{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d745b36-8f92-4c0e-a42b-21d8de06dc8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import statsmodels.api as sm\n",
    "    import sklearn.linear_model as lm\n",
    "    import sklearn.model_selection as skm\n",
    "    import statsmodels.formula.api as smf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from ISLP.models import ModelSpec as MS\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "    from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss,\n",
    "                                mean_squared_error)\n",
    "    from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)\n",
    "    from ISLP.bart import BART\n",
    "    import xgboost as xgb\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    #import doubleml as dml\n",
    "    import graphviz\n",
    "    import networkx as nx\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel\n",
    "    \n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install ISLP -q\n",
    "    !pip install stargazer -q\n",
    "    !pip install xgboost -q\n",
    "    !pip install doubleml -q\n",
    "    !pip install dowhy -q\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import statsmodels.api as sm\n",
    "    import sklearn.linear_model as lm\n",
    "    import sklearn.model_selection as skm\n",
    "    import statsmodels.formula.api as smf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from ISLP.models import ModelSpec as MS\n",
    "    from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "    from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss,\n",
    "                                mean_squared_error)\n",
    "    from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)\n",
    "    from ISLP.bart import BART\n",
    "    import xgboost as xgb\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    #import doubleml as dml\n",
    "    import graphviz\n",
    "    import networkx as nx\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3175a2c0-27be-498a-bae5-6078dd085bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"spillovers.dta\"\n",
    "\n",
    "data = pd.read_stata(data_loc, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7398c00a-ae54-4b7b-aa65-a93e6eb2b121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cusip': 'CUSIP',\n",
       " 'year': 'year',\n",
       " 'xrd': 'Expenditure on R&D',\n",
       " 'ppent': 'Net book value of property, plant and equipment',\n",
       " 'spillsic': 'SIC correlation weighted R&D of other firms, 1996 values',\n",
       " 'spillcovsic': '',\n",
       " 'spillmalsic': '',\n",
       " 'spillmalcovsic': '',\n",
       " 'spilltec': 'Patent NClass correlation weighted R&D of other firms, 1996 values',\n",
       " 'spillcovtec': '',\n",
       " 'spillmaltec': '',\n",
       " 'spillmalcovtec': '',\n",
       " 'spilltloc': '',\n",
       " 'spillsloc': '',\n",
       " 'spilltectloc': '',\n",
       " 'spilltecsloc': '',\n",
       " 'spillsicsloc': '',\n",
       " 'spillsictloc': '',\n",
       " 'lstate': '',\n",
       " 'lfirm': '',\n",
       " 'firm_dum': '',\n",
       " 'hxrd': '',\n",
       " 'spillsicIV': '',\n",
       " 'spilltecIV': '',\n",
       " 'spillsicIV_mal': '',\n",
       " 'spilltecIV_mal': '',\n",
       " 'p005': '',\n",
       " 'sales_ind': 'Control variable - Total sales weighted by sic sales matrix',\n",
       " 'sales_ind_ns': 'Control variable - Total Value Shipments in Manufacturing from National Statisti',\n",
       " 'patents_ind': 'Control variable - Total number of patents weighted by tech matrix',\n",
       " 'pat_count': 'Patents per firm applied for that year',\n",
       " 'i': 'Unique firm level indentifier, like cusip but in numeric values',\n",
       " 'tic': 'company ticker',\n",
       " 'comn': 'company name',\n",
       " 'sic': 'primary SIC code (firm)',\n",
       " 'at': 'total assets (firm)',\n",
       " 'sales': 'total sales (firm)',\n",
       " 'oibdp': 'op. income before depr. (firm)',\n",
       " 'ib': 'income before extraord. items (firm)',\n",
       " 'emp': 'employment (firm)',\n",
       " 'dldte': 'data of deletion',\n",
       " 'dlrsn': 'reason of deletion',\n",
       " 'segnum': 'number of business segments',\n",
       " 'invt': 'Total inventories',\n",
       " 'intan': 'Total intangibles',\n",
       " 'ivaeq': 'Investments and advances - equity method',\n",
       " 'ivao': 'Investments and advances - other',\n",
       " 'act': 'Current assets - total',\n",
       " 'ao': 'Assets other',\n",
       " 'oiadp': 'Operating profits before amortization and depreciation',\n",
       " 'gics': '',\n",
       " 'poa': 'profits over assets',\n",
       " 'pos': 'profits over sales',\n",
       " 'mkvaf': 'Market value',\n",
       " 'dt': 'Total debt',\n",
       " 'pstk': 'Preferred Equity Total (par/stated value)',\n",
       " 'xad': 'Advertising expense',\n",
       " 'pi': 'Pre-tax income',\n",
       " 'aqi': '',\n",
       " 'aqs': '',\n",
       " 'ppegt': 'Plant, property and equipment, gross book value',\n",
       " 'capx': 'Capital expenditures',\n",
       " 'capxv': 'Capital expenditure through acquisition as well',\n",
       " 'xrent': '',\n",
       " 'xint': '',\n",
       " 'xintd': '',\n",
       " 'xlr': 'Labour expenses',\n",
       " 'xpr': '',\n",
       " 'xsga': '',\n",
       " 'cogs': '',\n",
       " 'invfg': '',\n",
       " 'invwip': '',\n",
       " 'invo': '',\n",
       " 'invrm': '',\n",
       " 'invval1': '',\n",
       " 'lifr': '',\n",
       " 'dp': '',\n",
       " 'la': 'log10(assets)',\n",
       " 'ls': 'log10(sales)',\n",
       " 'pindex': 'CPI price index used to deflate all variables',\n",
       " 'infl': '',\n",
       " 'inf': '',\n",
       " 'lsic': '',\n",
       " 'lcovsic': '',\n",
       " 'lmalsic': '',\n",
       " 'lmalcovsic': '',\n",
       " 'ltec': '',\n",
       " 'lcovtec': '',\n",
       " 'lmaltec': '',\n",
       " 'lmalcovtec': '',\n",
       " 'ltloc': '',\n",
       " 'lsloc': '',\n",
       " 'ltectloc': '',\n",
       " 'ltecsloc': '',\n",
       " 'lsicsloc': '',\n",
       " 'lsictloc': '',\n",
       " 'ltecIV': '',\n",
       " 'lsicIV': '',\n",
       " 'ltecIV_mal': '',\n",
       " 'lsicIV_mal': '',\n",
       " 'lxrd': 'Log R&D expenditure',\n",
       " 'lpatents_ind': 'Log Total patents in n-class weighted industries (by firm year) - tech shock con',\n",
       " 'lpatents_indt': '',\n",
       " 'lpatents_indm': '',\n",
       " 'ttt': '',\n",
       " 'lpatpat': '',\n",
       " 'lpat_count': 'Log of patent count by year - missing values -1 and missing indicator lpat_count',\n",
       " 'lpat_cite_norm': 'Log of cite weighted patent count by year',\n",
       " 'lsales': 'Log sales',\n",
       " 'lppent': 'Log of ppent - i.e. net tangible fixed assets',\n",
       " 'lemp': 'Log count of employees',\n",
       " 'pat_cite': 'Cites per firm',\n",
       " 'pat_cite_norm': 'Cites per firm, normalized to average 1 per year',\n",
       " 'dsales': '',\n",
       " 'demp': '',\n",
       " 'sales_emp': '',\n",
       " 'ppent_emp': '',\n",
       " 'dy': '',\n",
       " 'dym': '',\n",
       " 'jumpyear': '',\n",
       " 'oldnum': '',\n",
       " 'num': '',\n",
       " 'prob': '',\n",
       " 'dyear': '',\n",
       " 'lxrd1': 'Lag Log R&D expenditure',\n",
       " 'lsales1': 'Lag Log sales',\n",
       " 'sales1': '',\n",
       " 'lppent1': 'Lag Log of ppent - i.e. net tangible fixed assets',\n",
       " 'lemp1': 'Lag Log count of employees',\n",
       " 'lpatents_ind1': 'Lag Log Total patents in n-class weighted industries (by firm year) - tech shock',\n",
       " 'lpatents_ind2': 'Twice Lagged Log Total patents in n-class weighted industries (by firm year) - t',\n",
       " 'lpat_cite_norm1': 'Lag of Log of cite weighted patent count by year',\n",
       " 'lpat_count1': 'Lag of Log of patent count by year - missing values -1 and missing indicator lpa',\n",
       " 'pat_count1': '',\n",
       " 'gpatent': 'Cite weighted and year normalized stock of firm patents',\n",
       " 'gpatent_count': 'Stock of firm patent count',\n",
       " 'kpat_cite': '',\n",
       " 'rxrd': 'Expenditure on R&D, 1996 values',\n",
       " 'grd': 'Stock of R&D expenditures',\n",
       " 'rhxrd': '',\n",
       " 'ghxrd': 'Stock of instrument predicted R&D expenditures',\n",
       " 'rspillsic': '',\n",
       " 'gspillsic': '',\n",
       " 'lgspillsic': 'Log stock of sic weighted R&D (spillovers)',\n",
       " 'lgspillsic1': 'Lagged Log stock of sic weighted R&D (spillovers)',\n",
       " 'rspillcovsic': '',\n",
       " 'gspillcovsic': '',\n",
       " 'lgspillcovsic': '',\n",
       " 'lgspillcovsic1': '',\n",
       " 'rspillmalsic': '',\n",
       " 'gspillmalsic': '',\n",
       " 'lgspillmalsic': '',\n",
       " 'lgspillmalsic1': '',\n",
       " 'rspillmalcovsic': '',\n",
       " 'gspillmalcovsic': '',\n",
       " 'lgspillmalcovsic': '',\n",
       " 'lgspillmalcovsic1': '',\n",
       " 'rspilltec': '',\n",
       " 'gspilltec': '',\n",
       " 'lgspilltec': 'Log stock of tec weighted R&D (spillovers)',\n",
       " 'lgspilltec1': 'Lag Log stock of tec weighted R&D (spillovers)',\n",
       " 'rspillcovtec': '',\n",
       " 'gspillcovtec': '',\n",
       " 'lgspillcovtec': '',\n",
       " 'lgspillcovtec1': '',\n",
       " 'rspillmaltec': '',\n",
       " 'gspillmaltec': '',\n",
       " 'lgspillmaltec': '',\n",
       " 'lgspillmaltec1': '',\n",
       " 'rspillmalcovtec': '',\n",
       " 'gspillmalcovtec': '',\n",
       " 'lgspillmalcovtec': '',\n",
       " 'lgspillmalcovtec1': '',\n",
       " 'rspilltloc': '',\n",
       " 'gspilltloc': '',\n",
       " 'lgspilltloc': 'Log stock of location weighted R&D (spillovers)',\n",
       " 'lgspilltloc1': 'Lagged Log stock of location weighted R&D (spillovers)',\n",
       " 'rspillsloc': '',\n",
       " 'gspillsloc': '',\n",
       " 'lgspillsloc': 'Log stock of sales location weighted R&D (spillovers)',\n",
       " 'lgspillsloc1': 'Lagged Log stock of location weighted R&D (spillovers)',\n",
       " 'rspilltectloc': '',\n",
       " 'gspilltectloc': '',\n",
       " 'lgspilltectloc': '',\n",
       " 'lgspilltectloc1': '',\n",
       " 'rspilltecsloc': '',\n",
       " 'gspilltecsloc': '',\n",
       " 'lgspilltecsloc': '',\n",
       " 'lgspilltecsloc1': '',\n",
       " 'rspillsicsloc': '',\n",
       " 'gspillsicsloc': '',\n",
       " 'lgspillsicsloc': '',\n",
       " 'lgspillsicsloc1': '',\n",
       " 'rspillsictloc': '',\n",
       " 'gspillsictloc': '',\n",
       " 'lgspillsictloc': '',\n",
       " 'lgspillsictloc1': '',\n",
       " 'rspilltecIV': '',\n",
       " 'gspilltecIV': '',\n",
       " 'lgspilltecIV': '',\n",
       " 'lgspilltecIV1': '',\n",
       " 'rspillsicIV': '',\n",
       " 'gspillsicIV': '',\n",
       " 'lgspillsicIV': '',\n",
       " 'lgspillsicIV1': '',\n",
       " 'rspilltecIV_mal': '',\n",
       " 'gspilltecIV_mal': '',\n",
       " 'lgspilltecIV_mal': '',\n",
       " 'lgspilltecIV_mal1': '',\n",
       " 'rspillsicIV_mal': '',\n",
       " 'gspillsicIV_mal': '',\n",
       " 'lgspillsicIV_mal': '',\n",
       " 'lgspillsicIV_mal1': '',\n",
       " 'lkpat_cite': '',\n",
       " 'rppent': 'Net book value of property, plant and equipment in 1996 values',\n",
       " 'rcapx': '',\n",
       " 'kstock': 'Capital stock calculated by the perpetual inventory method from ppent initial va',\n",
       " 'lgrd': 'Log of stock of R&D expenditures',\n",
       " 'lghxrd': 'Log of stock of  instrument predicted R&D expenditures',\n",
       " 'lgpatent': 'Log of Cite weighted and year normalized stock of firm patents',\n",
       " 'lgpatent_count': 'Log Stock of firm patent count',\n",
       " 'lkstock': 'Log of Capital stock calculated by the perpetual inventory method from ppent ini',\n",
       " 'lcogs1': '',\n",
       " 'lgrd1': 'Lag Log of stock of R&D expenditures',\n",
       " 'lghxrd1': '',\n",
       " 'grd1': '',\n",
       " 'lgpatent1': 'Lag Log of Cite weighted and year normalized stock of firm patents',\n",
       " 'lgpatent_count1': 'Lag Log Stock of firm patent count',\n",
       " 'gpatent_count1': '',\n",
       " 'lkstock1': 'Lag Log of Capital stock calculated by the perpetual inventory method from ppent',\n",
       " 'dlsales': '',\n",
       " 'dlemp': '',\n",
       " 'dlppent': '',\n",
       " 'profit': '',\n",
       " 'lprofit': '',\n",
       " 'rmkvaf': '',\n",
       " 'rpstk': '',\n",
       " 'rdt': '',\n",
       " 'rinvt': '',\n",
       " 'rivaeq': '',\n",
       " 'rivao': '',\n",
       " 'rintan': '',\n",
       " 'ract': '',\n",
       " 'value': '',\n",
       " 'value_e': '',\n",
       " 'value_d': '',\n",
       " 'qkstock': '',\n",
       " 'dum_qkstock': \"Some minor numbers missing for kstock in Tobin's q - set to zero when missing\",\n",
       " 'tobinq': \"Tobin's Q calculated following Hall, Jaffe and Trajtenberg, 2000\",\n",
       " 'tobinq_e': \"Equity component of Tobin's Q calculated following Hall, Jaffe and Trajtenberg, \",\n",
       " 'tobinq_d': \"Debt component of Tobin's Q calculated following Hall, Jaffe and Trajtenberg, 20\",\n",
       " 'rawtobinq': '',\n",
       " 'lq': \"Log Tobin's Q\",\n",
       " 'lq_e': \"Log Tobin's Equity Q - tobinq_e\",\n",
       " 'lq_d': \"Log Tobin's Debt Q - tobinq_d\",\n",
       " 'lq1': \"Lag Log Tobin's Q\",\n",
       " 'grd_k': 'R&D stock divided by capital stock',\n",
       " 'gpat_k': 'Patent stock divided by capital stock',\n",
       " 'gpatcount_k': '',\n",
       " 'gtec_k': '',\n",
       " 'gsic_k': '',\n",
       " 'pat_k': '',\n",
       " 'grd_k_dum': 'R&D stock over capital missing dummy',\n",
       " 'grd_k1': 'lagged R&D stock over capital',\n",
       " 'gtec_k1': '',\n",
       " 'gsic_k1': '',\n",
       " 'pat_k1': '',\n",
       " 'grd_k1_dum': 'Lagged R&D stock over capital missing dummy',\n",
       " 'gpat_k1': 'Lagged Patent stock divided by capital stock',\n",
       " 'gpatcount_k1': '',\n",
       " 'gpat_k1_dum': 'Missing value indicator for lagged Patent stock divided by capital stock',\n",
       " 'gpatcount_k1_dum': '',\n",
       " 'gtecxrd_k': '',\n",
       " 'lkpat_cite1': '',\n",
       " 'pat_cite_norm1': '',\n",
       " 'yy12': 'year==  1981.0000',\n",
       " 'yy13': 'year==  1982.0000',\n",
       " 'yy14': 'year==  1983.0000',\n",
       " 'yy15': 'year==  1984.0000',\n",
       " 'yy16': 'year==  1985.0000',\n",
       " 'yy17': 'year==  1986.0000',\n",
       " 'yy18': 'year==  1987.0000',\n",
       " 'yy19': 'year==  1988.0000',\n",
       " 'yy20': 'year==  1989.0000',\n",
       " 'yy21': 'year==  1990.0000',\n",
       " 'yy22': 'year==  1991.0000',\n",
       " 'yy23': 'year==  1992.0000',\n",
       " 'yy24': 'year==  1993.0000',\n",
       " 'yy25': 'year==  1994.0000',\n",
       " 'yy26': 'year==  1995.0000',\n",
       " 'yy27': 'year==  1996.0000',\n",
       " 'yy28': 'year==  1997.0000',\n",
       " 'yy29': 'year==  1998.0000',\n",
       " 'yy30': 'year==  1999.0000',\n",
       " 'yy31': 'year==  2000.0000',\n",
       " 'yy32': 'year==  2001.0000',\n",
       " 'pat_all': 'Total granted patents per firm applied for over 1970-1999',\n",
       " 'ccog': '',\n",
       " 'sic3': 'Three digit SIC code',\n",
       " 'sic2': '',\n",
       " 'indwage': '',\n",
       " 'nwage': '',\n",
       " 'materials': '',\n",
       " 'rati': '',\n",
       " 'lmat1': '',\n",
       " 'grd_kt2': 'R&D stock over capital^2',\n",
       " 'grd_kt3': 'R&D stock over capital^3',\n",
       " 'grd_kt4': 'R&D stock over capital^4',\n",
       " 'grd_kt5': 'R&D stock over capital^5',\n",
       " 'grd_kt6': '',\n",
       " 'gpat_kt2': 'Patent stock over capital^2',\n",
       " 'gpat_kt3': 'Patent stock over capital^3',\n",
       " 'gpat_kt4': 'Patent stock over capital^4',\n",
       " 'gpat_kt5': 'Patent stock over capital^5',\n",
       " 'f_year': '',\n",
       " 'fyear': 'First year using actual data in patents stuff',\n",
       " 'myear': '',\n",
       " 'prior_years': 'Number of years used to make initial conditions',\n",
       " 'riorpat': '',\n",
       " 'riorpat_cite': '',\n",
       " 'riorlgpat': '',\n",
       " 'riorgrd': '',\n",
       " 'riorppent': '',\n",
       " 'riorsales': '',\n",
       " 'riorgspilltec': '',\n",
       " 'riorgspillsic': '',\n",
       " 'riorlq': '',\n",
       " 'riorgtec_k': '',\n",
       " 'riorgsic_k': '',\n",
       " 'riorgrd_k': '',\n",
       " 'riorlsales_ind': '',\n",
       " 'priorpat': '',\n",
       " 'priorpat_cite': '',\n",
       " 'priorlgpat': '',\n",
       " 'priorgrd': '',\n",
       " 'priorppent': '',\n",
       " 'priorsales': '',\n",
       " 'priorgspilltec': '',\n",
       " 'priorgspillsic': '',\n",
       " 'priorlq': '',\n",
       " 'priorgtec_k': '',\n",
       " 'priorgsic_k': '',\n",
       " 'priorgrd_k': '',\n",
       " 'priorlsales_ind': '',\n",
       " 'lsales_ind': 'Logged total sales weighted by SIC matrix',\n",
       " 'lsales_ind_ns': '',\n",
       " 'lsales_ind_ns_dum': '',\n",
       " 'lsales_ind1': 'Lagged Logged total sales weighted by SIC matrix',\n",
       " 'lsales_ind_ns1': '',\n",
       " 'lsales_ind_ns_dum1': '',\n",
       " 'lsales_ind2': 'Twice Lagged Logged total sales weighted by SIC matrix',\n",
       " 'priordum_grd_zero': '',\n",
       " 'lpriorgrd': 'Logged initial conditions stock of R&D (pre-sample values)',\n",
       " 'lpriorppent': 'Logged initial conditions ppent (pre-sample values)',\n",
       " 'lpriorsales': 'Logged initial conditions sales (pre-sample values)',\n",
       " 'lpriorgspilltec': 'Logged initial conditions lgspilltec (pre-sample values)',\n",
       " 'lpriorgspillsic': 'Logged initial conditions lgspillsic (pre-sample values)',\n",
       " 'lpriorpat': 'Logged initial conditions patent count (pre-sample values)',\n",
       " 'lpriorpat_cite': '',\n",
       " 'lpriorgrd_dum': 'Missing indicator for Logged initial conditions stock of R&D (pre-sample values)',\n",
       " 'lpriorgrd1': 'Lagged Logged initial conditions stock of R&D (pre-sample values)',\n",
       " 'lpriorgrd1_dum': 'Missing indicator for lagged Logged initial conditions stock of R&D (pre-sample ',\n",
       " 'lpriorpat_dum': 'Missing indicator for Logged initial conditions patent count (pre-sample values)',\n",
       " 'lpriorpat_cite_dum': '',\n",
       " 'lpriorpat1': 'Lagged Logged initial conditions patent count (pre-sample values)',\n",
       " 'lpriorpat1_dum': 'Missing indicator for Lagged Logged initial conditions patent count (pre-sample ',\n",
       " 'lpind_ind': 'Price index, yearly at 3-digit SIC level',\n",
       " 'pat_cite1': '',\n",
       " 'lpat_cite1': '',\n",
       " 'lpat_cite1_dum': '',\n",
       " 'lpat_cite': '',\n",
       " 'lgrd_dum': 'Missing indicator for Log of stock of R&D expenditures',\n",
       " 'lgrd1_dum': 'Missing indicator for Lag Log of stock of R&D expenditures',\n",
       " 'lpat_count_dum': 'Dummy for missing value of log of patent count by year',\n",
       " 'lpat_count1_dum': 'Dummy for missing value of lag of log of patent count by year',\n",
       " 'lgpatent_dum': 'Missing indicator for Log of Cite weighted and year normalized stock of firm pat',\n",
       " 'lgpatent1_dum': 'Missing indicator for lagged Log of Cite weighted and year normalized stock of f',\n",
       " 'lgpatent_count_dum': 'Missing indicator for Log Stock of firm patent count',\n",
       " 'lgpatent_count1_dum': 'Missing indicator for lagged Log Stock of firm patent count',\n",
       " 'code': 'CUSIP',\n",
       " 'pind_ind': 'Price index, yearly at 3-digit SIC level',\n",
       " 'psmooth': 'Fitted values',\n",
       " 'plin': '',\n",
       " 'pind_indm': '',\n",
       " 'rsales': 'Sales in 1996 values',\n",
       " 'lrsales': 'Log Sales in 1996 values',\n",
       " 'lrsales1': 'Lag Log Sales in 1996 values',\n",
       " 'lqq': '',\n",
       " 'lxrd_sales': '',\n",
       " 'lxrd_sales1': '',\n",
       " 'lfirm_dum': '',\n",
       " 'gfirm': '',\n",
       " 'lgfirm': '',\n",
       " 'lgfirm_dum': '',\n",
       " 'state': '',\n",
       " 'gstate': '',\n",
       " 'lgstate': '',\n",
       " 'lgfirm1': '',\n",
       " 'lgfirm_dum1': '',\n",
       " 'lgstate1': '',\n",
       " 'lfirm1': '',\n",
       " 'lfirm_dum1': '',\n",
       " 'lstate1': '',\n",
       " 'noj': 'Count observations per firm (includes pre-sample patent observations)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.variable_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caed78af-4e44-4cdf-ad1d-04e70a2b2cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['i', 'year', 'rmkvaf', 'grd', 'grd_k1', 'rxrd', 'gspillsic',\n",
       "       'gspilltec', 'pat_count', 'pat_cite', 'rsales', 'rppent', 'emp',\n",
       "       'gspilltecIV', 'gspillsicIV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_of_int = ['i','year','rmkvaf','grd','grd_k1','rxrd','gspillsic','gspilltec','pat_count','pat_cite','rsales',\n",
    "              'rppent','emp','gspilltecIV','gspillsicIV']\n",
    "\n",
    "data = pd.read_stata(data_loc)\n",
    "df = data[vars_of_int]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf406bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177/3340598184.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['year'] = df['year'].astype(str)\n",
      "/tmp/ipykernel_177/3340598184.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['i'] = df['i'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# convert categorical columns to strings\n",
    "df['year'] = df['year'].astype(str)\n",
    "df['i'] = df['i'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ada76bb-9a0a-461e-b92b-39cfb388358b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmkvaf</th>\n",
       "      <th>gspilltecIV</th>\n",
       "      <th>gspillsicIV</th>\n",
       "      <th>pat_count</th>\n",
       "      <th>rsales</th>\n",
       "      <th>rppent</th>\n",
       "      <th>emp</th>\n",
       "      <th>rxrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "      <td>11736.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3449.07</td>\n",
       "      <td>20747.15</td>\n",
       "      <td>5980.43</td>\n",
       "      <td>18.43</td>\n",
       "      <td>2728.76</td>\n",
       "      <td>1289.43</td>\n",
       "      <td>18.36</td>\n",
       "      <td>101.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13571.64</td>\n",
       "      <td>15482.72</td>\n",
       "      <td>8665.92</td>\n",
       "      <td>80.25</td>\n",
       "      <td>8369.00</td>\n",
       "      <td>3991.58</td>\n",
       "      <td>53.39</td>\n",
       "      <td>457.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.43</td>\n",
       "      <td>230.28</td>\n",
       "      <td>4.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>86.55</td>\n",
       "      <td>8901.98</td>\n",
       "      <td>596.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>113.19</td>\n",
       "      <td>26.77</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>395.46</td>\n",
       "      <td>17242.42</td>\n",
       "      <td>2027.41</td>\n",
       "      <td>1.00</td>\n",
       "      <td>433.49</td>\n",
       "      <td>116.75</td>\n",
       "      <td>3.71</td>\n",
       "      <td>4.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1934.38</td>\n",
       "      <td>29073.87</td>\n",
       "      <td>7382.01</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1876.15</td>\n",
       "      <td>710.58</td>\n",
       "      <td>13.53</td>\n",
       "      <td>29.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>485566.69</td>\n",
       "      <td>91985.96</td>\n",
       "      <td>55576.60</td>\n",
       "      <td>2405.00</td>\n",
       "      <td>137513.77</td>\n",
       "      <td>72707.55</td>\n",
       "      <td>876.80</td>\n",
       "      <td>8900.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rmkvaf  gspilltecIV  gspillsicIV  pat_count     rsales    rppent  \\\n",
       "count   11736.00     11736.00     11736.00   11736.00   11736.00  11736.00   \n",
       "mean     3449.07     20747.15      5980.43      18.43    2728.76   1289.43   \n",
       "std     13571.64     15482.72      8665.92      80.25    8369.00   3991.58   \n",
       "min         0.43       230.28         4.46       0.00       1.48      0.96   \n",
       "25%        86.55      8901.98       596.31       0.00     113.19     26.77   \n",
       "50%       395.46     17242.42      2027.41       1.00     433.49    116.75   \n",
       "75%      1934.38     29073.87      7382.01       6.00    1876.15    710.58   \n",
       "max    485566.69     91985.96     55576.60    2405.00  137513.77  72707.55   \n",
       "\n",
       "            emp      rxrd  \n",
       "count  11736.00  11736.00  \n",
       "mean      18.36    101.31  \n",
       "std       53.39    457.04  \n",
       "min        0.10      0.00  \n",
       "25%        1.08      0.00  \n",
       "50%        3.71      4.53  \n",
       "75%       13.53     29.16  \n",
       "max      876.80   8900.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_of_int_noindex = ['rmkvaf','gspilltecIV','gspillsicIV','pat_count','rsales','rppent','emp','rxrd']\n",
    "\n",
    "# delete NaN values\n",
    "\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "#for i in range(0, len(var_of_int_noindex)):\n",
    "#    df = df[df[var_of_int_noindex[i]].isna() == False]\n",
    "\n",
    "#df = df.loc[df['rmkvaf'].isna() == False]\n",
    "\n",
    "df_sum_stats = df[var_of_int_noindex].describe()\n",
    "df_sum_stats = df_sum_stats.round(2)\n",
    "df_sum_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7521d293-0372-4623-b152-86df3a5fb4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      " & count & mean & std & min & 25% & 50% & 75% & max \\\\\n",
      "\\midrule\n",
      "rmkvaf & 11736.000000 & 3449.070000 & 13571.640000 & 0.430000 & 86.550000 & 395.460000 & 1934.380000 & 485566.690000 \\\\\n",
      "gspilltecIV & 11736.000000 & 20747.150000 & 15482.720000 & 230.280000 & 8901.980000 & 17242.420000 & 29073.870000 & 91985.960000 \\\\\n",
      "gspillsicIV & 11736.000000 & 5980.430000 & 8665.920000 & 4.460000 & 596.310000 & 2027.410000 & 7382.010000 & 55576.600000 \\\\\n",
      "pat_count & 11736.000000 & 18.430000 & 80.250000 & 0.000000 & 0.000000 & 1.000000 & 6.000000 & 2405.000000 \\\\\n",
      "rsales & 11736.000000 & 2728.760000 & 8369.000000 & 1.480000 & 113.190000 & 433.490000 & 1876.150000 & 137513.770000 \\\\\n",
      "rppent & 11736.000000 & 1289.430000 & 3991.580000 & 0.960000 & 26.770000 & 116.750000 & 710.580000 & 72707.550000 \\\\\n",
      "emp & 11736.000000 & 18.360000 & 53.390000 & 0.100000 & 1.080000 & 3.710000 & 13.530000 & 876.800000 \\\\\n",
      "rxrd & 11736.000000 & 101.310000 & 457.040000 & 0.000000 & 0.000000 & 4.530000 & 29.160000 & 8900.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_sum_stats.T.to_excel(\"sum_stats.xlsx\") # export summary statistics\n",
    "print(df_sum_stats.T.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee5bf3",
   "metadata": {},
   "source": [
    "# OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2734b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear model\n",
    "# gen fixed effects\n",
    "time_effects = pd.get_dummies(df['year'])\n",
    "#time_effects.columns = time_effects.columns.astype(str)\n",
    "firm_effects = pd.get_dummies(df['i'])\n",
    "#time_effects.columns = time_effects.columns.astype(str)\n",
    "\n",
    "\n",
    "df = pd.merge(df, time_effects, left_on=df.index, right_on=time_effects.index, how='left')\n",
    "\n",
    "df = df.rename(columns={'key_0': 'old_key'})\n",
    "\n",
    "df = pd.merge(df, firm_effects, left_on=df['old_key'], right_on=firm_effects.index, how='left')\n",
    "\n",
    "fixed_effects = list(time_effects.columns.values)\n",
    "for col in firm_effects.columns.values:\n",
    "    fixed_effects.append(col)\n",
    "\n",
    "y_var = df['rmkvaf']\n",
    "\n",
    "# get df for x vars + fixed effects\n",
    "x_vars = ['gspilltecIV','gspillsicIV','pat_count','rsales','rppent','emp','rxrd']\n",
    "for col in fixed_effects:\n",
    "    x_vars.append(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8119f9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.664</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.641</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   28.98</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:18:31</td>     <th>  Log-Likelihood:    </th> <td>-1.2192e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 11736</td>      <th>  AIC:               </th>  <td>2.454e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 10985</td>      <th>  BIC:               </th>  <td>2.509e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   750</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>-6271.8534</td> <td>  532.282</td> <td>  -11.783</td> <td> 0.000</td> <td>-7315.222</td> <td>-5228.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th> <td>    0.1630</td> <td>    0.028</td> <td>    5.768</td> <td> 0.000</td> <td>    0.108</td> <td>    0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th> <td>    0.4887</td> <td>    0.052</td> <td>    9.386</td> <td> 0.000</td> <td>    0.387</td> <td>    0.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>   <td>  -29.4376</td> <td>    1.715</td> <td>  -17.168</td> <td> 0.000</td> <td>  -32.799</td> <td>  -26.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>      <td>    1.0154</td> <td>    0.041</td> <td>   24.569</td> <td> 0.000</td> <td>    0.934</td> <td>    1.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>      <td>    0.5175</td> <td>    0.087</td> <td>    5.962</td> <td> 0.000</td> <td>    0.347</td> <td>    0.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>         <td>   -2.5097</td> <td>    7.030</td> <td>   -0.357</td> <td> 0.721</td> <td>  -16.291</td> <td>   11.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>        <td>    8.6216</td> <td>    0.669</td> <td>   12.878</td> <td> 0.000</td> <td>    7.309</td> <td>    9.934</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981</th>        <td>  762.8261</td> <td>  381.627</td> <td>    1.999</td> <td> 0.046</td> <td>   14.769</td> <td> 1510.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>        <td>  883.7420</td> <td>  369.574</td> <td>    2.391</td> <td> 0.017</td> <td>  159.310</td> <td> 1608.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>        <td>  821.5707</td> <td>  357.444</td> <td>    2.298</td> <td> 0.022</td> <td>  120.917</td> <td> 1522.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>        <td>  310.9095</td> <td>  346.828</td> <td>    0.896</td> <td> 0.370</td> <td> -368.935</td> <td>  990.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>        <td>  112.7209</td> <td>  339.088</td> <td>    0.332</td> <td> 0.740</td> <td> -551.953</td> <td>  777.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>        <td> -152.9965</td> <td>  330.737</td> <td>   -0.463</td> <td> 0.644</td> <td> -801.300</td> <td>  495.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>        <td> -423.0556</td> <td>  324.228</td> <td>   -1.305</td> <td> 0.192</td> <td>-1058.600</td> <td>  212.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>        <td> -777.3124</td> <td>  321.303</td> <td>   -2.419</td> <td> 0.016</td> <td>-1407.124</td> <td> -147.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>        <td> -661.1419</td> <td>  317.261</td> <td>   -2.084</td> <td> 0.037</td> <td>-1283.031</td> <td>  -39.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>        <td>-1153.5785</td> <td>  313.432</td> <td>   -3.680</td> <td> 0.000</td> <td>-1767.961</td> <td> -539.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>        <td> -798.2414</td> <td>  312.074</td> <td>   -2.558</td> <td> 0.011</td> <td>-1409.962</td> <td> -186.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>        <td> -982.2983</td> <td>  312.534</td> <td>   -3.143</td> <td> 0.002</td> <td>-1594.921</td> <td> -369.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>        <td> -999.6691</td> <td>  313.254</td> <td>   -3.191</td> <td> 0.001</td> <td>-1613.703</td> <td> -385.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>        <td>-1323.3880</td> <td>  317.300</td> <td>   -4.171</td> <td> 0.000</td> <td>-1945.352</td> <td> -701.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>        <td> -816.2328</td> <td>  325.043</td> <td>   -2.511</td> <td> 0.012</td> <td>-1453.375</td> <td> -179.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>        <td> -732.9858</td> <td>  338.616</td> <td>   -2.165</td> <td> 0.030</td> <td>-1396.733</td> <td>  -69.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>        <td> -349.2790</td> <td>  356.979</td> <td>   -0.978</td> <td> 0.328</td> <td>-1049.022</td> <td>  350.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>        <td> -247.3364</td> <td>  381.255</td> <td>   -0.649</td> <td> 0.517</td> <td> -994.665</td> <td>  499.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>        <td>  253.8930</td> <td>  408.826</td> <td>    0.621</td> <td> 0.535</td> <td> -547.480</td> <td> 1055.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>     <td> 3097.6615</td> <td> 1880.996</td> <td>    1.647</td> <td> 0.100</td> <td> -589.430</td> <td> 6784.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>     <td> 2317.7560</td> <td> 2276.237</td> <td>    1.018</td> <td> 0.309</td> <td>-2144.079</td> <td> 6779.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>     <td> 1438.1417</td> <td> 1886.554</td> <td>    0.762</td> <td> 0.446</td> <td>-2259.844</td> <td> 5136.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>     <td> 2713.9470</td> <td> 1892.028</td> <td>    1.434</td> <td> 0.151</td> <td> -994.769</td> <td> 6422.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>     <td> 5086.2191</td> <td> 1901.323</td> <td>    2.675</td> <td> 0.007</td> <td> 1359.283</td> <td> 8813.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>      <td> 4105.1769</td> <td> 1889.911</td> <td>    2.172</td> <td> 0.030</td> <td>  400.612</td> <td> 7809.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>     <td> 1739.8066</td> <td> 1873.446</td> <td>    0.929</td> <td> 0.353</td> <td>-1932.485</td> <td> 5412.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>     <td>-3584.2178</td> <td> 1887.069</td> <td>   -1.899</td> <td> 0.058</td> <td>-7283.212</td> <td>  114.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>     <td> 3969.6072</td> <td> 4695.384</td> <td>    0.845</td> <td> 0.398</td> <td>-5234.191</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>     <td>-3492.5868</td> <td> 1884.197</td> <td>   -1.854</td> <td> 0.064</td> <td>-7185.951</td> <td>  200.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>      <td> 2642.8898</td> <td> 4696.864</td> <td>    0.563</td> <td> 0.574</td> <td>-6563.809</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>     <td> 6527.9249</td> <td> 1936.233</td> <td>    3.371</td> <td> 0.001</td> <td> 2732.560</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>     <td> 1516.7663</td> <td> 1875.072</td> <td>    0.809</td> <td> 0.419</td> <td>-2158.713</td> <td> 5192.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>     <td> 6692.8481</td> <td> 1941.452</td> <td>    3.447</td> <td> 0.001</td> <td> 2887.252</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>      <td>-1796.1429</td> <td> 1888.737</td> <td>   -0.951</td> <td> 0.342</td> <td>-5498.407</td> <td> 1906.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>     <td>-4038.5599</td> <td> 2457.273</td> <td>   -1.644</td> <td> 0.100</td> <td>-8855.258</td> <td>  778.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>     <td>-4707.9285</td> <td> 3442.390</td> <td>   -1.368</td> <td> 0.171</td> <td>-1.15e+04</td> <td> 2039.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>     <td> 5329.6656</td> <td> 2134.048</td> <td>    2.497</td> <td> 0.013</td> <td> 1146.548</td> <td> 9512.784</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>     <td> 2940.1622</td> <td> 2144.003</td> <td>    1.371</td> <td> 0.170</td> <td>-1262.470</td> <td> 7142.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>     <td>-5356.9855</td> <td> 2061.047</td> <td>   -2.599</td> <td> 0.009</td> <td>-9397.008</td> <td>-1316.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>     <td> 4996.0691</td> <td> 1899.773</td> <td>    2.630</td> <td> 0.009</td> <td> 1272.172</td> <td> 8719.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>     <td> 6330.3022</td> <td> 1936.864</td> <td>    3.268</td> <td> 0.001</td> <td> 2533.701</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>     <td>  820.3515</td> <td> 2217.856</td> <td>    0.370</td> <td> 0.711</td> <td>-3527.045</td> <td> 5167.748</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>     <td> 4715.7756</td> <td> 1901.887</td> <td>    2.480</td> <td> 0.013</td> <td>  987.735</td> <td> 8443.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>     <td> 2154.7388</td> <td> 1894.796</td> <td>    1.137</td> <td> 0.255</td> <td>-1559.402</td> <td> 5868.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>     <td>-2.005e+04</td> <td> 2162.637</td> <td>   -9.272</td> <td> 0.000</td> <td>-2.43e+04</td> <td>-1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>     <td> 3818.9932</td> <td> 1889.577</td> <td>    2.021</td> <td> 0.043</td> <td>  115.081</td> <td> 7522.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>     <td>-5177.8756</td> <td> 3440.106</td> <td>   -1.505</td> <td> 0.132</td> <td>-1.19e+04</td> <td> 1565.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>      <td> 1614.5984</td> <td> 1995.996</td> <td>    0.809</td> <td> 0.419</td> <td>-2297.914</td> <td> 5527.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>     <td> 2316.7998</td> <td> 1873.315</td> <td>    1.237</td> <td> 0.216</td> <td>-1355.234</td> <td> 5988.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>     <td>-3228.6671</td> <td> 1955.239</td> <td>   -1.651</td> <td> 0.099</td> <td>-7061.288</td> <td>  603.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>     <td>-5555.8122</td> <td> 2042.980</td> <td>   -2.719</td> <td> 0.007</td> <td>-9560.420</td> <td>-1551.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>     <td>-2355.5645</td> <td> 1884.123</td> <td>   -1.250</td> <td> 0.211</td> <td>-6048.786</td> <td> 1337.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>     <td> 3660.2121</td> <td> 1885.367</td> <td>    1.941</td> <td> 0.052</td> <td>  -35.446</td> <td> 7355.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>     <td>  680.7273</td> <td> 1975.735</td> <td>    0.345</td> <td> 0.730</td> <td>-3192.068</td> <td> 4553.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>     <td> 5118.8058</td> <td> 2095.825</td> <td>    2.442</td> <td> 0.015</td> <td> 1010.612</td> <td> 9227.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>     <td> 5608.1798</td> <td> 1917.312</td> <td>    2.925</td> <td> 0.003</td> <td> 1849.902</td> <td> 9366.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>      <td>-7472.1228</td> <td> 2056.588</td> <td>   -3.633</td> <td> 0.000</td> <td>-1.15e+04</td> <td>-3440.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>     <td>-2605.6534</td> <td> 1957.284</td> <td>   -1.331</td> <td> 0.183</td> <td>-6442.283</td> <td> 1230.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>     <td>-2.479e+04</td> <td> 2419.308</td> <td>  -10.248</td> <td> 0.000</td> <td>-2.95e+04</td> <td>-2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>     <td> 4204.1963</td> <td> 1895.074</td> <td>    2.218</td> <td> 0.027</td> <td>  489.511</td> <td> 7918.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>     <td>-4935.1607</td> <td> 2255.821</td> <td>   -2.188</td> <td> 0.029</td> <td>-9356.975</td> <td> -513.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>     <td> 6215.7763</td> <td> 1983.684</td> <td>    3.133</td> <td> 0.002</td> <td> 2327.398</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>     <td>-1.323e+04</td> <td> 2018.961</td> <td>   -6.553</td> <td> 0.000</td> <td>-1.72e+04</td> <td>-9272.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>     <td>-1388.9617</td> <td> 1962.277</td> <td>   -0.708</td> <td> 0.479</td> <td>-5235.378</td> <td> 2457.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>     <td>-1032.5703</td> <td> 2297.680</td> <td>   -0.449</td> <td> 0.653</td> <td>-5536.437</td> <td> 3471.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>     <td> 1611.7732</td> <td> 1882.464</td> <td>    0.856</td> <td> 0.392</td> <td>-2078.195</td> <td> 5301.741</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>     <td> 3411.8443</td> <td> 1991.393</td> <td>    1.713</td> <td> 0.087</td> <td> -491.644</td> <td> 7315.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>     <td> -123.4474</td> <td> 5749.638</td> <td>   -0.021</td> <td> 0.983</td> <td>-1.14e+04</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>     <td>-1206.3294</td> <td> 2063.080</td> <td>   -0.585</td> <td> 0.559</td> <td>-5250.338</td> <td> 2837.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>     <td> 5584.0522</td> <td> 1900.543</td> <td>    2.938</td> <td> 0.003</td> <td> 1858.647</td> <td> 9309.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>     <td> 6007.0246</td> <td> 1903.516</td> <td>    3.156</td> <td> 0.002</td> <td> 2275.790</td> <td> 9738.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>     <td> 2989.2197</td> <td> 1882.796</td> <td>    1.588</td> <td> 0.112</td> <td> -701.399</td> <td> 6679.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>     <td>-7577.6388</td> <td> 1947.644</td> <td>   -3.891</td> <td> 0.000</td> <td>-1.14e+04</td> <td>-3759.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>     <td> 3555.7350</td> <td> 1884.570</td> <td>    1.887</td> <td> 0.059</td> <td> -138.361</td> <td> 7249.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>     <td> 4227.1709</td> <td> 1892.182</td> <td>    2.234</td> <td> 0.026</td> <td>  518.154</td> <td> 7936.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>     <td> 3204.7584</td> <td> 1882.393</td> <td>    1.702</td> <td> 0.089</td> <td> -485.071</td> <td> 6894.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>     <td> 2942.7785</td> <td> 1898.192</td> <td>    1.550</td> <td> 0.121</td> <td> -778.019</td> <td> 6663.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>     <td> 2796.6662</td> <td> 1900.522</td> <td>    1.472</td> <td> 0.141</td> <td> -928.699</td> <td> 6522.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>     <td> 7476.3004</td> <td> 2074.980</td> <td>    3.603</td> <td> 0.000</td> <td> 3408.967</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>     <td> 2444.8875</td> <td> 2583.273</td> <td>    0.946</td> <td> 0.344</td> <td>-2618.793</td> <td> 7508.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>     <td> 5373.4594</td> <td> 1920.030</td> <td>    2.799</td> <td> 0.005</td> <td> 1609.854</td> <td> 9137.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>     <td> 5983.2672</td> <td> 1973.759</td> <td>    3.031</td> <td> 0.002</td> <td> 2114.344</td> <td> 9852.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>     <td> 2812.0683</td> <td> 1899.421</td> <td>    1.480</td> <td> 0.139</td> <td> -911.138</td> <td> 6535.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>      <td>  211.7163</td> <td> 2175.428</td> <td>    0.097</td> <td> 0.922</td> <td>-4052.514</td> <td> 4475.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>     <td> 3739.5996</td> <td> 1898.088</td> <td>    1.970</td> <td> 0.049</td> <td>   19.005</td> <td> 7460.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>     <td> 2308.6559</td> <td> 1884.980</td> <td>    1.225</td> <td> 0.221</td> <td>-1386.244</td> <td> 6003.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>     <td> 4188.3126</td> <td> 1888.746</td> <td>    2.218</td> <td> 0.027</td> <td>  486.030</td> <td> 7890.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>     <td>-8646.4572</td> <td> 2050.829</td> <td>   -4.216</td> <td> 0.000</td> <td>-1.27e+04</td> <td>-4626.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>     <td>  559.7743</td> <td> 2178.114</td> <td>    0.257</td> <td> 0.797</td> <td>-3709.721</td> <td> 4829.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>     <td> 3564.9093</td> <td> 1883.462</td> <td>    1.893</td> <td> 0.058</td> <td> -127.016</td> <td> 7256.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>     <td> 3523.4088</td> <td> 3082.617</td> <td>    1.143</td> <td> 0.253</td> <td>-2519.076</td> <td> 9565.893</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>     <td> -2.17e+04</td> <td> 2308.416</td> <td>   -9.400</td> <td> 0.000</td> <td>-2.62e+04</td> <td>-1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>      <td> 2933.4044</td> <td> 1882.939</td> <td>    1.558</td> <td> 0.119</td> <td> -757.496</td> <td> 6624.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>     <td> 1081.1555</td> <td> 2524.920</td> <td>    0.428</td> <td> 0.669</td> <td>-3868.143</td> <td> 6030.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>     <td> 2228.5844</td> <td> 1939.011</td> <td>    1.149</td> <td> 0.250</td> <td>-1572.227</td> <td> 6029.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>     <td>-3219.8794</td> <td> 1994.714</td> <td>   -1.614</td> <td> 0.107</td> <td>-7129.878</td> <td>  690.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>      <td> 4365.8342</td> <td> 1903.475</td> <td>    2.294</td> <td> 0.022</td> <td>  634.680</td> <td> 8096.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>     <td> 4720.6397</td> <td> 1900.811</td> <td>    2.483</td> <td> 0.013</td> <td>  994.708</td> <td> 8446.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>     <td> 3186.8529</td> <td> 1881.234</td> <td>    1.694</td> <td> 0.090</td> <td> -500.705</td> <td> 6874.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>     <td>  520.7405</td> <td> 1885.531</td> <td>    0.276</td> <td> 0.782</td> <td>-3175.240</td> <td> 4216.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>     <td> 4045.0552</td> <td> 2056.849</td> <td>    1.967</td> <td> 0.049</td> <td>   13.262</td> <td> 8076.849</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>      <td> 1776.5387</td> <td> 1873.271</td> <td>    0.948</td> <td> 0.343</td> <td>-1895.409</td> <td> 5448.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>     <td>  205.1234</td> <td> 1871.942</td> <td>    0.110</td> <td> 0.913</td> <td>-3464.220</td> <td> 3874.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>     <td> 5502.7284</td> <td> 1916.367</td> <td>    2.871</td> <td> 0.004</td> <td> 1746.304</td> <td> 9259.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>     <td> 4764.9719</td> <td> 1887.086</td> <td>    2.525</td> <td> 0.012</td> <td> 1065.943</td> <td> 8464.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>     <td>-2807.4697</td> <td> 3423.675</td> <td>   -0.820</td> <td> 0.412</td> <td>-9518.488</td> <td> 3903.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>     <td>-6006.0413</td> <td> 2178.952</td> <td>   -2.756</td> <td> 0.006</td> <td>-1.03e+04</td> <td>-1734.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>     <td>-8833.2777</td> <td> 2022.181</td> <td>   -4.368</td> <td> 0.000</td> <td>-1.28e+04</td> <td>-4869.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>     <td> 2151.7198</td> <td> 1916.982</td> <td>    1.122</td> <td> 0.262</td> <td>-1605.909</td> <td> 5909.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>     <td>-4909.5155</td> <td> 1887.924</td> <td>   -2.600</td> <td> 0.009</td> <td>-8610.187</td> <td>-1208.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>    <td>-2.297e+04</td> <td> 5892.521</td> <td>   -3.898</td> <td> 0.000</td> <td>-3.45e+04</td> <td>-1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>     <td>-3081.4194</td> <td> 1983.440</td> <td>   -1.554</td> <td> 0.120</td> <td>-6969.319</td> <td>  806.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>     <td> -345.4494</td> <td> 1880.824</td> <td>   -0.184</td> <td> 0.854</td> <td>-4032.202</td> <td> 3341.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>     <td> 4446.9242</td> <td> 1922.452</td> <td>    2.313</td> <td> 0.021</td> <td>  678.572</td> <td> 8215.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>     <td>-2952.4483</td> <td> 1975.979</td> <td>   -1.494</td> <td> 0.135</td> <td>-6825.723</td> <td>  920.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>     <td> 3677.2970</td> <td> 1888.017</td> <td>    1.948</td> <td> 0.051</td> <td>  -23.557</td> <td> 7378.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>     <td> 6202.3365</td> <td> 1931.978</td> <td>    3.210</td> <td> 0.001</td> <td> 2415.312</td> <td> 9989.361</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>     <td> 2202.9150</td> <td> 1892.068</td> <td>    1.164</td> <td> 0.244</td> <td>-1505.880</td> <td> 5911.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>     <td>-2455.8304</td> <td> 2451.133</td> <td>   -1.002</td> <td> 0.316</td> <td>-7260.493</td> <td> 2348.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>     <td> 5045.6218</td> <td> 1907.963</td> <td>    2.645</td> <td> 0.008</td> <td> 1305.670</td> <td> 8785.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>     <td> 8014.9787</td> <td> 1901.563</td> <td>    4.215</td> <td> 0.000</td> <td> 4287.574</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>      <td>-6558.7605</td> <td> 2019.911</td> <td>   -3.247</td> <td> 0.001</td> <td>-1.05e+04</td> <td>-2599.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>     <td>-9542.3217</td> <td> 2056.890</td> <td>   -4.639</td> <td> 0.000</td> <td>-1.36e+04</td> <td>-5510.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>     <td> 5933.0902</td> <td> 1938.826</td> <td>    3.060</td> <td> 0.002</td> <td> 2132.642</td> <td> 9733.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>     <td>-8859.2408</td> <td> 2033.636</td> <td>   -4.356</td> <td> 0.000</td> <td>-1.28e+04</td> <td>-4872.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>     <td> 1751.7498</td> <td> 2031.089</td> <td>    0.862</td> <td> 0.388</td> <td>-2229.550</td> <td> 5733.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>     <td> 4499.7539</td> <td> 2058.475</td> <td>    2.186</td> <td> 0.029</td> <td>  464.772</td> <td> 8534.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>     <td>-5780.8803</td> <td> 3386.028</td> <td>   -1.707</td> <td> 0.088</td> <td>-1.24e+04</td> <td>  856.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>     <td>-1.427e+04</td> <td> 2558.449</td> <td>   -5.576</td> <td> 0.000</td> <td>-1.93e+04</td> <td>-9250.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>     <td> 1914.7910</td> <td> 2191.834</td> <td>    0.874</td> <td> 0.382</td> <td>-2381.599</td> <td> 6211.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>     <td> -794.1008</td> <td> 5806.390</td> <td>   -0.137</td> <td> 0.891</td> <td>-1.22e+04</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>     <td> 6483.0051</td> <td> 2313.539</td> <td>    2.802</td> <td> 0.005</td> <td> 1948.052</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>     <td> 4039.2914</td> <td> 2731.204</td> <td>    1.479</td> <td> 0.139</td> <td>-1314.360</td> <td> 9392.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>      <td> 1198.7045</td> <td> 1896.415</td> <td>    0.632</td> <td> 0.527</td> <td>-2518.610</td> <td> 4916.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>     <td>-1.807e+04</td> <td> 2537.008</td> <td>   -7.121</td> <td> 0.000</td> <td> -2.3e+04</td> <td>-1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>     <td> 5.987e+04</td> <td> 2260.555</td> <td>   26.485</td> <td> 0.000</td> <td> 5.54e+04</td> <td> 6.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>     <td> -506.9822</td> <td> 3335.896</td> <td>   -0.152</td> <td> 0.879</td> <td>-7045.938</td> <td> 6031.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>     <td>-9251.7575</td> <td> 2280.522</td> <td>   -4.057</td> <td> 0.000</td> <td>-1.37e+04</td> <td>-4781.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>     <td>-6362.1395</td> <td> 2337.764</td> <td>   -2.721</td> <td> 0.007</td> <td>-1.09e+04</td> <td>-1779.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>     <td>-4470.9142</td> <td> 2200.236</td> <td>   -2.032</td> <td> 0.042</td> <td>-8783.772</td> <td> -158.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>     <td> 4591.1516</td> <td> 2227.440</td> <td>    2.061</td> <td> 0.039</td> <td>  224.968</td> <td> 8957.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>     <td> 1427.6404</td> <td> 2271.244</td> <td>    0.629</td> <td> 0.530</td> <td>-3024.407</td> <td> 5879.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>      <td>   88.1293</td> <td> 1867.488</td> <td>    0.047</td> <td> 0.962</td> <td>-3572.484</td> <td> 3748.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>     <td> 1364.7553</td> <td> 2578.535</td> <td>    0.529</td> <td> 0.597</td> <td>-3689.638</td> <td> 6419.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>     <td> -566.7502</td> <td> 4696.797</td> <td>   -0.121</td> <td> 0.904</td> <td>-9773.318</td> <td> 8639.818</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>      <td>  110.7821</td> <td> 1928.931</td> <td>    0.057</td> <td> 0.954</td> <td>-3670.270</td> <td> 3891.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>     <td> 3456.8019</td> <td> 2467.873</td> <td>    1.401</td> <td> 0.161</td> <td>-1380.672</td> <td> 8294.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>     <td> 3965.7460</td> <td> 2282.077</td> <td>    1.738</td> <td> 0.082</td> <td> -507.537</td> <td> 8439.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>     <td> -127.9446</td> <td> 2482.050</td> <td>   -0.052</td> <td> 0.959</td> <td>-4993.209</td> <td> 4737.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>     <td> 1919.1874</td> <td> 2186.990</td> <td>    0.878</td> <td> 0.380</td> <td>-2367.707</td> <td> 6206.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>     <td>  884.6439</td> <td> 4704.257</td> <td>    0.188</td> <td> 0.851</td> <td>-8336.546</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>     <td> 6304.3213</td> <td> 2248.139</td> <td>    2.804</td> <td> 0.005</td> <td> 1897.564</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>     <td>-1.712e+04</td> <td> 2529.614</td> <td>   -6.766</td> <td> 0.000</td> <td>-2.21e+04</td> <td>-1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>      <td> 5220.0849</td> <td> 1972.052</td> <td>    2.647</td> <td> 0.008</td> <td> 1354.509</td> <td> 9085.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>     <td> -1.16e+04</td> <td> 2814.360</td> <td>   -4.120</td> <td> 0.000</td> <td>-1.71e+04</td> <td>-6078.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>      <td> 4990.9330</td> <td> 1942.406</td> <td>    2.569</td> <td> 0.010</td> <td> 1183.468</td> <td> 8798.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>      <td> 3304.1604</td> <td> 1881.880</td> <td>    1.756</td> <td> 0.079</td> <td> -384.664</td> <td> 6992.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>     <td> 5487.1519</td> <td> 2296.375</td> <td>    2.389</td> <td> 0.017</td> <td>  985.843</td> <td> 9988.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>     <td> -141.3897</td> <td> 2348.886</td> <td>   -0.060</td> <td> 0.952</td> <td>-4745.630</td> <td> 4462.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>      <td>-5147.0947</td> <td> 2077.474</td> <td>   -2.478</td> <td> 0.013</td> <td>-9219.318</td> <td>-1074.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>     <td>-3791.2207</td> <td> 4697.960</td> <td>   -0.807</td> <td> 0.420</td> <td> -1.3e+04</td> <td> 5417.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>      <td>-6146.9690</td> <td> 2353.305</td> <td>   -2.612</td> <td> 0.009</td> <td>-1.08e+04</td> <td>-1534.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>     <td>-2598.5396</td> <td> 3116.002</td> <td>   -0.834</td> <td> 0.404</td> <td>-8706.465</td> <td> 3509.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>     <td>-2.024e+04</td> <td> 2805.355</td> <td>   -7.216</td> <td> 0.000</td> <td>-2.57e+04</td> <td>-1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>     <td> -292.9437</td> <td> 2344.123</td> <td>   -0.125</td> <td> 0.901</td> <td>-4887.847</td> <td> 4301.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>     <td> 4288.2331</td> <td> 2274.826</td> <td>    1.885</td> <td> 0.059</td> <td> -170.836</td> <td> 8747.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>     <td>-4098.7445</td> <td> 2274.930</td> <td>   -1.802</td> <td> 0.072</td> <td>-8558.018</td> <td>  360.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>     <td> 5903.7335</td> <td> 2404.934</td> <td>    2.455</td> <td> 0.014</td> <td> 1189.629</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>     <td>-4312.5560</td> <td> 2504.596</td> <td>   -1.722</td> <td> 0.085</td> <td>-9222.014</td> <td>  596.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>     <td> 6564.1270</td> <td> 2414.808</td> <td>    2.718</td> <td> 0.007</td> <td> 1830.669</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>      <td>-1.002e+04</td> <td> 2336.561</td> <td>   -4.290</td> <td> 0.000</td> <td>-1.46e+04</td> <td>-5442.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>     <td> 1928.0302</td> <td> 2357.844</td> <td>    0.818</td> <td> 0.414</td> <td>-2693.769</td> <td> 6549.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>      <td>-4788.4202</td> <td> 1992.772</td> <td>   -2.403</td> <td> 0.016</td> <td>-8694.612</td> <td> -882.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>      <td>-5058.1898</td> <td> 1924.186</td> <td>   -2.629</td> <td> 0.009</td> <td>-8829.941</td> <td>-1286.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>     <td> 3305.2508</td> <td> 2584.883</td> <td>    1.279</td> <td> 0.201</td> <td>-1761.586</td> <td> 8372.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>     <td> 5461.6883</td> <td> 3357.406</td> <td>    1.627</td> <td> 0.104</td> <td>-1119.432</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>     <td>-1.084e+04</td> <td> 2488.694</td> <td>   -4.355</td> <td> 0.000</td> <td>-1.57e+04</td> <td>-5960.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>      <td> 1006.3612</td> <td> 2191.304</td> <td>    0.459</td> <td> 0.646</td> <td>-3288.990</td> <td> 5301.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>     <td> 2110.4264</td> <td> 2286.866</td> <td>    0.923</td> <td> 0.356</td> <td>-2372.243</td> <td> 6593.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>     <td> 1052.9005</td> <td> 2326.922</td> <td>    0.452</td> <td> 0.651</td> <td>-3508.286</td> <td> 5614.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>     <td>-6713.4106</td> <td> 2461.951</td> <td>   -2.727</td> <td> 0.006</td> <td>-1.15e+04</td> <td>-1887.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>     <td> -815.5019</td> <td> 2463.229</td> <td>   -0.331</td> <td> 0.741</td> <td>-5643.875</td> <td> 4012.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>      <td> 2804.6111</td> <td> 4092.214</td> <td>    0.685</td> <td> 0.493</td> <td>-5216.864</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>     <td> 5029.9668</td> <td> 2495.067</td> <td>    2.016</td> <td> 0.044</td> <td>  139.187</td> <td> 9920.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>     <td> 2075.1080</td> <td> 8133.047</td> <td>    0.255</td> <td> 0.799</td> <td>-1.39e+04</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>      <td> 3135.8476</td> <td> 2599.375</td> <td>    1.206</td> <td> 0.228</td> <td>-1959.395</td> <td> 8231.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>      <td> 5597.0788</td> <td> 2478.743</td> <td>    2.258</td> <td> 0.024</td> <td>  738.296</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>     <td> 3312.9826</td> <td> 4701.123</td> <td>    0.705</td> <td> 0.481</td> <td>-5902.065</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>     <td> 4088.8308</td> <td> 2479.191</td> <td>    1.649</td> <td> 0.099</td> <td> -770.831</td> <td> 8948.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>      <td> 5305.6897</td> <td> 1905.409</td> <td>    2.785</td> <td> 0.005</td> <td> 1570.745</td> <td> 9040.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>     <td> 2872.0224</td> <td> 2461.569</td> <td>    1.167</td> <td> 0.243</td> <td>-1953.095</td> <td> 7697.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>     <td>-2197.3109</td> <td> 2585.502</td> <td>   -0.850</td> <td> 0.395</td> <td>-7265.360</td> <td> 2870.738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>      <td> 2255.5097</td> <td> 1879.439</td> <td>    1.200</td> <td> 0.230</td> <td>-1428.530</td> <td> 5939.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>     <td>-2753.9887</td> <td> 2704.343</td> <td>   -1.018</td> <td> 0.309</td> <td>-8054.988</td> <td> 2547.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>      <td> 5170.2956</td> <td> 1903.709</td> <td>    2.716</td> <td> 0.007</td> <td> 1438.683</td> <td> 8901.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>     <td>-2.216e+04</td> <td> 3042.021</td> <td>   -7.286</td> <td> 0.000</td> <td>-2.81e+04</td> <td>-1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>     <td> 1047.2724</td> <td> 2467.055</td> <td>    0.425</td> <td> 0.671</td> <td>-3788.600</td> <td> 5883.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>     <td> 4394.1636</td> <td> 2892.305</td> <td>    1.519</td> <td> 0.129</td> <td>-1275.275</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>      <td>-1.197e+04</td> <td> 3230.716</td> <td>   -3.705</td> <td> 0.000</td> <td>-1.83e+04</td> <td>-5635.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>      <td> 1150.9509</td> <td> 1903.069</td> <td>    0.605</td> <td> 0.545</td> <td>-2579.407</td> <td> 4881.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>      <td> 8009.7727</td> <td> 2062.475</td> <td>    3.884</td> <td> 0.000</td> <td> 3966.951</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>      <td> 4464.8770</td> <td> 1899.969</td> <td>    2.350</td> <td> 0.019</td> <td>  740.596</td> <td> 8189.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>     <td> -473.4188</td> <td> 2720.089</td> <td>   -0.174</td> <td> 0.862</td> <td>-5805.283</td> <td> 4858.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>      <td>-5408.3889</td> <td> 1921.430</td> <td>   -2.815</td> <td> 0.005</td> <td>-9174.738</td> <td>-1642.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>      <td> 1999.1696</td> <td> 1874.500</td> <td>    1.067</td> <td> 0.286</td> <td>-1675.187</td> <td> 5673.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>      <td>-1.241e+04</td> <td> 2289.758</td> <td>   -5.419</td> <td> 0.000</td> <td>-1.69e+04</td> <td>-7918.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>     <td>-1.141e+04</td> <td> 2782.711</td> <td>   -4.099</td> <td> 0.000</td> <td>-1.69e+04</td> <td>-5952.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>     <td>-6651.5417</td> <td> 2934.612</td> <td>   -2.267</td> <td> 0.023</td> <td>-1.24e+04</td> <td> -899.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>      <td>-1267.5784</td> <td> 1873.117</td> <td>   -0.677</td> <td> 0.499</td> <td>-4939.225</td> <td> 2404.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>      <td> 4617.1754</td> <td> 1892.086</td> <td>    2.440</td> <td> 0.015</td> <td>  908.347</td> <td> 8326.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>      <td> 5917.8126</td> <td> 1934.732</td> <td>    3.059</td> <td> 0.002</td> <td> 2125.389</td> <td> 9710.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>     <td>-2142.6639</td> <td> 2713.492</td> <td>   -0.790</td> <td> 0.430</td> <td>-7461.597</td> <td> 3176.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>     <td>-2843.3099</td> <td> 2581.325</td> <td>   -1.101</td> <td> 0.271</td> <td>-7903.171</td> <td> 2216.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>      <td>-1.609e+04</td> <td> 2040.450</td> <td>   -7.885</td> <td> 0.000</td> <td>-2.01e+04</td> <td>-1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>      <td> 1038.3302</td> <td> 1938.531</td> <td>    0.536</td> <td> 0.592</td> <td>-2761.540</td> <td> 4838.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>     <td> 3126.0374</td> <td> 2596.217</td> <td>    1.204</td> <td> 0.229</td> <td>-1963.015</td> <td> 8215.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>      <td> 1818.2271</td> <td> 1899.530</td> <td>    0.957</td> <td> 0.338</td> <td>-1905.194</td> <td> 5541.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>      <td> 4811.4862</td> <td> 1900.383</td> <td>    2.532</td> <td> 0.011</td> <td> 1086.393</td> <td> 8536.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>      <td> 4688.6298</td> <td> 3113.418</td> <td>    1.506</td> <td> 0.132</td> <td>-1414.230</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>      <td> 3868.9635</td> <td> 1989.197</td> <td>    1.945</td> <td> 0.052</td> <td>  -30.220</td> <td> 7768.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>      <td>-3198.3213</td> <td> 2107.779</td> <td>   -1.517</td> <td> 0.129</td> <td>-7329.947</td> <td>  933.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>      <td> 4715.3642</td> <td> 1941.975</td> <td>    2.428</td> <td> 0.015</td> <td>  908.743</td> <td> 8521.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>      <td>-1.351e+04</td> <td> 2088.466</td> <td>   -6.467</td> <td> 0.000</td> <td>-1.76e+04</td> <td>-9411.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>     <td> 1696.6514</td> <td> 2583.745</td> <td>    0.657</td> <td> 0.511</td> <td>-3367.953</td> <td> 6761.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>      <td>  928.9560</td> <td> 1897.774</td> <td>    0.489</td> <td> 0.624</td> <td>-2791.023</td> <td> 4648.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>      <td>-9134.3632</td> <td> 2467.813</td> <td>   -3.701</td> <td> 0.000</td> <td> -1.4e+04</td> <td>-4297.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>     <td> 4455.5629</td> <td> 3661.166</td> <td>    1.217</td> <td> 0.224</td> <td>-2720.982</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>      <td> -492.4923</td> <td> 4068.535</td> <td>   -0.121</td> <td> 0.904</td> <td>-8467.552</td> <td> 7482.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>      <td> 4544.4415</td> <td> 2017.943</td> <td>    2.252</td> <td> 0.024</td> <td>  588.911</td> <td> 8499.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>      <td>-2060.4339</td> <td> 1984.365</td> <td>   -1.038</td> <td> 0.299</td> <td>-5950.147</td> <td> 1829.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>      <td> 3267.3750</td> <td> 1996.792</td> <td>    1.636</td> <td> 0.102</td> <td> -646.696</td> <td> 7181.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>      <td>-1552.3608</td> <td> 1906.690</td> <td>   -0.814</td> <td> 0.416</td> <td>-5289.817</td> <td> 2185.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>      <td> 1437.4441</td> <td> 1910.381</td> <td>    0.752</td> <td> 0.452</td> <td>-2307.247</td> <td> 5182.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>      <td> 5411.2870</td> <td> 1885.935</td> <td>    2.869</td> <td> 0.004</td> <td> 1714.515</td> <td> 9108.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>      <td> 4053.1417</td> <td> 1891.695</td> <td>    2.143</td> <td> 0.032</td> <td>  345.080</td> <td> 7761.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>      <td>-9269.3673</td> <td> 3298.509</td> <td>   -2.810</td> <td> 0.005</td> <td>-1.57e+04</td> <td>-2803.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>      <td> 1536.3298</td> <td> 1931.166</td> <td>    0.796</td> <td> 0.426</td> <td>-2249.102</td> <td> 5321.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>      <td> 2131.2548</td> <td> 1883.802</td> <td>    1.131</td> <td> 0.258</td> <td>-1561.337</td> <td> 5823.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>      <td> 4099.7529</td> <td> 2474.961</td> <td>    1.656</td> <td> 0.098</td> <td> -751.616</td> <td> 8951.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>      <td> 1597.1970</td> <td> 1868.781</td> <td>    0.855</td> <td> 0.393</td> <td>-2065.949</td> <td> 5260.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>      <td> 2075.1150</td> <td> 1882.009</td> <td>    1.103</td> <td> 0.270</td> <td>-1613.962</td> <td> 5764.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>      <td> 6198.3376</td> <td> 1930.235</td> <td>    3.211</td> <td> 0.001</td> <td> 2414.730</td> <td> 9981.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>     <td> 2.915e+04</td> <td> 2731.987</td> <td>   10.671</td> <td> 0.000</td> <td> 2.38e+04</td> <td> 3.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>      <td>-1.242e+04</td> <td> 2176.183</td> <td>   -5.706</td> <td> 0.000</td> <td>-1.67e+04</td> <td>-8151.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>      <td> 1721.6545</td> <td> 1887.294</td> <td>    0.912</td> <td> 0.362</td> <td>-1977.782</td> <td> 5421.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>      <td> 1959.3626</td> <td> 1870.848</td> <td>    1.047</td> <td> 0.295</td> <td>-1707.836</td> <td> 5626.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>     <td> -618.7069</td> <td> 2726.033</td> <td>   -0.227</td> <td> 0.820</td> <td>-5962.222</td> <td> 4724.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>     <td> 4677.4226</td> <td> 2626.157</td> <td>    1.781</td> <td> 0.075</td> <td> -470.317</td> <td> 9825.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>      <td> 1676.4498</td> <td> 2015.720</td> <td>    0.832</td> <td> 0.406</td> <td>-2274.724</td> <td> 5627.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>      <td> 1.113e+04</td> <td> 3081.311</td> <td>    3.611</td> <td> 0.000</td> <td> 5085.407</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>     <td>-2.285e+04</td> <td> 3150.156</td> <td>   -7.254</td> <td> 0.000</td> <td> -2.9e+04</td> <td>-1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>      <td> 3159.7736</td> <td> 1879.231</td> <td>    1.681</td> <td> 0.093</td> <td> -523.858</td> <td> 6843.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>      <td> 3.556e+04</td> <td> 2586.631</td> <td>   13.749</td> <td> 0.000</td> <td> 3.05e+04</td> <td> 4.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>      <td> 5969.6054</td> <td> 1973.136</td> <td>    3.025</td> <td> 0.002</td> <td> 2101.903</td> <td> 9837.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>      <td>-4042.4772</td> <td> 1940.943</td> <td>   -2.083</td> <td> 0.037</td> <td>-7847.075</td> <td> -237.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>      <td> 2682.7886</td> <td> 1888.591</td> <td>    1.421</td> <td> 0.155</td> <td>-1019.189</td> <td> 6384.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>     <td> 6567.2487</td> <td> 2635.889</td> <td>    2.491</td> <td> 0.013</td> <td> 1400.431</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>      <td>  443.9865</td> <td> 5746.731</td> <td>    0.077</td> <td> 0.938</td> <td>-1.08e+04</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>      <td> 3892.7923</td> <td> 2219.233</td> <td>    1.754</td> <td> 0.079</td> <td> -457.304</td> <td> 8242.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>     <td>-7958.3788</td> <td> 2774.205</td> <td>   -2.869</td> <td> 0.004</td> <td>-1.34e+04</td> <td>-2520.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>      <td> 2326.0620</td> <td> 1885.338</td> <td>    1.234</td> <td> 0.217</td> <td>-1369.541</td> <td> 6021.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>     <td> 4176.8950</td> <td> 2593.046</td> <td>    1.611</td> <td> 0.107</td> <td> -905.942</td> <td> 9259.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>      <td>-7591.5514</td> <td> 1959.471</td> <td>   -3.874</td> <td> 0.000</td> <td>-1.14e+04</td> <td>-3750.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>     <td>-1903.9381</td> <td> 2578.726</td> <td>   -0.738</td> <td> 0.460</td> <td>-6958.706</td> <td> 3150.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>      <td>-1.613e+04</td> <td> 2116.014</td> <td>   -7.622</td> <td> 0.000</td> <td>-2.03e+04</td> <td> -1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>      <td>-1662.6714</td> <td> 1886.111</td> <td>   -0.882</td> <td> 0.378</td> <td>-5359.787</td> <td> 2034.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>      <td> 5170.2469</td> <td> 3340.562</td> <td>    1.548</td> <td> 0.122</td> <td>-1377.855</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>      <td>-3191.1118</td> <td> 2131.966</td> <td>   -1.497</td> <td> 0.134</td> <td>-7370.148</td> <td>  987.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>     <td> 1834.4738</td> <td> 2725.102</td> <td>    0.673</td> <td> 0.501</td> <td>-3507.216</td> <td> 7176.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>     <td>-1.052e+04</td> <td> 3152.490</td> <td>   -3.338</td> <td> 0.001</td> <td>-1.67e+04</td> <td>-4344.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>      <td>-8659.6696</td> <td> 4202.820</td> <td>   -2.060</td> <td> 0.039</td> <td>-1.69e+04</td> <td> -421.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>      <td>  484.3946</td> <td> 2190.931</td> <td>    0.221</td> <td> 0.825</td> <td>-3810.224</td> <td> 4779.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>     <td>-1.653e+04</td> <td> 3592.553</td> <td>   -4.601</td> <td> 0.000</td> <td>-2.36e+04</td> <td>-9486.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>      <td> 5358.6897</td> <td> 1907.486</td> <td>    2.809</td> <td> 0.005</td> <td> 1619.674</td> <td> 9097.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>      <td>-1986.4490</td> <td> 2016.237</td> <td>   -0.985</td> <td> 0.325</td> <td>-5938.636</td> <td> 1965.738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>      <td>  1.16e+04</td> <td> 2184.113</td> <td>    5.311</td> <td> 0.000</td> <td> 7319.280</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>      <td> 7423.7475</td> <td> 1930.621</td> <td>    3.845</td> <td> 0.000</td> <td> 3639.384</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>      <td> -332.1383</td> <td> 1947.184</td> <td>   -0.171</td> <td> 0.865</td> <td>-4148.970</td> <td> 3484.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>      <td> 1652.2575</td> <td> 1871.029</td> <td>    0.883</td> <td> 0.377</td> <td>-2015.297</td> <td> 5319.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>      <td> 4578.3084</td> <td> 3665.158</td> <td>    1.249</td> <td> 0.212</td> <td>-2606.061</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>     <td> 2861.8215</td> <td> 2898.198</td> <td>    0.987</td> <td> 0.323</td> <td>-2819.169</td> <td> 8542.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>     <td>-1.315e+04</td> <td> 3182.313</td> <td>   -4.132</td> <td> 0.000</td> <td>-1.94e+04</td> <td>-6911.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>      <td> 5574.0696</td> <td> 1912.008</td> <td>    2.915</td> <td> 0.004</td> <td> 1826.189</td> <td> 9321.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>     <td> 5318.9119</td> <td> 3105.737</td> <td>    1.713</td> <td> 0.087</td> <td> -768.891</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>      <td>-5529.7674</td> <td> 2025.889</td> <td>   -2.730</td> <td> 0.006</td> <td>-9500.874</td> <td>-1558.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>      <td>-1.051e+04</td> <td> 2309.623</td> <td>   -4.549</td> <td> 0.000</td> <td> -1.5e+04</td> <td>-5978.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>      <td> 4352.4929</td> <td> 2124.037</td> <td>    2.049</td> <td> 0.040</td> <td>  188.999</td> <td> 8515.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>     <td> 3954.1172</td> <td> 2893.904</td> <td>    1.366</td> <td> 0.172</td> <td>-1718.455</td> <td> 9626.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>      <td> 4373.0373</td> <td> 1893.929</td> <td>    2.309</td> <td> 0.021</td> <td>  660.595</td> <td> 8085.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>     <td> 3503.0947</td> <td> 8133.263</td> <td>    0.431</td> <td> 0.667</td> <td>-1.24e+04</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>     <td> 3759.6474</td> <td> 2882.452</td> <td>    1.304</td> <td> 0.192</td> <td>-1890.476</td> <td> 9409.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>      <td>-1.093e+04</td> <td> 2185.521</td> <td>   -5.001</td> <td> 0.000</td> <td>-1.52e+04</td> <td>-6644.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>      <td> 5753.7139</td> <td> 2919.607</td> <td>    1.971</td> <td> 0.049</td> <td>   30.758</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>     <td> 5048.6253</td> <td> 4702.360</td> <td>    1.074</td> <td> 0.283</td> <td>-4168.846</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>      <td>-8546.1978</td> <td> 2151.373</td> <td>   -3.972</td> <td> 0.000</td> <td>-1.28e+04</td> <td>-4329.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>      <td> 3908.1790</td> <td> 1894.826</td> <td>    2.063</td> <td> 0.039</td> <td>  193.979</td> <td> 7622.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>      <td>-2007.5654</td> <td> 2519.353</td> <td>   -0.797</td> <td> 0.426</td> <td>-6945.951</td> <td> 2930.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>     <td> 3481.3869</td> <td> 3088.153</td> <td>    1.127</td> <td> 0.260</td> <td>-2571.948</td> <td> 9534.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>      <td> 1230.9401</td> <td> 1891.180</td> <td>    0.651</td> <td> 0.515</td> <td>-2476.113</td> <td> 4937.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>      <td> 1702.4949</td> <td> 1892.650</td> <td>    0.900</td> <td> 0.368</td> <td>-2007.440</td> <td> 5412.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>      <td> 1272.6585</td> <td> 1963.208</td> <td>    0.648</td> <td> 0.517</td> <td>-2575.582</td> <td> 5120.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>      <td> 7372.6205</td> <td> 1892.581</td> <td>    3.896</td> <td> 0.000</td> <td> 3662.821</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>      <td> 1438.7965</td> <td> 1948.563</td> <td>    0.738</td> <td> 0.460</td> <td>-2380.739</td> <td> 5258.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>      <td> 3769.5931</td> <td> 1886.511</td> <td>    1.998</td> <td> 0.046</td> <td>   71.692</td> <td> 7467.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>      <td>-1.143e+04</td> <td> 2040.584</td> <td>   -5.603</td> <td> 0.000</td> <td>-1.54e+04</td> <td>-7433.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>      <td> 5089.1314</td> <td> 1905.976</td> <td>    2.670</td> <td> 0.008</td> <td> 1353.075</td> <td> 8825.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>      <td>-7331.9480</td> <td> 2190.234</td> <td>   -3.348</td> <td> 0.001</td> <td>-1.16e+04</td> <td>-3038.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>     <td>-1.914e+04</td> <td> 3341.098</td> <td>   -5.729</td> <td> 0.000</td> <td>-2.57e+04</td> <td>-1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>     <td> 2209.6861</td> <td> 1882.569</td> <td>    1.174</td> <td> 0.241</td> <td>-1480.488</td> <td> 5899.860</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>     <td>-1.405e+04</td> <td> 3303.995</td> <td>   -4.252</td> <td> 0.000</td> <td>-2.05e+04</td> <td>-7571.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>      <td> 2393.1235</td> <td> 2130.471</td> <td>    1.123</td> <td> 0.261</td> <td>-1782.982</td> <td> 6569.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>      <td> 4097.7828</td> <td> 2721.613</td> <td>    1.506</td> <td> 0.132</td> <td>-1237.069</td> <td> 9432.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>      <td>-1681.6497</td> <td> 1926.097</td> <td>   -0.873</td> <td> 0.383</td> <td>-5457.146</td> <td> 2093.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>     <td>-1.319e+04</td> <td> 3481.781</td> <td>   -3.789</td> <td> 0.000</td> <td>   -2e+04</td> <td>-6366.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>      <td>-2.204e+04</td> <td> 3579.193</td> <td>   -6.159</td> <td> 0.000</td> <td>-2.91e+04</td> <td> -1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>      <td> 5925.8278</td> <td> 2397.776</td> <td>    2.471</td> <td> 0.013</td> <td> 1225.755</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>      <td> 3143.7179</td> <td> 1879.256</td> <td>    1.673</td> <td> 0.094</td> <td> -539.962</td> <td> 6827.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>      <td>  560.6943</td> <td> 2901.686</td> <td>    0.193</td> <td> 0.847</td> <td>-5127.132</td> <td> 6248.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>      <td>-3739.0166</td> <td> 1889.827</td> <td>   -1.978</td> <td> 0.048</td> <td>-7443.418</td> <td>  -34.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>      <td> 3217.3556</td> <td> 1887.883</td> <td>    1.704</td> <td> 0.088</td> <td> -483.236</td> <td> 6917.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>      <td>-1.229e+04</td> <td> 2679.152</td> <td>   -4.589</td> <td> 0.000</td> <td>-1.75e+04</td> <td>-7042.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>      <td>-7388.6022</td> <td> 2246.838</td> <td>   -3.288</td> <td> 0.001</td> <td>-1.18e+04</td> <td>-2984.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>      <td> 5754.9913</td> <td> 1915.818</td> <td>    3.004</td> <td> 0.003</td> <td> 1999.643</td> <td> 9510.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>      <td> 2605.6552</td> <td> 1927.441</td> <td>    1.352</td> <td> 0.176</td> <td>-1172.475</td> <td> 6383.786</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>      <td>-1.921e+04</td> <td> 3282.864</td> <td>   -5.850</td> <td> 0.000</td> <td>-2.56e+04</td> <td>-1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>      <td> 4147.8770</td> <td> 2008.866</td> <td>    2.065</td> <td> 0.039</td> <td>  210.137</td> <td> 8085.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>      <td>-3190.9219</td> <td> 2246.715</td> <td>   -1.420</td> <td> 0.156</td> <td>-7594.888</td> <td> 1213.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>      <td> 2356.5688</td> <td> 3657.437</td> <td>    0.644</td> <td> 0.519</td> <td>-4812.666</td> <td> 9525.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>      <td> 5429.2066</td> <td> 1883.668</td> <td>    2.882</td> <td> 0.004</td> <td> 1736.879</td> <td> 9121.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>      <td> 3032.2169</td> <td> 1894.223</td> <td>    1.601</td> <td> 0.109</td> <td> -680.801</td> <td> 6745.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>      <td> 5.164e+04</td> <td> 1917.758</td> <td>   26.928</td> <td> 0.000</td> <td> 4.79e+04</td> <td> 5.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>      <td> 3460.1219</td> <td> 2366.002</td> <td>    1.462</td> <td> 0.144</td> <td>-1177.668</td> <td> 8097.912</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>      <td> 3116.6330</td> <td> 1879.878</td> <td>    1.658</td> <td> 0.097</td> <td> -568.266</td> <td> 6801.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>      <td> 4337.1510</td> <td> 1876.725</td> <td>    2.311</td> <td> 0.021</td> <td>  658.432</td> <td> 8015.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>      <td>-2281.1656</td> <td> 2127.841</td> <td>   -1.072</td> <td> 0.284</td> <td>-6452.116</td> <td> 1889.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>      <td> -835.9173</td> <td> 2130.537</td> <td>   -0.392</td> <td> 0.695</td> <td>-5012.154</td> <td> 3340.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>      <td> 1511.2721</td> <td> 2026.724</td> <td>    0.746</td> <td> 0.456</td> <td>-2461.471</td> <td> 5484.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>      <td> 2668.5172</td> <td> 2060.752</td> <td>    1.295</td> <td> 0.195</td> <td>-1370.928</td> <td> 6707.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>      <td> 3852.1762</td> <td> 1902.254</td> <td>    2.025</td> <td> 0.043</td> <td>  123.416</td> <td> 7580.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>      <td> 3155.9190</td> <td> 1888.647</td> <td>    1.671</td> <td> 0.095</td> <td> -546.168</td> <td> 6858.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>      <td>-2.072e+04</td> <td> 2258.740</td> <td>   -9.173</td> <td> 0.000</td> <td>-2.51e+04</td> <td>-1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>      <td>-5818.3772</td> <td> 2210.720</td> <td>   -2.632</td> <td> 0.009</td> <td>-1.02e+04</td> <td>-1484.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>      <td> 3265.0074</td> <td> 2459.861</td> <td>    1.327</td> <td> 0.184</td> <td>-1556.764</td> <td> 8086.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>      <td> 1994.0750</td> <td> 1882.429</td> <td>    1.059</td> <td> 0.289</td> <td>-1695.825</td> <td> 5683.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>      <td>-4224.6553</td> <td> 2026.781</td> <td>   -2.084</td> <td> 0.037</td> <td>-8197.511</td> <td> -251.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>      <td>-2333.9904</td> <td> 1879.165</td> <td>   -1.242</td> <td> 0.214</td> <td>-6017.493</td> <td> 1349.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>      <td> 1014.3769</td> <td> 2733.802</td> <td>    0.371</td> <td> 0.711</td> <td>-4344.366</td> <td> 6373.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>      <td>  188.2776</td> <td> 1929.690</td> <td>    0.098</td> <td> 0.922</td> <td>-3594.262</td> <td> 3970.818</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>      <td> 4457.5709</td> <td> 1874.420</td> <td>    2.378</td> <td> 0.017</td> <td>  783.371</td> <td> 8131.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>      <td> 4428.8413</td> <td> 4073.143</td> <td>    1.087</td> <td> 0.277</td> <td>-3555.252</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>      <td> -732.3446</td> <td> 1868.067</td> <td>   -0.392</td> <td> 0.695</td> <td>-4394.091</td> <td> 2929.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>      <td> 6232.2652</td> <td> 1936.461</td> <td>    3.218</td> <td> 0.001</td> <td> 2436.453</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>      <td> 2857.3739</td> <td> 1890.238</td> <td>    1.512</td> <td> 0.131</td> <td> -847.832</td> <td> 6562.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>      <td> 5362.2778</td> <td> 1956.066</td> <td>    2.741</td> <td> 0.006</td> <td> 1528.037</td> <td> 9196.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>      <td>-8258.7073</td> <td> 1962.105</td> <td>   -4.209</td> <td> 0.000</td> <td>-1.21e+04</td> <td>-4412.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>      <td>-6335.2032</td> <td> 2022.153</td> <td>   -3.133</td> <td> 0.002</td> <td>-1.03e+04</td> <td>-2371.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>      <td> -489.0604</td> <td> 2008.441</td> <td>   -0.244</td> <td> 0.808</td> <td>-4425.967</td> <td> 3447.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>      <td>-1.215e+04</td> <td> 2003.453</td> <td>   -6.065</td> <td> 0.000</td> <td>-1.61e+04</td> <td>-8223.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>      <td>-1489.2333</td> <td> 2513.791</td> <td>   -0.592</td> <td> 0.554</td> <td>-6416.717</td> <td> 3438.250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>      <td>-2595.1828</td> <td> 1923.358</td> <td>   -1.349</td> <td> 0.177</td> <td>-6365.310</td> <td> 1174.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>      <td>-1.142e+04</td> <td> 2373.660</td> <td>   -4.811</td> <td> 0.000</td> <td>-1.61e+04</td> <td>-6767.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>      <td>-4822.9571</td> <td> 2489.769</td> <td>   -1.937</td> <td> 0.053</td> <td>-9703.352</td> <td>   57.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>      <td> -1.12e+04</td> <td> 2076.298</td> <td>   -5.395</td> <td> 0.000</td> <td>-1.53e+04</td> <td>-7131.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>      <td> 2464.5681</td> <td> 1884.029</td> <td>    1.308</td> <td> 0.191</td> <td>-1228.468</td> <td> 6157.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>      <td>-1.186e+04</td> <td> 2457.404</td> <td>   -4.826</td> <td> 0.000</td> <td>-1.67e+04</td> <td>-7042.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>      <td> 4451.3701</td> <td> 1907.381</td> <td>    2.334</td> <td> 0.020</td> <td>  712.561</td> <td> 8190.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>      <td>-1036.0352</td> <td> 1895.076</td> <td>   -0.547</td> <td> 0.585</td> <td>-4750.725</td> <td> 2678.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>      <td>-2798.6990</td> <td> 2826.112</td> <td>   -0.990</td> <td> 0.322</td> <td>-8338.388</td> <td> 2740.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>      <td>-4538.1360</td> <td> 2000.831</td> <td>   -2.268</td> <td> 0.023</td> <td>-8460.125</td> <td> -616.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>      <td> 4250.9880</td> <td> 1891.959</td> <td>    2.247</td> <td> 0.025</td> <td>  542.408</td> <td> 7959.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>      <td> -172.6957</td> <td> 3113.710</td> <td>   -0.055</td> <td> 0.956</td> <td>-6276.127</td> <td> 5930.736</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>      <td>-3700.5847</td> <td> 1976.063</td> <td>   -1.873</td> <td> 0.061</td> <td>-7574.024</td> <td>  172.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>      <td> 4642.4080</td> <td> 2004.110</td> <td>    2.316</td> <td> 0.021</td> <td>  713.992</td> <td> 8570.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>      <td> 5759.4857</td> <td> 1915.542</td> <td>    3.007</td> <td> 0.003</td> <td> 2004.679</td> <td> 9514.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>      <td> 3284.6978</td> <td> 1987.071</td> <td>    1.653</td> <td> 0.098</td> <td> -610.318</td> <td> 7179.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>      <td>  1.19e+04</td> <td> 1930.180</td> <td>    6.167</td> <td> 0.000</td> <td> 8119.077</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>      <td>-1117.7330</td> <td> 1921.797</td> <td>   -0.582</td> <td> 0.561</td> <td>-4884.800</td> <td> 2649.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>      <td> 5218.1397</td> <td> 1898.819</td> <td>    2.748</td> <td> 0.006</td> <td> 1496.113</td> <td> 8940.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>      <td>-2524.2024</td> <td> 1940.344</td> <td>   -1.301</td> <td> 0.193</td> <td>-6327.626</td> <td> 1279.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>      <td> 3295.9047</td> <td> 1871.494</td> <td>    1.761</td> <td> 0.078</td> <td> -372.560</td> <td> 6964.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>      <td>-9060.2877</td> <td> 2100.872</td> <td>   -4.313</td> <td> 0.000</td> <td>-1.32e+04</td> <td>-4942.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>      <td> 8033.8976</td> <td> 1929.372</td> <td>    4.164</td> <td> 0.000</td> <td> 4251.980</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>      <td> 1920.6403</td> <td> 3323.066</td> <td>    0.578</td> <td> 0.563</td> <td>-4593.168</td> <td> 8434.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>      <td>-1.593e+04</td> <td> 2564.847</td> <td>   -6.211</td> <td> 0.000</td> <td> -2.1e+04</td> <td>-1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>      <td> 1843.7623</td> <td> 2458.852</td> <td>    0.750</td> <td> 0.453</td> <td>-2976.030</td> <td> 6663.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>      <td>-3593.3345</td> <td> 1887.749</td> <td>   -1.904</td> <td> 0.057</td> <td>-7293.662</td> <td>  106.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>      <td> 5016.9613</td> <td> 2591.186</td> <td>    1.936</td> <td> 0.053</td> <td>  -62.230</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>      <td> 2955.8533</td> <td> 2457.595</td> <td>    1.203</td> <td> 0.229</td> <td>-1861.475</td> <td> 7773.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>      <td> 5447.8893</td> <td> 1907.622</td> <td>    2.856</td> <td> 0.004</td> <td> 1708.608</td> <td> 9187.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>      <td> 5426.4492</td> <td> 2265.829</td> <td>    2.395</td> <td> 0.017</td> <td>  985.018</td> <td> 9867.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>      <td>-1.103e+04</td> <td> 2145.369</td> <td>   -5.140</td> <td> 0.000</td> <td>-1.52e+04</td> <td>-6821.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>      <td> 3301.5102</td> <td> 1884.484</td> <td>    1.752</td> <td> 0.080</td> <td> -392.418</td> <td> 6995.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>      <td>-9874.4637</td> <td> 2047.650</td> <td>   -4.822</td> <td> 0.000</td> <td>-1.39e+04</td> <td>-5860.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>      <td> 3163.6636</td> <td> 1883.366</td> <td>    1.680</td> <td> 0.093</td> <td> -528.074</td> <td> 6855.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>      <td> 4919.3231</td> <td> 1900.807</td> <td>    2.588</td> <td> 0.010</td> <td> 1193.399</td> <td> 8645.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>      <td> 2279.9522</td> <td> 2110.236</td> <td>    1.080</td> <td> 0.280</td> <td>-1856.491</td> <td> 6416.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>      <td> 1286.5416</td> <td> 2011.000</td> <td>    0.640</td> <td> 0.522</td> <td>-2655.381</td> <td> 5228.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>      <td> 1858.7087</td> <td> 2025.498</td> <td>    0.918</td> <td> 0.359</td> <td>-2111.633</td> <td> 5829.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>      <td> 1245.7250</td> <td> 3327.189</td> <td>    0.374</td> <td> 0.708</td> <td>-5276.164</td> <td> 7767.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>      <td> -473.9774</td> <td> 1923.678</td> <td>   -0.246</td> <td> 0.805</td> <td>-4244.732</td> <td> 3296.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>      <td>  799.5226</td> <td> 1953.912</td> <td>    0.409</td> <td> 0.682</td> <td>-3030.496</td> <td> 4629.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>      <td> 3959.4555</td> <td> 1998.635</td> <td>    1.981</td> <td> 0.048</td> <td>   41.772</td> <td> 7877.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>      <td> 2171.8836</td> <td> 1890.235</td> <td>    1.149</td> <td> 0.251</td> <td>-1533.317</td> <td> 5877.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>      <td>-8255.6752</td> <td> 2183.381</td> <td>   -3.781</td> <td> 0.000</td> <td>-1.25e+04</td> <td>-3975.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>      <td>-3304.0404</td> <td> 1990.996</td> <td>   -1.659</td> <td> 0.097</td> <td>-7206.751</td> <td>  598.670</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>      <td> 1904.8450</td> <td> 1924.315</td> <td>    0.990</td> <td> 0.322</td> <td>-1867.158</td> <td> 5676.848</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>      <td> 2165.8909</td> <td> 8126.759</td> <td>    0.267</td> <td> 0.790</td> <td>-1.38e+04</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>      <td> 3235.1016</td> <td> 1999.182</td> <td>    1.618</td> <td> 0.106</td> <td> -683.654</td> <td> 7153.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>      <td> 6033.0314</td> <td> 1935.629</td> <td>    3.117</td> <td> 0.002</td> <td> 2238.849</td> <td> 9827.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>      <td> 5120.6568</td> <td> 1905.415</td> <td>    2.687</td> <td> 0.007</td> <td> 1385.700</td> <td> 8855.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>      <td>  557.2426</td> <td> 1868.586</td> <td>    0.298</td> <td> 0.766</td> <td>-3105.522</td> <td> 4220.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>      <td>-8057.7539</td> <td> 2111.057</td> <td>   -3.817</td> <td> 0.000</td> <td>-1.22e+04</td> <td>-3919.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>      <td> 4916.5681</td> <td> 1901.960</td> <td>    2.585</td> <td> 0.010</td> <td> 1188.385</td> <td> 8644.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>      <td>-6424.5107</td> <td> 2021.872</td> <td>   -3.178</td> <td> 0.001</td> <td>-1.04e+04</td> <td>-2461.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>      <td>-2091.5853</td> <td> 1985.279</td> <td>   -1.054</td> <td> 0.292</td> <td>-5983.090</td> <td> 1799.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>      <td> 3731.0165</td> <td> 1941.283</td> <td>    1.922</td> <td> 0.055</td> <td>  -74.247</td> <td> 7536.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>      <td> 2815.4626</td> <td> 1887.217</td> <td>    1.492</td> <td> 0.136</td> <td> -883.822</td> <td> 6514.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>      <td> 5420.4120</td> <td> 1910.857</td> <td>    2.837</td> <td> 0.005</td> <td> 1674.788</td> <td> 9166.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>      <td> 3660.7730</td> <td> 1920.037</td> <td>    1.907</td> <td> 0.057</td> <td> -102.845</td> <td> 7424.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>      <td> 4667.2138</td> <td> 1895.196</td> <td>    2.463</td> <td> 0.014</td> <td>  952.289</td> <td> 8382.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>      <td> 4039.1996</td> <td> 1940.071</td> <td>    2.082</td> <td> 0.037</td> <td>  236.311</td> <td> 7842.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>      <td>-1.085e+05</td> <td> 3590.235</td> <td>  -30.217</td> <td> 0.000</td> <td>-1.16e+05</td> <td>-1.01e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>      <td> -1.01e+04</td> <td> 2350.003</td> <td>   -4.299</td> <td> 0.000</td> <td>-1.47e+04</td> <td>-5496.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>      <td> 1095.9244</td> <td> 1892.671</td> <td>    0.579</td> <td> 0.563</td> <td>-2614.052</td> <td> 4805.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>      <td> 2263.7762</td> <td> 1887.300</td> <td>    1.199</td> <td> 0.230</td> <td>-1435.671</td> <td> 5963.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>      <td>  783.9623</td> <td> 1907.591</td> <td>    0.411</td> <td> 0.681</td> <td>-2955.259</td> <td> 4523.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>      <td> 2265.5152</td> <td> 1886.613</td> <td>    1.201</td> <td> 0.230</td> <td>-1432.585</td> <td> 5963.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>      <td>-1.139e+04</td> <td> 3058.833</td> <td>   -3.723</td> <td> 0.000</td> <td>-1.74e+04</td> <td>-5392.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>      <td> 1.069e+04</td> <td> 1904.454</td> <td>    5.614</td> <td> 0.000</td> <td> 6958.725</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>      <td> 6337.7995</td> <td> 1935.443</td> <td>    3.275</td> <td> 0.001</td> <td> 2543.983</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>      <td>-4590.1084</td> <td> 2122.137</td> <td>   -2.163</td> <td> 0.031</td> <td>-8749.879</td> <td> -430.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>      <td>-8094.2878</td> <td> 2272.709</td> <td>   -3.562</td> <td> 0.000</td> <td>-1.25e+04</td> <td>-3639.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>      <td>-1779.6929</td> <td> 1877.645</td> <td>   -0.948</td> <td> 0.343</td> <td>-5460.216</td> <td> 1900.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>      <td> 3546.3590</td> <td> 1886.846</td> <td>    1.880</td> <td> 0.060</td> <td> -152.198</td> <td> 7244.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>      <td> -558.5515</td> <td> 1868.774</td> <td>   -0.299</td> <td> 0.765</td> <td>-4221.684</td> <td> 3104.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>      <td>-1.137e+04</td> <td> 2090.169</td> <td>   -5.441</td> <td> 0.000</td> <td>-1.55e+04</td> <td>-7274.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>      <td> 3.313e+04</td> <td> 3130.737</td> <td>   10.582</td> <td> 0.000</td> <td>  2.7e+04</td> <td> 3.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>      <td> 4633.1839</td> <td> 2276.152</td> <td>    2.036</td> <td> 0.042</td> <td>  171.516</td> <td> 9094.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>      <td> 4698.1594</td> <td> 2312.439</td> <td>    2.032</td> <td> 0.042</td> <td>  165.362</td> <td> 9230.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>      <td>-1.588e+05</td> <td> 5755.681</td> <td>  -27.590</td> <td> 0.000</td> <td> -1.7e+05</td> <td>-1.48e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>      <td> -953.2594</td> <td> 1865.655</td> <td>   -0.511</td> <td> 0.609</td> <td>-4610.279</td> <td> 2703.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>      <td> 5310.5607</td> <td> 1910.748</td> <td>    2.779</td> <td> 0.005</td> <td> 1565.151</td> <td> 9055.971</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>      <td>-8979.5137</td> <td> 2287.952</td> <td>   -3.925</td> <td> 0.000</td> <td>-1.35e+04</td> <td>-4494.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>      <td>-1763.8665</td> <td> 1879.130</td> <td>   -0.939</td> <td> 0.348</td> <td>-5447.300</td> <td> 1919.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>      <td>-5338.2348</td> <td> 1962.337</td> <td>   -2.720</td> <td> 0.007</td> <td>-9184.768</td> <td>-1491.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>      <td>  745.1892</td> <td> 2713.019</td> <td>    0.275</td> <td> 0.784</td> <td>-4572.816</td> <td> 6063.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>      <td> 1267.9682</td> <td> 2338.619</td> <td>    0.542</td> <td> 0.588</td> <td>-3316.146</td> <td> 5852.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>      <td> 1.422e+04</td> <td> 1883.672</td> <td>    7.550</td> <td> 0.000</td> <td> 1.05e+04</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>      <td>  542.5111</td> <td> 2261.694</td> <td>    0.240</td> <td> 0.810</td> <td>-3890.815</td> <td> 4975.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>      <td> 4365.6893</td> <td> 1893.161</td> <td>    2.306</td> <td> 0.021</td> <td>  654.753</td> <td> 8076.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>      <td> 5457.5375</td> <td> 1955.951</td> <td>    2.790</td> <td> 0.005</td> <td> 1623.521</td> <td> 9291.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>      <td> 5406.9499</td> <td> 2379.839</td> <td>    2.272</td> <td> 0.023</td> <td>  742.037</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>      <td>-5137.8977</td> <td> 1943.679</td> <td>   -2.643</td> <td> 0.008</td> <td>-8947.858</td> <td>-1327.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>      <td>-7120.7766</td> <td> 2035.355</td> <td>   -3.499</td> <td> 0.000</td> <td>-1.11e+04</td> <td>-3131.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>      <td> 3617.3288</td> <td> 1884.658</td> <td>    1.919</td> <td> 0.055</td> <td>  -76.940</td> <td> 7311.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>      <td> 2423.9649</td> <td> 1882.094</td> <td>    1.288</td> <td> 0.198</td> <td>-1265.278</td> <td> 6113.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>      <td> 2636.1828</td> <td> 1888.007</td> <td>    1.396</td> <td> 0.163</td> <td>-1064.650</td> <td> 6337.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>      <td> -370.0230</td> <td> 2018.458</td> <td>   -0.183</td> <td> 0.855</td> <td>-4326.563</td> <td> 3586.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>      <td> 4023.7011</td> <td> 1886.119</td> <td>    2.133</td> <td> 0.033</td> <td>  326.568</td> <td> 7720.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>      <td> 5523.7227</td> <td> 1916.870</td> <td>    2.882</td> <td> 0.004</td> <td> 1766.312</td> <td> 9281.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>      <td> 3337.8511</td> <td> 1912.468</td> <td>    1.745</td> <td> 0.081</td> <td> -410.929</td> <td> 7086.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>      <td> 6233.8984</td> <td> 1935.267</td> <td>    3.221</td> <td> 0.001</td> <td> 2440.426</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>      <td> 2087.6910</td> <td> 2469.127</td> <td>    0.846</td> <td> 0.398</td> <td>-2752.241</td> <td> 6927.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>      <td> 5725.0042</td> <td> 1937.789</td> <td>    2.954</td> <td> 0.003</td> <td> 1926.588</td> <td> 9523.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>      <td>-1.848e+04</td> <td> 2271.698</td> <td>   -8.135</td> <td> 0.000</td> <td>-2.29e+04</td> <td> -1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>      <td> 2884.6863</td> <td> 1878.406</td> <td>    1.536</td> <td> 0.125</td> <td> -797.327</td> <td> 6566.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>      <td> 4921.0805</td> <td> 1902.737</td> <td>    2.586</td> <td> 0.010</td> <td> 1191.374</td> <td> 8650.787</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>      <td> 4154.6414</td> <td> 2204.114</td> <td>    1.885</td> <td> 0.059</td> <td> -165.818</td> <td> 8475.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>      <td>  982.5964</td> <td> 1870.064</td> <td>    0.525</td> <td> 0.599</td> <td>-2683.066</td> <td> 4648.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>      <td> 4611.2221</td> <td> 1996.389</td> <td>    2.310</td> <td> 0.021</td> <td>  697.940</td> <td> 8524.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>      <td> 6443.3547</td> <td> 1892.985</td> <td>    3.404</td> <td> 0.001</td> <td> 2732.763</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>      <td> 5182.5680</td> <td> 1953.809</td> <td>    2.653</td> <td> 0.008</td> <td> 1352.751</td> <td> 9012.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>      <td> 4371.1879</td> <td> 1891.184</td> <td>    2.311</td> <td> 0.021</td> <td>  664.127</td> <td> 8078.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>      <td> 3920.2341</td> <td> 1881.911</td> <td>    2.083</td> <td> 0.037</td> <td>  231.350</td> <td> 7609.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>      <td>-4919.3500</td> <td> 1989.347</td> <td>   -2.473</td> <td> 0.013</td> <td>-8818.828</td> <td>-1019.872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>      <td> 5579.7669</td> <td> 2589.433</td> <td>    2.155</td> <td> 0.031</td> <td>  504.012</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>      <td>-2.149e+04</td> <td> 2189.269</td> <td>   -9.815</td> <td> 0.000</td> <td>-2.58e+04</td> <td>-1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>      <td> 6284.4911</td> <td> 1911.721</td> <td>    3.287</td> <td> 0.001</td> <td> 2537.173</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>      <td> 1593.5800</td> <td> 1984.465</td> <td>    0.803</td> <td> 0.422</td> <td>-2296.329</td> <td> 5483.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>      <td> 4680.9703</td> <td> 1892.279</td> <td>    2.474</td> <td> 0.013</td> <td>  971.764</td> <td> 8390.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>      <td> 3732.5040</td> <td> 1896.819</td> <td>    1.968</td> <td> 0.049</td> <td>   14.398</td> <td> 7450.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>      <td> 3685.4231</td> <td> 1889.670</td> <td>    1.950</td> <td> 0.051</td> <td>  -18.671</td> <td> 7389.517</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>      <td> 1412.3279</td> <td> 1903.400</td> <td>    0.742</td> <td> 0.458</td> <td>-2318.679</td> <td> 5143.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>      <td> 3425.6120</td> <td> 1886.884</td> <td>    1.815</td> <td> 0.069</td> <td> -273.020</td> <td> 7124.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>      <td>-2.354e+04</td> <td> 2204.399</td> <td>  -10.681</td> <td> 0.000</td> <td>-2.79e+04</td> <td>-1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>      <td> 4756.7109</td> <td> 1890.668</td> <td>    2.516</td> <td> 0.012</td> <td> 1050.662</td> <td> 8462.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>      <td>-1763.3549</td> <td> 1956.028</td> <td>   -0.901</td> <td> 0.367</td> <td>-5597.522</td> <td> 2070.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>      <td> 1594.0186</td> <td> 1904.603</td> <td>    0.837</td> <td> 0.403</td> <td>-2139.345</td> <td> 5327.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>      <td> -551.9034</td> <td> 1965.277</td> <td>   -0.281</td> <td> 0.779</td> <td>-4404.199</td> <td> 3300.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>      <td> 2.313e+04</td> <td> 2050.513</td> <td>   11.281</td> <td> 0.000</td> <td> 1.91e+04</td> <td> 2.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>      <td>-1253.0232</td> <td> 2221.970</td> <td>   -0.564</td> <td> 0.573</td> <td>-5608.484</td> <td> 3102.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>      <td>-3476.7879</td> <td> 2528.257</td> <td>   -1.375</td> <td> 0.169</td> <td>-8432.626</td> <td> 1479.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>      <td>-6657.4342</td> <td> 1921.654</td> <td>   -3.464</td> <td> 0.001</td> <td>-1.04e+04</td> <td>-2890.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>      <td> 3754.1280</td> <td> 1885.722</td> <td>    1.991</td> <td> 0.047</td> <td>   57.773</td> <td> 7450.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>      <td> 6085.6302</td> <td> 2158.752</td> <td>    2.819</td> <td> 0.005</td> <td> 1854.088</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>      <td> 2066.3853</td> <td> 3764.955</td> <td>    0.549</td> <td> 0.583</td> <td>-5313.603</td> <td> 9446.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>      <td> 4963.4106</td> <td> 1874.865</td> <td>    2.647</td> <td> 0.008</td> <td> 1288.338</td> <td> 8638.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>      <td>-1.751e+04</td> <td> 2080.749</td> <td>   -8.417</td> <td> 0.000</td> <td>-2.16e+04</td> <td>-1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>     <td>-1.509e+04</td> <td> 4441.536</td> <td>   -3.398</td> <td> 0.001</td> <td>-2.38e+04</td> <td>-6384.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>      <td> 5098.8954</td> <td> 1913.924</td> <td>    2.664</td> <td> 0.008</td> <td> 1347.261</td> <td> 8850.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>      <td> 3175.7833</td> <td> 1892.683</td> <td>    1.678</td> <td> 0.093</td> <td> -534.215</td> <td> 6885.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>      <td>-6512.5249</td> <td> 2013.810</td> <td>   -3.234</td> <td> 0.001</td> <td>-1.05e+04</td> <td>-2565.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>      <td>-6666.9320</td> <td> 1931.756</td> <td>   -3.451</td> <td> 0.001</td> <td>-1.05e+04</td> <td>-2880.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>      <td>-4005.8304</td> <td> 2232.235</td> <td>   -1.795</td> <td> 0.073</td> <td>-8381.412</td> <td>  369.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>     <td>-1.243e+04</td> <td> 3702.050</td> <td>   -3.358</td> <td> 0.001</td> <td>-1.97e+04</td> <td>-5174.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>      <td> -159.9485</td> <td> 2036.176</td> <td>   -0.079</td> <td> 0.937</td> <td>-4151.221</td> <td> 3831.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>      <td> 3633.6054</td> <td> 1882.449</td> <td>    1.930</td> <td> 0.054</td> <td>  -56.333</td> <td> 7323.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>     <td> 2001.2059</td> <td> 4082.166</td> <td>    0.490</td> <td> 0.624</td> <td>-6000.574</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>      <td> 3792.1030</td> <td> 1891.311</td> <td>    2.005</td> <td> 0.045</td> <td>   84.794</td> <td> 7499.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>      <td> 4252.9273</td> <td> 1892.229</td> <td>    2.248</td> <td> 0.025</td> <td>  543.817</td> <td> 7962.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>      <td> 5331.3623</td> <td> 1929.983</td> <td>    2.762</td> <td> 0.006</td> <td> 1548.249</td> <td> 9114.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>     <td> 4107.4431</td> <td> 4086.784</td> <td>    1.005</td> <td> 0.315</td> <td>-3903.389</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>      <td>-1840.2931</td> <td> 2289.573</td> <td>   -0.804</td> <td> 0.422</td> <td>-6328.269</td> <td> 2647.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>     <td> 1.096e+04</td> <td> 4774.766</td> <td>    2.295</td> <td> 0.022</td> <td> 1598.888</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>      <td> 5315.3816</td> <td> 2126.320</td> <td>    2.500</td> <td> 0.012</td> <td> 1147.411</td> <td> 9483.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>      <td>-2706.9925</td> <td> 1966.170</td> <td>   -1.377</td> <td> 0.169</td> <td>-6561.039</td> <td> 1147.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>      <td> 1950.1248</td> <td> 1886.585</td> <td>    1.034</td> <td> 0.301</td> <td>-1747.922</td> <td> 5648.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>      <td> 3856.2871</td> <td> 1997.887</td> <td>    1.930</td> <td> 0.054</td> <td>  -59.931</td> <td> 7772.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>      <td> -1.98e+04</td> <td> 2839.541</td> <td>   -6.974</td> <td> 0.000</td> <td>-2.54e+04</td> <td>-1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>      <td> 4079.6361</td> <td> 3344.074</td> <td>    1.220</td> <td> 0.223</td> <td>-2475.351</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>      <td> 4782.8866</td> <td> 1911.134</td> <td>    2.503</td> <td> 0.012</td> <td> 1036.721</td> <td> 8529.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>      <td>-3966.7694</td> <td> 1970.640</td> <td>   -2.013</td> <td> 0.044</td> <td>-7829.579</td> <td> -103.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>      <td> 3140.2437</td> <td> 1880.024</td> <td>    1.670</td> <td> 0.095</td> <td> -544.942</td> <td> 6825.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>      <td> 5344.1170</td> <td> 2069.760</td> <td>    2.582</td> <td> 0.010</td> <td> 1287.015</td> <td> 9401.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>      <td> 9761.2418</td> <td> 1895.523</td> <td>    5.150</td> <td> 0.000</td> <td> 6045.676</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>      <td> 4837.2092</td> <td> 1903.822</td> <td>    2.541</td> <td> 0.011</td> <td> 1105.375</td> <td> 8569.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>      <td> 5252.0872</td> <td> 5653.339</td> <td>    0.929</td> <td> 0.353</td> <td>-5829.475</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>      <td> 4720.5567</td> <td> 1890.634</td> <td>    2.497</td> <td> 0.013</td> <td> 1014.574</td> <td> 8426.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>      <td> -734.7835</td> <td> 1932.553</td> <td>   -0.380</td> <td> 0.704</td> <td>-4522.934</td> <td> 3053.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>      <td> 6089.8782</td> <td> 1928.200</td> <td>    3.158</td> <td> 0.002</td> <td> 2310.259</td> <td> 9869.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>      <td>-2496.1172</td> <td> 2012.976</td> <td>   -1.240</td> <td> 0.215</td> <td>-6441.912</td> <td> 1449.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>      <td> 4445.7448</td> <td> 1898.100</td> <td>    2.342</td> <td> 0.019</td> <td>  725.128</td> <td> 8166.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>      <td> 4813.7868</td> <td> 1924.016</td> <td>    2.502</td> <td> 0.012</td> <td> 1042.369</td> <td> 8585.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>      <td> 3598.1008</td> <td> 1929.493</td> <td>    1.865</td> <td> 0.062</td> <td> -184.053</td> <td> 7380.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>      <td> -734.9224</td> <td> 1960.341</td> <td>   -0.375</td> <td> 0.708</td> <td>-4577.544</td> <td> 3107.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>      <td>-4877.3634</td> <td> 2139.537</td> <td>   -2.280</td> <td> 0.023</td> <td>-9071.240</td> <td> -683.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>      <td> 2631.2582</td> <td> 8140.134</td> <td>    0.323</td> <td> 0.747</td> <td>-1.33e+04</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>      <td> 3122.6930</td> <td> 1884.421</td> <td>    1.657</td> <td> 0.098</td> <td> -571.110</td> <td> 6816.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>      <td> 5860.6445</td> <td> 2301.144</td> <td>    2.547</td> <td> 0.011</td> <td> 1349.988</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>      <td> 2890.7532</td> <td> 2113.603</td> <td>    1.368</td> <td> 0.171</td> <td>-1252.290</td> <td> 7033.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>      <td>-7043.9021</td> <td> 2023.324</td> <td>   -3.481</td> <td> 0.001</td> <td> -1.1e+04</td> <td>-3077.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>      <td> -480.0719</td> <td> 1920.270</td> <td>   -0.250</td> <td> 0.803</td> <td>-4244.146</td> <td> 3284.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>      <td> 5210.9517</td> <td> 1903.130</td> <td>    2.738</td> <td> 0.006</td> <td> 1480.474</td> <td> 8941.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>      <td> 4613.0426</td> <td> 1902.342</td> <td>    2.425</td> <td> 0.015</td> <td>  884.111</td> <td> 8341.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>      <td> 4431.0832</td> <td> 2121.854</td> <td>    2.088</td> <td> 0.037</td> <td>  271.868</td> <td> 8590.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>      <td> 5011.0435</td> <td> 2079.583</td> <td>    2.410</td> <td> 0.016</td> <td>  934.687</td> <td> 9087.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>      <td>-2017.2034</td> <td> 2275.032</td> <td>   -0.887</td> <td> 0.375</td> <td>-6476.675</td> <td> 2442.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>      <td> 4085.2015</td> <td> 1890.317</td> <td>    2.161</td> <td> 0.031</td> <td>  379.841</td> <td> 7790.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>      <td> 3637.9988</td> <td> 1882.029</td> <td>    1.933</td> <td> 0.053</td> <td>  -51.116</td> <td> 7327.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>      <td> 1230.8940</td> <td> 4069.683</td> <td>    0.302</td> <td> 0.762</td> <td>-6746.416</td> <td> 9208.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>      <td> 5944.6150</td> <td> 1910.454</td> <td>    3.112</td> <td> 0.002</td> <td> 2199.781</td> <td> 9689.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>      <td> 3876.8676</td> <td> 2145.664</td> <td>    1.807</td> <td> 0.071</td> <td> -329.020</td> <td> 8082.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>      <td> -563.3170</td> <td> 1968.969</td> <td>   -0.286</td> <td> 0.775</td> <td>-4422.852</td> <td> 3296.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>      <td> 2825.0452</td> <td> 4084.063</td> <td>    0.692</td> <td> 0.489</td> <td>-5180.454</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>      <td> 5143.2369</td> <td> 1902.430</td> <td>    2.704</td> <td> 0.007</td> <td> 1414.132</td> <td> 8872.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>      <td> 4871.3773</td> <td> 1917.705</td> <td>    2.540</td> <td> 0.011</td> <td> 1112.330</td> <td> 8630.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>      <td>-1.321e+04</td> <td> 2050.847</td> <td>   -6.440</td> <td> 0.000</td> <td>-1.72e+04</td> <td>-9187.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>      <td> 7016.4411</td> <td> 1992.023</td> <td>    3.522</td> <td> 0.000</td> <td> 3111.717</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>      <td>-2.048e+04</td> <td> 2422.263</td> <td>   -8.453</td> <td> 0.000</td> <td>-2.52e+04</td> <td>-1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>      <td> 5985.7283</td> <td> 2400.219</td> <td>    2.494</td> <td> 0.013</td> <td> 1280.867</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>      <td> 4924.6282</td> <td> 1908.446</td> <td>    2.580</td> <td> 0.010</td> <td> 1183.731</td> <td> 8665.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>      <td> 3868.1145</td> <td> 1885.957</td> <td>    2.051</td> <td> 0.040</td> <td>  171.299</td> <td> 7564.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>      <td> 3531.1264</td> <td> 1897.188</td> <td>    1.861</td> <td> 0.063</td> <td> -187.704</td> <td> 7249.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>      <td> -176.0146</td> <td> 1979.119</td> <td>   -0.089</td> <td> 0.929</td> <td>-4055.444</td> <td> 3703.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>      <td> 3797.8864</td> <td> 2136.856</td> <td>    1.777</td> <td> 0.076</td> <td> -390.736</td> <td> 7986.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>      <td> -668.9741</td> <td> 2727.946</td> <td>   -0.245</td> <td> 0.806</td> <td>-6016.239</td> <td> 4678.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>      <td> 1287.7612</td> <td> 1923.314</td> <td>    0.670</td> <td> 0.503</td> <td>-2482.281</td> <td> 5057.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>      <td> 1372.8494</td> <td> 1909.405</td> <td>    0.719</td> <td> 0.472</td> <td>-2369.928</td> <td> 5115.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>      <td> 2643.0183</td> <td> 1900.603</td> <td>    1.391</td> <td> 0.164</td> <td>-1082.507</td> <td> 6368.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>      <td>-5777.6863</td> <td> 2549.116</td> <td>   -2.267</td> <td> 0.023</td> <td>-1.08e+04</td> <td> -780.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>      <td> 6908.9206</td> <td> 1898.554</td> <td>    3.639</td> <td> 0.000</td> <td> 3187.413</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>      <td> 5820.7381</td> <td> 1886.029</td> <td>    3.086</td> <td> 0.002</td> <td> 2123.781</td> <td> 9517.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>      <td> 2066.6982</td> <td> 2076.405</td> <td>    0.995</td> <td> 0.320</td> <td>-2003.428</td> <td> 6136.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>      <td> 6503.0343</td> <td> 1923.673</td> <td>    3.381</td> <td> 0.001</td> <td> 2732.289</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>      <td> 6174.9739</td> <td> 3129.278</td> <td>    1.973</td> <td> 0.048</td> <td>   41.027</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>      <td> 3294.9471</td> <td> 1884.523</td> <td>    1.748</td> <td> 0.080</td> <td> -399.057</td> <td> 6988.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>      <td> 2149.9549</td> <td> 2020.801</td> <td>    1.064</td> <td> 0.287</td> <td>-1811.180</td> <td> 6111.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>      <td> 3684.4739</td> <td> 1884.310</td> <td>    1.955</td> <td> 0.051</td> <td>   -9.113</td> <td> 7378.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>      <td> 4642.5032</td> <td> 1893.941</td> <td>    2.451</td> <td> 0.014</td> <td>  930.039</td> <td> 8354.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>      <td> 7302.9869</td> <td> 1896.443</td> <td>    3.851</td> <td> 0.000</td> <td> 3585.617</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>      <td>  181.7039</td> <td> 1884.279</td> <td>    0.096</td> <td> 0.923</td> <td>-3511.822</td> <td> 3875.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>      <td> -274.3024</td> <td> 1937.642</td> <td>   -0.142</td> <td> 0.887</td> <td>-4072.429</td> <td> 3523.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>      <td>  1.28e+04</td> <td> 1904.390</td> <td>    6.722</td> <td> 0.000</td> <td> 9067.861</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>      <td> 2256.8554</td> <td> 3094.499</td> <td>    0.729</td> <td> 0.466</td> <td>-3808.919</td> <td> 8322.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>      <td>  454.8082</td> <td> 2183.158</td> <td>    0.208</td> <td> 0.835</td> <td>-3824.575</td> <td> 4734.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>      <td> 2.545e+04</td> <td> 2148.988</td> <td>   11.841</td> <td> 0.000</td> <td> 2.12e+04</td> <td> 2.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>      <td> 3535.9150</td> <td> 1881.772</td> <td>    1.879</td> <td> 0.060</td> <td> -152.697</td> <td> 7224.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>      <td>  237.8283</td> <td> 1986.583</td> <td>    0.120</td> <td> 0.905</td> <td>-3656.233</td> <td> 4131.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>      <td>-6685.3518</td> <td> 2284.765</td> <td>   -2.926</td> <td> 0.003</td> <td>-1.12e+04</td> <td>-2206.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>      <td> 4494.6574</td> <td> 2888.074</td> <td>    1.556</td> <td> 0.120</td> <td>-1166.486</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>      <td> 1181.2310</td> <td> 1918.089</td> <td>    0.616</td> <td> 0.538</td> <td>-2578.568</td> <td> 4941.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>      <td>-7911.4411</td> <td> 2855.978</td> <td>   -2.770</td> <td> 0.006</td> <td>-1.35e+04</td> <td>-2313.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>      <td>-4271.2095</td> <td> 1936.237</td> <td>   -2.206</td> <td> 0.027</td> <td>-8066.583</td> <td> -475.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>      <td> 4345.8538</td> <td> 1890.458</td> <td>    2.299</td> <td> 0.022</td> <td>  640.217</td> <td> 8051.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>      <td> 3110.0251</td> <td> 1889.359</td> <td>    1.646</td> <td> 0.100</td> <td> -593.458</td> <td> 6813.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>      <td> -200.0609</td> <td> 1875.303</td> <td>   -0.107</td> <td> 0.915</td> <td>-3875.992</td> <td> 3475.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>      <td> 5677.9998</td> <td> 2294.109</td> <td>    2.475</td> <td> 0.013</td> <td> 1181.134</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>      <td> 3068.3648</td> <td> 1999.314</td> <td>    1.535</td> <td> 0.125</td> <td> -850.650</td> <td> 6987.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>      <td>-6368.5974</td> <td> 2092.013</td> <td>   -3.044</td> <td> 0.002</td> <td>-1.05e+04</td> <td>-2267.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>      <td> 3312.6048</td> <td> 3643.490</td> <td>    0.909</td> <td> 0.363</td> <td>-3829.292</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>      <td> 4314.1537</td> <td> 1883.555</td> <td>    2.290</td> <td> 0.022</td> <td>  622.048</td> <td> 8006.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>      <td> 4406.8673</td> <td> 1902.507</td> <td>    2.316</td> <td> 0.021</td> <td>  677.610</td> <td> 8136.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>      <td> 1261.0932</td> <td> 1909.746</td> <td>    0.660</td> <td> 0.509</td> <td>-2482.352</td> <td> 5004.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>      <td> 4258.7072</td> <td> 1895.126</td> <td>    2.247</td> <td> 0.025</td> <td>  543.920</td> <td> 7973.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>      <td>   94.9133</td> <td> 1998.861</td> <td>    0.047</td> <td> 0.962</td> <td>-3823.213</td> <td> 4013.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>      <td>-6767.6640</td> <td> 2431.073</td> <td>   -2.784</td> <td> 0.005</td> <td>-1.15e+04</td> <td>-2002.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>      <td> 3508.6228</td> <td> 1886.606</td> <td>    1.860</td> <td> 0.063</td> <td> -189.464</td> <td> 7206.710</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>      <td>  864.6487</td> <td> 2161.340</td> <td>    0.400</td> <td> 0.689</td> <td>-3371.966</td> <td> 5101.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>      <td> 4079.5367</td> <td> 1894.075</td> <td>    2.154</td> <td> 0.031</td> <td>  366.810</td> <td> 7792.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>      <td> 4368.0875</td> <td> 1892.540</td> <td>    2.308</td> <td> 0.021</td> <td>  658.369</td> <td> 8077.806</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>      <td> -444.5436</td> <td> 1938.669</td> <td>   -0.229</td> <td> 0.819</td> <td>-4244.684</td> <td> 3355.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>      <td> 5242.7246</td> <td> 2062.069</td> <td>    2.542</td> <td> 0.011</td> <td> 1200.698</td> <td> 9284.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>      <td> 3203.1421</td> <td> 2117.987</td> <td>    1.512</td> <td> 0.130</td> <td> -948.494</td> <td> 7354.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>      <td>-1982.7323</td> <td> 1872.804</td> <td>   -1.059</td> <td> 0.290</td> <td>-5653.765</td> <td> 1688.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>      <td> 3939.9208</td> <td> 1887.339</td> <td>    2.088</td> <td> 0.037</td> <td>  240.396</td> <td> 7639.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>      <td>-8822.8734</td> <td> 2053.343</td> <td>   -4.297</td> <td> 0.000</td> <td>-1.28e+04</td> <td>-4797.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>      <td> 3643.2673</td> <td> 1880.924</td> <td>    1.937</td> <td> 0.053</td> <td>  -43.683</td> <td> 7330.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>      <td>-1668.5671</td> <td> 1880.801</td> <td>   -0.887</td> <td> 0.375</td> <td>-5355.275</td> <td> 2018.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>      <td> 4887.6887</td> <td> 1895.424</td> <td>    2.579</td> <td> 0.010</td> <td> 1172.316</td> <td> 8603.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>      <td>-6569.8690</td> <td> 2034.245</td> <td>   -3.230</td> <td> 0.001</td> <td>-1.06e+04</td> <td>-2582.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>      <td> -278.4626</td> <td> 1986.987</td> <td>   -0.140</td> <td> 0.889</td> <td>-4173.315</td> <td> 3616.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>      <td>-1014.3734</td> <td> 1976.040</td> <td>   -0.513</td> <td> 0.608</td> <td>-4887.768</td> <td> 2859.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>      <td> 7500.9507</td> <td> 1914.777</td> <td>    3.917</td> <td> 0.000</td> <td> 3747.643</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>      <td> 4187.0950</td> <td> 1887.493</td> <td>    2.218</td> <td> 0.027</td> <td>  487.269</td> <td> 7886.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>      <td> 4451.0966</td> <td> 2005.459</td> <td>    2.219</td> <td> 0.026</td> <td>  520.035</td> <td> 8382.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>      <td>  860.7939</td> <td> 1869.151</td> <td>    0.461</td> <td> 0.645</td> <td>-2803.078</td> <td> 4524.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>      <td> 2057.4060</td> <td> 1885.421</td> <td>    1.091</td> <td> 0.275</td> <td>-1638.359</td> <td> 5753.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>      <td>-1.955e+04</td> <td> 2201.157</td> <td>   -8.881</td> <td> 0.000</td> <td>-2.39e+04</td> <td>-1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>      <td> 2426.3775</td> <td> 1987.219</td> <td>    1.221</td> <td> 0.222</td> <td>-1468.929</td> <td> 6321.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>      <td> 5924.6963</td> <td> 1920.790</td> <td>    3.085</td> <td> 0.002</td> <td> 2159.602</td> <td> 9689.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>      <td>-6813.2524</td> <td> 2034.913</td> <td>   -3.348</td> <td> 0.001</td> <td>-1.08e+04</td> <td>-2824.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>      <td> 3427.6567</td> <td> 2727.958</td> <td>    1.256</td> <td> 0.209</td> <td>-1919.632</td> <td> 8774.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>      <td> 1583.0928</td> <td> 1924.083</td> <td>    0.823</td> <td> 0.411</td> <td>-2188.455</td> <td> 5354.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>      <td>-1.386e+04</td> <td> 2219.412</td> <td>   -6.247</td> <td> 0.000</td> <td>-1.82e+04</td> <td>-9513.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>      <td>-1.125e+04</td> <td> 2138.260</td> <td>   -5.260</td> <td> 0.000</td> <td>-1.54e+04</td> <td>-7056.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>      <td> 3747.6061</td> <td> 1890.531</td> <td>    1.982</td> <td> 0.047</td> <td>   41.826</td> <td> 7453.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>      <td> 4967.5181</td> <td> 1914.276</td> <td>    2.595</td> <td> 0.009</td> <td> 1215.193</td> <td> 8719.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>      <td> 4448.7369</td> <td> 1893.131</td> <td>    2.350</td> <td> 0.019</td> <td>  737.860</td> <td> 8159.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>      <td>-1564.8370</td> <td> 2068.102</td> <td>   -0.757</td> <td> 0.449</td> <td>-5618.689</td> <td> 2489.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>      <td> 5167.8110</td> <td> 1904.168</td> <td>    2.714</td> <td> 0.007</td> <td> 1435.300</td> <td> 8900.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>      <td>  336.5507</td> <td> 1986.336</td> <td>    0.169</td> <td> 0.865</td> <td>-3557.026</td> <td> 4230.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>      <td>-1014.2924</td> <td> 2411.287</td> <td>   -0.421</td> <td> 0.674</td> <td>-5740.849</td> <td> 3712.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>      <td>  811.2235</td> <td> 2008.822</td> <td>    0.404</td> <td> 0.686</td> <td>-3126.429</td> <td> 4748.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>      <td>  111.2975</td> <td> 1911.608</td> <td>    0.058</td> <td> 0.954</td> <td>-3635.798</td> <td> 3858.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>      <td>-2390.3624</td> <td> 2139.162</td> <td>   -1.117</td> <td> 0.264</td> <td>-6583.505</td> <td> 1802.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>      <td> 5671.7298</td> <td> 1921.079</td> <td>    2.952</td> <td> 0.003</td> <td> 1906.070</td> <td> 9437.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>      <td>  887.0208</td> <td> 1937.131</td> <td>    0.458</td> <td> 0.647</td> <td>-2910.104</td> <td> 4684.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>      <td>-1.195e+04</td> <td> 2040.351</td> <td>   -5.859</td> <td> 0.000</td> <td> -1.6e+04</td> <td>-7954.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>      <td> 1302.6717</td> <td> 2006.620</td> <td>    0.649</td> <td> 0.516</td> <td>-2630.664</td> <td> 5236.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>      <td>  884.4232</td> <td> 1963.476</td> <td>    0.450</td> <td> 0.652</td> <td>-2964.342</td> <td> 4733.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>      <td> 4944.8247</td> <td> 1881.092</td> <td>    2.629</td> <td> 0.009</td> <td> 1257.545</td> <td> 8632.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>      <td> 4982.4455</td> <td> 2045.671</td> <td>    2.436</td> <td> 0.015</td> <td>  972.562</td> <td> 8992.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>      <td> 3670.3352</td> <td> 1888.262</td> <td>    1.944</td> <td> 0.052</td> <td>  -30.998</td> <td> 7371.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>      <td> 3236.9587</td> <td> 1883.212</td> <td>    1.719</td> <td> 0.086</td> <td> -454.475</td> <td> 6928.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>      <td> 1240.3781</td> <td> 1908.909</td> <td>    0.650</td> <td> 0.516</td> <td>-2501.426</td> <td> 4982.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>      <td>-8111.7072</td> <td> 2318.225</td> <td>   -3.499</td> <td> 0.000</td> <td>-1.27e+04</td> <td>-3567.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>      <td> 6388.1362</td> <td> 2303.552</td> <td>    2.773</td> <td> 0.006</td> <td> 1872.761</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>      <td> 3671.8709</td> <td> 1886.483</td> <td>    1.946</td> <td> 0.052</td> <td>  -25.974</td> <td> 7369.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>      <td> 6394.2133</td> <td> 2734.879</td> <td>    2.338</td> <td> 0.019</td> <td> 1033.358</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>      <td> 8019.4538</td> <td> 2187.179</td> <td>    3.667</td> <td> 0.000</td> <td> 3732.189</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>      <td>-1046.7635</td> <td> 1930.939</td> <td>   -0.542</td> <td> 0.588</td> <td>-4831.751</td> <td> 2738.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>      <td> 2.288e+04</td> <td> 2174.771</td> <td>   10.523</td> <td> 0.000</td> <td> 1.86e+04</td> <td> 2.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>      <td>-8252.7873</td> <td> 2048.657</td> <td>   -4.028</td> <td> 0.000</td> <td>-1.23e+04</td> <td>-4237.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>      <td> 5182.1662</td> <td> 1912.584</td> <td>    2.710</td> <td> 0.007</td> <td> 1433.157</td> <td> 8931.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>      <td>  285.1798</td> <td> 2085.795</td> <td>    0.137</td> <td> 0.891</td> <td>-3803.354</td> <td> 4373.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>      <td>-6174.7927</td> <td> 2267.068</td> <td>   -2.724</td> <td> 0.006</td> <td>-1.06e+04</td> <td>-1730.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>      <td> 5357.3152</td> <td> 1891.680</td> <td>    2.832</td> <td> 0.005</td> <td> 1649.281</td> <td> 9065.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>      <td> 5343.3350</td> <td> 1921.755</td> <td>    2.780</td> <td> 0.005</td> <td> 1576.349</td> <td> 9110.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>      <td> 3082.5063</td> <td> 1882.670</td> <td>    1.637</td> <td> 0.102</td> <td> -607.866</td> <td> 6772.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>      <td>-1618.1451</td> <td> 1874.980</td> <td>   -0.863</td> <td> 0.388</td> <td>-5293.443</td> <td> 2057.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>      <td> 4370.1464</td> <td> 3672.134</td> <td>    1.190</td> <td> 0.234</td> <td>-2827.897</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>      <td>-1669.7543</td> <td> 1880.379</td> <td>   -0.888</td> <td> 0.375</td> <td>-5355.635</td> <td> 2016.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>      <td> -102.7675</td> <td> 2184.219</td> <td>   -0.047</td> <td> 0.962</td> <td>-4384.230</td> <td> 4178.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>      <td>  619.4863</td> <td> 1886.940</td> <td>    0.328</td> <td> 0.743</td> <td>-3079.255</td> <td> 4318.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>      <td> 4089.5465</td> <td> 1889.663</td> <td>    2.164</td> <td> 0.030</td> <td>  385.466</td> <td> 7793.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>      <td> 5250.1882</td> <td> 1901.428</td> <td>    2.761</td> <td> 0.006</td> <td> 1523.048</td> <td> 8977.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>      <td> -167.6682</td> <td> 1946.070</td> <td>   -0.086</td> <td> 0.931</td> <td>-3982.316</td> <td> 3646.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>      <td> 9881.2575</td> <td> 2003.848</td> <td>    4.931</td> <td> 0.000</td> <td> 5953.354</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>      <td> 6358.0137</td> <td> 1939.505</td> <td>    3.278</td> <td> 0.001</td> <td> 2556.236</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>      <td> 3921.8310</td> <td> 1888.789</td> <td>    2.076</td> <td> 0.038</td> <td>  219.464</td> <td> 7624.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>      <td> 4019.5858</td> <td> 1886.207</td> <td>    2.131</td> <td> 0.033</td> <td>  322.281</td> <td> 7716.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>      <td> 5179.6950</td> <td> 1922.581</td> <td>    2.694</td> <td> 0.007</td> <td> 1411.090</td> <td> 8948.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>      <td>-3798.5887</td> <td> 2037.891</td> <td>   -1.864</td> <td> 0.062</td> <td>-7793.223</td> <td>  196.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>      <td> 2015.6252</td> <td> 1876.670</td> <td>    1.074</td> <td> 0.283</td> <td>-1662.987</td> <td> 5694.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>      <td>  783.6540</td> <td> 1875.076</td> <td>    0.418</td> <td> 0.676</td> <td>-2891.833</td> <td> 4459.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>      <td>-1.876e+04</td> <td> 2234.007</td> <td>   -8.396</td> <td> 0.000</td> <td>-2.31e+04</td> <td>-1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>      <td>-8901.2931</td> <td> 2173.056</td> <td>   -4.096</td> <td> 0.000</td> <td>-1.32e+04</td> <td>-4641.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>      <td> 5563.5665</td> <td> 2223.931</td> <td>    2.502</td> <td> 0.012</td> <td> 1204.262</td> <td> 9922.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>      <td> 1076.0450</td> <td> 1871.540</td> <td>    0.575</td> <td> 0.565</td> <td>-2592.510</td> <td> 4744.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>      <td> -332.1457</td> <td> 1903.188</td> <td>   -0.175</td> <td> 0.861</td> <td>-4062.737</td> <td> 3398.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>      <td>-6534.2926</td> <td> 2323.696</td> <td>   -2.812</td> <td> 0.005</td> <td>-1.11e+04</td> <td>-1979.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>      <td>-1080.8584</td> <td> 2074.877</td> <td>   -0.521</td> <td> 0.602</td> <td>-5147.991</td> <td> 2986.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>      <td> -939.2564</td> <td> 1868.324</td> <td>   -0.503</td> <td> 0.615</td> <td>-4601.507</td> <td> 2722.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>      <td>-2806.9536</td> <td> 1947.227</td> <td>   -1.442</td> <td> 0.149</td> <td>-6623.869</td> <td> 1009.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>      <td> 4738.5874</td> <td> 3383.557</td> <td>    1.400</td> <td> 0.161</td> <td>-1893.793</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>      <td> 2640.2302</td> <td> 2267.952</td> <td>    1.164</td> <td> 0.244</td> <td>-1805.365</td> <td> 7085.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>      <td> 5178.7693</td> <td> 1907.866</td> <td>    2.714</td> <td> 0.007</td> <td> 1439.008</td> <td> 8918.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>      <td> 2567.5309</td> <td> 1883.220</td> <td>    1.363</td> <td> 0.173</td> <td>-1123.919</td> <td> 6258.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>      <td>-1799.1070</td> <td> 3371.606</td> <td>   -0.534</td> <td> 0.594</td> <td>-8408.061</td> <td> 4809.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>      <td>-5071.6784</td> <td> 2007.828</td> <td>   -2.526</td> <td> 0.012</td> <td>-9007.382</td> <td>-1135.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>      <td>-6693.0668</td> <td> 1999.891</td> <td>   -3.347</td> <td> 0.001</td> <td>-1.06e+04</td> <td>-2772.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>      <td> 4530.0976</td> <td> 1889.274</td> <td>    2.398</td> <td> 0.017</td> <td>  826.781</td> <td> 8233.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>      <td> 5414.8369</td> <td> 2502.877</td> <td>    2.163</td> <td> 0.031</td> <td>  508.747</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>      <td> 6377.0626</td> <td> 1939.103</td> <td>    3.289</td> <td> 0.001</td> <td> 2576.071</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>      <td> 5066.9414</td> <td> 1894.447</td> <td>    2.675</td> <td> 0.007</td> <td> 1353.485</td> <td> 8780.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>      <td> -928.3173</td> <td> 1912.641</td> <td>   -0.485</td> <td> 0.627</td> <td>-4677.437</td> <td> 2820.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>      <td>-1586.1615</td> <td> 1999.894</td> <td>   -0.793</td> <td> 0.428</td> <td>-5506.313</td> <td> 2333.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>      <td>-1086.7601</td> <td> 3079.383</td> <td>   -0.353</td> <td> 0.724</td> <td>-7122.905</td> <td> 4949.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>      <td>-2250.3692</td> <td> 1868.021</td> <td>   -1.205</td> <td> 0.228</td> <td>-5912.026</td> <td> 1411.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>      <td> 2906.9673</td> <td> 1933.000</td> <td>    1.504</td> <td> 0.133</td> <td> -882.061</td> <td> 6695.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>      <td> 1876.8362</td> <td> 1908.153</td> <td>    0.984</td> <td> 0.325</td> <td>-1863.486</td> <td> 5617.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>      <td>-2.787e+04</td> <td> 3795.325</td> <td>   -7.343</td> <td> 0.000</td> <td>-3.53e+04</td> <td>-2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>      <td> 4832.4677</td> <td> 2290.196</td> <td>    2.110</td> <td> 0.035</td> <td>  343.272</td> <td> 9321.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>      <td>  638.3682</td> <td> 2006.770</td> <td>    0.318</td> <td> 0.750</td> <td>-3295.263</td> <td> 4571.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>      <td>-3209.2322</td> <td> 2167.623</td> <td>   -1.481</td> <td> 0.139</td> <td>-7458.163</td> <td> 1039.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>      <td> 8727.5253</td> <td> 1945.950</td> <td>    4.485</td> <td> 0.000</td> <td> 4913.114</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>      <td>-3357.0062</td> <td> 1885.295</td> <td>   -1.781</td> <td> 0.075</td> <td>-7052.524</td> <td>  338.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>      <td>-9433.9635</td> <td> 2046.163</td> <td>   -4.611</td> <td> 0.000</td> <td>-1.34e+04</td> <td>-5423.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>      <td>-2.456e+04</td> <td> 3444.483</td> <td>   -7.131</td> <td> 0.000</td> <td>-3.13e+04</td> <td>-1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>      <td>  893.6168</td> <td> 1923.595</td> <td>    0.465</td> <td> 0.642</td> <td>-2876.975</td> <td> 4664.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>      <td>-3563.2028</td> <td> 2729.749</td> <td>   -1.305</td> <td> 0.192</td> <td>-8914.003</td> <td> 1787.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>      <td>-4032.0335</td> <td> 1921.783</td> <td>   -2.098</td> <td> 0.036</td> <td>-7799.073</td> <td> -264.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>      <td>-4766.8112</td> <td> 3333.138</td> <td>   -1.430</td> <td> 0.153</td> <td>-1.13e+04</td> <td> 1766.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>      <td> 5174.2224</td> <td> 1938.394</td> <td>    2.669</td> <td> 0.008</td> <td> 1374.621</td> <td> 8973.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>      <td> -263.9671</td> <td> 1993.288</td> <td>   -0.132</td> <td> 0.895</td> <td>-4171.171</td> <td> 3643.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>      <td>-1187.2024</td> <td> 2063.936</td> <td>   -0.575</td> <td> 0.565</td> <td>-5232.888</td> <td> 2858.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>      <td>-1.397e+04</td> <td> 4527.328</td> <td>   -3.086</td> <td> 0.002</td> <td>-2.28e+04</td> <td>-5097.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>      <td> -877.7466</td> <td> 1992.241</td> <td>   -0.441</td> <td> 0.660</td> <td>-4782.897</td> <td> 3027.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>      <td> 2761.0191</td> <td> 1894.635</td> <td>    1.457</td> <td> 0.145</td> <td> -952.807</td> <td> 6474.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>      <td> 4007.7347</td> <td> 1875.265</td> <td>    2.137</td> <td> 0.033</td> <td>  331.878</td> <td> 7683.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>      <td>-6828.8306</td> <td> 1935.540</td> <td>   -3.528</td> <td> 0.000</td> <td>-1.06e+04</td> <td>-3034.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>      <td>-7685.7043</td> <td> 2015.237</td> <td>   -3.814</td> <td> 0.000</td> <td>-1.16e+04</td> <td>-3735.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>      <td> 3747.0948</td> <td> 1890.350</td> <td>    1.982</td> <td> 0.047</td> <td>   41.669</td> <td> 7452.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>      <td>-5368.8020</td> <td> 1920.256</td> <td>   -2.796</td> <td> 0.005</td> <td>-9132.850</td> <td>-1604.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>      <td> 5064.2150</td> <td> 1906.029</td> <td>    2.657</td> <td> 0.008</td> <td> 1328.056</td> <td> 8800.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>      <td> 1030.4091</td> <td> 1892.700</td> <td>    0.544</td> <td> 0.586</td> <td>-2679.624</td> <td> 4740.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>      <td>-7979.6142</td> <td> 2104.282</td> <td>   -3.792</td> <td> 0.000</td> <td>-1.21e+04</td> <td>-3854.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>      <td> 3494.9073</td> <td> 1987.897</td> <td>    1.758</td> <td> 0.079</td> <td> -401.728</td> <td> 7391.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>      <td> -3.02e+04</td> <td> 2085.706</td> <td>  -14.480</td> <td> 0.000</td> <td>-3.43e+04</td> <td>-2.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>      <td> 5988.0513</td> <td> 1929.137</td> <td>    3.104</td> <td> 0.002</td> <td> 2206.595</td> <td> 9769.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>      <td>-4997.0058</td> <td> 1912.519</td> <td>   -2.613</td> <td> 0.009</td> <td>-8745.887</td> <td>-1248.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>      <td>-2307.9747</td> <td> 2751.378</td> <td>   -0.839</td> <td> 0.402</td> <td>-7701.171</td> <td> 3085.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>      <td> 1439.8701</td> <td> 1900.249</td> <td>    0.758</td> <td> 0.449</td> <td>-2284.959</td> <td> 5164.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>      <td> 5676.8617</td> <td> 1932.396</td> <td>    2.938</td> <td> 0.003</td> <td> 1889.018</td> <td> 9464.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9999.0</th>      <td>-9230.0755</td> <td> 2004.514</td> <td>   -4.605</td> <td> 0.000</td> <td>-1.32e+04</td> <td>-5300.868</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22714.975</td> <th>  Durbin-Watson:     </th>   <td>   0.750</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>140074505.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>14.746</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>537.398</td>  <th>  Cond. No.          </th>   <td>2.53e+17</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.36e-22. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &       0.664    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &       0.641    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &       28.98    \\\\\n",
       "\\textbf{Date:}             & Mon, 14 Oct 2024 & \\textbf{  Prob (F-statistic):} &       0.00     \\\\\n",
       "\\textbf{Time:}             &     16:18:31     & \\textbf{  Log-Likelihood:    } &  -1.2192e+05   \\\\\n",
       "\\textbf{No. Observations:} &       11736      & \\textbf{  AIC:               } &   2.454e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       10985      & \\textbf{  BIC:               } &   2.509e+05    \\\\\n",
       "\\textbf{Df Model:}         &         750      & \\textbf{                     } &                \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &                \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                     & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}       &   -6271.8534  &      532.282     &   -11.783  &         0.000        &    -7315.222    &    -5228.485     \\\\\n",
       "\\textbf{gspilltecIV} &       0.1630  &        0.028     &     5.768  &         0.000        &        0.108    &        0.218     \\\\\n",
       "\\textbf{gspillsicIV} &       0.4887  &        0.052     &     9.386  &         0.000        &        0.387    &        0.591     \\\\\n",
       "\\textbf{pat\\_count}  &     -29.4376  &        1.715     &   -17.168  &         0.000        &      -32.799    &      -26.076     \\\\\n",
       "\\textbf{rsales}      &       1.0154  &        0.041     &    24.569  &         0.000        &        0.934    &        1.096     \\\\\n",
       "\\textbf{rppent}      &       0.5175  &        0.087     &     5.962  &         0.000        &        0.347    &        0.688     \\\\\n",
       "\\textbf{emp}         &      -2.5097  &        7.030     &    -0.357  &         0.721        &      -16.291    &       11.271     \\\\\n",
       "\\textbf{rxrd}        &       8.6216  &        0.669     &    12.878  &         0.000        &        7.309    &        9.934     \\\\\n",
       "\\textbf{1981}        &     762.8261  &      381.627     &     1.999  &         0.046        &       14.769    &     1510.883     \\\\\n",
       "\\textbf{1982}        &     883.7420  &      369.574     &     2.391  &         0.017        &      159.310    &     1608.174     \\\\\n",
       "\\textbf{1983}        &     821.5707  &      357.444     &     2.298  &         0.022        &      120.917    &     1522.225     \\\\\n",
       "\\textbf{1984}        &     310.9095  &      346.828     &     0.896  &         0.370        &     -368.935    &      990.754     \\\\\n",
       "\\textbf{1985}        &     112.7209  &      339.088     &     0.332  &         0.740        &     -551.953    &      777.395     \\\\\n",
       "\\textbf{1986}        &    -152.9965  &      330.737     &    -0.463  &         0.644        &     -801.300    &      495.307     \\\\\n",
       "\\textbf{1987}        &    -423.0556  &      324.228     &    -1.305  &         0.192        &    -1058.600    &      212.489     \\\\\n",
       "\\textbf{1988}        &    -777.3124  &      321.303     &    -2.419  &         0.016        &    -1407.124    &     -147.500     \\\\\n",
       "\\textbf{1989}        &    -661.1419  &      317.261     &    -2.084  &         0.037        &    -1283.031    &      -39.252     \\\\\n",
       "\\textbf{1990}        &   -1153.5785  &      313.432     &    -3.680  &         0.000        &    -1767.961    &     -539.196     \\\\\n",
       "\\textbf{1991}        &    -798.2414  &      312.074     &    -2.558  &         0.011        &    -1409.962    &     -186.521     \\\\\n",
       "\\textbf{1992}        &    -982.2983  &      312.534     &    -3.143  &         0.002        &    -1594.921    &     -369.676     \\\\\n",
       "\\textbf{1993}        &    -999.6691  &      313.254     &    -3.191  &         0.001        &    -1613.703    &     -385.635     \\\\\n",
       "\\textbf{1994}        &   -1323.3880  &      317.300     &    -4.171  &         0.000        &    -1945.352    &     -701.424     \\\\\n",
       "\\textbf{1995}        &    -816.2328  &      325.043     &    -2.511  &         0.012        &    -1453.375    &     -179.091     \\\\\n",
       "\\textbf{1996}        &    -732.9858  &      338.616     &    -2.165  &         0.030        &    -1396.733    &      -69.238     \\\\\n",
       "\\textbf{1997}        &    -349.2790  &      356.979     &    -0.978  &         0.328        &    -1049.022    &      350.464     \\\\\n",
       "\\textbf{1998}        &    -247.3364  &      381.255     &    -0.649  &         0.517        &     -994.665    &      499.993     \\\\\n",
       "\\textbf{1999}        &     253.8930  &      408.826     &     0.621  &         0.535        &     -547.480    &     1055.266     \\\\\n",
       "\\textbf{10005.0}     &    3097.6615  &     1880.996     &     1.647  &         0.100        &     -589.430    &     6784.753     \\\\\n",
       "\\textbf{10006.0}     &    2317.7560  &     2276.237     &     1.018  &         0.309        &    -2144.079    &     6779.591     \\\\\n",
       "\\textbf{10008.0}     &    1438.1417  &     1886.554     &     0.762  &         0.446        &    -2259.844    &     5136.127     \\\\\n",
       "\\textbf{10016.0}     &    2713.9470  &     1892.028     &     1.434  &         0.151        &     -994.769    &     6422.663     \\\\\n",
       "\\textbf{10030.0}     &    5086.2191  &     1901.323     &     2.675  &         0.007        &     1359.283    &     8813.155     \\\\\n",
       "\\textbf{1004.0}      &    4105.1769  &     1889.911     &     2.172  &         0.030        &      400.612    &     7809.742     \\\\\n",
       "\\textbf{10056.0}     &    1739.8066  &     1873.446     &     0.929  &         0.353        &    -1932.485    &     5412.099     \\\\\n",
       "\\textbf{10085.0}     &   -3584.2178  &     1887.069     &    -1.899  &         0.058        &    -7283.212    &      114.777     \\\\\n",
       "\\textbf{10092.0}     &    3969.6072  &     4695.384     &     0.845  &         0.398        &    -5234.191    &     1.32e+04     \\\\\n",
       "\\textbf{10097.0}     &   -3492.5868  &     1884.197     &    -1.854  &         0.064        &    -7185.951    &      200.778     \\\\\n",
       "\\textbf{1010.0}      &    2642.8898  &     4696.864     &     0.563  &         0.574        &    -6563.809    &     1.18e+04     \\\\\n",
       "\\textbf{10109.0}     &    6527.9249  &     1936.233     &     3.371  &         0.001        &     2732.560    &     1.03e+04     \\\\\n",
       "\\textbf{10115.0}     &    1516.7663  &     1875.072     &     0.809  &         0.419        &    -2158.713    &     5192.245     \\\\\n",
       "\\textbf{10124.0}     &    6692.8481  &     1941.452     &     3.447  &         0.001        &     2887.252    &     1.05e+04     \\\\\n",
       "\\textbf{1013.0}      &   -1796.1429  &     1888.737     &    -0.951  &         0.342        &    -5498.407    &     1906.121     \\\\\n",
       "\\textbf{10150.0}     &   -4038.5599  &     2457.273     &    -1.644  &         0.100        &    -8855.258    &      778.138     \\\\\n",
       "\\textbf{10159.0}     &   -4707.9285  &     3442.390     &    -1.368  &         0.171        &    -1.15e+04    &     2039.776     \\\\\n",
       "\\textbf{10174.0}     &    5329.6656  &     2134.048     &     2.497  &         0.013        &     1146.548    &     9512.784     \\\\\n",
       "\\textbf{10185.0}     &    2940.1622  &     2144.003     &     1.371  &         0.170        &    -1262.470    &     7142.794     \\\\\n",
       "\\textbf{10195.0}     &   -5356.9855  &     2061.047     &    -2.599  &         0.009        &    -9397.008    &    -1316.963     \\\\\n",
       "\\textbf{10198.0}     &    4996.0691  &     1899.773     &     2.630  &         0.009        &     1272.172    &     8719.966     \\\\\n",
       "\\textbf{10215.0}     &    6330.3022  &     1936.864     &     3.268  &         0.001        &     2533.701    &     1.01e+04     \\\\\n",
       "\\textbf{10232.0}     &     820.3515  &     2217.856     &     0.370  &         0.711        &    -3527.045    &     5167.748     \\\\\n",
       "\\textbf{10236.0}     &    4715.7756  &     1901.887     &     2.480  &         0.013        &      987.735    &     8443.816     \\\\\n",
       "\\textbf{10286.0}     &    2154.7388  &     1894.796     &     1.137  &         0.255        &    -1559.402    &     5868.880     \\\\\n",
       "\\textbf{10301.0}     &   -2.005e+04  &     2162.637     &    -9.272  &         0.000        &    -2.43e+04    &    -1.58e+04     \\\\\n",
       "\\textbf{10312.0}     &    3818.9932  &     1889.577     &     2.021  &         0.043        &      115.081    &     7522.905     \\\\\n",
       "\\textbf{10332.0}     &   -5177.8756  &     3440.106     &    -1.505  &         0.132        &    -1.19e+04    &     1565.351     \\\\\n",
       "\\textbf{1036.0}      &    1614.5984  &     1995.996     &     0.809  &         0.419        &    -2297.914    &     5527.110     \\\\\n",
       "\\textbf{10374.0}     &    2316.7998  &     1873.315     &     1.237  &         0.216        &    -1355.234    &     5988.834     \\\\\n",
       "\\textbf{10386.0}     &   -3228.6671  &     1955.239     &    -1.651  &         0.099        &    -7061.288    &      603.954     \\\\\n",
       "\\textbf{10391.0}     &   -5555.8122  &     2042.980     &    -2.719  &         0.007        &    -9560.420    &    -1551.204     \\\\\n",
       "\\textbf{10407.0}     &   -2355.5645  &     1884.123     &    -1.250  &         0.211        &    -6048.786    &     1337.656     \\\\\n",
       "\\textbf{10420.0}     &    3660.2121  &     1885.367     &     1.941  &         0.052        &      -35.446    &     7355.870     \\\\\n",
       "\\textbf{10422.0}     &     680.7273  &     1975.735     &     0.345  &         0.730        &    -3192.068    &     4553.523     \\\\\n",
       "\\textbf{10426.0}     &    5118.8058  &     2095.825     &     2.442  &         0.015        &     1010.612    &     9227.000     \\\\\n",
       "\\textbf{10441.0}     &    5608.1798  &     1917.312     &     2.925  &         0.003        &     1849.902    &     9366.457     \\\\\n",
       "\\textbf{1045.0}      &   -7472.1228  &     2056.588     &    -3.633  &         0.000        &    -1.15e+04    &    -3440.841     \\\\\n",
       "\\textbf{10453.0}     &   -2605.6534  &     1957.284     &    -1.331  &         0.183        &    -6442.283    &     1230.976     \\\\\n",
       "\\textbf{10482.0}     &   -2.479e+04  &     2419.308     &   -10.248  &         0.000        &    -2.95e+04    &    -2.01e+04     \\\\\n",
       "\\textbf{10498.0}     &    4204.1963  &     1895.074     &     2.218  &         0.027        &      489.511    &     7918.882     \\\\\n",
       "\\textbf{10499.0}     &   -4935.1607  &     2255.821     &    -2.188  &         0.029        &    -9356.975    &     -513.347     \\\\\n",
       "\\textbf{10511.0}     &    6215.7763  &     1983.684     &     3.133  &         0.002        &     2327.398    &     1.01e+04     \\\\\n",
       "\\textbf{10519.0}     &   -1.323e+04  &     2018.961     &    -6.553  &         0.000        &    -1.72e+04    &    -9272.681     \\\\\n",
       "\\textbf{10530.0}     &   -1388.9617  &     1962.277     &    -0.708  &         0.479        &    -5235.378    &     2457.455     \\\\\n",
       "\\textbf{10537.0}     &   -1032.5703  &     2297.680     &    -0.449  &         0.653        &    -5536.437    &     3471.296     \\\\\n",
       "\\textbf{10540.0}     &    1611.7732  &     1882.464     &     0.856  &         0.392        &    -2078.195    &     5301.741     \\\\\n",
       "\\textbf{10541.0}     &    3411.8443  &     1991.393     &     1.713  &         0.087        &     -491.644    &     7315.333     \\\\\n",
       "\\textbf{10550.0}     &    -123.4474  &     5749.638     &    -0.021  &         0.983        &    -1.14e+04    &     1.11e+04     \\\\\n",
       "\\textbf{10553.0}     &   -1206.3294  &     2063.080     &    -0.585  &         0.559        &    -5250.338    &     2837.679     \\\\\n",
       "\\textbf{10565.0}     &    5584.0522  &     1900.543     &     2.938  &         0.003        &     1858.647    &     9309.458     \\\\\n",
       "\\textbf{10580.0}     &    6007.0246  &     1903.516     &     3.156  &         0.002        &     2275.790    &     9738.259     \\\\\n",
       "\\textbf{10581.0}     &    2989.2197  &     1882.796     &     1.588  &         0.112        &     -701.399    &     6679.839     \\\\\n",
       "\\textbf{10588.0}     &   -7577.6388  &     1947.644     &    -3.891  &         0.000        &    -1.14e+04    &    -3759.906     \\\\\n",
       "\\textbf{10597.0}     &    3555.7350  &     1884.570     &     1.887  &         0.059        &     -138.361    &     7249.831     \\\\\n",
       "\\textbf{10599.0}     &    4227.1709  &     1892.182     &     2.234  &         0.026        &      518.154    &     7936.188     \\\\\n",
       "\\textbf{10618.0}     &    3204.7584  &     1882.393     &     1.702  &         0.089        &     -485.071    &     6894.588     \\\\\n",
       "\\textbf{10656.0}     &    2942.7785  &     1898.192     &     1.550  &         0.121        &     -778.019    &     6663.576     \\\\\n",
       "\\textbf{10658.0}     &    2796.6662  &     1900.522     &     1.472  &         0.141        &     -928.699    &     6522.031     \\\\\n",
       "\\textbf{10726.0}     &    7476.3004  &     2074.980     &     3.603  &         0.000        &     3408.967    &     1.15e+04     \\\\\n",
       "\\textbf{10734.0}     &    2444.8875  &     2583.273     &     0.946  &         0.344        &    -2618.793    &     7508.568     \\\\\n",
       "\\textbf{10735.0}     &    5373.4594  &     1920.030     &     2.799  &         0.005        &     1609.854    &     9137.065     \\\\\n",
       "\\textbf{10764.0}     &    5983.2672  &     1973.759     &     3.031  &         0.002        &     2114.344    &     9852.190     \\\\\n",
       "\\textbf{10777.0}     &    2812.0683  &     1899.421     &     1.480  &         0.139        &     -911.138    &     6535.275     \\\\\n",
       "\\textbf{1078.0}      &     211.7163  &     2175.428     &     0.097  &         0.922        &    -4052.514    &     4475.946     \\\\\n",
       "\\textbf{10793.0}     &    3739.5996  &     1898.088     &     1.970  &         0.049        &       19.005    &     7460.194     \\\\\n",
       "\\textbf{10816.0}     &    2308.6559  &     1884.980     &     1.225  &         0.221        &    -1386.244    &     6003.555     \\\\\n",
       "\\textbf{10839.0}     &    4188.3126  &     1888.746     &     2.218  &         0.027        &      486.030    &     7890.595     \\\\\n",
       "\\textbf{10857.0}     &   -8646.4572  &     2050.829     &    -4.216  &         0.000        &    -1.27e+04    &    -4626.463     \\\\\n",
       "\\textbf{10867.0}     &     559.7743  &     2178.114     &     0.257  &         0.797        &    -3709.721    &     4829.269     \\\\\n",
       "\\textbf{10906.0}     &    3564.9093  &     1883.462     &     1.893  &         0.058        &     -127.016    &     7256.834     \\\\\n",
       "\\textbf{10950.0}     &    3523.4088  &     3082.617     &     1.143  &         0.253        &    -2519.076    &     9565.893     \\\\\n",
       "\\textbf{10983.0}     &    -2.17e+04  &     2308.416     &    -9.400  &         0.000        &    -2.62e+04    &    -1.72e+04     \\\\\n",
       "\\textbf{1099.0}      &    2933.4044  &     1882.939     &     1.558  &         0.119        &     -757.496    &     6624.305     \\\\\n",
       "\\textbf{10991.0}     &    1081.1555  &     2524.920     &     0.428  &         0.669        &    -3868.143    &     6030.454     \\\\\n",
       "\\textbf{11012.0}     &    2228.5844  &     1939.011     &     1.149  &         0.250        &    -1572.227    &     6029.395     \\\\\n",
       "\\textbf{11038.0}     &   -3219.8794  &     1994.714     &    -1.614  &         0.107        &    -7129.878    &      690.119     \\\\\n",
       "\\textbf{1104.0}      &    4365.8342  &     1903.475     &     2.294  &         0.022        &      634.680    &     8096.988     \\\\\n",
       "\\textbf{11060.0}     &    4720.6397  &     1900.811     &     2.483  &         0.013        &      994.708    &     8446.572     \\\\\n",
       "\\textbf{11094.0}     &    3186.8529  &     1881.234     &     1.694  &         0.090        &     -500.705    &     6874.411     \\\\\n",
       "\\textbf{11096.0}     &     520.7405  &     1885.531     &     0.276  &         0.782        &    -3175.240    &     4216.721     \\\\\n",
       "\\textbf{11113.0}     &    4045.0552  &     2056.849     &     1.967  &         0.049        &       13.262    &     8076.849     \\\\\n",
       "\\textbf{1115.0}      &    1776.5387  &     1873.271     &     0.948  &         0.343        &    -1895.409    &     5448.487     \\\\\n",
       "\\textbf{11161.0}     &     205.1234  &     1871.942     &     0.110  &         0.913        &    -3464.220    &     3874.467     \\\\\n",
       "\\textbf{11225.0}     &    5502.7284  &     1916.367     &     2.871  &         0.004        &     1746.304    &     9259.152     \\\\\n",
       "\\textbf{11228.0}     &    4764.9719  &     1887.086     &     2.525  &         0.012        &     1065.943    &     8464.000     \\\\\n",
       "\\textbf{11236.0}     &   -2807.4697  &     3423.675     &    -0.820  &         0.412        &    -9518.488    &     3903.548     \\\\\n",
       "\\textbf{11288.0}     &   -6006.0413  &     2178.952     &    -2.756  &         0.006        &    -1.03e+04    &    -1734.904     \\\\\n",
       "\\textbf{11312.0}     &   -8833.2777  &     2022.181     &    -4.368  &         0.000        &    -1.28e+04    &    -4869.440     \\\\\n",
       "\\textbf{11361.0}     &    2151.7198  &     1916.982     &     1.122  &         0.262        &    -1605.909    &     5909.349     \\\\\n",
       "\\textbf{11399.0}     &   -4909.5155  &     1887.924     &    -2.600  &         0.009        &    -8610.187    &    -1208.844     \\\\\n",
       "\\textbf{114303.0}    &   -2.297e+04  &     5892.521     &    -3.898  &         0.000        &    -3.45e+04    &    -1.14e+04     \\\\\n",
       "\\textbf{11456.0}     &   -3081.4194  &     1983.440     &    -1.554  &         0.120        &    -6969.319    &      806.480     \\\\\n",
       "\\textbf{11465.0}     &    -345.4494  &     1880.824     &    -0.184  &         0.854        &    -4032.202    &     3341.303     \\\\\n",
       "\\textbf{11502.0}     &    4446.9242  &     1922.452     &     2.313  &         0.021        &      678.572    &     8215.276     \\\\\n",
       "\\textbf{11506.0}     &   -2952.4483  &     1975.979     &    -1.494  &         0.135        &    -6825.723    &      920.827     \\\\\n",
       "\\textbf{11537.0}     &    3677.2970  &     1888.017     &     1.948  &         0.051        &      -23.557    &     7378.151     \\\\\n",
       "\\textbf{11566.0}     &    6202.3365  &     1931.978     &     3.210  &         0.001        &     2415.312    &     9989.361     \\\\\n",
       "\\textbf{11573.0}     &    2202.9150  &     1892.068     &     1.164  &         0.244        &    -1505.880    &     5911.710     \\\\\n",
       "\\textbf{11580.0}     &   -2455.8304  &     2451.133     &    -1.002  &         0.316        &    -7260.493    &     2348.832     \\\\\n",
       "\\textbf{11600.0}     &    5045.6218  &     1907.963     &     2.645  &         0.008        &     1305.670    &     8785.573     \\\\\n",
       "\\textbf{11609.0}     &    8014.9787  &     1901.563     &     4.215  &         0.000        &     4287.574    &     1.17e+04     \\\\\n",
       "\\textbf{1161.0}      &   -6558.7605  &     2019.911     &    -3.247  &         0.001        &    -1.05e+04    &    -2599.371     \\\\\n",
       "\\textbf{11636.0}     &   -9542.3217  &     2056.890     &    -4.639  &         0.000        &    -1.36e+04    &    -5510.447     \\\\\n",
       "\\textbf{11670.0}     &    5933.0902  &     1938.826     &     3.060  &         0.002        &     2132.642    &     9733.538     \\\\\n",
       "\\textbf{11678.0}     &   -8859.2408  &     2033.636     &    -4.356  &         0.000        &    -1.28e+04    &    -4872.948     \\\\\n",
       "\\textbf{11682.0}     &    1751.7498  &     2031.089     &     0.862  &         0.388        &    -2229.550    &     5733.049     \\\\\n",
       "\\textbf{11694.0}     &    4499.7539  &     2058.475     &     2.186  &         0.029        &      464.772    &     8534.736     \\\\\n",
       "\\textbf{11720.0}     &   -5780.8803  &     3386.028     &    -1.707  &         0.088        &    -1.24e+04    &      856.344     \\\\\n",
       "\\textbf{11721.0}     &   -1.427e+04  &     2558.449     &    -5.576  &         0.000        &    -1.93e+04    &    -9250.679     \\\\\n",
       "\\textbf{11722.0}     &    1914.7910  &     2191.834     &     0.874  &         0.382        &    -2381.599    &     6211.181     \\\\\n",
       "\\textbf{11793.0}     &    -794.1008  &     5806.390     &    -0.137  &         0.891        &    -1.22e+04    &     1.06e+04     \\\\\n",
       "\\textbf{11797.0}     &    6483.0051  &     2313.539     &     2.802  &         0.005        &     1948.052    &      1.1e+04     \\\\\n",
       "\\textbf{11914.0}     &    4039.2914  &     2731.204     &     1.479  &         0.139        &    -1314.360    &     9392.943     \\\\\n",
       "\\textbf{1209.0}      &    1198.7045  &     1896.415     &     0.632  &         0.527        &    -2518.610    &     4916.019     \\\\\n",
       "\\textbf{12136.0}     &   -1.807e+04  &     2537.008     &    -7.121  &         0.000        &     -2.3e+04    &    -1.31e+04     \\\\\n",
       "\\textbf{12141.0}     &    5.987e+04  &     2260.555     &    26.485  &         0.000        &     5.54e+04    &     6.43e+04     \\\\\n",
       "\\textbf{12181.0}     &    -506.9822  &     3335.896     &    -0.152  &         0.879        &    -7045.938    &     6031.974     \\\\\n",
       "\\textbf{12215.0}     &   -9251.7575  &     2280.522     &    -4.057  &         0.000        &    -1.37e+04    &    -4781.523     \\\\\n",
       "\\textbf{12216.0}     &   -6362.1395  &     2337.764     &    -2.721  &         0.007        &    -1.09e+04    &    -1779.701     \\\\\n",
       "\\textbf{12256.0}     &   -4470.9142  &     2200.236     &    -2.032  &         0.042        &    -8783.772    &     -158.056     \\\\\n",
       "\\textbf{12262.0}     &    4591.1516  &     2227.440     &     2.061  &         0.039        &      224.968    &     8957.336     \\\\\n",
       "\\textbf{12389.0}     &    1427.6404  &     2271.244     &     0.629  &         0.530        &    -3024.407    &     5879.688     \\\\\n",
       "\\textbf{1239.0}      &      88.1293  &     1867.488     &     0.047  &         0.962        &    -3572.484    &     3748.743     \\\\\n",
       "\\textbf{12390.0}     &    1364.7553  &     2578.535     &     0.529  &         0.597        &    -3689.638    &     6419.149     \\\\\n",
       "\\textbf{12397.0}     &    -566.7502  &     4696.797     &    -0.121  &         0.904        &    -9773.318    &     8639.818     \\\\\n",
       "\\textbf{1243.0}      &     110.7821  &     1928.931     &     0.057  &         0.954        &    -3670.270    &     3891.834     \\\\\n",
       "\\textbf{12548.0}     &    3456.8019  &     2467.873     &     1.401  &         0.161        &    -1380.672    &     8294.276     \\\\\n",
       "\\textbf{12570.0}     &    3965.7460  &     2282.077     &     1.738  &         0.082        &     -507.537    &     8439.029     \\\\\n",
       "\\textbf{12581.0}     &    -127.9446  &     2482.050     &    -0.052  &         0.959        &    -4993.209    &     4737.320     \\\\\n",
       "\\textbf{12592.0}     &    1919.1874  &     2186.990     &     0.878  &         0.380        &    -2367.707    &     6206.081     \\\\\n",
       "\\textbf{12604.0}     &     884.6439  &     4704.257     &     0.188  &         0.851        &    -8336.546    &     1.01e+04     \\\\\n",
       "\\textbf{12656.0}     &    6304.3213  &     2248.139     &     2.804  &         0.005        &     1897.564    &     1.07e+04     \\\\\n",
       "\\textbf{12679.0}     &   -1.712e+04  &     2529.614     &    -6.766  &         0.000        &    -2.21e+04    &    -1.22e+04     \\\\\n",
       "\\textbf{1278.0}      &    5220.0849  &     1972.052     &     2.647  &         0.008        &     1354.509    &     9085.661     \\\\\n",
       "\\textbf{12788.0}     &    -1.16e+04  &     2814.360     &    -4.120  &         0.000        &    -1.71e+04    &    -6078.871     \\\\\n",
       "\\textbf{1283.0}      &    4990.9330  &     1942.406     &     2.569  &         0.010        &     1183.468    &     8798.398     \\\\\n",
       "\\textbf{1297.0}      &    3304.1604  &     1881.880     &     1.756  &         0.079        &     -384.664    &     6992.985     \\\\\n",
       "\\textbf{12992.0}     &    5487.1519  &     2296.375     &     2.389  &         0.017        &      985.843    &     9988.461     \\\\\n",
       "\\textbf{13135.0}     &    -141.3897  &     2348.886     &    -0.060  &         0.952        &    -4745.630    &     4462.850     \\\\\n",
       "\\textbf{1327.0}      &   -5147.0947  &     2077.474     &    -2.478  &         0.013        &    -9219.318    &    -1074.871     \\\\\n",
       "\\textbf{13282.0}     &   -3791.2207  &     4697.960     &    -0.807  &         0.420        &     -1.3e+04    &     5417.625     \\\\\n",
       "\\textbf{1334.0}      &   -6146.9690  &     2353.305     &    -2.612  &         0.009        &    -1.08e+04    &    -1534.068     \\\\\n",
       "\\textbf{13351.0}     &   -2598.5396  &     3116.002     &    -0.834  &         0.404        &    -8706.465    &     3509.385     \\\\\n",
       "\\textbf{13365.0}     &   -2.024e+04  &     2805.355     &    -7.216  &         0.000        &    -2.57e+04    &    -1.47e+04     \\\\\n",
       "\\textbf{13369.0}     &    -292.9437  &     2344.123     &    -0.125  &         0.901        &    -4887.847    &     4301.959     \\\\\n",
       "\\textbf{13406.0}     &    4288.2331  &     2274.826     &     1.885  &         0.059        &     -170.836    &     8747.302     \\\\\n",
       "\\textbf{13407.0}     &   -4098.7445  &     2274.930     &    -1.802  &         0.072        &    -8558.018    &      360.529     \\\\\n",
       "\\textbf{13417.0}     &    5903.7335  &     2404.934     &     2.455  &         0.014        &     1189.629    &     1.06e+04     \\\\\n",
       "\\textbf{13525.0}     &   -4312.5560  &     2504.596     &    -1.722  &         0.085        &    -9222.014    &      596.902     \\\\\n",
       "\\textbf{13554.0}     &    6564.1270  &     2414.808     &     2.718  &         0.007        &     1830.669    &     1.13e+04     \\\\\n",
       "\\textbf{1359.0}      &   -1.002e+04  &     2336.561     &    -4.290  &         0.000        &    -1.46e+04    &    -5442.891     \\\\\n",
       "\\textbf{13623.0}     &    1928.0302  &     2357.844     &     0.818  &         0.414        &    -2693.769    &     6549.830     \\\\\n",
       "\\textbf{1372.0}      &   -4788.4202  &     1992.772     &    -2.403  &         0.016        &    -8694.612    &     -882.229     \\\\\n",
       "\\textbf{1380.0}      &   -5058.1898  &     1924.186     &    -2.629  &         0.009        &    -8829.941    &    -1286.438     \\\\\n",
       "\\textbf{13923.0}     &    3305.2508  &     2584.883     &     1.279  &         0.201        &    -1761.586    &     8372.087     \\\\\n",
       "\\textbf{13932.0}     &    5461.6883  &     3357.406     &     1.627  &         0.104        &    -1119.432    &      1.2e+04     \\\\\n",
       "\\textbf{13941.0}     &   -1.084e+04  &     2488.694     &    -4.355  &         0.000        &    -1.57e+04    &    -5960.052     \\\\\n",
       "\\textbf{1397.0}      &    1006.3612  &     2191.304     &     0.459  &         0.646        &    -3288.990    &     5301.712     \\\\\n",
       "\\textbf{14064.0}     &    2110.4264  &     2286.866     &     0.923  &         0.356        &    -2372.243    &     6593.096     \\\\\n",
       "\\textbf{14084.0}     &    1052.9005  &     2326.922     &     0.452  &         0.651        &    -3508.286    &     5614.087     \\\\\n",
       "\\textbf{14324.0}     &   -6713.4106  &     2461.951     &    -2.727  &         0.006        &    -1.15e+04    &    -1887.543     \\\\\n",
       "\\textbf{14462.0}     &    -815.5019  &     2463.229     &    -0.331  &         0.741        &    -5643.875    &     4012.871     \\\\\n",
       "\\textbf{1447.0}      &    2804.6111  &     4092.214     &     0.685  &         0.493        &    -5216.864    &     1.08e+04     \\\\\n",
       "\\textbf{14593.0}     &    5029.9668  &     2495.067     &     2.016  &         0.044        &      139.187    &     9920.746     \\\\\n",
       "\\textbf{14622.0}     &    2075.1080  &     8133.047     &     0.255  &         0.799        &    -1.39e+04    &      1.8e+04     \\\\\n",
       "\\textbf{1465.0}      &    3135.8476  &     2599.375     &     1.206  &         0.228        &    -1959.395    &     8231.090     \\\\\n",
       "\\textbf{1468.0}      &    5597.0788  &     2478.743     &     2.258  &         0.024        &      738.296    &     1.05e+04     \\\\\n",
       "\\textbf{14897.0}     &    3312.9826  &     4701.123     &     0.705  &         0.481        &    -5902.065    &     1.25e+04     \\\\\n",
       "\\textbf{14954.0}     &    4088.8308  &     2479.191     &     1.649  &         0.099        &     -770.831    &     8948.492     \\\\\n",
       "\\textbf{1496.0}      &    5305.6897  &     1905.409     &     2.785  &         0.005        &     1570.745    &     9040.634     \\\\\n",
       "\\textbf{15267.0}     &    2872.0224  &     2461.569     &     1.167  &         0.243        &    -1953.095    &     7697.140     \\\\\n",
       "\\textbf{15354.0}     &   -2197.3109  &     2585.502     &    -0.850  &         0.395        &    -7265.360    &     2870.738     \\\\\n",
       "\\textbf{1542.0}      &    2255.5097  &     1879.439     &     1.200  &         0.230        &    -1428.530    &     5939.549     \\\\\n",
       "\\textbf{15459.0}     &   -2753.9887  &     2704.343     &    -1.018  &         0.309        &    -8054.988    &     2547.011     \\\\\n",
       "\\textbf{1554.0}      &    5170.2956  &     1903.709     &     2.716  &         0.007        &     1438.683    &     8901.908     \\\\\n",
       "\\textbf{15708.0}     &   -2.216e+04  &     3042.021     &    -7.286  &         0.000        &    -2.81e+04    &    -1.62e+04     \\\\\n",
       "\\textbf{15711.0}     &    1047.2724  &     2467.055     &     0.425  &         0.671        &    -3788.600    &     5883.145     \\\\\n",
       "\\textbf{15761.0}     &    4394.1636  &     2892.305     &     1.519  &         0.129        &    -1275.275    &     1.01e+04     \\\\\n",
       "\\textbf{1581.0}      &   -1.197e+04  &     3230.716     &    -3.705  &         0.000        &    -1.83e+04    &    -5635.823     \\\\\n",
       "\\textbf{1593.0}      &    1150.9509  &     1903.069     &     0.605  &         0.545        &    -2579.407    &     4881.309     \\\\\n",
       "\\textbf{1602.0}      &    8009.7727  &     2062.475     &     3.884  &         0.000        &     3966.951    &     1.21e+04     \\\\\n",
       "\\textbf{1613.0}      &    4464.8770  &     1899.969     &     2.350  &         0.019        &      740.596    &     8189.158     \\\\\n",
       "\\textbf{16188.0}     &    -473.4188  &     2720.089     &    -0.174  &         0.862        &    -5805.283    &     4858.446     \\\\\n",
       "\\textbf{1632.0}      &   -5408.3889  &     1921.430     &    -2.815  &         0.005        &    -9174.738    &    -1642.040     \\\\\n",
       "\\textbf{1633.0}      &    1999.1696  &     1874.500     &     1.067  &         0.286        &    -1675.187    &     5673.527     \\\\\n",
       "\\textbf{1635.0}      &   -1.241e+04  &     2289.758     &    -5.419  &         0.000        &    -1.69e+04    &    -7918.747     \\\\\n",
       "\\textbf{16401.0}     &   -1.141e+04  &     2782.711     &    -4.099  &         0.000        &    -1.69e+04    &    -5952.839     \\\\\n",
       "\\textbf{16437.0}     &   -6651.5417  &     2934.612     &    -2.267  &         0.023        &    -1.24e+04    &     -899.174     \\\\\n",
       "\\textbf{1651.0}      &   -1267.5784  &     1873.117     &    -0.677  &         0.499        &    -4939.225    &     2404.069     \\\\\n",
       "\\textbf{1655.0}      &    4617.1754  &     1892.086     &     2.440  &         0.015        &      908.347    &     8326.004     \\\\\n",
       "\\textbf{1663.0}      &    5917.8126  &     1934.732     &     3.059  &         0.002        &     2125.389    &     9710.236     \\\\\n",
       "\\textbf{16710.0}     &   -2142.6639  &     2713.492     &    -0.790  &         0.430        &    -7461.597    &     3176.269     \\\\\n",
       "\\textbf{16729.0}     &   -2843.3099  &     2581.325     &    -1.101  &         0.271        &    -7903.171    &     2216.551     \\\\\n",
       "\\textbf{1690.0}      &   -1.609e+04  &     2040.450     &    -7.885  &         0.000        &    -2.01e+04    &    -1.21e+04     \\\\\n",
       "\\textbf{1703.0}      &    1038.3302  &     1938.531     &     0.536  &         0.592        &    -2761.540    &     4838.201     \\\\\n",
       "\\textbf{17202.0}     &    3126.0374  &     2596.217     &     1.204  &         0.229        &    -1963.015    &     8215.089     \\\\\n",
       "\\textbf{1722.0}      &    1818.2271  &     1899.530     &     0.957  &         0.338        &    -1905.194    &     5541.649     \\\\\n",
       "\\textbf{1728.0}      &    4811.4862  &     1900.383     &     2.532  &         0.011        &     1086.393    &     8536.579     \\\\\n",
       "\\textbf{1743.0}      &    4688.6298  &     3113.418     &     1.506  &         0.132        &    -1414.230    &     1.08e+04     \\\\\n",
       "\\textbf{1754.0}      &    3868.9635  &     1989.197     &     1.945  &         0.052        &      -30.220    &     7768.147     \\\\\n",
       "\\textbf{1762.0}      &   -3198.3213  &     2107.779     &    -1.517  &         0.129        &    -7329.947    &      933.305     \\\\\n",
       "\\textbf{1773.0}      &    4715.3642  &     1941.975     &     2.428  &         0.015        &      908.743    &     8521.986     \\\\\n",
       "\\textbf{1786.0}      &   -1.351e+04  &     2088.466     &    -6.467  &         0.000        &    -1.76e+04    &    -9411.312     \\\\\n",
       "\\textbf{18100.0}     &    1696.6514  &     2583.745     &     0.657  &         0.511        &    -3367.953    &     6761.256     \\\\\n",
       "\\textbf{1820.0}      &     928.9560  &     1897.774     &     0.489  &         0.624        &    -2791.023    &     4648.935     \\\\\n",
       "\\textbf{1848.0}      &   -9134.3632  &     2467.813     &    -3.701  &         0.000        &     -1.4e+04    &    -4297.007     \\\\\n",
       "\\textbf{18654.0}     &    4455.5629  &     3661.166     &     1.217  &         0.224        &    -2720.982    &     1.16e+04     \\\\\n",
       "\\textbf{1875.0}      &    -492.4923  &     4068.535     &    -0.121  &         0.904        &    -8467.552    &     7482.568     \\\\\n",
       "\\textbf{1884.0}      &    4544.4415  &     2017.943     &     2.252  &         0.024        &      588.911    &     8499.972     \\\\\n",
       "\\textbf{1913.0}      &   -2060.4339  &     1984.365     &    -1.038  &         0.299        &    -5950.147    &     1829.279     \\\\\n",
       "\\textbf{1919.0}      &    3267.3750  &     1996.792     &     1.636  &         0.102        &     -646.696    &     7181.446     \\\\\n",
       "\\textbf{1920.0}      &   -1552.3608  &     1906.690     &    -0.814  &         0.416        &    -5289.817    &     2185.095     \\\\\n",
       "\\textbf{1968.0}      &    1437.4441  &     1910.381     &     0.752  &         0.452        &    -2307.247    &     5182.135     \\\\\n",
       "\\textbf{1976.0}      &    5411.2870  &     1885.935     &     2.869  &         0.004        &     1714.515    &     9108.059     \\\\\n",
       "\\textbf{1981.0}      &    4053.1417  &     1891.695     &     2.143  &         0.032        &      345.080    &     7761.203     \\\\\n",
       "\\textbf{1988.0}      &   -9269.3673  &     3298.509     &    -2.810  &         0.005        &    -1.57e+04    &    -2803.696     \\\\\n",
       "\\textbf{1992.0}      &    1536.3298  &     1931.166     &     0.796  &         0.426        &    -2249.102    &     5321.762     \\\\\n",
       "\\textbf{2008.0}      &    2131.2548  &     1883.802     &     1.131  &         0.258        &    -1561.337    &     5823.846     \\\\\n",
       "\\textbf{2033.0}      &    4099.7529  &     2474.961     &     1.656  &         0.098        &     -751.616    &     8951.122     \\\\\n",
       "\\textbf{2044.0}      &    1597.1970  &     1868.781     &     0.855  &         0.393        &    -2065.949    &     5260.343     \\\\\n",
       "\\textbf{2049.0}      &    2075.1150  &     1882.009     &     1.103  &         0.270        &    -1613.962    &     5764.192     \\\\\n",
       "\\textbf{2061.0}      &    6198.3376  &     1930.235     &     3.211  &         0.001        &     2414.730    &     9981.945     \\\\\n",
       "\\textbf{20779.0}     &    2.915e+04  &     2731.987     &    10.671  &         0.000        &     2.38e+04    &     3.45e+04     \\\\\n",
       "\\textbf{2085.0}      &   -1.242e+04  &     2176.183     &    -5.706  &         0.000        &    -1.67e+04    &    -8151.426     \\\\\n",
       "\\textbf{2086.0}      &    1721.6545  &     1887.294     &     0.912  &         0.362        &    -1977.782    &     5421.091     \\\\\n",
       "\\textbf{2111.0}      &    1959.3626  &     1870.848     &     1.047  &         0.295        &    -1707.836    &     5626.561     \\\\\n",
       "\\textbf{21204.0}     &    -618.7069  &     2726.033     &    -0.227  &         0.820        &    -5962.222    &     4724.808     \\\\\n",
       "\\textbf{21238.0}     &    4677.4226  &     2626.157     &     1.781  &         0.075        &     -470.317    &     9825.163     \\\\\n",
       "\\textbf{2124.0}      &    1676.4498  &     2015.720     &     0.832  &         0.406        &    -2274.724    &     5627.623     \\\\\n",
       "\\textbf{2146.0}      &    1.113e+04  &     3081.311     &     3.611  &         0.000        &     5085.407    &     1.72e+04     \\\\\n",
       "\\textbf{21496.0}     &   -2.285e+04  &     3150.156     &    -7.254  &         0.000        &     -2.9e+04    &    -1.67e+04     \\\\\n",
       "\\textbf{2154.0}      &    3159.7736  &     1879.231     &     1.681  &         0.093        &     -523.858    &     6843.405     \\\\\n",
       "\\textbf{2176.0}      &    3.556e+04  &     2586.631     &    13.749  &         0.000        &     3.05e+04    &     4.06e+04     \\\\\n",
       "\\textbf{2188.0}      &    5969.6054  &     1973.136     &     3.025  &         0.002        &     2101.903    &     9837.308     \\\\\n",
       "\\textbf{2189.0}      &   -4042.4772  &     1940.943     &    -2.083  &         0.037        &    -7847.075    &     -237.879     \\\\\n",
       "\\textbf{2220.0}      &    2682.7886  &     1888.591     &     1.421  &         0.155        &    -1019.189    &     6384.766     \\\\\n",
       "\\textbf{22205.0}     &    6567.2487  &     2635.889     &     2.491  &         0.013        &     1400.431    &     1.17e+04     \\\\\n",
       "\\textbf{2226.0}      &     443.9865  &     5746.731     &     0.077  &         0.938        &    -1.08e+04    &     1.17e+04     \\\\\n",
       "\\textbf{2230.0}      &    3892.7923  &     2219.233     &     1.754  &         0.079        &     -457.304    &     8242.889     \\\\\n",
       "\\textbf{22325.0}     &   -7958.3788  &     2774.205     &    -2.869  &         0.004        &    -1.34e+04    &    -2520.437     \\\\\n",
       "\\textbf{2255.0}      &    2326.0620  &     1885.338     &     1.234  &         0.217        &    -1369.541    &     6021.665     \\\\\n",
       "\\textbf{22619.0}     &    4176.8950  &     2593.046     &     1.611  &         0.107        &     -905.942    &     9259.732     \\\\\n",
       "\\textbf{2267.0}      &   -7591.5514  &     1959.471     &    -3.874  &         0.000        &    -1.14e+04    &    -3750.635     \\\\\n",
       "\\textbf{22815.0}     &   -1903.9381  &     2578.726     &    -0.738  &         0.460        &    -6958.706    &     3150.829     \\\\\n",
       "\\textbf{2285.0}      &   -1.613e+04  &     2116.014     &    -7.622  &         0.000        &    -2.03e+04    &     -1.2e+04     \\\\\n",
       "\\textbf{2290.0}      &   -1662.6714  &     1886.111     &    -0.882  &         0.378        &    -5359.787    &     2034.445     \\\\\n",
       "\\textbf{2295.0}      &    5170.2469  &     3340.562     &     1.548  &         0.122        &    -1377.855    &     1.17e+04     \\\\\n",
       "\\textbf{2316.0}      &   -3191.1118  &     2131.966     &    -1.497  &         0.134        &    -7370.148    &      987.924     \\\\\n",
       "\\textbf{23220.0}     &    1834.4738  &     2725.102     &     0.673  &         0.501        &    -3507.216    &     7176.164     \\\\\n",
       "\\textbf{23224.0}     &   -1.052e+04  &     3152.490     &    -3.338  &         0.001        &    -1.67e+04    &    -4344.288     \\\\\n",
       "\\textbf{2343.0}      &   -8659.6696  &     4202.820     &    -2.060  &         0.039        &    -1.69e+04    &     -421.387     \\\\\n",
       "\\textbf{2352.0}      &     484.3946  &     2190.931     &     0.221  &         0.825        &    -3810.224    &     4779.013     \\\\\n",
       "\\textbf{23700.0}     &   -1.653e+04  &     3592.553     &    -4.601  &         0.000        &    -2.36e+04    &    -9486.301     \\\\\n",
       "\\textbf{2390.0}      &    5358.6897  &     1907.486     &     2.809  &         0.005        &     1619.674    &     9097.705     \\\\\n",
       "\\textbf{2393.0}      &   -1986.4490  &     2016.237     &    -0.985  &         0.325        &    -5938.636    &     1965.738     \\\\\n",
       "\\textbf{2403.0}      &     1.16e+04  &     2184.113     &     5.311  &         0.000        &     7319.280    &     1.59e+04     \\\\\n",
       "\\textbf{2435.0}      &    7423.7475  &     1930.621     &     3.845  &         0.000        &     3639.384    &     1.12e+04     \\\\\n",
       "\\textbf{2444.0}      &    -332.1383  &     1947.184     &    -0.171  &         0.865        &    -4148.970    &     3484.694     \\\\\n",
       "\\textbf{2448.0}      &    1652.2575  &     1871.029     &     0.883  &         0.377        &    -2015.297    &     5319.812     \\\\\n",
       "\\textbf{2469.0}      &    4578.3084  &     3665.158     &     1.249  &         0.212        &    -2606.061    &     1.18e+04     \\\\\n",
       "\\textbf{24720.0}     &    2861.8215  &     2898.198     &     0.987  &         0.323        &    -2819.169    &     8542.812     \\\\\n",
       "\\textbf{24800.0}     &   -1.315e+04  &     3182.313     &    -4.132  &         0.000        &    -1.94e+04    &    -6911.698     \\\\\n",
       "\\textbf{2482.0}      &    5574.0696  &     1912.008     &     2.915  &         0.004        &     1826.189    &     9321.950     \\\\\n",
       "\\textbf{24969.0}     &    5318.9119  &     3105.737     &     1.713  &         0.087        &     -768.891    &     1.14e+04     \\\\\n",
       "\\textbf{2498.0}      &   -5529.7674  &     2025.889     &    -2.730  &         0.006        &    -9500.874    &    -1558.661     \\\\\n",
       "\\textbf{2504.0}      &   -1.051e+04  &     2309.623     &    -4.549  &         0.000        &     -1.5e+04    &    -5978.127     \\\\\n",
       "\\textbf{2508.0}      &    4352.4929  &     2124.037     &     2.049  &         0.040        &      188.999    &     8515.987     \\\\\n",
       "\\textbf{25124.0}     &    3954.1172  &     2893.904     &     1.366  &         0.172        &    -1718.455    &     9626.689     \\\\\n",
       "\\textbf{2518.0}      &    4373.0373  &     1893.929     &     2.309  &         0.021        &      660.595    &     8085.479     \\\\\n",
       "\\textbf{25224.0}     &    3503.0947  &     8133.263     &     0.431  &         0.667        &    -1.24e+04    &     1.94e+04     \\\\\n",
       "\\textbf{25279.0}     &    3759.6474  &     2882.452     &     1.304  &         0.192        &    -1890.476    &     9409.771     \\\\\n",
       "\\textbf{2537.0}      &   -1.093e+04  &     2185.521     &    -5.001  &         0.000        &    -1.52e+04    &    -6644.813     \\\\\n",
       "\\textbf{2538.0}      &    5753.7139  &     2919.607     &     1.971  &         0.049        &       30.758    &     1.15e+04     \\\\\n",
       "\\textbf{25389.0}     &    5048.6253  &     4702.360     &     1.074  &         0.283        &    -4168.846    &     1.43e+04     \\\\\n",
       "\\textbf{2547.0}      &   -8546.1978  &     2151.373     &    -3.972  &         0.000        &    -1.28e+04    &    -4329.120     \\\\\n",
       "\\textbf{2553.0}      &    3908.1790  &     1894.826     &     2.063  &         0.039        &      193.979    &     7622.379     \\\\\n",
       "\\textbf{2574.0}      &   -2007.5654  &     2519.353     &    -0.797  &         0.426        &    -6945.951    &     2930.820     \\\\\n",
       "\\textbf{25747.0}     &    3481.3869  &     3088.153     &     1.127  &         0.260        &    -2571.948    &     9534.722     \\\\\n",
       "\\textbf{2577.0}      &    1230.9401  &     1891.180     &     0.651  &         0.515        &    -2476.113    &     4937.993     \\\\\n",
       "\\textbf{2593.0}      &    1702.4949  &     1892.650     &     0.900  &         0.368        &    -2007.440    &     5412.430     \\\\\n",
       "\\textbf{2596.0}      &    1272.6585  &     1963.208     &     0.648  &         0.517        &    -2575.582    &     5120.899     \\\\\n",
       "\\textbf{2663.0}      &    7372.6205  &     1892.581     &     3.896  &         0.000        &     3662.821    &     1.11e+04     \\\\\n",
       "\\textbf{2771.0}      &    1438.7965  &     1948.563     &     0.738  &         0.460        &    -2380.739    &     5258.331     \\\\\n",
       "\\textbf{2787.0}      &    3769.5931  &     1886.511     &     1.998  &         0.046        &       71.692    &     7467.495     \\\\\n",
       "\\textbf{2797.0}      &   -1.143e+04  &     2040.584     &    -5.603  &         0.000        &    -1.54e+04    &    -7433.671     \\\\\n",
       "\\textbf{2802.0}      &    5089.1314  &     1905.976     &     2.670  &         0.008        &     1353.075    &     8825.188     \\\\\n",
       "\\textbf{2817.0}      &   -7331.9480  &     2190.234     &    -3.348  &         0.001        &    -1.16e+04    &    -3038.696     \\\\\n",
       "\\textbf{28678.0}     &   -1.914e+04  &     3341.098     &    -5.729  &         0.000        &    -2.57e+04    &    -1.26e+04     \\\\\n",
       "\\textbf{28701.0}     &    2209.6861  &     1882.569     &     1.174  &         0.241        &    -1480.488    &     5899.860     \\\\\n",
       "\\textbf{28742.0}     &   -1.405e+04  &     3303.995     &    -4.252  &         0.000        &    -2.05e+04    &    -7571.157     \\\\\n",
       "\\textbf{2888.0}      &    2393.1235  &     2130.471     &     1.123  &         0.261        &    -1782.982    &     6569.229     \\\\\n",
       "\\textbf{2897.0}      &    4097.7828  &     2721.613     &     1.506  &         0.132        &    -1237.069    &     9432.635     \\\\\n",
       "\\textbf{2917.0}      &   -1681.6497  &     1926.097     &    -0.873  &         0.383        &    -5457.146    &     2093.847     \\\\\n",
       "\\textbf{29392.0}     &   -1.319e+04  &     3481.781     &    -3.789  &         0.000        &       -2e+04    &    -6366.693     \\\\\n",
       "\\textbf{2950.0}      &   -2.204e+04  &     3579.193     &    -6.159  &         0.000        &    -2.91e+04    &     -1.5e+04     \\\\\n",
       "\\textbf{2951.0}      &    5925.8278  &     2397.776     &     2.471  &         0.013        &     1225.755    &     1.06e+04     \\\\\n",
       "\\textbf{2953.0}      &    3143.7179  &     1879.256     &     1.673  &         0.094        &     -539.962    &     6827.397     \\\\\n",
       "\\textbf{2960.0}      &     560.6943  &     2901.686     &     0.193  &         0.847        &    -5127.132    &     6248.521     \\\\\n",
       "\\textbf{2975.0}      &   -3739.0166  &     1889.827     &    -1.978  &         0.048        &    -7443.418    &      -34.615     \\\\\n",
       "\\textbf{2982.0}      &    3217.3556  &     1887.883     &     1.704  &         0.088        &     -483.236    &     6917.947     \\\\\n",
       "\\textbf{2991.0}      &   -1.229e+04  &     2679.152     &    -4.589  &         0.000        &    -1.75e+04    &    -7042.115     \\\\\n",
       "\\textbf{3011.0}      &   -7388.6022  &     2246.838     &    -3.288  &         0.001        &    -1.18e+04    &    -2984.395     \\\\\n",
       "\\textbf{3015.0}      &    5754.9913  &     1915.818     &     3.004  &         0.003        &     1999.643    &     9510.339     \\\\\n",
       "\\textbf{3026.0}      &    2605.6552  &     1927.441     &     1.352  &         0.176        &    -1172.475    &     6383.786     \\\\\n",
       "\\textbf{3031.0}      &   -1.921e+04  &     3282.864     &    -5.850  &         0.000        &    -2.56e+04    &    -1.28e+04     \\\\\n",
       "\\textbf{3062.0}      &    4147.8770  &     2008.866     &     2.065  &         0.039        &      210.137    &     8085.617     \\\\\n",
       "\\textbf{3093.0}      &   -3190.9219  &     2246.715     &    -1.420  &         0.156        &    -7594.888    &     1213.044     \\\\\n",
       "\\textbf{3107.0}      &    2356.5688  &     3657.437     &     0.644  &         0.519        &    -4812.666    &     9525.804     \\\\\n",
       "\\textbf{3121.0}      &    5429.2066  &     1883.668     &     2.882  &         0.004        &     1736.879    &     9121.534     \\\\\n",
       "\\textbf{3126.0}      &    3032.2169  &     1894.223     &     1.601  &         0.109        &     -680.801    &     6745.235     \\\\\n",
       "\\textbf{3144.0}      &    5.164e+04  &     1917.758     &    26.928  &         0.000        &     4.79e+04    &     5.54e+04     \\\\\n",
       "\\textbf{3156.0}      &    3460.1219  &     2366.002     &     1.462  &         0.144        &    -1177.668    &     8097.912     \\\\\n",
       "\\textbf{3157.0}      &    3116.6330  &     1879.878     &     1.658  &         0.097        &     -568.266    &     6801.532     \\\\\n",
       "\\textbf{3170.0}      &    4337.1510  &     1876.725     &     2.311  &         0.021        &      658.432    &     8015.870     \\\\\n",
       "\\textbf{3178.0}      &   -2281.1656  &     2127.841     &    -1.072  &         0.284        &    -6452.116    &     1889.785     \\\\\n",
       "\\textbf{3206.0}      &    -835.9173  &     2130.537     &    -0.392  &         0.695        &    -5012.154    &     3340.319     \\\\\n",
       "\\textbf{3229.0}      &    1511.2721  &     2026.724     &     0.746  &         0.456        &    -2461.471    &     5484.015     \\\\\n",
       "\\textbf{3235.0}      &    2668.5172  &     2060.752     &     1.295  &         0.195        &    -1370.928    &     6707.962     \\\\\n",
       "\\textbf{3246.0}      &    3852.1762  &     1902.254     &     2.025  &         0.043        &      123.416    &     7580.937     \\\\\n",
       "\\textbf{3248.0}      &    3155.9190  &     1888.647     &     1.671  &         0.095        &     -546.168    &     6858.006     \\\\\n",
       "\\textbf{3282.0}      &   -2.072e+04  &     2258.740     &    -9.173  &         0.000        &    -2.51e+04    &    -1.63e+04     \\\\\n",
       "\\textbf{3362.0}      &   -5818.3772  &     2210.720     &    -2.632  &         0.009        &    -1.02e+04    &    -1484.968     \\\\\n",
       "\\textbf{3372.0}      &    3265.0074  &     2459.861     &     1.327  &         0.184        &    -1556.764    &     8086.778     \\\\\n",
       "\\textbf{3422.0}      &    1994.0750  &     1882.429     &     1.059  &         0.289        &    -1695.825    &     5683.975     \\\\\n",
       "\\textbf{3497.0}      &   -4224.6553  &     2026.781     &    -2.084  &         0.037        &    -8197.511    &     -251.800     \\\\\n",
       "\\textbf{3502.0}      &   -2333.9904  &     1879.165     &    -1.242  &         0.214        &    -6017.493    &     1349.512     \\\\\n",
       "\\textbf{3504.0}      &    1014.3769  &     2733.802     &     0.371  &         0.711        &    -4344.366    &     6373.120     \\\\\n",
       "\\textbf{3505.0}      &     188.2776  &     1929.690     &     0.098  &         0.922        &    -3594.262    &     3970.818     \\\\\n",
       "\\textbf{3532.0}      &    4457.5709  &     1874.420     &     2.378  &         0.017        &      783.371    &     8131.771     \\\\\n",
       "\\textbf{3574.0}      &    4428.8413  &     4073.143     &     1.087  &         0.277        &    -3555.252    &     1.24e+04     \\\\\n",
       "\\textbf{3580.0}      &    -732.3446  &     1868.067     &    -0.392  &         0.695        &    -4394.091    &     2929.402     \\\\\n",
       "\\textbf{3612.0}      &    6232.2652  &     1936.461     &     3.218  &         0.001        &     2436.453    &        1e+04     \\\\\n",
       "\\textbf{3619.0}      &    2857.3739  &     1890.238     &     1.512  &         0.131        &     -847.832    &     6562.580     \\\\\n",
       "\\textbf{3622.0}      &    5362.2778  &     1956.066     &     2.741  &         0.006        &     1528.037    &     9196.518     \\\\\n",
       "\\textbf{3639.0}      &   -8258.7073  &     1962.105     &    -4.209  &         0.000        &    -1.21e+04    &    -4412.629     \\\\\n",
       "\\textbf{3650.0}      &   -6335.2032  &     2022.153     &    -3.133  &         0.002        &    -1.03e+04    &    -2371.419     \\\\\n",
       "\\textbf{3662.0}      &    -489.0604  &     2008.441     &    -0.244  &         0.808        &    -4425.967    &     3447.846     \\\\\n",
       "\\textbf{3734.0}      &   -1.215e+04  &     2003.453     &    -6.065  &         0.000        &    -1.61e+04    &    -8223.688     \\\\\n",
       "\\textbf{3735.0}      &   -1489.2333  &     2513.791     &    -0.592  &         0.554        &    -6416.717    &     3438.250     \\\\\n",
       "\\textbf{3761.0}      &   -2595.1828  &     1923.358     &    -1.349  &         0.177        &    -6365.310    &     1174.944     \\\\\n",
       "\\textbf{3779.0}      &   -1.142e+04  &     2373.660     &    -4.811  &         0.000        &    -1.61e+04    &    -6767.446     \\\\\n",
       "\\textbf{3781.0}      &   -4822.9571  &     2489.769     &    -1.937  &         0.053        &    -9703.352    &       57.438     \\\\\n",
       "\\textbf{3782.0}      &    -1.12e+04  &     2076.298     &    -5.395  &         0.000        &    -1.53e+04    &    -7131.984     \\\\\n",
       "\\textbf{3786.0}      &    2464.5681  &     1884.029     &     1.308  &         0.191        &    -1228.468    &     6157.604     \\\\\n",
       "\\textbf{3796.0}      &   -1.186e+04  &     2457.404     &    -4.826  &         0.000        &    -1.67e+04    &    -7042.753     \\\\\n",
       "\\textbf{3821.0}      &    4451.3701  &     1907.381     &     2.334  &         0.020        &      712.561    &     8190.180     \\\\\n",
       "\\textbf{3835.0}      &   -1036.0352  &     1895.076     &    -0.547  &         0.585        &    -4750.725    &     2678.655     \\\\\n",
       "\\textbf{3839.0}      &   -2798.6990  &     2826.112     &    -0.990  &         0.322        &    -8338.388    &     2740.990     \\\\\n",
       "\\textbf{3840.0}      &   -4538.1360  &     2000.831     &    -2.268  &         0.023        &    -8460.125    &     -616.147     \\\\\n",
       "\\textbf{3895.0}      &    4250.9880  &     1891.959     &     2.247  &         0.025        &      542.408    &     7959.568     \\\\\n",
       "\\textbf{3908.0}      &    -172.6957  &     3113.710     &    -0.055  &         0.956        &    -6276.127    &     5930.736     \\\\\n",
       "\\textbf{3911.0}      &   -3700.5847  &     1976.063     &    -1.873  &         0.061        &    -7574.024    &      172.855     \\\\\n",
       "\\textbf{3917.0}      &    4642.4080  &     2004.110     &     2.316  &         0.021        &      713.992    &     8570.824     \\\\\n",
       "\\textbf{3946.0}      &    5759.4857  &     1915.542     &     3.007  &         0.003        &     2004.679    &     9514.293     \\\\\n",
       "\\textbf{3971.0}      &    3284.6978  &     1987.071     &     1.653  &         0.098        &     -610.318    &     7179.714     \\\\\n",
       "\\textbf{3980.0}      &     1.19e+04  &     1930.180     &     6.167  &         0.000        &     8119.077    &     1.57e+04     \\\\\n",
       "\\textbf{4034.0}      &   -1117.7330  &     1921.797     &    -0.582  &         0.561        &    -4884.800    &     2649.334     \\\\\n",
       "\\textbf{4036.0}      &    5218.1397  &     1898.819     &     2.748  &         0.006        &     1496.113    &     8940.167     \\\\\n",
       "\\textbf{4040.0}      &   -2524.2024  &     1940.344     &    -1.301  &         0.193        &    -6327.626    &     1279.221     \\\\\n",
       "\\textbf{4058.0}      &    3295.9047  &     1871.494     &     1.761  &         0.078        &     -372.560    &     6964.369     \\\\\n",
       "\\textbf{4060.0}      &   -9060.2877  &     2100.872     &    -4.313  &         0.000        &    -1.32e+04    &    -4942.201     \\\\\n",
       "\\textbf{4062.0}      &    8033.8976  &     1929.372     &     4.164  &         0.000        &     4251.980    &     1.18e+04     \\\\\n",
       "\\textbf{4077.0}      &    1920.6403  &     3323.066     &     0.578  &         0.563        &    -4593.168    &     8434.448     \\\\\n",
       "\\textbf{4087.0}      &   -1.593e+04  &     2564.847     &    -6.211  &         0.000        &     -2.1e+04    &    -1.09e+04     \\\\\n",
       "\\textbf{4091.0}      &    1843.7623  &     2458.852     &     0.750  &         0.453        &    -2976.030    &     6663.555     \\\\\n",
       "\\textbf{4127.0}      &   -3593.3345  &     1887.749     &    -1.904  &         0.057        &    -7293.662    &      106.993     \\\\\n",
       "\\textbf{4138.0}      &    5016.9613  &     2591.186     &     1.936  &         0.053        &      -62.230    &     1.01e+04     \\\\\n",
       "\\textbf{4162.0}      &    2955.8533  &     2457.595     &     1.203  &         0.229        &    -1861.475    &     7773.181     \\\\\n",
       "\\textbf{4186.0}      &    5447.8893  &     1907.622     &     2.856  &         0.004        &     1708.608    &     9187.171     \\\\\n",
       "\\textbf{4194.0}      &    5426.4492  &     2265.829     &     2.395  &         0.017        &      985.018    &     9867.881     \\\\\n",
       "\\textbf{4199.0}      &   -1.103e+04  &     2145.369     &    -5.140  &         0.000        &    -1.52e+04    &    -6821.182     \\\\\n",
       "\\textbf{4213.0}      &    3301.5102  &     1884.484     &     1.752  &         0.080        &     -392.418    &     6995.439     \\\\\n",
       "\\textbf{4222.0}      &   -9874.4637  &     2047.650     &    -4.822  &         0.000        &    -1.39e+04    &    -5860.701     \\\\\n",
       "\\textbf{4223.0}      &    3163.6636  &     1883.366     &     1.680  &         0.093        &     -528.074    &     6855.401     \\\\\n",
       "\\textbf{4251.0}      &    4919.3231  &     1900.807     &     2.588  &         0.010        &     1193.399    &     8645.247     \\\\\n",
       "\\textbf{4265.0}      &    2279.9522  &     2110.236     &     1.080  &         0.280        &    -1856.491    &     6416.395     \\\\\n",
       "\\textbf{4274.0}      &    1286.5416  &     2011.000     &     0.640  &         0.522        &    -2655.381    &     5228.464     \\\\\n",
       "\\textbf{4321.0}      &    1858.7087  &     2025.498     &     0.918  &         0.359        &    -2111.633    &     5829.050     \\\\\n",
       "\\textbf{4335.0}      &    1245.7250  &     3327.189     &     0.374  &         0.708        &    -5276.164    &     7767.614     \\\\\n",
       "\\textbf{4340.0}      &    -473.9774  &     1923.678     &    -0.246  &         0.805        &    -4244.732    &     3296.777     \\\\\n",
       "\\textbf{4371.0}      &     799.5226  &     1953.912     &     0.409  &         0.682        &    -3030.496    &     4629.541     \\\\\n",
       "\\textbf{4415.0}      &    3959.4555  &     1998.635     &     1.981  &         0.048        &       41.772    &     7877.139     \\\\\n",
       "\\textbf{4450.0}      &    2171.8836  &     1890.235     &     1.149  &         0.251        &    -1533.317    &     5877.085     \\\\\n",
       "\\textbf{4476.0}      &   -8255.6752  &     2183.381     &    -3.781  &         0.000        &    -1.25e+04    &    -3975.855     \\\\\n",
       "\\textbf{4510.0}      &   -3304.0404  &     1990.996     &    -1.659  &         0.097        &    -7206.751    &      598.670     \\\\\n",
       "\\textbf{4520.0}      &    1904.8450  &     1924.315     &     0.990  &         0.322        &    -1867.158    &     5676.848     \\\\\n",
       "\\textbf{4551.0}      &    2165.8909  &     8126.759     &     0.267  &         0.790        &    -1.38e+04    &     1.81e+04     \\\\\n",
       "\\textbf{4568.0}      &    3235.1016  &     1999.182     &     1.618  &         0.106        &     -683.654    &     7153.858     \\\\\n",
       "\\textbf{4579.0}      &    6033.0314  &     1935.629     &     3.117  &         0.002        &     2238.849    &     9827.213     \\\\\n",
       "\\textbf{4585.0}      &    5120.6568  &     1905.415     &     2.687  &         0.007        &     1385.700    &     8855.613     \\\\\n",
       "\\textbf{4595.0}      &     557.2426  &     1868.586     &     0.298  &         0.766        &    -3105.522    &     4220.007     \\\\\n",
       "\\textbf{4600.0}      &   -8057.7539  &     2111.057     &    -3.817  &         0.000        &    -1.22e+04    &    -3919.703     \\\\\n",
       "\\textbf{4607.0}      &    4916.5681  &     1901.960     &     2.585  &         0.010        &     1188.385    &     8644.751     \\\\\n",
       "\\textbf{4608.0}      &   -6424.5107  &     2021.872     &    -3.178  &         0.001        &    -1.04e+04    &    -2461.279     \\\\\n",
       "\\textbf{4622.0}      &   -2091.5853  &     1985.279     &    -1.054  &         0.292        &    -5983.090    &     1799.919     \\\\\n",
       "\\textbf{4623.0}      &    3731.0165  &     1941.283     &     1.922  &         0.055        &      -74.247    &     7536.280     \\\\\n",
       "\\textbf{4768.0}      &    2815.4626  &     1887.217     &     1.492  &         0.136        &     -883.822    &     6514.747     \\\\\n",
       "\\textbf{4771.0}      &    5420.4120  &     1910.857     &     2.837  &         0.005        &     1674.788    &     9166.036     \\\\\n",
       "\\textbf{4800.0}      &    3660.7730  &     1920.037     &     1.907  &         0.057        &     -102.845    &     7424.391     \\\\\n",
       "\\textbf{4802.0}      &    4667.2138  &     1895.196     &     2.463  &         0.014        &      952.289    &     8382.138     \\\\\n",
       "\\textbf{4807.0}      &    4039.1996  &     1940.071     &     2.082  &         0.037        &      236.311    &     7842.088     \\\\\n",
       "\\textbf{4839.0}      &   -1.085e+05  &     3590.235     &   -30.217  &         0.000        &    -1.16e+05    &    -1.01e+05     \\\\\n",
       "\\textbf{4843.0}      &    -1.01e+04  &     2350.003     &    -4.299  &         0.000        &    -1.47e+04    &    -5496.966     \\\\\n",
       "\\textbf{4881.0}      &    1095.9244  &     1892.671     &     0.579  &         0.563        &    -2614.052    &     4805.901     \\\\\n",
       "\\textbf{4900.0}      &    2263.7762  &     1887.300     &     1.199  &         0.230        &    -1435.671    &     5963.224     \\\\\n",
       "\\textbf{4926.0}      &     783.9623  &     1907.591     &     0.411  &         0.681        &    -2955.259    &     4523.183     \\\\\n",
       "\\textbf{4941.0}      &    2265.5152  &     1886.613     &     1.201  &         0.230        &    -1432.585    &     5963.616     \\\\\n",
       "\\textbf{4961.0}      &   -1.139e+04  &     3058.833     &    -3.723  &         0.000        &    -1.74e+04    &    -5392.161     \\\\\n",
       "\\textbf{4988.0}      &    1.069e+04  &     1904.454     &     5.614  &         0.000        &     6958.725    &     1.44e+04     \\\\\n",
       "\\textbf{4993.0}      &    6337.7995  &     1935.443     &     3.275  &         0.001        &     2543.983    &     1.01e+04     \\\\\n",
       "\\textbf{5018.0}      &   -4590.1084  &     2122.137     &    -2.163  &         0.031        &    -8749.879    &     -430.337     \\\\\n",
       "\\textbf{5020.0}      &   -8094.2878  &     2272.709     &    -3.562  &         0.000        &    -1.25e+04    &    -3639.369     \\\\\n",
       "\\textbf{5027.0}      &   -1779.6929  &     1877.645     &    -0.948  &         0.343        &    -5460.216    &     1900.830     \\\\\n",
       "\\textbf{5032.0}      &    3546.3590  &     1886.846     &     1.880  &         0.060        &     -152.198    &     7244.916     \\\\\n",
       "\\textbf{5043.0}      &    -558.5515  &     1868.774     &    -0.299  &         0.765        &    -4221.684    &     3104.581     \\\\\n",
       "\\textbf{5046.0}      &   -1.137e+04  &     2090.169     &    -5.441  &         0.000        &    -1.55e+04    &    -7274.949     \\\\\n",
       "\\textbf{5047.0}      &    3.313e+04  &     3130.737     &    10.582  &         0.000        &      2.7e+04    &     3.93e+04     \\\\\n",
       "\\textbf{5065.0}      &    4633.1839  &     2276.152     &     2.036  &         0.042        &      171.516    &     9094.852     \\\\\n",
       "\\textbf{5071.0}      &    4698.1594  &     2312.439     &     2.032  &         0.042        &      165.362    &     9230.957     \\\\\n",
       "\\textbf{5073.0}      &   -1.588e+05  &     5755.681     &   -27.590  &         0.000        &     -1.7e+05    &    -1.48e+05     \\\\\n",
       "\\textbf{5087.0}      &    -953.2594  &     1865.655     &    -0.511  &         0.609        &    -4610.279    &     2703.760     \\\\\n",
       "\\textbf{5109.0}      &    5310.5607  &     1910.748     &     2.779  &         0.005        &     1565.151    &     9055.971     \\\\\n",
       "\\textbf{5116.0}      &   -8979.5137  &     2287.952     &    -3.925  &         0.000        &    -1.35e+04    &    -4494.716     \\\\\n",
       "\\textbf{5122.0}      &   -1763.8665  &     1879.130     &    -0.939  &         0.348        &    -5447.300    &     1919.567     \\\\\n",
       "\\textbf{5134.0}      &   -5338.2348  &     1962.337     &    -2.720  &         0.007        &    -9184.768    &    -1491.702     \\\\\n",
       "\\textbf{5142.0}      &     745.1892  &     2713.019     &     0.275  &         0.784        &    -4572.816    &     6063.194     \\\\\n",
       "\\textbf{5165.0}      &    1267.9682  &     2338.619     &     0.542  &         0.588        &    -3316.146    &     5852.082     \\\\\n",
       "\\textbf{5169.0}      &    1.422e+04  &     1883.672     &     7.550  &         0.000        &     1.05e+04    &     1.79e+04     \\\\\n",
       "\\textbf{5174.0}      &     542.5111  &     2261.694     &     0.240  &         0.810        &    -3890.815    &     4975.837     \\\\\n",
       "\\textbf{5179.0}      &    4365.6893  &     1893.161     &     2.306  &         0.021        &      654.753    &     8076.625     \\\\\n",
       "\\textbf{5181.0}      &    5457.5375  &     1955.951     &     2.790  &         0.005        &     1623.521    &     9291.554     \\\\\n",
       "\\textbf{5187.0}      &    5406.9499  &     2379.839     &     2.272  &         0.023        &      742.037    &     1.01e+04     \\\\\n",
       "\\textbf{5229.0}      &   -5137.8977  &     1943.679     &    -2.643  &         0.008        &    -8947.858    &    -1327.937     \\\\\n",
       "\\textbf{5234.0}      &   -7120.7766  &     2035.355     &    -3.499  &         0.000        &    -1.11e+04    &    -3131.115     \\\\\n",
       "\\textbf{5237.0}      &    3617.3288  &     1884.658     &     1.919  &         0.055        &      -76.940    &     7311.598     \\\\\n",
       "\\textbf{5252.0}      &    2423.9649  &     1882.094     &     1.288  &         0.198        &    -1265.278    &     6113.208     \\\\\n",
       "\\textbf{5254.0}      &    2636.1828  &     1888.007     &     1.396  &         0.163        &    -1064.650    &     6337.015     \\\\\n",
       "\\textbf{5306.0}      &    -370.0230  &     2018.458     &    -0.183  &         0.855        &    -4326.563    &     3586.517     \\\\\n",
       "\\textbf{5338.0}      &    4023.7011  &     1886.119     &     2.133  &         0.033        &      326.568    &     7720.835     \\\\\n",
       "\\textbf{5377.0}      &    5523.7227  &     1916.870     &     2.882  &         0.004        &     1766.312    &     9281.133     \\\\\n",
       "\\textbf{5439.0}      &    3337.8511  &     1912.468     &     1.745  &         0.081        &     -410.929    &     7086.632     \\\\\n",
       "\\textbf{5456.0}      &    6233.8984  &     1935.267     &     3.221  &         0.001        &     2440.426    &        1e+04     \\\\\n",
       "\\textbf{5464.0}      &    2087.6910  &     2469.127     &     0.846  &         0.398        &    -2752.241    &     6927.623     \\\\\n",
       "\\textbf{5476.0}      &    5725.0042  &     1937.789     &     2.954  &         0.003        &     1926.588    &     9523.420     \\\\\n",
       "\\textbf{5492.0}      &   -1.848e+04  &     2271.698     &    -8.135  &         0.000        &    -2.29e+04    &     -1.4e+04     \\\\\n",
       "\\textbf{5496.0}      &    2884.6863  &     1878.406     &     1.536  &         0.125        &     -797.327    &     6566.699     \\\\\n",
       "\\textbf{5505.0}      &    4921.0805  &     1902.737     &     2.586  &         0.010        &     1191.374    &     8650.787     \\\\\n",
       "\\textbf{5518.0}      &    4154.6414  &     2204.114     &     1.885  &         0.059        &     -165.818    &     8475.101     \\\\\n",
       "\\textbf{5520.0}      &     982.5964  &     1870.064     &     0.525  &         0.599        &    -2683.066    &     4648.259     \\\\\n",
       "\\textbf{5545.0}      &    4611.2221  &     1996.389     &     2.310  &         0.021        &      697.940    &     8524.504     \\\\\n",
       "\\textbf{5568.0}      &    6443.3547  &     1892.985     &     3.404  &         0.001        &     2732.763    &     1.02e+04     \\\\\n",
       "\\textbf{5569.0}      &    5182.5680  &     1953.809     &     2.653  &         0.008        &     1352.751    &     9012.385     \\\\\n",
       "\\textbf{5578.0}      &    4371.1879  &     1891.184     &     2.311  &         0.021        &      664.127    &     8078.249     \\\\\n",
       "\\textbf{5581.0}      &    3920.2341  &     1881.911     &     2.083  &         0.037        &      231.350    &     7609.118     \\\\\n",
       "\\textbf{5589.0}      &   -4919.3500  &     1989.347     &    -2.473  &         0.013        &    -8818.828    &    -1019.872     \\\\\n",
       "\\textbf{5597.0}      &    5579.7669  &     2589.433     &     2.155  &         0.031        &      504.012    &     1.07e+04     \\\\\n",
       "\\textbf{5606.0}      &   -2.149e+04  &     2189.269     &    -9.815  &         0.000        &    -2.58e+04    &    -1.72e+04     \\\\\n",
       "\\textbf{5639.0}      &    6284.4911  &     1911.721     &     3.287  &         0.001        &     2537.173    &        1e+04     \\\\\n",
       "\\textbf{5667.0}      &    1593.5800  &     1984.465     &     0.803  &         0.422        &    -2296.329    &     5483.489     \\\\\n",
       "\\textbf{5690.0}      &    4680.9703  &     1892.279     &     2.474  &         0.013        &      971.764    &     8390.177     \\\\\n",
       "\\textbf{5709.0}      &    3732.5040  &     1896.819     &     1.968  &         0.049        &       14.398    &     7450.610     \\\\\n",
       "\\textbf{5726.0}      &    3685.4231  &     1889.670     &     1.950  &         0.051        &      -18.671    &     7389.517     \\\\\n",
       "\\textbf{5764.0}      &    1412.3279  &     1903.400     &     0.742  &         0.458        &    -2318.679    &     5143.335     \\\\\n",
       "\\textbf{5772.0}      &    3425.6120  &     1886.884     &     1.815  &         0.069        &     -273.020    &     7124.244     \\\\\n",
       "\\textbf{5860.0}      &   -2.354e+04  &     2204.399     &   -10.681  &         0.000        &    -2.79e+04    &    -1.92e+04     \\\\\n",
       "\\textbf{5878.0}      &    4756.7109  &     1890.668     &     2.516  &         0.012        &     1050.662    &     8462.760     \\\\\n",
       "\\textbf{5903.0}      &   -1763.3549  &     1956.028     &    -0.901  &         0.367        &    -5597.522    &     2070.813     \\\\\n",
       "\\textbf{5905.0}      &    1594.0186  &     1904.603     &     0.837  &         0.403        &    -2139.345    &     5327.383     \\\\\n",
       "\\textbf{5959.0}      &    -551.9034  &     1965.277     &    -0.281  &         0.779        &    -4404.199    &     3300.393     \\\\\n",
       "\\textbf{6008.0}      &    2.313e+04  &     2050.513     &    11.281  &         0.000        &     1.91e+04    &     2.72e+04     \\\\\n",
       "\\textbf{6034.0}      &   -1253.0232  &     2221.970     &    -0.564  &         0.573        &    -5608.484    &     3102.438     \\\\\n",
       "\\textbf{6035.0}      &   -3476.7879  &     2528.257     &    -1.375  &         0.169        &    -8432.626    &     1479.051     \\\\\n",
       "\\textbf{6036.0}      &   -6657.4342  &     1921.654     &    -3.464  &         0.001        &    -1.04e+04    &    -2890.646     \\\\\n",
       "\\textbf{6039.0}      &    3754.1280  &     1885.722     &     1.991  &         0.047        &       57.773    &     7450.483     \\\\\n",
       "\\textbf{6044.0}      &    6085.6302  &     2158.752     &     2.819  &         0.005        &     1854.088    &     1.03e+04     \\\\\n",
       "\\textbf{6066.0}      &    2066.3853  &     3764.955     &     0.549  &         0.583        &    -5313.603    &     9446.374     \\\\\n",
       "\\textbf{6078.0}      &    4963.4106  &     1874.865     &     2.647  &         0.008        &     1288.338    &     8638.483     \\\\\n",
       "\\textbf{6081.0}      &   -1.751e+04  &     2080.749     &    -8.417  &         0.000        &    -2.16e+04    &    -1.34e+04     \\\\\n",
       "\\textbf{60893.0}     &   -1.509e+04  &     4441.536     &    -3.398  &         0.001        &    -2.38e+04    &    -6384.761     \\\\\n",
       "\\textbf{6097.0}      &    5098.8954  &     1913.924     &     2.664  &         0.008        &     1347.261    &     8850.530     \\\\\n",
       "\\textbf{6102.0}      &    3175.7833  &     1892.683     &     1.678  &         0.093        &     -534.215    &     6885.782     \\\\\n",
       "\\textbf{6104.0}      &   -6512.5249  &     2013.810     &    -3.234  &         0.001        &    -1.05e+04    &    -2565.094     \\\\\n",
       "\\textbf{6109.0}      &   -6666.9320  &     1931.756     &    -3.451  &         0.001        &    -1.05e+04    &    -2880.342     \\\\\n",
       "\\textbf{6127.0}      &   -4005.8304  &     2232.235     &    -1.795  &         0.073        &    -8381.412    &      369.751     \\\\\n",
       "\\textbf{61552.0}     &   -1.243e+04  &     3702.050     &    -3.358  &         0.001        &    -1.97e+04    &    -5174.322     \\\\\n",
       "\\textbf{6158.0}      &    -159.9485  &     2036.176     &    -0.079  &         0.937        &    -4151.221    &     3831.324     \\\\\n",
       "\\textbf{6171.0}      &    3633.6054  &     1882.449     &     1.930  &         0.054        &      -56.333    &     7323.544     \\\\\n",
       "\\textbf{61780.0}     &    2001.2059  &     4082.166     &     0.490  &         0.624        &    -6000.574    &        1e+04     \\\\\n",
       "\\textbf{6207.0}      &    3792.1030  &     1891.311     &     2.005  &         0.045        &       84.794    &     7499.412     \\\\\n",
       "\\textbf{6214.0}      &    4252.9273  &     1892.229     &     2.248  &         0.025        &      543.817    &     7962.037     \\\\\n",
       "\\textbf{6216.0}      &    5331.3623  &     1929.983     &     2.762  &         0.006        &     1548.249    &     9114.475     \\\\\n",
       "\\textbf{62221.0}     &    4107.4431  &     4086.784     &     1.005  &         0.315        &    -3903.389    &     1.21e+04     \\\\\n",
       "\\textbf{6259.0}      &   -1840.2931  &     2289.573     &    -0.804  &         0.422        &    -6328.269    &     2647.683     \\\\\n",
       "\\textbf{62599.0}     &    1.096e+04  &     4774.766     &     2.295  &         0.022        &     1598.888    &     2.03e+04     \\\\\n",
       "\\textbf{6266.0}      &    5315.3816  &     2126.320     &     2.500  &         0.012        &     1147.411    &     9483.352     \\\\\n",
       "\\textbf{6268.0}      &   -2706.9925  &     1966.170     &    -1.377  &         0.169        &    -6561.039    &     1147.054     \\\\\n",
       "\\textbf{6288.0}      &    1950.1248  &     1886.585     &     1.034  &         0.301        &    -1747.922    &     5648.171     \\\\\n",
       "\\textbf{6297.0}      &    3856.2871  &     1997.887     &     1.930  &         0.054        &      -59.931    &     7772.505     \\\\\n",
       "\\textbf{6307.0}      &    -1.98e+04  &     2839.541     &    -6.974  &         0.000        &    -2.54e+04    &    -1.42e+04     \\\\\n",
       "\\textbf{6313.0}      &    4079.6361  &     3344.074     &     1.220  &         0.223        &    -2475.351    &     1.06e+04     \\\\\n",
       "\\textbf{6314.0}      &    4782.8866  &     1911.134     &     2.503  &         0.012        &     1036.721    &     8529.053     \\\\\n",
       "\\textbf{6326.0}      &   -3966.7694  &     1970.640     &    -2.013  &         0.044        &    -7829.579    &     -103.960     \\\\\n",
       "\\textbf{6349.0}      &    3140.2437  &     1880.024     &     1.670  &         0.095        &     -544.942    &     6825.429     \\\\\n",
       "\\textbf{6357.0}      &    5344.1170  &     2069.760     &     2.582  &         0.010        &     1287.015    &     9401.219     \\\\\n",
       "\\textbf{6375.0}      &    9761.2418  &     1895.523     &     5.150  &         0.000        &     6045.676    &     1.35e+04     \\\\\n",
       "\\textbf{6376.0}      &    4837.2092  &     1903.822     &     2.541  &         0.011        &     1105.375    &     8569.044     \\\\\n",
       "\\textbf{6379.0}      &    5252.0872  &     5653.339     &     0.929  &         0.353        &    -5829.475    &     1.63e+04     \\\\\n",
       "\\textbf{6386.0}      &    4720.5567  &     1890.634     &     2.497  &         0.013        &     1014.574    &     8426.539     \\\\\n",
       "\\textbf{6403.0}      &    -734.7835  &     1932.553     &    -0.380  &         0.704        &    -4522.934    &     3053.367     \\\\\n",
       "\\textbf{6410.0}      &    6089.8782  &     1928.200     &     3.158  &         0.002        &     2310.259    &     9869.497     \\\\\n",
       "\\textbf{6416.0}      &   -2496.1172  &     2012.976     &    -1.240  &         0.215        &    -6441.912    &     1449.678     \\\\\n",
       "\\textbf{6424.0}      &    4445.7448  &     1898.100     &     2.342  &         0.019        &      725.128    &     8166.362     \\\\\n",
       "\\textbf{6433.0}      &    4813.7868  &     1924.016     &     2.502  &         0.012        &     1042.369    &     8585.205     \\\\\n",
       "\\textbf{6435.0}      &    3598.1008  &     1929.493     &     1.865  &         0.062        &     -184.053    &     7380.254     \\\\\n",
       "\\textbf{6492.0}      &    -734.9224  &     1960.341     &    -0.375  &         0.708        &    -4577.544    &     3107.699     \\\\\n",
       "\\textbf{6497.0}      &   -4877.3634  &     2139.537     &    -2.280  &         0.023        &    -9071.240    &     -683.486     \\\\\n",
       "\\textbf{6500.0}      &    2631.2582  &     8140.134     &     0.323  &         0.747        &    -1.33e+04    &     1.86e+04     \\\\\n",
       "\\textbf{6509.0}      &    3122.6930  &     1884.421     &     1.657  &         0.098        &     -571.110    &     6816.496     \\\\\n",
       "\\textbf{6527.0}      &    5860.6445  &     2301.144     &     2.547  &         0.011        &     1349.988    &     1.04e+04     \\\\\n",
       "\\textbf{6528.0}      &    2890.7532  &     2113.603     &     1.368  &         0.171        &    -1252.290    &     7033.796     \\\\\n",
       "\\textbf{6531.0}      &   -7043.9021  &     2023.324     &    -3.481  &         0.001        &     -1.1e+04    &    -3077.824     \\\\\n",
       "\\textbf{6532.0}      &    -480.0719  &     1920.270     &    -0.250  &         0.803        &    -4244.146    &     3284.002     \\\\\n",
       "\\textbf{6543.0}      &    5210.9517  &     1903.130     &     2.738  &         0.006        &     1480.474    &     8941.429     \\\\\n",
       "\\textbf{6548.0}      &    4613.0426  &     1902.342     &     2.425  &         0.015        &      884.111    &     8341.974     \\\\\n",
       "\\textbf{6550.0}      &    4431.0832  &     2121.854     &     2.088  &         0.037        &      271.868    &     8590.298     \\\\\n",
       "\\textbf{6552.0}      &    5011.0435  &     2079.583     &     2.410  &         0.016        &      934.687    &     9087.400     \\\\\n",
       "\\textbf{6565.0}      &   -2017.2034  &     2275.032     &    -0.887  &         0.375        &    -6476.675    &     2442.268     \\\\\n",
       "\\textbf{6571.0}      &    4085.2015  &     1890.317     &     2.161  &         0.031        &      379.841    &     7790.563     \\\\\n",
       "\\textbf{6573.0}      &    3637.9988  &     1882.029     &     1.933  &         0.053        &      -51.116    &     7327.114     \\\\\n",
       "\\textbf{6641.0}      &    1230.8940  &     4069.683     &     0.302  &         0.762        &    -6746.416    &     9208.204     \\\\\n",
       "\\textbf{6649.0}      &    5944.6150  &     1910.454     &     3.112  &         0.002        &     2199.781    &     9689.449     \\\\\n",
       "\\textbf{6730.0}      &    3876.8676  &     2145.664     &     1.807  &         0.071        &     -329.020    &     8082.755     \\\\\n",
       "\\textbf{6731.0}      &    -563.3170  &     1968.969     &    -0.286  &         0.775        &    -4422.852    &     3296.218     \\\\\n",
       "\\textbf{6742.0}      &    2825.0452  &     4084.063     &     0.692  &         0.489        &    -5180.454    &     1.08e+04     \\\\\n",
       "\\textbf{6745.0}      &    5143.2369  &     1902.430     &     2.704  &         0.007        &     1414.132    &     8872.342     \\\\\n",
       "\\textbf{6756.0}      &    4871.3773  &     1917.705     &     2.540  &         0.011        &     1112.330    &     8630.424     \\\\\n",
       "\\textbf{6765.0}      &   -1.321e+04  &     2050.847     &    -6.440  &         0.000        &    -1.72e+04    &    -9187.512     \\\\\n",
       "\\textbf{6768.0}      &    7016.4411  &     1992.023     &     3.522  &         0.000        &     3111.717    &     1.09e+04     \\\\\n",
       "\\textbf{6774.0}      &   -2.048e+04  &     2422.263     &    -8.453  &         0.000        &    -2.52e+04    &    -1.57e+04     \\\\\n",
       "\\textbf{6797.0}      &    5985.7283  &     2400.219     &     2.494  &         0.013        &     1280.867    &     1.07e+04     \\\\\n",
       "\\textbf{6803.0}      &    4924.6282  &     1908.446     &     2.580  &         0.010        &     1183.731    &     8665.525     \\\\\n",
       "\\textbf{6821.0}      &    3868.1145  &     1885.957     &     2.051  &         0.040        &      171.299    &     7564.930     \\\\\n",
       "\\textbf{6830.0}      &    3531.1264  &     1897.188     &     1.861  &         0.063        &     -187.704    &     7249.956     \\\\\n",
       "\\textbf{6845.0}      &    -176.0146  &     1979.119     &    -0.089  &         0.929        &    -4055.444    &     3703.415     \\\\\n",
       "\\textbf{6848.0}      &    3797.8864  &     2136.856     &     1.777  &         0.076        &     -390.736    &     7986.509     \\\\\n",
       "\\textbf{6873.0}      &    -668.9741  &     2727.946     &    -0.245  &         0.806        &    -6016.239    &     4678.291     \\\\\n",
       "\\textbf{6900.0}      &    1287.7612  &     1923.314     &     0.670  &         0.503        &    -2482.281    &     5057.803     \\\\\n",
       "\\textbf{6908.0}      &    1372.8494  &     1909.405     &     0.719  &         0.472        &    -2369.928    &     5115.627     \\\\\n",
       "\\textbf{6994.0}      &    2643.0183  &     1900.603     &     1.391  &         0.164        &    -1082.507    &     6368.543     \\\\\n",
       "\\textbf{7045.0}      &   -5777.6863  &     2549.116     &    -2.267  &         0.023        &    -1.08e+04    &     -780.960     \\\\\n",
       "\\textbf{7065.0}      &    6908.9206  &     1898.554     &     3.639  &         0.000        &     3187.413    &     1.06e+04     \\\\\n",
       "\\textbf{7085.0}      &    5820.7381  &     1886.029     &     3.086  &         0.002        &     2123.781    &     9517.695     \\\\\n",
       "\\textbf{7107.0}      &    2066.6982  &     2076.405     &     0.995  &         0.320        &    -2003.428    &     6136.825     \\\\\n",
       "\\textbf{7116.0}      &    6503.0343  &     1923.673     &     3.381  &         0.001        &     2732.289    &     1.03e+04     \\\\\n",
       "\\textbf{7117.0}      &    6174.9739  &     3129.278     &     1.973  &         0.048        &       41.027    &     1.23e+04     \\\\\n",
       "\\textbf{7121.0}      &    3294.9471  &     1884.523     &     1.748  &         0.080        &     -399.057    &     6988.952     \\\\\n",
       "\\textbf{7127.0}      &    2149.9549  &     2020.801     &     1.064  &         0.287        &    -1811.180    &     6111.089     \\\\\n",
       "\\textbf{7139.0}      &    3684.4739  &     1884.310     &     1.955  &         0.051        &       -9.113    &     7378.061     \\\\\n",
       "\\textbf{7146.0}      &    4642.5032  &     1893.941     &     2.451  &         0.014        &      930.039    &     8354.968     \\\\\n",
       "\\textbf{7163.0}      &    7302.9869  &     1896.443     &     3.851  &         0.000        &     3585.617    &      1.1e+04     \\\\\n",
       "\\textbf{7180.0}      &     181.7039  &     1884.279     &     0.096  &         0.923        &    -3511.822    &     3875.230     \\\\\n",
       "\\textbf{7183.0}      &    -274.3024  &     1937.642     &    -0.142  &         0.887        &    -4072.429    &     3523.824     \\\\\n",
       "\\textbf{7228.0}      &     1.28e+04  &     1904.390     &     6.722  &         0.000        &     9067.861    &     1.65e+04     \\\\\n",
       "\\textbf{7232.0}      &    2256.8554  &     3094.499     &     0.729  &         0.466        &    -3808.919    &     8322.630     \\\\\n",
       "\\textbf{7250.0}      &     454.8082  &     2183.158     &     0.208  &         0.835        &    -3824.575    &     4734.191     \\\\\n",
       "\\textbf{7257.0}      &    2.545e+04  &     2148.988     &    11.841  &         0.000        &     2.12e+04    &     2.97e+04     \\\\\n",
       "\\textbf{7260.0}      &    3535.9150  &     1881.772     &     1.879  &         0.060        &     -152.697    &     7224.527     \\\\\n",
       "\\textbf{7267.0}      &     237.8283  &     1986.583     &     0.120  &         0.905        &    -3656.233    &     4131.889     \\\\\n",
       "\\textbf{7268.0}      &   -6685.3518  &     2284.765     &    -2.926  &         0.003        &    -1.12e+04    &    -2206.801     \\\\\n",
       "\\textbf{7281.0}      &    4494.6574  &     2888.074     &     1.556  &         0.120        &    -1166.486    &     1.02e+04     \\\\\n",
       "\\textbf{7291.0}      &    1181.2310  &     1918.089     &     0.616  &         0.538        &    -2578.568    &     4941.030     \\\\\n",
       "\\textbf{7343.0}      &   -7911.4411  &     2855.978     &    -2.770  &         0.006        &    -1.35e+04    &    -2313.211     \\\\\n",
       "\\textbf{7346.0}      &   -4271.2095  &     1936.237     &    -2.206  &         0.027        &    -8066.583    &     -475.836     \\\\\n",
       "\\textbf{7401.0}      &    4345.8538  &     1890.458     &     2.299  &         0.022        &      640.217    &     8051.491     \\\\\n",
       "\\textbf{7409.0}      &    3110.0251  &     1889.359     &     1.646  &         0.100        &     -593.458    &     6813.508     \\\\\n",
       "\\textbf{7420.0}      &    -200.0609  &     1875.303     &    -0.107  &         0.915        &    -3875.992    &     3475.870     \\\\\n",
       "\\textbf{7435.0}      &    5677.9998  &     2294.109     &     2.475  &         0.013        &     1181.134    &     1.02e+04     \\\\\n",
       "\\textbf{7466.0}      &    3068.3648  &     1999.314     &     1.535  &         0.125        &     -850.650    &     6987.380     \\\\\n",
       "\\textbf{7486.0}      &   -6368.5974  &     2092.013     &    -3.044  &         0.002        &    -1.05e+04    &    -2267.875     \\\\\n",
       "\\textbf{7503.0}      &    3312.6048  &     3643.490     &     0.909  &         0.363        &    -3829.292    &     1.05e+04     \\\\\n",
       "\\textbf{7506.0}      &    4314.1537  &     1883.555     &     2.290  &         0.022        &      622.048    &     8006.260     \\\\\n",
       "\\textbf{7537.0}      &    4406.8673  &     1902.507     &     2.316  &         0.021        &      677.610    &     8136.124     \\\\\n",
       "\\textbf{7549.0}      &    1261.0932  &     1909.746     &     0.660  &         0.509        &    -2482.352    &     5004.539     \\\\\n",
       "\\textbf{7554.0}      &    4258.7072  &     1895.126     &     2.247  &         0.025        &      543.920    &     7973.495     \\\\\n",
       "\\textbf{7557.0}      &      94.9133  &     1998.861     &     0.047  &         0.962        &    -3823.213    &     4013.040     \\\\\n",
       "\\textbf{7585.0}      &   -6767.6640  &     2431.073     &    -2.784  &         0.005        &    -1.15e+04    &    -2002.324     \\\\\n",
       "\\textbf{7602.0}      &    3508.6228  &     1886.606     &     1.860  &         0.063        &     -189.464    &     7206.710     \\\\\n",
       "\\textbf{7620.0}      &     864.6487  &     2161.340     &     0.400  &         0.689        &    -3371.966    &     5101.263     \\\\\n",
       "\\textbf{7636.0}      &    4079.5367  &     1894.075     &     2.154  &         0.031        &      366.810    &     7792.264     \\\\\n",
       "\\textbf{7646.0}      &    4368.0875  &     1892.540     &     2.308  &         0.021        &      658.369    &     8077.806     \\\\\n",
       "\\textbf{7658.0}      &    -444.5436  &     1938.669     &    -0.229  &         0.819        &    -4244.684    &     3355.597     \\\\\n",
       "\\textbf{7683.0}      &    5242.7246  &     2062.069     &     2.542  &         0.011        &     1200.698    &     9284.751     \\\\\n",
       "\\textbf{7685.0}      &    3203.1421  &     2117.987     &     1.512  &         0.130        &     -948.494    &     7354.779     \\\\\n",
       "\\textbf{7692.0}      &   -1982.7323  &     1872.804     &    -1.059  &         0.290        &    -5653.765    &     1688.300     \\\\\n",
       "\\textbf{7762.0}      &    3939.9208  &     1887.339     &     2.088  &         0.037        &      240.396    &     7639.445     \\\\\n",
       "\\textbf{7772.0}      &   -8822.8734  &     2053.343     &    -4.297  &         0.000        &    -1.28e+04    &    -4797.952     \\\\\n",
       "\\textbf{7773.0}      &    3643.2673  &     1880.924     &     1.937  &         0.053        &      -43.683    &     7330.217     \\\\\n",
       "\\textbf{7777.0}      &   -1668.5671  &     1880.801     &    -0.887  &         0.375        &    -5355.275    &     2018.141     \\\\\n",
       "\\textbf{7835.0}      &    4887.6887  &     1895.424     &     2.579  &         0.010        &     1172.316    &     8603.061     \\\\\n",
       "\\textbf{7873.0}      &   -6569.8690  &     2034.245     &    -3.230  &         0.001        &    -1.06e+04    &    -2582.382     \\\\\n",
       "\\textbf{7883.0}      &    -278.4626  &     1986.987     &    -0.140  &         0.889        &    -4173.315    &     3616.390     \\\\\n",
       "\\textbf{7904.0}      &   -1014.3734  &     1976.040     &    -0.513  &         0.608        &    -4887.768    &     2859.021     \\\\\n",
       "\\textbf{7906.0}      &    7500.9507  &     1914.777     &     3.917  &         0.000        &     3747.643    &     1.13e+04     \\\\\n",
       "\\textbf{7921.0}      &    4187.0950  &     1887.493     &     2.218  &         0.027        &      487.269    &     7886.921     \\\\\n",
       "\\textbf{7923.0}      &    4451.0966  &     2005.459     &     2.219  &         0.026        &      520.035    &     8382.158     \\\\\n",
       "\\textbf{7935.0}      &     860.7939  &     1869.151     &     0.461  &         0.645        &    -2803.078    &     4524.666     \\\\\n",
       "\\textbf{7938.0}      &    2057.4060  &     1885.421     &     1.091  &         0.275        &    -1638.359    &     5753.171     \\\\\n",
       "\\textbf{7985.0}      &   -1.955e+04  &     2201.157     &    -8.881  &         0.000        &    -2.39e+04    &    -1.52e+04     \\\\\n",
       "\\textbf{8014.0}      &    2426.3775  &     1987.219     &     1.221  &         0.222        &    -1468.929    &     6321.684     \\\\\n",
       "\\textbf{8030.0}      &    5924.6963  &     1920.790     &     3.085  &         0.002        &     2159.602    &     9689.790     \\\\\n",
       "\\textbf{8046.0}      &   -6813.2524  &     2034.913     &    -3.348  &         0.001        &    -1.08e+04    &    -2824.456     \\\\\n",
       "\\textbf{8047.0}      &    3427.6567  &     2727.958     &     1.256  &         0.209        &    -1919.632    &     8774.946     \\\\\n",
       "\\textbf{8062.0}      &    1583.0928  &     1924.083     &     0.823  &         0.411        &    -2188.455    &     5354.641     \\\\\n",
       "\\textbf{8068.0}      &   -1.386e+04  &     2219.412     &    -6.247  &         0.000        &    -1.82e+04    &    -9513.728     \\\\\n",
       "\\textbf{8087.0}      &   -1.125e+04  &     2138.260     &    -5.260  &         0.000        &    -1.54e+04    &    -7056.886     \\\\\n",
       "\\textbf{8095.0}      &    3747.6061  &     1890.531     &     1.982  &         0.047        &       41.826    &     7453.386     \\\\\n",
       "\\textbf{8096.0}      &    4967.5181  &     1914.276     &     2.595  &         0.009        &     1215.193    &     8719.843     \\\\\n",
       "\\textbf{8109.0}      &    4448.7369  &     1893.131     &     2.350  &         0.019        &      737.860    &     8159.614     \\\\\n",
       "\\textbf{8123.0}      &   -1564.8370  &     2068.102     &    -0.757  &         0.449        &    -5618.689    &     2489.015     \\\\\n",
       "\\textbf{8150.0}      &    5167.8110  &     1904.168     &     2.714  &         0.007        &     1435.300    &     8900.322     \\\\\n",
       "\\textbf{8163.0}      &     336.5507  &     1986.336     &     0.169  &         0.865        &    -3557.026    &     4230.127     \\\\\n",
       "\\textbf{8176.0}      &   -1014.2924  &     2411.287     &    -0.421  &         0.674        &    -5740.849    &     3712.264     \\\\\n",
       "\\textbf{8202.0}      &     811.2235  &     2008.822     &     0.404  &         0.686        &    -3126.429    &     4748.876     \\\\\n",
       "\\textbf{8214.0}      &     111.2975  &     1911.608     &     0.058  &         0.954        &    -3635.798    &     3858.392     \\\\\n",
       "\\textbf{8215.0}      &   -2390.3624  &     2139.162     &    -1.117  &         0.264        &    -6583.505    &     1802.780     \\\\\n",
       "\\textbf{8219.0}      &    5671.7298  &     1921.079     &     2.952  &         0.003        &     1906.070    &     9437.390     \\\\\n",
       "\\textbf{8247.0}      &     887.0208  &     1937.131     &     0.458  &         0.647        &    -2910.104    &     4684.145     \\\\\n",
       "\\textbf{8253.0}      &   -1.195e+04  &     2040.351     &    -5.859  &         0.000        &     -1.6e+04    &    -7954.751     \\\\\n",
       "\\textbf{8290.0}      &    1302.6717  &     2006.620     &     0.649  &         0.516        &    -2630.664    &     5236.007     \\\\\n",
       "\\textbf{8293.0}      &     884.4232  &     1963.476     &     0.450  &         0.652        &    -2964.342    &     4733.189     \\\\\n",
       "\\textbf{8304.0}      &    4944.8247  &     1881.092     &     2.629  &         0.009        &     1257.545    &     8632.104     \\\\\n",
       "\\textbf{8334.0}      &    4982.4455  &     2045.671     &     2.436  &         0.015        &      972.562    &     8992.329     \\\\\n",
       "\\textbf{8348.0}      &    3670.3352  &     1888.262     &     1.944  &         0.052        &      -30.998    &     7371.669     \\\\\n",
       "\\textbf{8357.0}      &    3236.9587  &     1883.212     &     1.719  &         0.086        &     -454.475    &     6928.392     \\\\\n",
       "\\textbf{8358.0}      &    1240.3781  &     1908.909     &     0.650  &         0.516        &    -2501.426    &     4982.183     \\\\\n",
       "\\textbf{8446.0}      &   -8111.7072  &     2318.225     &    -3.499  &         0.000        &    -1.27e+04    &    -3567.568     \\\\\n",
       "\\textbf{8460.0}      &    6388.1362  &     2303.552     &     2.773  &         0.006        &     1872.761    &     1.09e+04     \\\\\n",
       "\\textbf{8463.0}      &    3671.8709  &     1886.483     &     1.946  &         0.052        &      -25.974    &     7369.716     \\\\\n",
       "\\textbf{8479.0}      &    6394.2133  &     2734.879     &     2.338  &         0.019        &     1033.358    &     1.18e+04     \\\\\n",
       "\\textbf{8530.0}      &    8019.4538  &     2187.179     &     3.667  &         0.000        &     3732.189    &     1.23e+04     \\\\\n",
       "\\textbf{8536.0}      &   -1046.7635  &     1930.939     &    -0.542  &         0.588        &    -4831.751    &     2738.224     \\\\\n",
       "\\textbf{8543.0}      &    2.288e+04  &     2174.771     &    10.523  &         0.000        &     1.86e+04    &     2.71e+04     \\\\\n",
       "\\textbf{8549.0}      &   -8252.7873  &     2048.657     &    -4.028  &         0.000        &    -1.23e+04    &    -4237.050     \\\\\n",
       "\\textbf{8551.0}      &    5182.1662  &     1912.584     &     2.710  &         0.007        &     1433.157    &     8931.176     \\\\\n",
       "\\textbf{8559.0}      &     285.1798  &     2085.795     &     0.137  &         0.891        &    -3803.354    &     4373.714     \\\\\n",
       "\\textbf{8573.0}      &   -6174.7927  &     2267.068     &    -2.724  &         0.006        &    -1.06e+04    &    -1730.932     \\\\\n",
       "\\textbf{8606.0}      &    5357.3152  &     1891.680     &     2.832  &         0.005        &     1649.281    &     9065.349     \\\\\n",
       "\\textbf{8607.0}      &    5343.3350  &     1921.755     &     2.780  &         0.005        &     1576.349    &     9110.321     \\\\\n",
       "\\textbf{8648.0}      &    3082.5063  &     1882.670     &     1.637  &         0.102        &     -607.866    &     6772.878     \\\\\n",
       "\\textbf{8657.0}      &   -1618.1451  &     1874.980     &    -0.863  &         0.388        &    -5293.443    &     2057.153     \\\\\n",
       "\\textbf{8675.0}      &    4370.1464  &     3672.134     &     1.190  &         0.234        &    -2827.897    &     1.16e+04     \\\\\n",
       "\\textbf{8681.0}      &   -1669.7543  &     1880.379     &    -0.888  &         0.375        &    -5355.635    &     2016.127     \\\\\n",
       "\\textbf{8687.0}      &    -102.7675  &     2184.219     &    -0.047  &         0.962        &    -4384.230    &     4178.695     \\\\\n",
       "\\textbf{8692.0}      &     619.4863  &     1886.940     &     0.328  &         0.743        &    -3079.255    &     4318.228     \\\\\n",
       "\\textbf{8699.0}      &    4089.5465  &     1889.663     &     2.164  &         0.030        &      385.466    &     7793.627     \\\\\n",
       "\\textbf{8717.0}      &    5250.1882  &     1901.428     &     2.761  &         0.006        &     1523.048    &     8977.329     \\\\\n",
       "\\textbf{8759.0}      &    -167.6682  &     1946.070     &    -0.086  &         0.931        &    -3982.316    &     3646.980     \\\\\n",
       "\\textbf{8762.0}      &    9881.2575  &     2003.848     &     4.931  &         0.000        &     5953.354    &     1.38e+04     \\\\\n",
       "\\textbf{8819.0}      &    6358.0137  &     1939.505     &     3.278  &         0.001        &     2556.236    &     1.02e+04     \\\\\n",
       "\\textbf{8850.0}      &    3921.8310  &     1888.789     &     2.076  &         0.038        &      219.464    &     7624.198     \\\\\n",
       "\\textbf{8852.0}      &    4019.5858  &     1886.207     &     2.131  &         0.033        &      322.281    &     7716.890     \\\\\n",
       "\\textbf{8859.0}      &    5179.6950  &     1922.581     &     2.694  &         0.007        &     1411.090    &     8948.300     \\\\\n",
       "\\textbf{8867.0}      &   -3798.5887  &     2037.891     &    -1.864  &         0.062        &    -7793.223    &      196.045     \\\\\n",
       "\\textbf{8881.0}      &    2015.6252  &     1876.670     &     1.074  &         0.283        &    -1662.987    &     5694.237     \\\\\n",
       "\\textbf{8958.0}      &     783.6540  &     1875.076     &     0.418  &         0.676        &    -2891.833    &     4459.141     \\\\\n",
       "\\textbf{8972.0}      &   -1.876e+04  &     2234.007     &    -8.396  &         0.000        &    -2.31e+04    &    -1.44e+04     \\\\\n",
       "\\textbf{8990.0}      &   -8901.2931  &     2173.056     &    -4.096  &         0.000        &    -1.32e+04    &    -4641.712     \\\\\n",
       "\\textbf{9004.0}      &    5563.5665  &     2223.931     &     2.502  &         0.012        &     1204.262    &     9922.871     \\\\\n",
       "\\textbf{9016.0}      &    1076.0450  &     1871.540     &     0.575  &         0.565        &    -2592.510    &     4744.600     \\\\\n",
       "\\textbf{9048.0}      &    -332.1457  &     1903.188     &    -0.175  &         0.861        &    -4062.737    &     3398.445     \\\\\n",
       "\\textbf{9051.0}      &   -6534.2926  &     2323.696     &    -2.812  &         0.005        &    -1.11e+04    &    -1979.430     \\\\\n",
       "\\textbf{9071.0}      &   -1080.8584  &     2074.877     &    -0.521  &         0.602        &    -5147.991    &     2986.274     \\\\\n",
       "\\textbf{9112.0}      &    -939.2564  &     1868.324     &    -0.503  &         0.615        &    -4601.507    &     2722.994     \\\\\n",
       "\\textbf{9114.0}      &   -2806.9536  &     1947.227     &    -1.442  &         0.149        &    -6623.869    &     1009.962     \\\\\n",
       "\\textbf{9132.0}      &    4738.5874  &     3383.557     &     1.400  &         0.161        &    -1893.793    &     1.14e+04     \\\\\n",
       "\\textbf{9173.0}      &    2640.2302  &     2267.952     &     1.164  &         0.244        &    -1805.365    &     7085.825     \\\\\n",
       "\\textbf{9180.0}      &    5178.7693  &     1907.866     &     2.714  &         0.007        &     1439.008    &     8918.531     \\\\\n",
       "\\textbf{9186.0}      &    2567.5309  &     1883.220     &     1.363  &         0.173        &    -1123.919    &     6258.981     \\\\\n",
       "\\textbf{9191.0}      &   -1799.1070  &     3371.606     &    -0.534  &         0.594        &    -8408.061    &     4809.847     \\\\\n",
       "\\textbf{9216.0}      &   -5071.6784  &     2007.828     &    -2.526  &         0.012        &    -9007.382    &    -1135.975     \\\\\n",
       "\\textbf{9217.0}      &   -6693.0668  &     1999.891     &    -3.347  &         0.001        &    -1.06e+04    &    -2772.920     \\\\\n",
       "\\textbf{9225.0}      &    4530.0976  &     1889.274     &     2.398  &         0.017        &      826.781    &     8233.414     \\\\\n",
       "\\textbf{9230.0}      &    5414.8369  &     2502.877     &     2.163  &         0.031        &      508.747    &     1.03e+04     \\\\\n",
       "\\textbf{9259.0}      &    6377.0626  &     1939.103     &     3.289  &         0.001        &     2576.071    &     1.02e+04     \\\\\n",
       "\\textbf{9293.0}      &    5066.9414  &     1894.447     &     2.675  &         0.007        &     1353.485    &     8780.398     \\\\\n",
       "\\textbf{9299.0}      &    -928.3173  &     1912.641     &    -0.485  &         0.627        &    -4677.437    &     2820.803     \\\\\n",
       "\\textbf{9308.0}      &   -1586.1615  &     1999.894     &    -0.793  &         0.428        &    -5506.313    &     2333.990     \\\\\n",
       "\\textbf{9311.0}      &   -1086.7601  &     3079.383     &    -0.353  &         0.724        &    -7122.905    &     4949.385     \\\\\n",
       "\\textbf{9313.0}      &   -2250.3692  &     1868.021     &    -1.205  &         0.228        &    -5912.026    &     1411.288     \\\\\n",
       "\\textbf{9325.0}      &    2906.9673  &     1933.000     &     1.504  &         0.133        &     -882.061    &     6695.996     \\\\\n",
       "\\textbf{9332.0}      &    1876.8362  &     1908.153     &     0.984  &         0.325        &    -1863.486    &     5617.159     \\\\\n",
       "\\textbf{9340.0}      &   -2.787e+04  &     3795.325     &    -7.343  &         0.000        &    -3.53e+04    &    -2.04e+04     \\\\\n",
       "\\textbf{9372.0}      &    4832.4677  &     2290.196     &     2.110  &         0.035        &      343.272    &     9321.664     \\\\\n",
       "\\textbf{9411.0}      &     638.3682  &     2006.770     &     0.318  &         0.750        &    -3295.263    &     4571.999     \\\\\n",
       "\\textbf{9459.0}      &   -3209.2322  &     2167.623     &    -1.481  &         0.139        &    -7458.163    &     1039.699     \\\\\n",
       "\\textbf{9465.0}      &    8727.5253  &     1945.950     &     4.485  &         0.000        &     4913.114    &     1.25e+04     \\\\\n",
       "\\textbf{9472.0}      &   -3357.0062  &     1885.295     &    -1.781  &         0.075        &    -7052.524    &      338.511     \\\\\n",
       "\\textbf{9483.0}      &   -9433.9635  &     2046.163     &    -4.611  &         0.000        &    -1.34e+04    &    -5423.116     \\\\\n",
       "\\textbf{9563.0}      &   -2.456e+04  &     3444.483     &    -7.131  &         0.000        &    -3.13e+04    &    -1.78e+04     \\\\\n",
       "\\textbf{9590.0}      &     893.6168  &     1923.595     &     0.465  &         0.642        &    -2876.975    &     4664.209     \\\\\n",
       "\\textbf{9598.0}      &   -3563.2028  &     2729.749     &    -1.305  &         0.192        &    -8914.003    &     1787.597     \\\\\n",
       "\\textbf{9599.0}      &   -4032.0335  &     1921.783     &    -2.098  &         0.036        &    -7799.073    &     -264.994     \\\\\n",
       "\\textbf{9602.0}      &   -4766.8112  &     3333.138     &    -1.430  &         0.153        &    -1.13e+04    &     1766.739     \\\\\n",
       "\\textbf{9619.0}      &    5174.2224  &     1938.394     &     2.669  &         0.008        &     1374.621    &     8973.823     \\\\\n",
       "\\textbf{9643.0}      &    -263.9671  &     1993.288     &    -0.132  &         0.895        &    -4171.171    &     3643.236     \\\\\n",
       "\\textbf{9650.0}      &   -1187.2024  &     2063.936     &    -0.575  &         0.565        &    -5232.888    &     2858.483     \\\\\n",
       "\\textbf{9653.0}      &   -1.397e+04  &     4527.328     &    -3.086  &         0.002        &    -2.28e+04    &    -5097.260     \\\\\n",
       "\\textbf{9667.0}      &    -877.7466  &     1992.241     &    -0.441  &         0.660        &    -4782.897    &     3027.404     \\\\\n",
       "\\textbf{9698.0}      &    2761.0191  &     1894.635     &     1.457  &         0.145        &     -952.807    &     6474.845     \\\\\n",
       "\\textbf{9699.0}      &    4007.7347  &     1875.265     &     2.137  &         0.033        &      331.878    &     7683.592     \\\\\n",
       "\\textbf{9719.0}      &   -6828.8306  &     1935.540     &    -3.528  &         0.000        &    -1.06e+04    &    -3034.825     \\\\\n",
       "\\textbf{9742.0}      &   -7685.7043  &     2015.237     &    -3.814  &         0.000        &    -1.16e+04    &    -3735.478     \\\\\n",
       "\\textbf{9761.0}      &    3747.0948  &     1890.350     &     1.982  &         0.047        &       41.669    &     7452.520     \\\\\n",
       "\\textbf{9771.0}      &   -5368.8020  &     1920.256     &    -2.796  &         0.005        &    -9132.850    &    -1604.754     \\\\\n",
       "\\textbf{9772.0}      &    5064.2150  &     1906.029     &     2.657  &         0.008        &     1328.056    &     8800.374     \\\\\n",
       "\\textbf{9778.0}      &    1030.4091  &     1892.700     &     0.544  &         0.586        &    -2679.624    &     4740.442     \\\\\n",
       "\\textbf{9799.0}      &   -7979.6142  &     2104.282     &    -3.792  &         0.000        &    -1.21e+04    &    -3854.842     \\\\\n",
       "\\textbf{9815.0}      &    3494.9073  &     1987.897     &     1.758  &         0.079        &     -401.728    &     7391.543     \\\\\n",
       "\\textbf{9818.0}      &    -3.02e+04  &     2085.706     &   -14.480  &         0.000        &    -3.43e+04    &    -2.61e+04     \\\\\n",
       "\\textbf{9837.0}      &    5988.0513  &     1929.137     &     3.104  &         0.002        &     2206.595    &     9769.508     \\\\\n",
       "\\textbf{9922.0}      &   -4997.0058  &     1912.519     &    -2.613  &         0.009        &    -8745.887    &    -1248.125     \\\\\n",
       "\\textbf{9954.0}      &   -2307.9747  &     2751.378     &    -0.839  &         0.402        &    -7701.171    &     3085.222     \\\\\n",
       "\\textbf{9963.0}      &    1439.8701  &     1900.249     &     0.758  &         0.449        &    -2284.959    &     5164.700     \\\\\n",
       "\\textbf{9988.0}      &    5676.8617  &     1932.396     &     2.938  &         0.003        &     1889.018    &     9464.705     \\\\\n",
       "\\textbf{9999.0}      &   -9230.0755  &     2004.514     &    -4.605  &         0.000        &    -1.32e+04    &    -5300.868     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 22714.975 & \\textbf{  Durbin-Watson:     } &       0.750    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 140074505.190  \\\\\n",
       "\\textbf{Skew:}          &   14.746  & \\textbf{  Prob(JB):          } &        0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  537.398  & \\textbf{  Cond. No.          } &    2.53e+17    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 1.36e-22. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.664\n",
       "Model:                            OLS   Adj. R-squared:                  0.641\n",
       "Method:                 Least Squares   F-statistic:                     28.98\n",
       "Date:                Mon, 14 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        16:18:31   Log-Likelihood:            -1.2192e+05\n",
       "No. Observations:               11736   AIC:                         2.454e+05\n",
       "Df Residuals:                   10985   BIC:                         2.509e+05\n",
       "Df Model:                         750                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const       -6271.8534    532.282    -11.783      0.000   -7315.222   -5228.485\n",
       "gspilltecIV     0.1630      0.028      5.768      0.000       0.108       0.218\n",
       "gspillsicIV     0.4887      0.052      9.386      0.000       0.387       0.591\n",
       "pat_count     -29.4376      1.715    -17.168      0.000     -32.799     -26.076\n",
       "rsales          1.0154      0.041     24.569      0.000       0.934       1.096\n",
       "rppent          0.5175      0.087      5.962      0.000       0.347       0.688\n",
       "emp            -2.5097      7.030     -0.357      0.721     -16.291      11.271\n",
       "rxrd            8.6216      0.669     12.878      0.000       7.309       9.934\n",
       "1981          762.8261    381.627      1.999      0.046      14.769    1510.883\n",
       "1982          883.7420    369.574      2.391      0.017     159.310    1608.174\n",
       "1983          821.5707    357.444      2.298      0.022     120.917    1522.225\n",
       "1984          310.9095    346.828      0.896      0.370    -368.935     990.754\n",
       "1985          112.7209    339.088      0.332      0.740    -551.953     777.395\n",
       "1986         -152.9965    330.737     -0.463      0.644    -801.300     495.307\n",
       "1987         -423.0556    324.228     -1.305      0.192   -1058.600     212.489\n",
       "1988         -777.3124    321.303     -2.419      0.016   -1407.124    -147.500\n",
       "1989         -661.1419    317.261     -2.084      0.037   -1283.031     -39.252\n",
       "1990        -1153.5785    313.432     -3.680      0.000   -1767.961    -539.196\n",
       "1991         -798.2414    312.074     -2.558      0.011   -1409.962    -186.521\n",
       "1992         -982.2983    312.534     -3.143      0.002   -1594.921    -369.676\n",
       "1993         -999.6691    313.254     -3.191      0.001   -1613.703    -385.635\n",
       "1994        -1323.3880    317.300     -4.171      0.000   -1945.352    -701.424\n",
       "1995         -816.2328    325.043     -2.511      0.012   -1453.375    -179.091\n",
       "1996         -732.9858    338.616     -2.165      0.030   -1396.733     -69.238\n",
       "1997         -349.2790    356.979     -0.978      0.328   -1049.022     350.464\n",
       "1998         -247.3364    381.255     -0.649      0.517    -994.665     499.993\n",
       "1999          253.8930    408.826      0.621      0.535    -547.480    1055.266\n",
       "10005.0      3097.6615   1880.996      1.647      0.100    -589.430    6784.753\n",
       "10006.0      2317.7560   2276.237      1.018      0.309   -2144.079    6779.591\n",
       "10008.0      1438.1417   1886.554      0.762      0.446   -2259.844    5136.127\n",
       "10016.0      2713.9470   1892.028      1.434      0.151    -994.769    6422.663\n",
       "10030.0      5086.2191   1901.323      2.675      0.007    1359.283    8813.155\n",
       "1004.0       4105.1769   1889.911      2.172      0.030     400.612    7809.742\n",
       "10056.0      1739.8066   1873.446      0.929      0.353   -1932.485    5412.099\n",
       "10085.0     -3584.2178   1887.069     -1.899      0.058   -7283.212     114.777\n",
       "10092.0      3969.6072   4695.384      0.845      0.398   -5234.191    1.32e+04\n",
       "10097.0     -3492.5868   1884.197     -1.854      0.064   -7185.951     200.778\n",
       "1010.0       2642.8898   4696.864      0.563      0.574   -6563.809    1.18e+04\n",
       "10109.0      6527.9249   1936.233      3.371      0.001    2732.560    1.03e+04\n",
       "10115.0      1516.7663   1875.072      0.809      0.419   -2158.713    5192.245\n",
       "10124.0      6692.8481   1941.452      3.447      0.001    2887.252    1.05e+04\n",
       "1013.0      -1796.1429   1888.737     -0.951      0.342   -5498.407    1906.121\n",
       "10150.0     -4038.5599   2457.273     -1.644      0.100   -8855.258     778.138\n",
       "10159.0     -4707.9285   3442.390     -1.368      0.171   -1.15e+04    2039.776\n",
       "10174.0      5329.6656   2134.048      2.497      0.013    1146.548    9512.784\n",
       "10185.0      2940.1622   2144.003      1.371      0.170   -1262.470    7142.794\n",
       "10195.0     -5356.9855   2061.047     -2.599      0.009   -9397.008   -1316.963\n",
       "10198.0      4996.0691   1899.773      2.630      0.009    1272.172    8719.966\n",
       "10215.0      6330.3022   1936.864      3.268      0.001    2533.701    1.01e+04\n",
       "10232.0       820.3515   2217.856      0.370      0.711   -3527.045    5167.748\n",
       "10236.0      4715.7756   1901.887      2.480      0.013     987.735    8443.816\n",
       "10286.0      2154.7388   1894.796      1.137      0.255   -1559.402    5868.880\n",
       "10301.0     -2.005e+04   2162.637     -9.272      0.000   -2.43e+04   -1.58e+04\n",
       "10312.0      3818.9932   1889.577      2.021      0.043     115.081    7522.905\n",
       "10332.0     -5177.8756   3440.106     -1.505      0.132   -1.19e+04    1565.351\n",
       "1036.0       1614.5984   1995.996      0.809      0.419   -2297.914    5527.110\n",
       "10374.0      2316.7998   1873.315      1.237      0.216   -1355.234    5988.834\n",
       "10386.0     -3228.6671   1955.239     -1.651      0.099   -7061.288     603.954\n",
       "10391.0     -5555.8122   2042.980     -2.719      0.007   -9560.420   -1551.204\n",
       "10407.0     -2355.5645   1884.123     -1.250      0.211   -6048.786    1337.656\n",
       "10420.0      3660.2121   1885.367      1.941      0.052     -35.446    7355.870\n",
       "10422.0       680.7273   1975.735      0.345      0.730   -3192.068    4553.523\n",
       "10426.0      5118.8058   2095.825      2.442      0.015    1010.612    9227.000\n",
       "10441.0      5608.1798   1917.312      2.925      0.003    1849.902    9366.457\n",
       "1045.0      -7472.1228   2056.588     -3.633      0.000   -1.15e+04   -3440.841\n",
       "10453.0     -2605.6534   1957.284     -1.331      0.183   -6442.283    1230.976\n",
       "10482.0     -2.479e+04   2419.308    -10.248      0.000   -2.95e+04   -2.01e+04\n",
       "10498.0      4204.1963   1895.074      2.218      0.027     489.511    7918.882\n",
       "10499.0     -4935.1607   2255.821     -2.188      0.029   -9356.975    -513.347\n",
       "10511.0      6215.7763   1983.684      3.133      0.002    2327.398    1.01e+04\n",
       "10519.0     -1.323e+04   2018.961     -6.553      0.000   -1.72e+04   -9272.681\n",
       "10530.0     -1388.9617   1962.277     -0.708      0.479   -5235.378    2457.455\n",
       "10537.0     -1032.5703   2297.680     -0.449      0.653   -5536.437    3471.296\n",
       "10540.0      1611.7732   1882.464      0.856      0.392   -2078.195    5301.741\n",
       "10541.0      3411.8443   1991.393      1.713      0.087    -491.644    7315.333\n",
       "10550.0      -123.4474   5749.638     -0.021      0.983   -1.14e+04    1.11e+04\n",
       "10553.0     -1206.3294   2063.080     -0.585      0.559   -5250.338    2837.679\n",
       "10565.0      5584.0522   1900.543      2.938      0.003    1858.647    9309.458\n",
       "10580.0      6007.0246   1903.516      3.156      0.002    2275.790    9738.259\n",
       "10581.0      2989.2197   1882.796      1.588      0.112    -701.399    6679.839\n",
       "10588.0     -7577.6388   1947.644     -3.891      0.000   -1.14e+04   -3759.906\n",
       "10597.0      3555.7350   1884.570      1.887      0.059    -138.361    7249.831\n",
       "10599.0      4227.1709   1892.182      2.234      0.026     518.154    7936.188\n",
       "10618.0      3204.7584   1882.393      1.702      0.089    -485.071    6894.588\n",
       "10656.0      2942.7785   1898.192      1.550      0.121    -778.019    6663.576\n",
       "10658.0      2796.6662   1900.522      1.472      0.141    -928.699    6522.031\n",
       "10726.0      7476.3004   2074.980      3.603      0.000    3408.967    1.15e+04\n",
       "10734.0      2444.8875   2583.273      0.946      0.344   -2618.793    7508.568\n",
       "10735.0      5373.4594   1920.030      2.799      0.005    1609.854    9137.065\n",
       "10764.0      5983.2672   1973.759      3.031      0.002    2114.344    9852.190\n",
       "10777.0      2812.0683   1899.421      1.480      0.139    -911.138    6535.275\n",
       "1078.0        211.7163   2175.428      0.097      0.922   -4052.514    4475.946\n",
       "10793.0      3739.5996   1898.088      1.970      0.049      19.005    7460.194\n",
       "10816.0      2308.6559   1884.980      1.225      0.221   -1386.244    6003.555\n",
       "10839.0      4188.3126   1888.746      2.218      0.027     486.030    7890.595\n",
       "10857.0     -8646.4572   2050.829     -4.216      0.000   -1.27e+04   -4626.463\n",
       "10867.0       559.7743   2178.114      0.257      0.797   -3709.721    4829.269\n",
       "10906.0      3564.9093   1883.462      1.893      0.058    -127.016    7256.834\n",
       "10950.0      3523.4088   3082.617      1.143      0.253   -2519.076    9565.893\n",
       "10983.0      -2.17e+04   2308.416     -9.400      0.000   -2.62e+04   -1.72e+04\n",
       "1099.0       2933.4044   1882.939      1.558      0.119    -757.496    6624.305\n",
       "10991.0      1081.1555   2524.920      0.428      0.669   -3868.143    6030.454\n",
       "11012.0      2228.5844   1939.011      1.149      0.250   -1572.227    6029.395\n",
       "11038.0     -3219.8794   1994.714     -1.614      0.107   -7129.878     690.119\n",
       "1104.0       4365.8342   1903.475      2.294      0.022     634.680    8096.988\n",
       "11060.0      4720.6397   1900.811      2.483      0.013     994.708    8446.572\n",
       "11094.0      3186.8529   1881.234      1.694      0.090    -500.705    6874.411\n",
       "11096.0       520.7405   1885.531      0.276      0.782   -3175.240    4216.721\n",
       "11113.0      4045.0552   2056.849      1.967      0.049      13.262    8076.849\n",
       "1115.0       1776.5387   1873.271      0.948      0.343   -1895.409    5448.487\n",
       "11161.0       205.1234   1871.942      0.110      0.913   -3464.220    3874.467\n",
       "11225.0      5502.7284   1916.367      2.871      0.004    1746.304    9259.152\n",
       "11228.0      4764.9719   1887.086      2.525      0.012    1065.943    8464.000\n",
       "11236.0     -2807.4697   3423.675     -0.820      0.412   -9518.488    3903.548\n",
       "11288.0     -6006.0413   2178.952     -2.756      0.006   -1.03e+04   -1734.904\n",
       "11312.0     -8833.2777   2022.181     -4.368      0.000   -1.28e+04   -4869.440\n",
       "11361.0      2151.7198   1916.982      1.122      0.262   -1605.909    5909.349\n",
       "11399.0     -4909.5155   1887.924     -2.600      0.009   -8610.187   -1208.844\n",
       "114303.0    -2.297e+04   5892.521     -3.898      0.000   -3.45e+04   -1.14e+04\n",
       "11456.0     -3081.4194   1983.440     -1.554      0.120   -6969.319     806.480\n",
       "11465.0      -345.4494   1880.824     -0.184      0.854   -4032.202    3341.303\n",
       "11502.0      4446.9242   1922.452      2.313      0.021     678.572    8215.276\n",
       "11506.0     -2952.4483   1975.979     -1.494      0.135   -6825.723     920.827\n",
       "11537.0      3677.2970   1888.017      1.948      0.051     -23.557    7378.151\n",
       "11566.0      6202.3365   1931.978      3.210      0.001    2415.312    9989.361\n",
       "11573.0      2202.9150   1892.068      1.164      0.244   -1505.880    5911.710\n",
       "11580.0     -2455.8304   2451.133     -1.002      0.316   -7260.493    2348.832\n",
       "11600.0      5045.6218   1907.963      2.645      0.008    1305.670    8785.573\n",
       "11609.0      8014.9787   1901.563      4.215      0.000    4287.574    1.17e+04\n",
       "1161.0      -6558.7605   2019.911     -3.247      0.001   -1.05e+04   -2599.371\n",
       "11636.0     -9542.3217   2056.890     -4.639      0.000   -1.36e+04   -5510.447\n",
       "11670.0      5933.0902   1938.826      3.060      0.002    2132.642    9733.538\n",
       "11678.0     -8859.2408   2033.636     -4.356      0.000   -1.28e+04   -4872.948\n",
       "11682.0      1751.7498   2031.089      0.862      0.388   -2229.550    5733.049\n",
       "11694.0      4499.7539   2058.475      2.186      0.029     464.772    8534.736\n",
       "11720.0     -5780.8803   3386.028     -1.707      0.088   -1.24e+04     856.344\n",
       "11721.0     -1.427e+04   2558.449     -5.576      0.000   -1.93e+04   -9250.679\n",
       "11722.0      1914.7910   2191.834      0.874      0.382   -2381.599    6211.181\n",
       "11793.0      -794.1008   5806.390     -0.137      0.891   -1.22e+04    1.06e+04\n",
       "11797.0      6483.0051   2313.539      2.802      0.005    1948.052     1.1e+04\n",
       "11914.0      4039.2914   2731.204      1.479      0.139   -1314.360    9392.943\n",
       "1209.0       1198.7045   1896.415      0.632      0.527   -2518.610    4916.019\n",
       "12136.0     -1.807e+04   2537.008     -7.121      0.000    -2.3e+04   -1.31e+04\n",
       "12141.0      5.987e+04   2260.555     26.485      0.000    5.54e+04    6.43e+04\n",
       "12181.0      -506.9822   3335.896     -0.152      0.879   -7045.938    6031.974\n",
       "12215.0     -9251.7575   2280.522     -4.057      0.000   -1.37e+04   -4781.523\n",
       "12216.0     -6362.1395   2337.764     -2.721      0.007   -1.09e+04   -1779.701\n",
       "12256.0     -4470.9142   2200.236     -2.032      0.042   -8783.772    -158.056\n",
       "12262.0      4591.1516   2227.440      2.061      0.039     224.968    8957.336\n",
       "12389.0      1427.6404   2271.244      0.629      0.530   -3024.407    5879.688\n",
       "1239.0         88.1293   1867.488      0.047      0.962   -3572.484    3748.743\n",
       "12390.0      1364.7553   2578.535      0.529      0.597   -3689.638    6419.149\n",
       "12397.0      -566.7502   4696.797     -0.121      0.904   -9773.318    8639.818\n",
       "1243.0        110.7821   1928.931      0.057      0.954   -3670.270    3891.834\n",
       "12548.0      3456.8019   2467.873      1.401      0.161   -1380.672    8294.276\n",
       "12570.0      3965.7460   2282.077      1.738      0.082    -507.537    8439.029\n",
       "12581.0      -127.9446   2482.050     -0.052      0.959   -4993.209    4737.320\n",
       "12592.0      1919.1874   2186.990      0.878      0.380   -2367.707    6206.081\n",
       "12604.0       884.6439   4704.257      0.188      0.851   -8336.546    1.01e+04\n",
       "12656.0      6304.3213   2248.139      2.804      0.005    1897.564    1.07e+04\n",
       "12679.0     -1.712e+04   2529.614     -6.766      0.000   -2.21e+04   -1.22e+04\n",
       "1278.0       5220.0849   1972.052      2.647      0.008    1354.509    9085.661\n",
       "12788.0      -1.16e+04   2814.360     -4.120      0.000   -1.71e+04   -6078.871\n",
       "1283.0       4990.9330   1942.406      2.569      0.010    1183.468    8798.398\n",
       "1297.0       3304.1604   1881.880      1.756      0.079    -384.664    6992.985\n",
       "12992.0      5487.1519   2296.375      2.389      0.017     985.843    9988.461\n",
       "13135.0      -141.3897   2348.886     -0.060      0.952   -4745.630    4462.850\n",
       "1327.0      -5147.0947   2077.474     -2.478      0.013   -9219.318   -1074.871\n",
       "13282.0     -3791.2207   4697.960     -0.807      0.420    -1.3e+04    5417.625\n",
       "1334.0      -6146.9690   2353.305     -2.612      0.009   -1.08e+04   -1534.068\n",
       "13351.0     -2598.5396   3116.002     -0.834      0.404   -8706.465    3509.385\n",
       "13365.0     -2.024e+04   2805.355     -7.216      0.000   -2.57e+04   -1.47e+04\n",
       "13369.0      -292.9437   2344.123     -0.125      0.901   -4887.847    4301.959\n",
       "13406.0      4288.2331   2274.826      1.885      0.059    -170.836    8747.302\n",
       "13407.0     -4098.7445   2274.930     -1.802      0.072   -8558.018     360.529\n",
       "13417.0      5903.7335   2404.934      2.455      0.014    1189.629    1.06e+04\n",
       "13525.0     -4312.5560   2504.596     -1.722      0.085   -9222.014     596.902\n",
       "13554.0      6564.1270   2414.808      2.718      0.007    1830.669    1.13e+04\n",
       "1359.0      -1.002e+04   2336.561     -4.290      0.000   -1.46e+04   -5442.891\n",
       "13623.0      1928.0302   2357.844      0.818      0.414   -2693.769    6549.830\n",
       "1372.0      -4788.4202   1992.772     -2.403      0.016   -8694.612    -882.229\n",
       "1380.0      -5058.1898   1924.186     -2.629      0.009   -8829.941   -1286.438\n",
       "13923.0      3305.2508   2584.883      1.279      0.201   -1761.586    8372.087\n",
       "13932.0      5461.6883   3357.406      1.627      0.104   -1119.432     1.2e+04\n",
       "13941.0     -1.084e+04   2488.694     -4.355      0.000   -1.57e+04   -5960.052\n",
       "1397.0       1006.3612   2191.304      0.459      0.646   -3288.990    5301.712\n",
       "14064.0      2110.4264   2286.866      0.923      0.356   -2372.243    6593.096\n",
       "14084.0      1052.9005   2326.922      0.452      0.651   -3508.286    5614.087\n",
       "14324.0     -6713.4106   2461.951     -2.727      0.006   -1.15e+04   -1887.543\n",
       "14462.0      -815.5019   2463.229     -0.331      0.741   -5643.875    4012.871\n",
       "1447.0       2804.6111   4092.214      0.685      0.493   -5216.864    1.08e+04\n",
       "14593.0      5029.9668   2495.067      2.016      0.044     139.187    9920.746\n",
       "14622.0      2075.1080   8133.047      0.255      0.799   -1.39e+04     1.8e+04\n",
       "1465.0       3135.8476   2599.375      1.206      0.228   -1959.395    8231.090\n",
       "1468.0       5597.0788   2478.743      2.258      0.024     738.296    1.05e+04\n",
       "14897.0      3312.9826   4701.123      0.705      0.481   -5902.065    1.25e+04\n",
       "14954.0      4088.8308   2479.191      1.649      0.099    -770.831    8948.492\n",
       "1496.0       5305.6897   1905.409      2.785      0.005    1570.745    9040.634\n",
       "15267.0      2872.0224   2461.569      1.167      0.243   -1953.095    7697.140\n",
       "15354.0     -2197.3109   2585.502     -0.850      0.395   -7265.360    2870.738\n",
       "1542.0       2255.5097   1879.439      1.200      0.230   -1428.530    5939.549\n",
       "15459.0     -2753.9887   2704.343     -1.018      0.309   -8054.988    2547.011\n",
       "1554.0       5170.2956   1903.709      2.716      0.007    1438.683    8901.908\n",
       "15708.0     -2.216e+04   3042.021     -7.286      0.000   -2.81e+04   -1.62e+04\n",
       "15711.0      1047.2724   2467.055      0.425      0.671   -3788.600    5883.145\n",
       "15761.0      4394.1636   2892.305      1.519      0.129   -1275.275    1.01e+04\n",
       "1581.0      -1.197e+04   3230.716     -3.705      0.000   -1.83e+04   -5635.823\n",
       "1593.0       1150.9509   1903.069      0.605      0.545   -2579.407    4881.309\n",
       "1602.0       8009.7727   2062.475      3.884      0.000    3966.951    1.21e+04\n",
       "1613.0       4464.8770   1899.969      2.350      0.019     740.596    8189.158\n",
       "16188.0      -473.4188   2720.089     -0.174      0.862   -5805.283    4858.446\n",
       "1632.0      -5408.3889   1921.430     -2.815      0.005   -9174.738   -1642.040\n",
       "1633.0       1999.1696   1874.500      1.067      0.286   -1675.187    5673.527\n",
       "1635.0      -1.241e+04   2289.758     -5.419      0.000   -1.69e+04   -7918.747\n",
       "16401.0     -1.141e+04   2782.711     -4.099      0.000   -1.69e+04   -5952.839\n",
       "16437.0     -6651.5417   2934.612     -2.267      0.023   -1.24e+04    -899.174\n",
       "1651.0      -1267.5784   1873.117     -0.677      0.499   -4939.225    2404.069\n",
       "1655.0       4617.1754   1892.086      2.440      0.015     908.347    8326.004\n",
       "1663.0       5917.8126   1934.732      3.059      0.002    2125.389    9710.236\n",
       "16710.0     -2142.6639   2713.492     -0.790      0.430   -7461.597    3176.269\n",
       "16729.0     -2843.3099   2581.325     -1.101      0.271   -7903.171    2216.551\n",
       "1690.0      -1.609e+04   2040.450     -7.885      0.000   -2.01e+04   -1.21e+04\n",
       "1703.0       1038.3302   1938.531      0.536      0.592   -2761.540    4838.201\n",
       "17202.0      3126.0374   2596.217      1.204      0.229   -1963.015    8215.089\n",
       "1722.0       1818.2271   1899.530      0.957      0.338   -1905.194    5541.649\n",
       "1728.0       4811.4862   1900.383      2.532      0.011    1086.393    8536.579\n",
       "1743.0       4688.6298   3113.418      1.506      0.132   -1414.230    1.08e+04\n",
       "1754.0       3868.9635   1989.197      1.945      0.052     -30.220    7768.147\n",
       "1762.0      -3198.3213   2107.779     -1.517      0.129   -7329.947     933.305\n",
       "1773.0       4715.3642   1941.975      2.428      0.015     908.743    8521.986\n",
       "1786.0      -1.351e+04   2088.466     -6.467      0.000   -1.76e+04   -9411.312\n",
       "18100.0      1696.6514   2583.745      0.657      0.511   -3367.953    6761.256\n",
       "1820.0        928.9560   1897.774      0.489      0.624   -2791.023    4648.935\n",
       "1848.0      -9134.3632   2467.813     -3.701      0.000    -1.4e+04   -4297.007\n",
       "18654.0      4455.5629   3661.166      1.217      0.224   -2720.982    1.16e+04\n",
       "1875.0       -492.4923   4068.535     -0.121      0.904   -8467.552    7482.568\n",
       "1884.0       4544.4415   2017.943      2.252      0.024     588.911    8499.972\n",
       "1913.0      -2060.4339   1984.365     -1.038      0.299   -5950.147    1829.279\n",
       "1919.0       3267.3750   1996.792      1.636      0.102    -646.696    7181.446\n",
       "1920.0      -1552.3608   1906.690     -0.814      0.416   -5289.817    2185.095\n",
       "1968.0       1437.4441   1910.381      0.752      0.452   -2307.247    5182.135\n",
       "1976.0       5411.2870   1885.935      2.869      0.004    1714.515    9108.059\n",
       "1981.0       4053.1417   1891.695      2.143      0.032     345.080    7761.203\n",
       "1988.0      -9269.3673   3298.509     -2.810      0.005   -1.57e+04   -2803.696\n",
       "1992.0       1536.3298   1931.166      0.796      0.426   -2249.102    5321.762\n",
       "2008.0       2131.2548   1883.802      1.131      0.258   -1561.337    5823.846\n",
       "2033.0       4099.7529   2474.961      1.656      0.098    -751.616    8951.122\n",
       "2044.0       1597.1970   1868.781      0.855      0.393   -2065.949    5260.343\n",
       "2049.0       2075.1150   1882.009      1.103      0.270   -1613.962    5764.192\n",
       "2061.0       6198.3376   1930.235      3.211      0.001    2414.730    9981.945\n",
       "20779.0      2.915e+04   2731.987     10.671      0.000    2.38e+04    3.45e+04\n",
       "2085.0      -1.242e+04   2176.183     -5.706      0.000   -1.67e+04   -8151.426\n",
       "2086.0       1721.6545   1887.294      0.912      0.362   -1977.782    5421.091\n",
       "2111.0       1959.3626   1870.848      1.047      0.295   -1707.836    5626.561\n",
       "21204.0      -618.7069   2726.033     -0.227      0.820   -5962.222    4724.808\n",
       "21238.0      4677.4226   2626.157      1.781      0.075    -470.317    9825.163\n",
       "2124.0       1676.4498   2015.720      0.832      0.406   -2274.724    5627.623\n",
       "2146.0       1.113e+04   3081.311      3.611      0.000    5085.407    1.72e+04\n",
       "21496.0     -2.285e+04   3150.156     -7.254      0.000    -2.9e+04   -1.67e+04\n",
       "2154.0       3159.7736   1879.231      1.681      0.093    -523.858    6843.405\n",
       "2176.0       3.556e+04   2586.631     13.749      0.000    3.05e+04    4.06e+04\n",
       "2188.0       5969.6054   1973.136      3.025      0.002    2101.903    9837.308\n",
       "2189.0      -4042.4772   1940.943     -2.083      0.037   -7847.075    -237.879\n",
       "2220.0       2682.7886   1888.591      1.421      0.155   -1019.189    6384.766\n",
       "22205.0      6567.2487   2635.889      2.491      0.013    1400.431    1.17e+04\n",
       "2226.0        443.9865   5746.731      0.077      0.938   -1.08e+04    1.17e+04\n",
       "2230.0       3892.7923   2219.233      1.754      0.079    -457.304    8242.889\n",
       "22325.0     -7958.3788   2774.205     -2.869      0.004   -1.34e+04   -2520.437\n",
       "2255.0       2326.0620   1885.338      1.234      0.217   -1369.541    6021.665\n",
       "22619.0      4176.8950   2593.046      1.611      0.107    -905.942    9259.732\n",
       "2267.0      -7591.5514   1959.471     -3.874      0.000   -1.14e+04   -3750.635\n",
       "22815.0     -1903.9381   2578.726     -0.738      0.460   -6958.706    3150.829\n",
       "2285.0      -1.613e+04   2116.014     -7.622      0.000   -2.03e+04    -1.2e+04\n",
       "2290.0      -1662.6714   1886.111     -0.882      0.378   -5359.787    2034.445\n",
       "2295.0       5170.2469   3340.562      1.548      0.122   -1377.855    1.17e+04\n",
       "2316.0      -3191.1118   2131.966     -1.497      0.134   -7370.148     987.924\n",
       "23220.0      1834.4738   2725.102      0.673      0.501   -3507.216    7176.164\n",
       "23224.0     -1.052e+04   3152.490     -3.338      0.001   -1.67e+04   -4344.288\n",
       "2343.0      -8659.6696   4202.820     -2.060      0.039   -1.69e+04    -421.387\n",
       "2352.0        484.3946   2190.931      0.221      0.825   -3810.224    4779.013\n",
       "23700.0     -1.653e+04   3592.553     -4.601      0.000   -2.36e+04   -9486.301\n",
       "2390.0       5358.6897   1907.486      2.809      0.005    1619.674    9097.705\n",
       "2393.0      -1986.4490   2016.237     -0.985      0.325   -5938.636    1965.738\n",
       "2403.0        1.16e+04   2184.113      5.311      0.000    7319.280    1.59e+04\n",
       "2435.0       7423.7475   1930.621      3.845      0.000    3639.384    1.12e+04\n",
       "2444.0       -332.1383   1947.184     -0.171      0.865   -4148.970    3484.694\n",
       "2448.0       1652.2575   1871.029      0.883      0.377   -2015.297    5319.812\n",
       "2469.0       4578.3084   3665.158      1.249      0.212   -2606.061    1.18e+04\n",
       "24720.0      2861.8215   2898.198      0.987      0.323   -2819.169    8542.812\n",
       "24800.0     -1.315e+04   3182.313     -4.132      0.000   -1.94e+04   -6911.698\n",
       "2482.0       5574.0696   1912.008      2.915      0.004    1826.189    9321.950\n",
       "24969.0      5318.9119   3105.737      1.713      0.087    -768.891    1.14e+04\n",
       "2498.0      -5529.7674   2025.889     -2.730      0.006   -9500.874   -1558.661\n",
       "2504.0      -1.051e+04   2309.623     -4.549      0.000    -1.5e+04   -5978.127\n",
       "2508.0       4352.4929   2124.037      2.049      0.040     188.999    8515.987\n",
       "25124.0      3954.1172   2893.904      1.366      0.172   -1718.455    9626.689\n",
       "2518.0       4373.0373   1893.929      2.309      0.021     660.595    8085.479\n",
       "25224.0      3503.0947   8133.263      0.431      0.667   -1.24e+04    1.94e+04\n",
       "25279.0      3759.6474   2882.452      1.304      0.192   -1890.476    9409.771\n",
       "2537.0      -1.093e+04   2185.521     -5.001      0.000   -1.52e+04   -6644.813\n",
       "2538.0       5753.7139   2919.607      1.971      0.049      30.758    1.15e+04\n",
       "25389.0      5048.6253   4702.360      1.074      0.283   -4168.846    1.43e+04\n",
       "2547.0      -8546.1978   2151.373     -3.972      0.000   -1.28e+04   -4329.120\n",
       "2553.0       3908.1790   1894.826      2.063      0.039     193.979    7622.379\n",
       "2574.0      -2007.5654   2519.353     -0.797      0.426   -6945.951    2930.820\n",
       "25747.0      3481.3869   3088.153      1.127      0.260   -2571.948    9534.722\n",
       "2577.0       1230.9401   1891.180      0.651      0.515   -2476.113    4937.993\n",
       "2593.0       1702.4949   1892.650      0.900      0.368   -2007.440    5412.430\n",
       "2596.0       1272.6585   1963.208      0.648      0.517   -2575.582    5120.899\n",
       "2663.0       7372.6205   1892.581      3.896      0.000    3662.821    1.11e+04\n",
       "2771.0       1438.7965   1948.563      0.738      0.460   -2380.739    5258.331\n",
       "2787.0       3769.5931   1886.511      1.998      0.046      71.692    7467.495\n",
       "2797.0      -1.143e+04   2040.584     -5.603      0.000   -1.54e+04   -7433.671\n",
       "2802.0       5089.1314   1905.976      2.670      0.008    1353.075    8825.188\n",
       "2817.0      -7331.9480   2190.234     -3.348      0.001   -1.16e+04   -3038.696\n",
       "28678.0     -1.914e+04   3341.098     -5.729      0.000   -2.57e+04   -1.26e+04\n",
       "28701.0      2209.6861   1882.569      1.174      0.241   -1480.488    5899.860\n",
       "28742.0     -1.405e+04   3303.995     -4.252      0.000   -2.05e+04   -7571.157\n",
       "2888.0       2393.1235   2130.471      1.123      0.261   -1782.982    6569.229\n",
       "2897.0       4097.7828   2721.613      1.506      0.132   -1237.069    9432.635\n",
       "2917.0      -1681.6497   1926.097     -0.873      0.383   -5457.146    2093.847\n",
       "29392.0     -1.319e+04   3481.781     -3.789      0.000      -2e+04   -6366.693\n",
       "2950.0      -2.204e+04   3579.193     -6.159      0.000   -2.91e+04    -1.5e+04\n",
       "2951.0       5925.8278   2397.776      2.471      0.013    1225.755    1.06e+04\n",
       "2953.0       3143.7179   1879.256      1.673      0.094    -539.962    6827.397\n",
       "2960.0        560.6943   2901.686      0.193      0.847   -5127.132    6248.521\n",
       "2975.0      -3739.0166   1889.827     -1.978      0.048   -7443.418     -34.615\n",
       "2982.0       3217.3556   1887.883      1.704      0.088    -483.236    6917.947\n",
       "2991.0      -1.229e+04   2679.152     -4.589      0.000   -1.75e+04   -7042.115\n",
       "3011.0      -7388.6022   2246.838     -3.288      0.001   -1.18e+04   -2984.395\n",
       "3015.0       5754.9913   1915.818      3.004      0.003    1999.643    9510.339\n",
       "3026.0       2605.6552   1927.441      1.352      0.176   -1172.475    6383.786\n",
       "3031.0      -1.921e+04   3282.864     -5.850      0.000   -2.56e+04   -1.28e+04\n",
       "3062.0       4147.8770   2008.866      2.065      0.039     210.137    8085.617\n",
       "3093.0      -3190.9219   2246.715     -1.420      0.156   -7594.888    1213.044\n",
       "3107.0       2356.5688   3657.437      0.644      0.519   -4812.666    9525.804\n",
       "3121.0       5429.2066   1883.668      2.882      0.004    1736.879    9121.534\n",
       "3126.0       3032.2169   1894.223      1.601      0.109    -680.801    6745.235\n",
       "3144.0       5.164e+04   1917.758     26.928      0.000    4.79e+04    5.54e+04\n",
       "3156.0       3460.1219   2366.002      1.462      0.144   -1177.668    8097.912\n",
       "3157.0       3116.6330   1879.878      1.658      0.097    -568.266    6801.532\n",
       "3170.0       4337.1510   1876.725      2.311      0.021     658.432    8015.870\n",
       "3178.0      -2281.1656   2127.841     -1.072      0.284   -6452.116    1889.785\n",
       "3206.0       -835.9173   2130.537     -0.392      0.695   -5012.154    3340.319\n",
       "3229.0       1511.2721   2026.724      0.746      0.456   -2461.471    5484.015\n",
       "3235.0       2668.5172   2060.752      1.295      0.195   -1370.928    6707.962\n",
       "3246.0       3852.1762   1902.254      2.025      0.043     123.416    7580.937\n",
       "3248.0       3155.9190   1888.647      1.671      0.095    -546.168    6858.006\n",
       "3282.0      -2.072e+04   2258.740     -9.173      0.000   -2.51e+04   -1.63e+04\n",
       "3362.0      -5818.3772   2210.720     -2.632      0.009   -1.02e+04   -1484.968\n",
       "3372.0       3265.0074   2459.861      1.327      0.184   -1556.764    8086.778\n",
       "3422.0       1994.0750   1882.429      1.059      0.289   -1695.825    5683.975\n",
       "3497.0      -4224.6553   2026.781     -2.084      0.037   -8197.511    -251.800\n",
       "3502.0      -2333.9904   1879.165     -1.242      0.214   -6017.493    1349.512\n",
       "3504.0       1014.3769   2733.802      0.371      0.711   -4344.366    6373.120\n",
       "3505.0        188.2776   1929.690      0.098      0.922   -3594.262    3970.818\n",
       "3532.0       4457.5709   1874.420      2.378      0.017     783.371    8131.771\n",
       "3574.0       4428.8413   4073.143      1.087      0.277   -3555.252    1.24e+04\n",
       "3580.0       -732.3446   1868.067     -0.392      0.695   -4394.091    2929.402\n",
       "3612.0       6232.2652   1936.461      3.218      0.001    2436.453       1e+04\n",
       "3619.0       2857.3739   1890.238      1.512      0.131    -847.832    6562.580\n",
       "3622.0       5362.2778   1956.066      2.741      0.006    1528.037    9196.518\n",
       "3639.0      -8258.7073   1962.105     -4.209      0.000   -1.21e+04   -4412.629\n",
       "3650.0      -6335.2032   2022.153     -3.133      0.002   -1.03e+04   -2371.419\n",
       "3662.0       -489.0604   2008.441     -0.244      0.808   -4425.967    3447.846\n",
       "3734.0      -1.215e+04   2003.453     -6.065      0.000   -1.61e+04   -8223.688\n",
       "3735.0      -1489.2333   2513.791     -0.592      0.554   -6416.717    3438.250\n",
       "3761.0      -2595.1828   1923.358     -1.349      0.177   -6365.310    1174.944\n",
       "3779.0      -1.142e+04   2373.660     -4.811      0.000   -1.61e+04   -6767.446\n",
       "3781.0      -4822.9571   2489.769     -1.937      0.053   -9703.352      57.438\n",
       "3782.0       -1.12e+04   2076.298     -5.395      0.000   -1.53e+04   -7131.984\n",
       "3786.0       2464.5681   1884.029      1.308      0.191   -1228.468    6157.604\n",
       "3796.0      -1.186e+04   2457.404     -4.826      0.000   -1.67e+04   -7042.753\n",
       "3821.0       4451.3701   1907.381      2.334      0.020     712.561    8190.180\n",
       "3835.0      -1036.0352   1895.076     -0.547      0.585   -4750.725    2678.655\n",
       "3839.0      -2798.6990   2826.112     -0.990      0.322   -8338.388    2740.990\n",
       "3840.0      -4538.1360   2000.831     -2.268      0.023   -8460.125    -616.147\n",
       "3895.0       4250.9880   1891.959      2.247      0.025     542.408    7959.568\n",
       "3908.0       -172.6957   3113.710     -0.055      0.956   -6276.127    5930.736\n",
       "3911.0      -3700.5847   1976.063     -1.873      0.061   -7574.024     172.855\n",
       "3917.0       4642.4080   2004.110      2.316      0.021     713.992    8570.824\n",
       "3946.0       5759.4857   1915.542      3.007      0.003    2004.679    9514.293\n",
       "3971.0       3284.6978   1987.071      1.653      0.098    -610.318    7179.714\n",
       "3980.0        1.19e+04   1930.180      6.167      0.000    8119.077    1.57e+04\n",
       "4034.0      -1117.7330   1921.797     -0.582      0.561   -4884.800    2649.334\n",
       "4036.0       5218.1397   1898.819      2.748      0.006    1496.113    8940.167\n",
       "4040.0      -2524.2024   1940.344     -1.301      0.193   -6327.626    1279.221\n",
       "4058.0       3295.9047   1871.494      1.761      0.078    -372.560    6964.369\n",
       "4060.0      -9060.2877   2100.872     -4.313      0.000   -1.32e+04   -4942.201\n",
       "4062.0       8033.8976   1929.372      4.164      0.000    4251.980    1.18e+04\n",
       "4077.0       1920.6403   3323.066      0.578      0.563   -4593.168    8434.448\n",
       "4087.0      -1.593e+04   2564.847     -6.211      0.000    -2.1e+04   -1.09e+04\n",
       "4091.0       1843.7623   2458.852      0.750      0.453   -2976.030    6663.555\n",
       "4127.0      -3593.3345   1887.749     -1.904      0.057   -7293.662     106.993\n",
       "4138.0       5016.9613   2591.186      1.936      0.053     -62.230    1.01e+04\n",
       "4162.0       2955.8533   2457.595      1.203      0.229   -1861.475    7773.181\n",
       "4186.0       5447.8893   1907.622      2.856      0.004    1708.608    9187.171\n",
       "4194.0       5426.4492   2265.829      2.395      0.017     985.018    9867.881\n",
       "4199.0      -1.103e+04   2145.369     -5.140      0.000   -1.52e+04   -6821.182\n",
       "4213.0       3301.5102   1884.484      1.752      0.080    -392.418    6995.439\n",
       "4222.0      -9874.4637   2047.650     -4.822      0.000   -1.39e+04   -5860.701\n",
       "4223.0       3163.6636   1883.366      1.680      0.093    -528.074    6855.401\n",
       "4251.0       4919.3231   1900.807      2.588      0.010    1193.399    8645.247\n",
       "4265.0       2279.9522   2110.236      1.080      0.280   -1856.491    6416.395\n",
       "4274.0       1286.5416   2011.000      0.640      0.522   -2655.381    5228.464\n",
       "4321.0       1858.7087   2025.498      0.918      0.359   -2111.633    5829.050\n",
       "4335.0       1245.7250   3327.189      0.374      0.708   -5276.164    7767.614\n",
       "4340.0       -473.9774   1923.678     -0.246      0.805   -4244.732    3296.777\n",
       "4371.0        799.5226   1953.912      0.409      0.682   -3030.496    4629.541\n",
       "4415.0       3959.4555   1998.635      1.981      0.048      41.772    7877.139\n",
       "4450.0       2171.8836   1890.235      1.149      0.251   -1533.317    5877.085\n",
       "4476.0      -8255.6752   2183.381     -3.781      0.000   -1.25e+04   -3975.855\n",
       "4510.0      -3304.0404   1990.996     -1.659      0.097   -7206.751     598.670\n",
       "4520.0       1904.8450   1924.315      0.990      0.322   -1867.158    5676.848\n",
       "4551.0       2165.8909   8126.759      0.267      0.790   -1.38e+04    1.81e+04\n",
       "4568.0       3235.1016   1999.182      1.618      0.106    -683.654    7153.858\n",
       "4579.0       6033.0314   1935.629      3.117      0.002    2238.849    9827.213\n",
       "4585.0       5120.6568   1905.415      2.687      0.007    1385.700    8855.613\n",
       "4595.0        557.2426   1868.586      0.298      0.766   -3105.522    4220.007\n",
       "4600.0      -8057.7539   2111.057     -3.817      0.000   -1.22e+04   -3919.703\n",
       "4607.0       4916.5681   1901.960      2.585      0.010    1188.385    8644.751\n",
       "4608.0      -6424.5107   2021.872     -3.178      0.001   -1.04e+04   -2461.279\n",
       "4622.0      -2091.5853   1985.279     -1.054      0.292   -5983.090    1799.919\n",
       "4623.0       3731.0165   1941.283      1.922      0.055     -74.247    7536.280\n",
       "4768.0       2815.4626   1887.217      1.492      0.136    -883.822    6514.747\n",
       "4771.0       5420.4120   1910.857      2.837      0.005    1674.788    9166.036\n",
       "4800.0       3660.7730   1920.037      1.907      0.057    -102.845    7424.391\n",
       "4802.0       4667.2138   1895.196      2.463      0.014     952.289    8382.138\n",
       "4807.0       4039.1996   1940.071      2.082      0.037     236.311    7842.088\n",
       "4839.0      -1.085e+05   3590.235    -30.217      0.000   -1.16e+05   -1.01e+05\n",
       "4843.0       -1.01e+04   2350.003     -4.299      0.000   -1.47e+04   -5496.966\n",
       "4881.0       1095.9244   1892.671      0.579      0.563   -2614.052    4805.901\n",
       "4900.0       2263.7762   1887.300      1.199      0.230   -1435.671    5963.224\n",
       "4926.0        783.9623   1907.591      0.411      0.681   -2955.259    4523.183\n",
       "4941.0       2265.5152   1886.613      1.201      0.230   -1432.585    5963.616\n",
       "4961.0      -1.139e+04   3058.833     -3.723      0.000   -1.74e+04   -5392.161\n",
       "4988.0       1.069e+04   1904.454      5.614      0.000    6958.725    1.44e+04\n",
       "4993.0       6337.7995   1935.443      3.275      0.001    2543.983    1.01e+04\n",
       "5018.0      -4590.1084   2122.137     -2.163      0.031   -8749.879    -430.337\n",
       "5020.0      -8094.2878   2272.709     -3.562      0.000   -1.25e+04   -3639.369\n",
       "5027.0      -1779.6929   1877.645     -0.948      0.343   -5460.216    1900.830\n",
       "5032.0       3546.3590   1886.846      1.880      0.060    -152.198    7244.916\n",
       "5043.0       -558.5515   1868.774     -0.299      0.765   -4221.684    3104.581\n",
       "5046.0      -1.137e+04   2090.169     -5.441      0.000   -1.55e+04   -7274.949\n",
       "5047.0       3.313e+04   3130.737     10.582      0.000     2.7e+04    3.93e+04\n",
       "5065.0       4633.1839   2276.152      2.036      0.042     171.516    9094.852\n",
       "5071.0       4698.1594   2312.439      2.032      0.042     165.362    9230.957\n",
       "5073.0      -1.588e+05   5755.681    -27.590      0.000    -1.7e+05   -1.48e+05\n",
       "5087.0       -953.2594   1865.655     -0.511      0.609   -4610.279    2703.760\n",
       "5109.0       5310.5607   1910.748      2.779      0.005    1565.151    9055.971\n",
       "5116.0      -8979.5137   2287.952     -3.925      0.000   -1.35e+04   -4494.716\n",
       "5122.0      -1763.8665   1879.130     -0.939      0.348   -5447.300    1919.567\n",
       "5134.0      -5338.2348   1962.337     -2.720      0.007   -9184.768   -1491.702\n",
       "5142.0        745.1892   2713.019      0.275      0.784   -4572.816    6063.194\n",
       "5165.0       1267.9682   2338.619      0.542      0.588   -3316.146    5852.082\n",
       "5169.0       1.422e+04   1883.672      7.550      0.000    1.05e+04    1.79e+04\n",
       "5174.0        542.5111   2261.694      0.240      0.810   -3890.815    4975.837\n",
       "5179.0       4365.6893   1893.161      2.306      0.021     654.753    8076.625\n",
       "5181.0       5457.5375   1955.951      2.790      0.005    1623.521    9291.554\n",
       "5187.0       5406.9499   2379.839      2.272      0.023     742.037    1.01e+04\n",
       "5229.0      -5137.8977   1943.679     -2.643      0.008   -8947.858   -1327.937\n",
       "5234.0      -7120.7766   2035.355     -3.499      0.000   -1.11e+04   -3131.115\n",
       "5237.0       3617.3288   1884.658      1.919      0.055     -76.940    7311.598\n",
       "5252.0       2423.9649   1882.094      1.288      0.198   -1265.278    6113.208\n",
       "5254.0       2636.1828   1888.007      1.396      0.163   -1064.650    6337.015\n",
       "5306.0       -370.0230   2018.458     -0.183      0.855   -4326.563    3586.517\n",
       "5338.0       4023.7011   1886.119      2.133      0.033     326.568    7720.835\n",
       "5377.0       5523.7227   1916.870      2.882      0.004    1766.312    9281.133\n",
       "5439.0       3337.8511   1912.468      1.745      0.081    -410.929    7086.632\n",
       "5456.0       6233.8984   1935.267      3.221      0.001    2440.426       1e+04\n",
       "5464.0       2087.6910   2469.127      0.846      0.398   -2752.241    6927.623\n",
       "5476.0       5725.0042   1937.789      2.954      0.003    1926.588    9523.420\n",
       "5492.0      -1.848e+04   2271.698     -8.135      0.000   -2.29e+04    -1.4e+04\n",
       "5496.0       2884.6863   1878.406      1.536      0.125    -797.327    6566.699\n",
       "5505.0       4921.0805   1902.737      2.586      0.010    1191.374    8650.787\n",
       "5518.0       4154.6414   2204.114      1.885      0.059    -165.818    8475.101\n",
       "5520.0        982.5964   1870.064      0.525      0.599   -2683.066    4648.259\n",
       "5545.0       4611.2221   1996.389      2.310      0.021     697.940    8524.504\n",
       "5568.0       6443.3547   1892.985      3.404      0.001    2732.763    1.02e+04\n",
       "5569.0       5182.5680   1953.809      2.653      0.008    1352.751    9012.385\n",
       "5578.0       4371.1879   1891.184      2.311      0.021     664.127    8078.249\n",
       "5581.0       3920.2341   1881.911      2.083      0.037     231.350    7609.118\n",
       "5589.0      -4919.3500   1989.347     -2.473      0.013   -8818.828   -1019.872\n",
       "5597.0       5579.7669   2589.433      2.155      0.031     504.012    1.07e+04\n",
       "5606.0      -2.149e+04   2189.269     -9.815      0.000   -2.58e+04   -1.72e+04\n",
       "5639.0       6284.4911   1911.721      3.287      0.001    2537.173       1e+04\n",
       "5667.0       1593.5800   1984.465      0.803      0.422   -2296.329    5483.489\n",
       "5690.0       4680.9703   1892.279      2.474      0.013     971.764    8390.177\n",
       "5709.0       3732.5040   1896.819      1.968      0.049      14.398    7450.610\n",
       "5726.0       3685.4231   1889.670      1.950      0.051     -18.671    7389.517\n",
       "5764.0       1412.3279   1903.400      0.742      0.458   -2318.679    5143.335\n",
       "5772.0       3425.6120   1886.884      1.815      0.069    -273.020    7124.244\n",
       "5860.0      -2.354e+04   2204.399    -10.681      0.000   -2.79e+04   -1.92e+04\n",
       "5878.0       4756.7109   1890.668      2.516      0.012    1050.662    8462.760\n",
       "5903.0      -1763.3549   1956.028     -0.901      0.367   -5597.522    2070.813\n",
       "5905.0       1594.0186   1904.603      0.837      0.403   -2139.345    5327.383\n",
       "5959.0       -551.9034   1965.277     -0.281      0.779   -4404.199    3300.393\n",
       "6008.0       2.313e+04   2050.513     11.281      0.000    1.91e+04    2.72e+04\n",
       "6034.0      -1253.0232   2221.970     -0.564      0.573   -5608.484    3102.438\n",
       "6035.0      -3476.7879   2528.257     -1.375      0.169   -8432.626    1479.051\n",
       "6036.0      -6657.4342   1921.654     -3.464      0.001   -1.04e+04   -2890.646\n",
       "6039.0       3754.1280   1885.722      1.991      0.047      57.773    7450.483\n",
       "6044.0       6085.6302   2158.752      2.819      0.005    1854.088    1.03e+04\n",
       "6066.0       2066.3853   3764.955      0.549      0.583   -5313.603    9446.374\n",
       "6078.0       4963.4106   1874.865      2.647      0.008    1288.338    8638.483\n",
       "6081.0      -1.751e+04   2080.749     -8.417      0.000   -2.16e+04   -1.34e+04\n",
       "60893.0     -1.509e+04   4441.536     -3.398      0.001   -2.38e+04   -6384.761\n",
       "6097.0       5098.8954   1913.924      2.664      0.008    1347.261    8850.530\n",
       "6102.0       3175.7833   1892.683      1.678      0.093    -534.215    6885.782\n",
       "6104.0      -6512.5249   2013.810     -3.234      0.001   -1.05e+04   -2565.094\n",
       "6109.0      -6666.9320   1931.756     -3.451      0.001   -1.05e+04   -2880.342\n",
       "6127.0      -4005.8304   2232.235     -1.795      0.073   -8381.412     369.751\n",
       "61552.0     -1.243e+04   3702.050     -3.358      0.001   -1.97e+04   -5174.322\n",
       "6158.0       -159.9485   2036.176     -0.079      0.937   -4151.221    3831.324\n",
       "6171.0       3633.6054   1882.449      1.930      0.054     -56.333    7323.544\n",
       "61780.0      2001.2059   4082.166      0.490      0.624   -6000.574       1e+04\n",
       "6207.0       3792.1030   1891.311      2.005      0.045      84.794    7499.412\n",
       "6214.0       4252.9273   1892.229      2.248      0.025     543.817    7962.037\n",
       "6216.0       5331.3623   1929.983      2.762      0.006    1548.249    9114.475\n",
       "62221.0      4107.4431   4086.784      1.005      0.315   -3903.389    1.21e+04\n",
       "6259.0      -1840.2931   2289.573     -0.804      0.422   -6328.269    2647.683\n",
       "62599.0      1.096e+04   4774.766      2.295      0.022    1598.888    2.03e+04\n",
       "6266.0       5315.3816   2126.320      2.500      0.012    1147.411    9483.352\n",
       "6268.0      -2706.9925   1966.170     -1.377      0.169   -6561.039    1147.054\n",
       "6288.0       1950.1248   1886.585      1.034      0.301   -1747.922    5648.171\n",
       "6297.0       3856.2871   1997.887      1.930      0.054     -59.931    7772.505\n",
       "6307.0       -1.98e+04   2839.541     -6.974      0.000   -2.54e+04   -1.42e+04\n",
       "6313.0       4079.6361   3344.074      1.220      0.223   -2475.351    1.06e+04\n",
       "6314.0       4782.8866   1911.134      2.503      0.012    1036.721    8529.053\n",
       "6326.0      -3966.7694   1970.640     -2.013      0.044   -7829.579    -103.960\n",
       "6349.0       3140.2437   1880.024      1.670      0.095    -544.942    6825.429\n",
       "6357.0       5344.1170   2069.760      2.582      0.010    1287.015    9401.219\n",
       "6375.0       9761.2418   1895.523      5.150      0.000    6045.676    1.35e+04\n",
       "6376.0       4837.2092   1903.822      2.541      0.011    1105.375    8569.044\n",
       "6379.0       5252.0872   5653.339      0.929      0.353   -5829.475    1.63e+04\n",
       "6386.0       4720.5567   1890.634      2.497      0.013    1014.574    8426.539\n",
       "6403.0       -734.7835   1932.553     -0.380      0.704   -4522.934    3053.367\n",
       "6410.0       6089.8782   1928.200      3.158      0.002    2310.259    9869.497\n",
       "6416.0      -2496.1172   2012.976     -1.240      0.215   -6441.912    1449.678\n",
       "6424.0       4445.7448   1898.100      2.342      0.019     725.128    8166.362\n",
       "6433.0       4813.7868   1924.016      2.502      0.012    1042.369    8585.205\n",
       "6435.0       3598.1008   1929.493      1.865      0.062    -184.053    7380.254\n",
       "6492.0       -734.9224   1960.341     -0.375      0.708   -4577.544    3107.699\n",
       "6497.0      -4877.3634   2139.537     -2.280      0.023   -9071.240    -683.486\n",
       "6500.0       2631.2582   8140.134      0.323      0.747   -1.33e+04    1.86e+04\n",
       "6509.0       3122.6930   1884.421      1.657      0.098    -571.110    6816.496\n",
       "6527.0       5860.6445   2301.144      2.547      0.011    1349.988    1.04e+04\n",
       "6528.0       2890.7532   2113.603      1.368      0.171   -1252.290    7033.796\n",
       "6531.0      -7043.9021   2023.324     -3.481      0.001    -1.1e+04   -3077.824\n",
       "6532.0       -480.0719   1920.270     -0.250      0.803   -4244.146    3284.002\n",
       "6543.0       5210.9517   1903.130      2.738      0.006    1480.474    8941.429\n",
       "6548.0       4613.0426   1902.342      2.425      0.015     884.111    8341.974\n",
       "6550.0       4431.0832   2121.854      2.088      0.037     271.868    8590.298\n",
       "6552.0       5011.0435   2079.583      2.410      0.016     934.687    9087.400\n",
       "6565.0      -2017.2034   2275.032     -0.887      0.375   -6476.675    2442.268\n",
       "6571.0       4085.2015   1890.317      2.161      0.031     379.841    7790.563\n",
       "6573.0       3637.9988   1882.029      1.933      0.053     -51.116    7327.114\n",
       "6641.0       1230.8940   4069.683      0.302      0.762   -6746.416    9208.204\n",
       "6649.0       5944.6150   1910.454      3.112      0.002    2199.781    9689.449\n",
       "6730.0       3876.8676   2145.664      1.807      0.071    -329.020    8082.755\n",
       "6731.0       -563.3170   1968.969     -0.286      0.775   -4422.852    3296.218\n",
       "6742.0       2825.0452   4084.063      0.692      0.489   -5180.454    1.08e+04\n",
       "6745.0       5143.2369   1902.430      2.704      0.007    1414.132    8872.342\n",
       "6756.0       4871.3773   1917.705      2.540      0.011    1112.330    8630.424\n",
       "6765.0      -1.321e+04   2050.847     -6.440      0.000   -1.72e+04   -9187.512\n",
       "6768.0       7016.4411   1992.023      3.522      0.000    3111.717    1.09e+04\n",
       "6774.0      -2.048e+04   2422.263     -8.453      0.000   -2.52e+04   -1.57e+04\n",
       "6797.0       5985.7283   2400.219      2.494      0.013    1280.867    1.07e+04\n",
       "6803.0       4924.6282   1908.446      2.580      0.010    1183.731    8665.525\n",
       "6821.0       3868.1145   1885.957      2.051      0.040     171.299    7564.930\n",
       "6830.0       3531.1264   1897.188      1.861      0.063    -187.704    7249.956\n",
       "6845.0       -176.0146   1979.119     -0.089      0.929   -4055.444    3703.415\n",
       "6848.0       3797.8864   2136.856      1.777      0.076    -390.736    7986.509\n",
       "6873.0       -668.9741   2727.946     -0.245      0.806   -6016.239    4678.291\n",
       "6900.0       1287.7612   1923.314      0.670      0.503   -2482.281    5057.803\n",
       "6908.0       1372.8494   1909.405      0.719      0.472   -2369.928    5115.627\n",
       "6994.0       2643.0183   1900.603      1.391      0.164   -1082.507    6368.543\n",
       "7045.0      -5777.6863   2549.116     -2.267      0.023   -1.08e+04    -780.960\n",
       "7065.0       6908.9206   1898.554      3.639      0.000    3187.413    1.06e+04\n",
       "7085.0       5820.7381   1886.029      3.086      0.002    2123.781    9517.695\n",
       "7107.0       2066.6982   2076.405      0.995      0.320   -2003.428    6136.825\n",
       "7116.0       6503.0343   1923.673      3.381      0.001    2732.289    1.03e+04\n",
       "7117.0       6174.9739   3129.278      1.973      0.048      41.027    1.23e+04\n",
       "7121.0       3294.9471   1884.523      1.748      0.080    -399.057    6988.952\n",
       "7127.0       2149.9549   2020.801      1.064      0.287   -1811.180    6111.089\n",
       "7139.0       3684.4739   1884.310      1.955      0.051      -9.113    7378.061\n",
       "7146.0       4642.5032   1893.941      2.451      0.014     930.039    8354.968\n",
       "7163.0       7302.9869   1896.443      3.851      0.000    3585.617     1.1e+04\n",
       "7180.0        181.7039   1884.279      0.096      0.923   -3511.822    3875.230\n",
       "7183.0       -274.3024   1937.642     -0.142      0.887   -4072.429    3523.824\n",
       "7228.0        1.28e+04   1904.390      6.722      0.000    9067.861    1.65e+04\n",
       "7232.0       2256.8554   3094.499      0.729      0.466   -3808.919    8322.630\n",
       "7250.0        454.8082   2183.158      0.208      0.835   -3824.575    4734.191\n",
       "7257.0       2.545e+04   2148.988     11.841      0.000    2.12e+04    2.97e+04\n",
       "7260.0       3535.9150   1881.772      1.879      0.060    -152.697    7224.527\n",
       "7267.0        237.8283   1986.583      0.120      0.905   -3656.233    4131.889\n",
       "7268.0      -6685.3518   2284.765     -2.926      0.003   -1.12e+04   -2206.801\n",
       "7281.0       4494.6574   2888.074      1.556      0.120   -1166.486    1.02e+04\n",
       "7291.0       1181.2310   1918.089      0.616      0.538   -2578.568    4941.030\n",
       "7343.0      -7911.4411   2855.978     -2.770      0.006   -1.35e+04   -2313.211\n",
       "7346.0      -4271.2095   1936.237     -2.206      0.027   -8066.583    -475.836\n",
       "7401.0       4345.8538   1890.458      2.299      0.022     640.217    8051.491\n",
       "7409.0       3110.0251   1889.359      1.646      0.100    -593.458    6813.508\n",
       "7420.0       -200.0609   1875.303     -0.107      0.915   -3875.992    3475.870\n",
       "7435.0       5677.9998   2294.109      2.475      0.013    1181.134    1.02e+04\n",
       "7466.0       3068.3648   1999.314      1.535      0.125    -850.650    6987.380\n",
       "7486.0      -6368.5974   2092.013     -3.044      0.002   -1.05e+04   -2267.875\n",
       "7503.0       3312.6048   3643.490      0.909      0.363   -3829.292    1.05e+04\n",
       "7506.0       4314.1537   1883.555      2.290      0.022     622.048    8006.260\n",
       "7537.0       4406.8673   1902.507      2.316      0.021     677.610    8136.124\n",
       "7549.0       1261.0932   1909.746      0.660      0.509   -2482.352    5004.539\n",
       "7554.0       4258.7072   1895.126      2.247      0.025     543.920    7973.495\n",
       "7557.0         94.9133   1998.861      0.047      0.962   -3823.213    4013.040\n",
       "7585.0      -6767.6640   2431.073     -2.784      0.005   -1.15e+04   -2002.324\n",
       "7602.0       3508.6228   1886.606      1.860      0.063    -189.464    7206.710\n",
       "7620.0        864.6487   2161.340      0.400      0.689   -3371.966    5101.263\n",
       "7636.0       4079.5367   1894.075      2.154      0.031     366.810    7792.264\n",
       "7646.0       4368.0875   1892.540      2.308      0.021     658.369    8077.806\n",
       "7658.0       -444.5436   1938.669     -0.229      0.819   -4244.684    3355.597\n",
       "7683.0       5242.7246   2062.069      2.542      0.011    1200.698    9284.751\n",
       "7685.0       3203.1421   2117.987      1.512      0.130    -948.494    7354.779\n",
       "7692.0      -1982.7323   1872.804     -1.059      0.290   -5653.765    1688.300\n",
       "7762.0       3939.9208   1887.339      2.088      0.037     240.396    7639.445\n",
       "7772.0      -8822.8734   2053.343     -4.297      0.000   -1.28e+04   -4797.952\n",
       "7773.0       3643.2673   1880.924      1.937      0.053     -43.683    7330.217\n",
       "7777.0      -1668.5671   1880.801     -0.887      0.375   -5355.275    2018.141\n",
       "7835.0       4887.6887   1895.424      2.579      0.010    1172.316    8603.061\n",
       "7873.0      -6569.8690   2034.245     -3.230      0.001   -1.06e+04   -2582.382\n",
       "7883.0       -278.4626   1986.987     -0.140      0.889   -4173.315    3616.390\n",
       "7904.0      -1014.3734   1976.040     -0.513      0.608   -4887.768    2859.021\n",
       "7906.0       7500.9507   1914.777      3.917      0.000    3747.643    1.13e+04\n",
       "7921.0       4187.0950   1887.493      2.218      0.027     487.269    7886.921\n",
       "7923.0       4451.0966   2005.459      2.219      0.026     520.035    8382.158\n",
       "7935.0        860.7939   1869.151      0.461      0.645   -2803.078    4524.666\n",
       "7938.0       2057.4060   1885.421      1.091      0.275   -1638.359    5753.171\n",
       "7985.0      -1.955e+04   2201.157     -8.881      0.000   -2.39e+04   -1.52e+04\n",
       "8014.0       2426.3775   1987.219      1.221      0.222   -1468.929    6321.684\n",
       "8030.0       5924.6963   1920.790      3.085      0.002    2159.602    9689.790\n",
       "8046.0      -6813.2524   2034.913     -3.348      0.001   -1.08e+04   -2824.456\n",
       "8047.0       3427.6567   2727.958      1.256      0.209   -1919.632    8774.946\n",
       "8062.0       1583.0928   1924.083      0.823      0.411   -2188.455    5354.641\n",
       "8068.0      -1.386e+04   2219.412     -6.247      0.000   -1.82e+04   -9513.728\n",
       "8087.0      -1.125e+04   2138.260     -5.260      0.000   -1.54e+04   -7056.886\n",
       "8095.0       3747.6061   1890.531      1.982      0.047      41.826    7453.386\n",
       "8096.0       4967.5181   1914.276      2.595      0.009    1215.193    8719.843\n",
       "8109.0       4448.7369   1893.131      2.350      0.019     737.860    8159.614\n",
       "8123.0      -1564.8370   2068.102     -0.757      0.449   -5618.689    2489.015\n",
       "8150.0       5167.8110   1904.168      2.714      0.007    1435.300    8900.322\n",
       "8163.0        336.5507   1986.336      0.169      0.865   -3557.026    4230.127\n",
       "8176.0      -1014.2924   2411.287     -0.421      0.674   -5740.849    3712.264\n",
       "8202.0        811.2235   2008.822      0.404      0.686   -3126.429    4748.876\n",
       "8214.0        111.2975   1911.608      0.058      0.954   -3635.798    3858.392\n",
       "8215.0      -2390.3624   2139.162     -1.117      0.264   -6583.505    1802.780\n",
       "8219.0       5671.7298   1921.079      2.952      0.003    1906.070    9437.390\n",
       "8247.0        887.0208   1937.131      0.458      0.647   -2910.104    4684.145\n",
       "8253.0      -1.195e+04   2040.351     -5.859      0.000    -1.6e+04   -7954.751\n",
       "8290.0       1302.6717   2006.620      0.649      0.516   -2630.664    5236.007\n",
       "8293.0        884.4232   1963.476      0.450      0.652   -2964.342    4733.189\n",
       "8304.0       4944.8247   1881.092      2.629      0.009    1257.545    8632.104\n",
       "8334.0       4982.4455   2045.671      2.436      0.015     972.562    8992.329\n",
       "8348.0       3670.3352   1888.262      1.944      0.052     -30.998    7371.669\n",
       "8357.0       3236.9587   1883.212      1.719      0.086    -454.475    6928.392\n",
       "8358.0       1240.3781   1908.909      0.650      0.516   -2501.426    4982.183\n",
       "8446.0      -8111.7072   2318.225     -3.499      0.000   -1.27e+04   -3567.568\n",
       "8460.0       6388.1362   2303.552      2.773      0.006    1872.761    1.09e+04\n",
       "8463.0       3671.8709   1886.483      1.946      0.052     -25.974    7369.716\n",
       "8479.0       6394.2133   2734.879      2.338      0.019    1033.358    1.18e+04\n",
       "8530.0       8019.4538   2187.179      3.667      0.000    3732.189    1.23e+04\n",
       "8536.0      -1046.7635   1930.939     -0.542      0.588   -4831.751    2738.224\n",
       "8543.0       2.288e+04   2174.771     10.523      0.000    1.86e+04    2.71e+04\n",
       "8549.0      -8252.7873   2048.657     -4.028      0.000   -1.23e+04   -4237.050\n",
       "8551.0       5182.1662   1912.584      2.710      0.007    1433.157    8931.176\n",
       "8559.0        285.1798   2085.795      0.137      0.891   -3803.354    4373.714\n",
       "8573.0      -6174.7927   2267.068     -2.724      0.006   -1.06e+04   -1730.932\n",
       "8606.0       5357.3152   1891.680      2.832      0.005    1649.281    9065.349\n",
       "8607.0       5343.3350   1921.755      2.780      0.005    1576.349    9110.321\n",
       "8648.0       3082.5063   1882.670      1.637      0.102    -607.866    6772.878\n",
       "8657.0      -1618.1451   1874.980     -0.863      0.388   -5293.443    2057.153\n",
       "8675.0       4370.1464   3672.134      1.190      0.234   -2827.897    1.16e+04\n",
       "8681.0      -1669.7543   1880.379     -0.888      0.375   -5355.635    2016.127\n",
       "8687.0       -102.7675   2184.219     -0.047      0.962   -4384.230    4178.695\n",
       "8692.0        619.4863   1886.940      0.328      0.743   -3079.255    4318.228\n",
       "8699.0       4089.5465   1889.663      2.164      0.030     385.466    7793.627\n",
       "8717.0       5250.1882   1901.428      2.761      0.006    1523.048    8977.329\n",
       "8759.0       -167.6682   1946.070     -0.086      0.931   -3982.316    3646.980\n",
       "8762.0       9881.2575   2003.848      4.931      0.000    5953.354    1.38e+04\n",
       "8819.0       6358.0137   1939.505      3.278      0.001    2556.236    1.02e+04\n",
       "8850.0       3921.8310   1888.789      2.076      0.038     219.464    7624.198\n",
       "8852.0       4019.5858   1886.207      2.131      0.033     322.281    7716.890\n",
       "8859.0       5179.6950   1922.581      2.694      0.007    1411.090    8948.300\n",
       "8867.0      -3798.5887   2037.891     -1.864      0.062   -7793.223     196.045\n",
       "8881.0       2015.6252   1876.670      1.074      0.283   -1662.987    5694.237\n",
       "8958.0        783.6540   1875.076      0.418      0.676   -2891.833    4459.141\n",
       "8972.0      -1.876e+04   2234.007     -8.396      0.000   -2.31e+04   -1.44e+04\n",
       "8990.0      -8901.2931   2173.056     -4.096      0.000   -1.32e+04   -4641.712\n",
       "9004.0       5563.5665   2223.931      2.502      0.012    1204.262    9922.871\n",
       "9016.0       1076.0450   1871.540      0.575      0.565   -2592.510    4744.600\n",
       "9048.0       -332.1457   1903.188     -0.175      0.861   -4062.737    3398.445\n",
       "9051.0      -6534.2926   2323.696     -2.812      0.005   -1.11e+04   -1979.430\n",
       "9071.0      -1080.8584   2074.877     -0.521      0.602   -5147.991    2986.274\n",
       "9112.0       -939.2564   1868.324     -0.503      0.615   -4601.507    2722.994\n",
       "9114.0      -2806.9536   1947.227     -1.442      0.149   -6623.869    1009.962\n",
       "9132.0       4738.5874   3383.557      1.400      0.161   -1893.793    1.14e+04\n",
       "9173.0       2640.2302   2267.952      1.164      0.244   -1805.365    7085.825\n",
       "9180.0       5178.7693   1907.866      2.714      0.007    1439.008    8918.531\n",
       "9186.0       2567.5309   1883.220      1.363      0.173   -1123.919    6258.981\n",
       "9191.0      -1799.1070   3371.606     -0.534      0.594   -8408.061    4809.847\n",
       "9216.0      -5071.6784   2007.828     -2.526      0.012   -9007.382   -1135.975\n",
       "9217.0      -6693.0668   1999.891     -3.347      0.001   -1.06e+04   -2772.920\n",
       "9225.0       4530.0976   1889.274      2.398      0.017     826.781    8233.414\n",
       "9230.0       5414.8369   2502.877      2.163      0.031     508.747    1.03e+04\n",
       "9259.0       6377.0626   1939.103      3.289      0.001    2576.071    1.02e+04\n",
       "9293.0       5066.9414   1894.447      2.675      0.007    1353.485    8780.398\n",
       "9299.0       -928.3173   1912.641     -0.485      0.627   -4677.437    2820.803\n",
       "9308.0      -1586.1615   1999.894     -0.793      0.428   -5506.313    2333.990\n",
       "9311.0      -1086.7601   3079.383     -0.353      0.724   -7122.905    4949.385\n",
       "9313.0      -2250.3692   1868.021     -1.205      0.228   -5912.026    1411.288\n",
       "9325.0       2906.9673   1933.000      1.504      0.133    -882.061    6695.996\n",
       "9332.0       1876.8362   1908.153      0.984      0.325   -1863.486    5617.159\n",
       "9340.0      -2.787e+04   3795.325     -7.343      0.000   -3.53e+04   -2.04e+04\n",
       "9372.0       4832.4677   2290.196      2.110      0.035     343.272    9321.664\n",
       "9411.0        638.3682   2006.770      0.318      0.750   -3295.263    4571.999\n",
       "9459.0      -3209.2322   2167.623     -1.481      0.139   -7458.163    1039.699\n",
       "9465.0       8727.5253   1945.950      4.485      0.000    4913.114    1.25e+04\n",
       "9472.0      -3357.0062   1885.295     -1.781      0.075   -7052.524     338.511\n",
       "9483.0      -9433.9635   2046.163     -4.611      0.000   -1.34e+04   -5423.116\n",
       "9563.0      -2.456e+04   3444.483     -7.131      0.000   -3.13e+04   -1.78e+04\n",
       "9590.0        893.6168   1923.595      0.465      0.642   -2876.975    4664.209\n",
       "9598.0      -3563.2028   2729.749     -1.305      0.192   -8914.003    1787.597\n",
       "9599.0      -4032.0335   1921.783     -2.098      0.036   -7799.073    -264.994\n",
       "9602.0      -4766.8112   3333.138     -1.430      0.153   -1.13e+04    1766.739\n",
       "9619.0       5174.2224   1938.394      2.669      0.008    1374.621    8973.823\n",
       "9643.0       -263.9671   1993.288     -0.132      0.895   -4171.171    3643.236\n",
       "9650.0      -1187.2024   2063.936     -0.575      0.565   -5232.888    2858.483\n",
       "9653.0      -1.397e+04   4527.328     -3.086      0.002   -2.28e+04   -5097.260\n",
       "9667.0       -877.7466   1992.241     -0.441      0.660   -4782.897    3027.404\n",
       "9698.0       2761.0191   1894.635      1.457      0.145    -952.807    6474.845\n",
       "9699.0       4007.7347   1875.265      2.137      0.033     331.878    7683.592\n",
       "9719.0      -6828.8306   1935.540     -3.528      0.000   -1.06e+04   -3034.825\n",
       "9742.0      -7685.7043   2015.237     -3.814      0.000   -1.16e+04   -3735.478\n",
       "9761.0       3747.0948   1890.350      1.982      0.047      41.669    7452.520\n",
       "9771.0      -5368.8020   1920.256     -2.796      0.005   -9132.850   -1604.754\n",
       "9772.0       5064.2150   1906.029      2.657      0.008    1328.056    8800.374\n",
       "9778.0       1030.4091   1892.700      0.544      0.586   -2679.624    4740.442\n",
       "9799.0      -7979.6142   2104.282     -3.792      0.000   -1.21e+04   -3854.842\n",
       "9815.0       3494.9073   1987.897      1.758      0.079    -401.728    7391.543\n",
       "9818.0       -3.02e+04   2085.706    -14.480      0.000   -3.43e+04   -2.61e+04\n",
       "9837.0       5988.0513   1929.137      3.104      0.002    2206.595    9769.508\n",
       "9922.0      -4997.0058   1912.519     -2.613      0.009   -8745.887   -1248.125\n",
       "9954.0      -2307.9747   2751.378     -0.839      0.402   -7701.171    3085.222\n",
       "9963.0       1439.8701   1900.249      0.758      0.449   -2284.959    5164.700\n",
       "9988.0       5676.8617   1932.396      2.938      0.003    1889.018    9464.705\n",
       "9999.0      -9230.0755   2004.514     -4.605      0.000   -1.32e+04   -5300.868\n",
       "==============================================================================\n",
       "Omnibus:                    22714.975   Durbin-Watson:                   0.750\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        140074505.190\n",
       "Skew:                          14.746   Prob(JB):                         0.00\n",
       "Kurtosis:                     537.398   Cond. No.                     2.53e+17\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.36e-22. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vars = df[x_vars]\n",
    "x_vars = sm.add_constant(x_vars)\n",
    "x_vars = x_vars.astype(float) # converts categorical booleans to floats\n",
    "\n",
    "lin_reg = sm.OLS(y_var,x_vars).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59501cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"1\"><em>Dependent variable: rmkvaf</em></td></tr><tr><td style=\"text-align:left\"></td><tr><td style=\"text-align:left\"></td><td>(1)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">const</td><td>-6271.853<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(532.282)</td></tr>\n",
       "<tr><td style=\"text-align:left\">gspilltecIV</td><td>0.163<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.028)</td></tr>\n",
       "<tr><td style=\"text-align:left\">gspillsicIV</td><td>0.489<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.052)</td></tr>\n",
       "<tr><td style=\"text-align:left\">pat_count</td><td>-29.438<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(1.715)</td></tr>\n",
       "<tr><td style=\"text-align:left\">rsales</td><td>1.015<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.041)</td></tr>\n",
       "<tr><td style=\"text-align:left\">rppent</td><td>0.518<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.087)</td></tr>\n",
       "<tr><td style=\"text-align:left\">emp</td><td>-2.510<sup></sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(7.030)</td></tr>\n",
       "<tr><td style=\"text-align:left\">rxrd</td><td>8.622<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.669)</td></tr>\n",
       "\n",
       "<tr><td style=\"text-align: left\">Firm & Time Effects</td><td>Yes</td></tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>11736</td></tr><tr><td style=\"text-align: left\">R<sup>2</sup></td><td>0.664</td></tr><tr><td style=\"text-align: left\">Adjusted R<sup>2</sup></td><td>0.641</td></tr><tr><td style=\"text-align: left\">Residual Std. Error</td><td>8127.834 (df=10985)</td></tr><tr><td style=\"text-align: left\">F Statistic</td><td>28.979<sup>***</sup> (df=750; 10985)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"1\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<stargazer.stargazer.Stargazer at 0x7f6f65201950>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export results, omit fixed effects\n",
    "stargazer = Stargazer([lin_reg])\n",
    "\n",
    "main_vars = [col for col in x_vars.columns if col not in fixed_effects]\n",
    "cov_labels = {'rmkvaf': 'Market Value',\n",
    "             'pat_count': 'Patent Count',\n",
    "             'rsales': 'Sales',\n",
    "             'rppent': '',\n",
    "             'emp': 'Employment',\n",
    "             'rxrd': 'R&D Expenditures'}\n",
    "\n",
    "\n",
    "stargazer.covariate_order(main_vars)\n",
    "stargazer.add_line(\"Firm & Time Effects\", ['Yes'])\n",
    "#stargazer.rename_covariates(cov_labels)\n",
    "stargazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e65e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!htbp] \\centering\n",
      "\\begin{tabular}{@{\\extracolsep{5pt}}lc}\n",
      "\\\\[-1.8ex]\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "& \\multicolumn{1}{c}{\\textit{Dependent variable: rmkvaf}} \\\n",
      "\\cr \\cline{2-2}\n",
      "\\\\[-1.8ex] & (1) \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " const & -6271.853$^{***}$ \\\\\n",
      "& (532.282) \\\\\n",
      " gspilltecIV & 0.163$^{***}$ \\\\\n",
      "& (0.028) \\\\\n",
      " gspillsicIV & 0.489$^{***}$ \\\\\n",
      "& (0.052) \\\\\n",
      " pat_count & -29.438$^{***}$ \\\\\n",
      "& (1.715) \\\\\n",
      " rsales & 1.015$^{***}$ \\\\\n",
      "& (0.041) \\\\\n",
      " rppent & 0.518$^{***}$ \\\\\n",
      "& (0.087) \\\\\n",
      " emp & -2.510$^{}$ \\\\\n",
      "& (7.030) \\\\\n",
      " rxrd & 8.622$^{***}$ \\\\\n",
      "& (0.669) \\\\\n",
      " Firm & Time Effects & Yes \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Observations & 11736 \\\\\n",
      " $R^2$ & 0.664 \\\\\n",
      " Adjusted $R^2$ & 0.641 \\\\\n",
      " Residual Std. Error & 8127.834 (df=10985) \\\\\n",
      " F Statistic & 28.979$^{***}$ (df=750; 10985) \\\\\n",
      "\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "\\textit{Note:} & \\multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# render to latex\n",
    "print(stargazer.render_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbb04b",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ad6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design = MS(df.columns.drop([col for col in df.columns if col not in x_vars.columns])).fit(df)\n",
    "# NOTE: including all of the fixed effects seems to kill the notebook. Running Ridge/LASSO on subset excluding FE's\n",
    "\n",
    "# Check: outlier in dataset driving a split in reg tree.\n",
    "## Drop two outlier firms: i = 5047, i = 12141, i = 6008\n",
    "df = df.loc[(df['i'] != '5047.0') & (df['i'] != '12141.0') & (df['i'] != '6008.0')]\n",
    "\n",
    "design = MS(df.columns.drop([col for col in df.columns if col not in x_vars.columns or col in fixed_effects])).fit(df)\n",
    "Y = np.array(df['rmkvaf'])\n",
    "X = design.transform(df)\n",
    "\n",
    "D = design.fit_transform(df)\n",
    "D = D.drop('intercept', axis=1)\n",
    "X = np.asarray(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "643f4695-8889-4a0f-8fd1-e7abe27e3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>rmkvaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>20779.0</td>\n",
       "      <td>194111.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5965</th>\n",
       "      <td>62599.0</td>\n",
       "      <td>190356.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>6066.0</td>\n",
       "      <td>183853.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>7257.0</td>\n",
       "      <td>168679.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4625</th>\n",
       "      <td>6066.0</td>\n",
       "      <td>163628.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8873</th>\n",
       "      <td>9230.0</td>\n",
       "      <td>1.155449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>13923.0</td>\n",
       "      <td>0.985430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9542</th>\n",
       "      <td>9799.0</td>\n",
       "      <td>0.718651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9543</th>\n",
       "      <td>9799.0</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9544</th>\n",
       "      <td>9799.0</td>\n",
       "      <td>0.432155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11684 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            i         rmkvaf\n",
       "785   20779.0  194111.171875\n",
       "5965  62599.0  190356.171875\n",
       "4626   6066.0  183853.468750\n",
       "6493   7257.0  168679.343750\n",
       "4625   6066.0  163628.187500\n",
       "...       ...            ...\n",
       "8873   9230.0       1.155449\n",
       "948   13923.0       0.985430\n",
       "9542   9799.0       0.718651\n",
       "9543   9799.0       0.617000\n",
       "9544   9799.0       0.432155\n",
       "\n",
       "[11684 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['i','rmkvaf']].sort_values('rmkvaf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968dedb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ModelSpec(terms=Index([&#x27;rxrd&#x27;, &#x27;pat_count&#x27;, &#x27;rsales&#x27;, &#x27;rppent&#x27;, &#x27;emp&#x27;, &#x27;gspilltecIV&#x27;,\n",
       "       &#x27;gspillsicIV&#x27;],\n",
       "      dtype=&#x27;object&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ModelSpec</label><div class=\"sk-toggleable__content\"><pre>ModelSpec(terms=Index([&#x27;rxrd&#x27;, &#x27;pat_count&#x27;, &#x27;rsales&#x27;, &#x27;rppent&#x27;, &#x27;emp&#x27;, &#x27;gspilltecIV&#x27;,\n",
       "       &#x27;gspillsicIV&#x27;],\n",
       "      dtype=&#x27;object&#x27;))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ModelSpec(terms=Index(['rxrd', 'pat_count', 'rsales', 'rppent', 'emp', 'gspilltecIV',\n",
       "       'gspillsicIV'],\n",
       "      dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6996449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692474979059.0005, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692450581502.8406, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692419802382.6101, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692380974844.9012, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692331998044.1666, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692270224966.1089, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692192321741.2958, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 692094091490.9111, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691970254220.4594, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691814172522.1726, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691617510899.404, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691369814485.1792, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691057990996.3798, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 690665678320.1027, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 690172479814.201, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 689553051273.4781, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 688776029217.4188, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 687802802175.9564, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 686586148542.9523, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 685068801127.6128, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 683182055669.4193, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 680844624500.3997, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 677962051588.1035, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 674427149426.1432, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 670122075770.6383, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 664922798296.9677, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 658706720659.7993, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 651364046408.1904, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 642812898664.3956, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 633017196004.0415, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 622004867930.3256, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 609882524534.4724, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 596841851467.655, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 583153591394.6376, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 569147449747.1246, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555180179457.696, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 541597994811.53894, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528701514996.15533, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 516720580355.7128, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505802945692.7384, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496016655840.682, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 487362606283.95233, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479792336330.8734, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 473226417772.6438, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 467570188763.427, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462725260024.04956, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 458596645900.6267, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 455096297229.41766, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 452144205667.0636, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 449668212416.5287, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 447603359618.9543, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 445891252734.6087, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 444479595194.1199, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 443321874245.7067, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 442377111399.43396, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 441609598452.76544, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440988575642.3176, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440487843262.5404, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440085320812.7626, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439762577828.82983, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439504361617.35724, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439298143126.185, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439133696185.58997, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439002719252.6392, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438898503595.42896, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438815647948.6616, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438749817057.7014, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438697540008.6773, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438656043552.26794, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438623115510.706, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438596993596.63306, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438576275407.4846, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438559845878.6792, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438546819011.81323, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438536491198.29755, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438528303913.4445, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438521813951.978, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438516669713.61426, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438512592329.7029, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438509360656.0952, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438506799348.26587, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438504769391.17883, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438503160581.599, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438501885562.6311, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438500875091.13904, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438500074284.1637, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438499439642.2453, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438498936689.28033, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438498538101.3142, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438498222223.19293, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497971892.66864, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497773508.4362, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497616291.34247, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497491698.85925, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497392961.0657, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497314712.8051, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497252702.2422, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497203559.8361, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497164615.2587, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 438497133752.309, tolerance: 138513639.2459772\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEUAAAAVCAYAAAAQAyPeAAAACXBIWXMAAA7EAAAOxAGVKw4bAAADIUlEQVRYCd2Y7XHTQBCGRYYClNCB6MAhFRB3YNwBcQfkp/0vEzogVMAkHQAVMLgDuwOMOzDvs74Vh3QRR6yxSXbmtHer9/ZLezenKzabTRG36XRaxeOn3k/Fe1RENJvN3mk4cJHGldqFWumyJ8iJkbhrekYlQHoxEjsTvzTBb9mtjxN8LfxxQr6TSDr5MNg9VX+dUib5dZD/EH+pdi3ZsonNwQlDUojlhvmWFA2ohK/ipwidNMYwDraMSXaudinMnfjOFHz4KEUrtVdq2D2WvJUUyb7r3ZXbDnORDdWvfc3FaV4RsK/F188RiAj+g/UaD4GGDREKKmTivSQk6CL4N6H/xzJG5iSbF+qXsW31+cr4Qgzmby7O9Ya55GHie8pYSqx0IhDdb42xDynViQ/2zEncPGETX8/lF1UP5eIMHOInD+WRHuwldckZIjz0rlUJkpHNqxi35z7LliXWJI+B91AubovePtExplIoty9bWfdTCWHZDMRTX6p7cg9vZderoEvbSS4uoYQ8DEkKm9oiAUiJqBLaoegkGG5tvpFDJC4XF02zLnmoSApKUuVoKH+EKmHNZlWVzzsAf5FpM4UjD5YUstqVebfBxurr1mX75l0fz6uDc0suruk/8ZVUSi6xIXcZy9XzYJyq1D8e1d0kly1zcU0FGluBkBQCdYUJnJ1HeF+prZOA/QpZvvjSJK8UX965uFgPca5ICiWTMhKD2YyheytFX6czsdvpvTw5/rs/sUJO4/OoSnJxsQ4SuyQpc7Wz+E2i7wEnKyUk5Kc4R+0+yDdB//q1TtngkLkSZzkbBftjDd4GUZGLc3zglliO+Z/UyGoXkTgSwqmxRXKAY7ZtUuIcwZPJa01sCDTP/fAD2G3Q+1k8PnHjPKdqPiYbK5z/FvyMKRfnc7A7sbsU3Sks1Aa73p1Ix0it3FXPIebL70ptgW2WD8SBrI9/Ga4eHlQl5sVhH1yZ2MHUkhJKk8uWv22497qtuaVeUsqPjkLcxG9L1CuFQPirTF4fZEbJDd37TOz/BiPueqXUN294GTI2esTB/XOyFSt3N3fi9Wn9F1Hku17ZY8bCAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle \\left( 7, \\  100\\right)$"
      ],
      "text/plain": [
       "(7, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = X - X.mean(0)[None,:]\n",
    "X_scale = X.std(0)\n",
    "Xs = Xs / X_scale[None,:]\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "soln_array = lm.ElasticNet.path(Xs,\n",
    "                                 Y,\n",
    "                                 l1_ratio=0.,\n",
    "                                 alphas=lambdas)[1]\n",
    "soln_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12466d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rxrd</th>\n",
       "      <th>pat_count</th>\n",
       "      <th>rsales</th>\n",
       "      <th>rppent</th>\n",
       "      <th>emp</th>\n",
       "      <th>gspilltecIV</th>\n",
       "      <th>gspillsicIV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative log(lambda)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9.167990</th>\n",
       "      <td>0.639117</td>\n",
       "      <td>0.386651</td>\n",
       "      <td>0.641837</td>\n",
       "      <td>0.580663</td>\n",
       "      <td>0.482584</td>\n",
       "      <td>0.281462</td>\n",
       "      <td>0.211692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.935406</th>\n",
       "      <td>0.806396</td>\n",
       "      <td>0.487844</td>\n",
       "      <td>0.809826</td>\n",
       "      <td>0.732642</td>\n",
       "      <td>0.608881</td>\n",
       "      <td>0.355125</td>\n",
       "      <td>0.267103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.702821</th>\n",
       "      <td>1.017432</td>\n",
       "      <td>0.615504</td>\n",
       "      <td>1.021757</td>\n",
       "      <td>0.924375</td>\n",
       "      <td>0.768208</td>\n",
       "      <td>0.448054</td>\n",
       "      <td>0.337010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.470237</th>\n",
       "      <td>1.283655</td>\n",
       "      <td>0.776541</td>\n",
       "      <td>1.289107</td>\n",
       "      <td>1.166247</td>\n",
       "      <td>0.969189</td>\n",
       "      <td>0.565280</td>\n",
       "      <td>0.425201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.237653</th>\n",
       "      <td>1.619474</td>\n",
       "      <td>0.979666</td>\n",
       "      <td>1.626344</td>\n",
       "      <td>1.471347</td>\n",
       "      <td>1.222693</td>\n",
       "      <td>0.713142</td>\n",
       "      <td>0.536452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.927523</th>\n",
       "      <td>2655.969827</td>\n",
       "      <td>329.826780</td>\n",
       "      <td>2902.989773</td>\n",
       "      <td>1852.216527</td>\n",
       "      <td>-1012.994917</td>\n",
       "      <td>186.880161</td>\n",
       "      <td>982.580331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.160108</th>\n",
       "      <td>2655.970437</td>\n",
       "      <td>329.826109</td>\n",
       "      <td>2902.995806</td>\n",
       "      <td>1852.215856</td>\n",
       "      <td>-1012.999948</td>\n",
       "      <td>186.879590</td>\n",
       "      <td>982.580723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.392692</th>\n",
       "      <td>2655.970921</td>\n",
       "      <td>329.825577</td>\n",
       "      <td>2903.000587</td>\n",
       "      <td>1852.215323</td>\n",
       "      <td>-1013.003935</td>\n",
       "      <td>186.879138</td>\n",
       "      <td>982.581034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.625276</th>\n",
       "      <td>2655.971304</td>\n",
       "      <td>329.825156</td>\n",
       "      <td>2903.004376</td>\n",
       "      <td>1852.214902</td>\n",
       "      <td>-1013.007095</td>\n",
       "      <td>186.878779</td>\n",
       "      <td>982.581280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.857861</th>\n",
       "      <td>2655.971608</td>\n",
       "      <td>329.824822</td>\n",
       "      <td>2903.007378</td>\n",
       "      <td>1852.214567</td>\n",
       "      <td>-1013.009599</td>\n",
       "      <td>186.878495</td>\n",
       "      <td>982.581475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             rxrd   pat_count       rsales       rppent  \\\n",
       "negative log(lambda)                                                      \n",
       "-9.167990                0.639117    0.386651     0.641837     0.580663   \n",
       "-8.935406                0.806396    0.487844     0.809826     0.732642   \n",
       "-8.702821                1.017432    0.615504     1.021757     0.924375   \n",
       "-8.470237                1.283655    0.776541     1.289107     1.166247   \n",
       "-8.237653                1.619474    0.979666     1.626344     1.471347   \n",
       "...                           ...         ...          ...          ...   \n",
       " 12.927523            2655.969827  329.826780  2902.989773  1852.216527   \n",
       " 13.160108            2655.970437  329.826109  2902.995806  1852.215856   \n",
       " 13.392692            2655.970921  329.825577  2903.000587  1852.215323   \n",
       " 13.625276            2655.971304  329.825156  2903.004376  1852.214902   \n",
       " 13.857861            2655.971608  329.824822  2903.007378  1852.214567   \n",
       "\n",
       "                              emp  gspilltecIV  gspillsicIV  \n",
       "negative log(lambda)                                         \n",
       "-9.167990                0.482584     0.281462     0.211692  \n",
       "-8.935406                0.608881     0.355125     0.267103  \n",
       "-8.702821                0.768208     0.448054     0.337010  \n",
       "-8.470237                0.969189     0.565280     0.425201  \n",
       "-8.237653                1.222693     0.713142     0.536452  \n",
       "...                           ...          ...          ...  \n",
       " 12.927523           -1012.994917   186.880161   982.580331  \n",
       " 13.160108           -1012.999948   186.879590   982.580723  \n",
       " 13.392692           -1013.003935   186.879138   982.581034  \n",
       " 13.625276           -1013.007095   186.878779   982.581280  \n",
       " 13.857861           -1013.009599   186.878495   982.581475  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))\n",
    "soln_path.index.name = 'negative log(lambda)'\n",
    "\n",
    "soln_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "222ea61d-8649-47d5-9913-dbf843cdde9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuMAAAK5CAYAAAAVaCA1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3gU1frA8e9sSU82vUEILXTpLSC9K03wgqIUQbBQ5ILl6rWg3p8FRVRQ9HrpVVEQEERApPcS6UgVCAmhpLfN7s7vjyULISEkbJJNwvt5nnl2d+adOe8mkLw5e+YcRVVVFSGEEEIIIUSJ0zg6ASGEEEIIIR5UUowLIYQQQgjhIFKMCyGEEEII4SBSjAshhBBCCOEgUowLIYQQQgjhIFKMCyGEEEII4SBSjAshhBBCCOEgUowLIYQQQgjhIDpHJyBusVgsXL58GU9PTxRFcXQ6QgghhBDiDqqqkpycTGhoKBqN/f3aUoyXIpcvXyYsLMzRaQghhBBCiHu4ePEiFStWtPs6UoyXIp6enoD1m+vl5eXgbIQQQgghxJ2SkpIICwuz1W32kmK8FMkemuLl5SXFuBBCCCFEKVZUQ4rlBk4hhBBCCCEcRIpxIYQQQgghHESKcSGEEEIIIRxExoyXMaqqYjKZMJvNjk5FFDGtVotOp5NpLYUQQogHiBTjZYjRaCQmJoa0tDRHpyKKiZubGyEhITg5OTk6FSGEEEKUACnGywiLxcK5c+fQarWEhobi5OQkPajliKqqGI1Grl69yrlz54iIiCiShQSEEEIIUbpJMV5GGI1GLBYLYWFhuLm5OTodUQxcXV3R6/X8/fffGI1GXFxcHJ2SEEIIIYpZue96mzFjBvXr17fN3R0ZGcmvv/5qO66qKpMmTSI0NBRXV1fat2/P0aNHc1wjMzOTsWPH4u/vj7u7O7179+bSpUs5YuLj4xk8eDAGgwGDwcDgwYNJSEgo8vcjvaXlm3x/hRBCiAdLuf/NX7FiRT766CP27dvHvn376NixI3369LEV3JMnT+azzz5j+vTp7N27l+DgYLp06UJycrLtGuPHj2f58uUsWbKEbdu2kZKSQs+ePXPcRDlo0CCioqJYu3Yta9euJSoqisGDB5f4+xVCCCGEEGWHoqqq6ugkSpqvry+ffPIJw4cPJzQ0lPHjx/Paa68B1l7woKAgPv74Y5577jkSExMJCAhg/vz5DBw4EIDLly8TFhbGmjVr6NatG8ePH6dOnTrs2rWLFi1aALBr1y4iIyM5ceIENWvWLFBeSUlJGAwGEhMTc63AmZGRwblz56hSpYoMXyjH5PsshBBClG751Wv3o9z3jN/ObDazZMkSUlNTiYyM5Ny5c8TGxtK1a1dbjLOzM+3atWPHjh0A7N+/n6ysrBwxoaGh1KtXzxazc+dODAaDrRAHaNmyJQaDwRaTl8zMTJKSknJsonCGDRtG3759HZ2GEEIIIcR9eSCK8cOHD+Ph4YGzszPPP/88y5cvp06dOsTGxgIQFBSUIz4oKMh2LDY2FicnJ3x8fPKNCQwMzNVuYGCgLSYvH374oW2MucFgICwszK73KYQQQgghypYHohivWbMmUVFR7Nq1ixdeeIGhQ4dy7Ngx2/E7pwhUVfWe0wbeGZNX/L2u8/rrr5OYmGjbLl68WNC3VKYZjcZCn5OVlVUMmQghhBBCONYDUYw7OTlRvXp1mjZtyocffkiDBg344osvCA4OBsjVex0XF2frLQ8ODsZoNBIfH59vzJUrV3K1e/Xq1Vy97rdzdna2zfKSvRWGqqqkGU0O2Qpzq0H79u0ZM2YMEyZMwN/fn5o1a1K/fn0yMzMBa6HdpEkTnnrqKQDOnz+Poij88MMPtG/fHhcXFxYsWIDZbGbChAl4e3vj5+fHq6++Wqg8hBBCCCFKmwdynnFVVcnMzKRKlSoEBwezfv16GjVqBFh7bTdv3szHH38MQJMmTdDr9axfv54BAwYAEBMTw5EjR5g8eTIAkZGRJCYmsmfPHpo3bw7A7t27SUxMpFWrVsX2PtKzzNR5+7diu35+jr3XDTengv/zmTt3Li+88ALbt28nKyuLPn368K9//YupU6fy1ltvce3aNb7++usc57z22mtMmTKF2bNn4+zszJQpU5g1axYzZ86kTp06TJkyheXLl9OxY8eifntCCCGEECWi3Bfjb7zxBj169CAsLIzk5GSWLFnCpk2bWLt2LYqiMH78eD744AMiIiKIiIjggw8+wM3NjUGDBgFgMBgYMWIEEydOxM/PD19fX15++WUeeughOnfuDEDt2rXp3r07I0eO5NtvvwVg1KhR9OzZs8AzqZR31atXt/3xArBgwQLatWuHp6cnU6ZM4ffff8dgMOQ4Z/z48fTr18/2+vPPP+f111+nf//+AHzzzTf89ptj/hgRQgghhCgK5b4Yv3LlCoMHDyYmJgaDwUD9+vVZu3YtXbp0AeDVV18lPT2dF198kfj4eFq0aMG6devw9PS0XWPq1KnodDoGDBhAeno6nTp1Ys6cOWi1WlvMwoULGTdunG3Wld69ezN9+vRifW+uei3H3utWrG3k13ZhNG3aNMfryMhIXn75Zd5//31ee+012rZtm+85iYmJxMTEEBkZadun0+lo2rSpDFURQgghRJlV7ovxmTNn5ntcURQmTZrEpEmT7hrj4uLCtGnTmDZt2l1jfH19WbBgwf2meV8URSnUUBFHcnd3z/HaYrGwfft2tFotp06dKtA5QgghhBDlzQNxA6cofT755BOOHz/O5s2b+e2335g9e3a+8QaDgZCQEHbt2mXbZzKZ2L9/f3GnKoQQQghRbKQYFyUuKiqKt99+m5kzZ9K6dWu++OILXnrpJc6ePZvveS+99BIfffQRy5cv58SJE7z44oskJCSUTNJCCCGEEMVAinFRojIyMnjqqacYNmwYvXr1AmDEiBF07tyZwYMHYzab73ruxIkTGTJkCMOGDSMyMhJPT08ee+yxkkpdCCGEEKLIKarc/VZqJCUlYTAYSExMzDXneEZGBufOnaNKlSq4uLg4KENR3OT7LIQQQpRu+dVr90N6xoUQQgghhHCQsjEVhxBCCCHEXVhUC2bVbH20mHO8vn1TUW89V1Us3HqefUxFzfv1bfuy3R6b/QjYpty1vb55/PZ9tx7uiL0zLg+3D2q4Pe5ugx3yu1aBY4txHEVh8stPo8BG+Ln6Fcm1SpIU40IIIYQoNJPFRJopjbSsNFKzUknLSiPdlE6GOYN0U7r1uSmDDFMGmeZMMs2ZGM1G66PF+phlzsJoMZJlySLLnGV7NKkmTBbrlmXJwmQxYVbNmC1mTKoJs8Wc47UQAN91/U6KcSGEEEKULVmWLK6nX+dGxg3iM+KJz4wnISPB9phsTCYpK4lkY7L1eWYSKVkpZJozHZ16gSkoaBRN7g0NKNieK4qCgpLnY/bx7OvdGQPc2ndzf44cbo+7Lf7213fG357/nc/vvH5esXlc9u6xBdh/1+vcJZeS5qn3vHdQKSTFuBBCCFFOGc1GYlNjuZx6mcsp1u1K2hWupl3lavpVrqZdJT4z3q42dBod7np33HRuuOnccNG54KJzwVXniqvOFWetc47NSeuEi84FvUaPk9Ypx2P2ptPocm+K9VGraNFqtOgUHRpFg1ajRatorc9vPuo0OlsBrVE0paZYFCIvUowLIYQQZZhFtRCTGsP5xPOcSzxn3ZLO8XfS31xNu1qg8bg6jQ5fZ1+8XbzxcfHBx9kHb2dvvF28MTgZ8HTytG1eTl64691tm5PWqQTepRDllxTjQgghRBlhNBs5nXCa49ePc/yGdTsVf4p0U/pdz3HWOhPqEUqoRygV3CsQ7B6Mv6s/gW6BtkeDswGNIhOsCeEIUowLIYQQpdSNjBscvHKQfVf2cSDuAH/F/4XJkvuGRb1GT7hXOJW9KlPFUIXKhsqEe4VT0aMivi6+MkxDiFJMinEhhBCilEgxprAzZic7L+9k/5X9nE08myvG4Gygtm9t6+ZXm1q+tQjzDEOnkV/pQpRF8j9XCCGEcKCLSRfZfGkzmy9tZt+Vfbl6vqt7V6dJUBOaBDWhQUADQtxDpKdbiHJEinFR6m3atIkOHToQHx+Pt7e3o9MpsPPnz1OlShUOHjxIw4YNHZ2OEKIUiU6JZtWZVfx67tdcvd+VvSrzcIWHaRbcjMaBjfF28XZMkkKIEiHFuBBCCFECko3JrP97PSvPrGT/lf22/VpFS5OgJrSt2JZ2FdtR2VDZcUkKIUqc3DpdlqkqGFMds91lyd28tG/fnjFjxjBmzBi8vb3x8/PjzTfftC3bu2DBApo2bYqnpyfBwcEMGjSIuLg4wNq73KFDBwB8fHxQFIVhw4bds02LxcLHH39M9erVcXZ2plKlSvzf//2f7fjhw4fp2LEjrq6u+Pn5MWrUKFJSUnLkPH78+BzX7Nu3b462K1euzAcffMDw4cPx9PSkUqVK/Pe//7Udr1KlCgCNGjVCURTat29f4K+ZEKL8OHrtKP/a+i86/NCBd3a8w/4r+1FQaBHSgv+0/g9bntjCzG4zGVp3qBTiQjyApGe8LMtKgw9CHdP2G5fByb3A4XPnzmXEiBHs3r2bffv2MWrUKMLDwxk5ciRGo5H333+fmjVrEhcXxz//+U+GDRvGmjVrCAsL46effqJ///6cPHkSLy8vXF1d79ne66+/znfffcfUqVN5+OGHiYmJ4cSJEwCkpaXRvXt3WrZsyd69e4mLi+PZZ59lzJgxzJkzp1BfhilTpvD+++/zxhtv8OOPP/LCCy/Qtm1batWqxZ49e2jevDkbNmygbt26ODnJXLxCPCgsqoUtl7Yw9+hc9l3ZZ9tf1VCVXtV60bNqT4Ldgx2YoRCitJBiXJSIsLAwpk6diqIo1KxZk8OHDzN16lRGjhzJ8OHDbXFVq1blyy+/pHnz5qSkpODh4YGvry8AgYGBBRoznpyczBdffMH06dMZOnQoANWqVePhhx8GYOHChaSnpzNv3jzc3a1/UEyfPp1evXrx8ccfExQUVOD39cgjj/Diiy8C8NprrzF16lQ2bdpErVq1CAgIAMDPz4/gYPmlK8SDIMOUwaqzq5h3dB7nk84DoFN0dK/SnadqP0Vdv7py86UQIgcpxssyvZu1h9pRbRdCy5Ytc/wCioyMZMqUKZjNZg4dOsSkSZOIiorixo0bWCwWAC5cuECdOnUKndrx48fJzMykU6dOdz3eoEEDWyEO0Lp1aywWCydPnixUMV6/fn3bc0VRCA4Otg2xEUI8OMwWMyvPrGR61HTi0qw/Azz1njxe83EG1RokveBCiLuSYrwsU5RCDRUpjTIyMujatStdu3ZlwYIFBAQEcOHCBbp164bRaLyva95rGIuqqnftmcrer9FobGPas2VlZeWK1+v1uc7P/mNCCFH+qarK9svb+Wz/Z5yKPwVAsHswQ+oMoV9EP9z1ZftntBCi+MkNnKJE7Nq1K9friIgITpw4wbVr1/joo49o06YNtWrVytWznD3W2mw2F6itiIgIXF1d+f333/M8XqdOHaKiokhNTbXt2759OxqNhho1agAQEBBATEyM7bjZbObIkSMFav9+8xZClC3Hrx9n5PqRvLDhBU7Fn8LTyZOXm77ML4/9wuA6g6UQF0IUiBTjokRcvHiRCRMmcPLkSRYvXsy0adN46aWXqFSpEk5OTkybNo2zZ8+ycuVK3n///RznhoeHoygKv/zyC1evXs0x60leXFxceO2113j11VeZN28eZ86cYdeuXcycOROAp556ChcXF4YOHcqRI0f4448/GDt2LIMHD7YNUenYsSOrV69m9erVnDhxghdffJGEhIRCvefAwEBcXV1Zu3YtV65cITExsVDnCyFKp9SsVP6z6z8M+GUAu2N2o9foGVpnKL/2+5WhdYfirHV2dIpCiDJEinFRIoYMGUJ6ejrNmzdn9OjRjB07llGjRhEQEMCcOXNYunQpderU4aOPPuLTTz/NcW6FChV49913+de//kVQUBBjxoy5Z3tvvfUWEydO5O2336Z27doMHDjQ1uPu5ubGb7/9xo0bN2jWrBmPP/44nTp1Yvr06bbzhw8fztChQxkyZAjt2rWjSpUqtikWC0qn0/Hll1/y7bffEhoaSp8+fQp1vhCi9NkVs4t+K/rx/cnvAehRpQcr+67k5WYvY3A2ODg7IURZpKh3DowVDpOUlITBYCAxMREvL68cxzIyMjh37hxVqlTBxcXFQRnen/bt29OwYUM+//xzR6dS6pXl77MQ5VmKMYUp+6fw418/AlDBowLvtnqXFiEtHJyZEKKk5Vev3Q+5gVMIIYTIx47oHbyz8x1iU2MBeKLmE/yzyT9xK+SsUkIIkRcpxkWZc68pD48dO0alSpVKMCMhRHlktpj5Kuorvjv8HQAVPSryXuv3aBbczMGZCSHKEynGRbHbtGlTkV4vNDSUqKiofI8LIYQ9EjMTeW3La2y/vB2AgTUHMqHJBOkNF0IUOSnGRZmj0+moXr26o9MQQpRTJ2+cZPwf47mUcgkXrQuTWk3i0aqPOjotIUQ5JcW4EEIIcdOas2t4Z8c7ZJgzqOBRgS86fEFN35qOTksIUY5JMS6EEOKBZ1EtfL7/c2YfnQ1Aq9BWTG47WaYrFEIUOynGhRBCPNBMFhOTdkxixZkVADz70LOMaTgGrUbr4MyEEA8CKcaFEEI8sIxmI//a+i/W/70eraLl3Vbv0qe6LNAlhCg5UowLIYR4IKVlpfHPTf9kx+Ud6DV6Pmn7CZ3COzk6LSHEA0bj6ASEKKxNmzahKAoJCQmOTkUIUUYlGZN4fsPz7Li8A1edK191+koKcSGEQ0jPuBBCiAfK9fTrPL/heU7cOIGnkydfd/qahoENHZ2WEOIBJT3josQZjUZHpyCEeEAlG5N5bv1znLhxAl8XX2Z3my2FuBDCoaQYL8NUVSUtK80hm6qqBc6zffv2jBkzhgkTJuDv70+XLl2YNGkSlSpVwtnZmdDQUMaNG2eLX7BgAU2bNsXT05Pg4GAGDRpEXFxcvm3s2LGDtm3b4urqSlhYGOPGjSM1NdV2/OuvvyYiIgIXFxeCgoJ4/PHHC/8FF0KUaZnmTMZtHMfJ+JP4ufgxp/scmUNcCOFwMkylDEs3pdNiUQuHtL170O5CLQs9d+5cXnjhBbZv387SpUv55JNPWLJkCXXr1iU2NpY///zTFms0Gnn//fepWbMmcXFx/POf/2TYsGGsWbMmz2sfPnyYbt268f777zNz5kyuXr3KmDFjGDNmDLNnz2bfvn2MGzeO+fPn06pVK27cuMHWrVvt/hoIIcoOs8XMa1teY9+Vfbjr3ZnReQZVDFUcnZYQQkgxLkpG9erVmTx5MgBubm4EBwfTuXNn9Ho9lSpVonnz5rbY4cOH255XrVqVL7/8kubNm5OSkoKHh0eua3/yyScMGjSI8ePHAxAREcGXX35Ju3btmDFjBhcuXMDd3Z2ePXvi6elJeHg4jRo1Kt43LIQoNVRV5f1d7/P7hd/Ra/R82eFLavvVdnRaQggBSDFeprnqXNk9aLfD2i6Mpk2b2p7/4x//4PPPP6dq1ap0796dRx55hF69eqHTWf85Hjx4kEmTJhEVFcWNGzewWCwAXLhwgTp16uS69v79+zl9+jQLFy607VNVFYvFwrlz5+jSpQvh4eG29rp3785jjz2Gm1vBe/aFEGXXtIPT+OnUT2gUDZPbTqZ5SPN7nySEECVEivEyTFGUQg0VcSR3d3fb87CwME6ePMn69evZsGEDL774Ip988gmbN2/GaDTStWtXunbtyoIFCwgICODChQt069btrjd+WiwWnnvuuRzjzrNVqlQJJycnDhw4wKZNm1i3bh1vv/02kyZNYu/evXh7exfXWxZClAILjy/ku8PfAfBmyzfpHN7ZwRnlpqoqGVkW0owm0rPMpBvNpGeZyTRZyMyykGm6+dxkxmiyYDSrZJksGM0WskwWsswWsiwqJrMFk0XFZFYxWSyYzCpmi4rJomJWVcxm63OLat2f/Zj93KJy6/G2faqqotqOWZ+rYHue/cht+1RuxVmP3Xqd/Z5vHbv1Ojvg9rjbX3PbOTevmmt/jjua7hKb8+t/z29RXpcrWHxhLl6Y6xbLVcu+BSNa0Lq6v6PTKDQpxoVDuLq60rt3b3r37s3o0aOpVasWhw8fRlVVrl27xkcffURYWBgA+/bty/dajRs35ujRo1SvXv2uMTqdjs6dO9O5c2feeecdvL292bhxI/369SvS9yWEKD02XtjIx3s+BmB0w9H8o8Y/iq0tVVVJSMsiLjmT6ymZxKdlkZBuJCEti/hUIwnpWSRnZJGSaSIlw0TyzceUTBNpRnOx5SWEKP2kGBclbs6cOZjNZlq0aIGbmxvz58/H1dWV8PBwLBYLTk5OTJs2jeeff54jR47w/vvv53u91157jZYtWzJ69GhGjhyJu7s7x48fZ/369UybNo1ffvmFs2fP0rZtW3x8fFizZg0Wi4WaNWUWBSHKq7OJZ3lj2xuoqAysOZDn6j9n1/VSM01cjE/j4o10Lt5I42J8GtHx6VxJzuRqUgZXUzLJMtvfX+ms0+DmpMVFb92cdZqbmxZnvfW5Xntrc9Ip6DTZrxV0WgWtRoNeo6DVKug1GjQaBZ1GQZu9KQoajYJWAxrFuk+jZG/YXisKOfYrtz1aj918zq3H7PMAFAUUrK9zPOfW8exXOfff3Jcjznr+7a+543he597pznNt++9yRl7xd7t2oRTJRQrbpAMaLWFermWzrC2bWYsyzdvbm48++ogJEyZgNpt56KGHWLVqFX5+foC1WH/jjTf48ssvady4MZ9++im9e/e+6/Xq16/P5s2b+fe//02bNm1QVZVq1aoxcOBAW3vLli1j0qRJZGRkEBERweLFi6lbt26JvF8hRMlKzUpl/B/jSc1KpXFgY15r/pqtSLvnuZkm/rqSzF9XkjkRa308GZvCtZTMAp3v46bH38MZHzcnvN30eLvp8XFzwuCmx8tFj6eLDg/nm9vN525OOlydtLjqtWg15b9gEkLkpKjFNaBJFFpSUhIGg4HExES8vLxyHMvIyODcuXNUqVIFFxcXB2Uoipt8n4Wwj6qqTNg0gQ0XNhDoGsj3vb7H3zXvMaSqqnL+ehp7z99g//l49v59g7NXU/OMBTC46gnzdSXMx40wXzcq+rgS5OVCoKczgV4uBHg446ST5TuEKO/yq9fuh/SMCyGEKDdmHpnJhgsb0Gl0fNbhs1yFeFxyBr8fj2Pzyavs+/sG11Jy3xge4OlMrWBPagR5UjPYk5pBnlQJcMfLRV9Sb0MI8QCRYlwIIUS5sCN6B18e+BKAN1q8QYOABgCcuZrC+mNXWHc0loMXE3LMnuGk1dAgzECTcF+aVfahYZg3fh7OjkhfCPGAkmJcCCFEmXcp+RKvbHkFFZX+Ef3pWrEvM7edY8meC5yKS8kR26Cigc61g4is5ke9CgZc9FoHZS2EEFKMCyGEKOOMZiP/3PRPkoxJVPOqTUp0T1r8uoGMLOuCYXqtQsuqfnStG0yX2kEEG+R+DCFE6SHFuBBCiDLtywNfcuLGCTSqB3/u70OU6QoAtYI9GRwZTq8GoTLeWwhRakkxLoQQosz63971zD06DxRIudQPvepLj4bBDG4ZTpNwnwJPaSiEEI4ixbgQQogy50h0Iv/36wEO8S4avQpJLXgpsi+DWlTCX27AFEKUIVKMCyGEKDMuJ6Tz8doTrIi6jEvo9+gNiXhogvhxyKdUMHg7Oj0hhCg0KcaFEEKUCSuionnz5yMkZ5jQeR5CbziIBg0zukkhLoQou2SpMPHAUxSFn3/+2dFpCCHuIjE9i5eWHOSlJVEkZ5h4KFzFv/IqAEbWH0nDwIaOTVAIIewgPeOixBmNRpycnBydhhCiDNh19joTf/iT6IR0tBqFsR2qccTyKedjk6nnV4/nGjzn6BSFEMIu0jMuil379u0ZM2YMEyZMwN/fny5duqAoCjNmzKBHjx64urpSpUoVli5dajvn/PnzKIrCkiVLaNWqFS4uLtStW5dNmzbluPaxY8d45JFH8PDwICgoiMGDB3Pt2rUcbY8bN45XX30VX19fgoODmTRpku145cqVAXjsscdQFMX2WgjhWFlmCx/9eoInv9tFdEI64X5uLH0+koCK+9gduwsXrQsftPkAvUamLBRClG1SjJdhqqpiSUtzyKbevp50AcydOxedTsf27dv59ttvAXjrrbfo378/f/75J08//TRPPvkkx48fz3HeK6+8wsSJEzl48CCtWrWid+/eXL9+HYCYmBjatWtHw4YN2bdvH2vXruXKlSsMGDAgV9vu7u7s3r2byZMn895777F+/XoA9u7dC8Ds2bOJiYmxvRZCOE5qpoln5+7jm81nUFV4olkYa8a1Idg3nS8OfAHAxKYTqWKo4uBMhRDCfjJMpQxT09M52biJQ9queWA/iptbgeOrV6/O5MmTc+z7xz/+wbPPPgvA+++/z/r165k2bRpff/21LWbMmDH0798fgBkzZrB27VpmzpzJq6++yowZM2jcuDEffPCBLX7WrFmEhYXx119/UaNGDQDq16/PO++8A0BERATTp0/n999/p0uXLgQEBADg7e1NcHDwfXwlhBBF6VpKJsPn7OXQpURc9Vo+G9CAHg+FoKoqr237gHRTOo0DGzOg5oB7X0wIIcqAct8z/uGHH9KsWTM8PT0JDAykb9++nDx5MkfMsGHDUBQlx9ayZcscMZmZmYwdOxZ/f3/c3d3p3bs3ly5dyhETHx/P4MGDMRgMGAwGBg8eTEJCQnG/xTKhadOmufZFRkbmen1nz/jtMTqdjqZNm9pi9u/fzx9//IGHh4dtq1WrFgBnzpyxnVe/fv0c1wwJCSEuLs6+NySEKHJ/X0/l8Rk7OHQpER83PYtGtqDHQyEArP97PZsvbUan0fFO5DtolHL/60sI8YAo9z3jmzdvZvTo0TRr1gyTycS///1vunbtyrFjx3B3d7fFde/endmzZ9te33mD4fjx41m1ahVLlizBz8+PiRMn0rNnT/bv349WqwVg0KBBXLp0ibVr1wIwatQoBg8ezKpVq4rlvSmurtQ8sL9Yrl2Qtgvj9q91vtctwGp52TEWi4VevXrx8ccf54oJCQmxPdfrc44pVRQFi8VSoHyEECXj8KVEnpmzh2spRir6uDJveHOqBngAkGxM5qM9HwEwot4IqnpXdWSqQghRpMp9MZ5dGGebPXs2gYGB7N+/n7Zt29r2Ozs733WYQmJiIjNnzmT+/Pl07twZgAULFhAWFsaGDRvo1q0bx48fZ+3atezatYsWLVoA8N133xEZGcnJkyepWbNmkb83RVEKNVSktNm1axdDhgzJ8bpRo0a5YrK/TyaTif379zNmzBgAGjduzE8//UTlypXR6e7/n7Jer8dsNt/3+UII+2z56yrPL9hPmtFMnRAv5gxvRqCni+34Fwe+4Gr6VSp7VWZk/ZEOzFQIIYreA/c5X2JiIgC+vr459m/atInAwEBq1KjByJEjcwxj2L9/P1lZWXTt2tW2LzQ0lHr16rFjxw4Adu7cicFgsBXiAC1btsRgMNhi7pSZmUlSUlKO7UGydOlSZs2axV9//cU777zDnj17bIV2tq+++orly5dz4sQJRo8eTXx8PMOHDwdg9OjR3LhxgyeffJI9e/Zw9uxZ1q1bx/DhwwtVXFeuXJnff/+d2NhY4uPji/Q9CiHyt+PMNZ6du480o5nW1f34/rmWOQrxqLgofjj5AwBvtXwLZ60sdS+EKF8eqGJcVVUmTJjAww8/TL169Wz7e/TowcKFC9m4cSNTpkxh7969dOzYkczMTABiY2NxcnLCx8cnx/WCgoKIjY21xQQGBuZqMzAw0BZzpw8//NA2vtxgMBAWFlZUb7VMePfdd1myZAn169dn7ty5LFy4kDp16uSI+eijj/j4449p0KABW7duZcWKFfj7+wPWP4i2b9+O2WymW7du1KtXj5deegmDwYBGU/B/2lOmTGH9+vWEhYXl6pkXQhSfY5eTeG7efoxmC93qBjFrWDM8XW4NK8uyZPHuzndRUelTrQ/NQ5o7MFshhCge5X6Yyu3GjBnDoUOH2LZtW479AwcOtD2vV68eTZs2JTw8nNWrV9OvX7+7Xk9V1RxjnPMa73xnzO1ef/11JkyYYHudlJRULgvyO+cGzxYaGsq6devyPbd27drs2rXrrscjIiJYtmxZodq+c7XNXr160atXr3zzEEIUrUvxaQybvYfkTBMtqvjyxRONcNZpc8TMOzqP0wmn8XH24eWmLzsoUyGEKF4PTM/42LFjWblyJX/88QcVK1bMNzYkJITw8HBOnToFQHBwMEajMdcQhri4OIKCgmwxV65cyXWtq1ev2mLu5OzsjJeXV45NCCHKu/hUI0Nn7SEuOZOaQZ78d0hTXPQ5C/GLyRf55s9vAHi52ct4u3g7IFMhhCh+5b4YV1WVMWPGsGzZMjZu3EiVKvdeJOL69etcvHjRNiNHkyZN0Ov1toViwLrgzJEjR2jVqhVgnYIvMTGRPXv22GJ2795NYmKiLUYIIR50GVlmnp23jzNXUwkxuDBneDMMrrlX0fx076dkmDNoEdyCXlXlkyshRPlV7oepjB49mkWLFrFixQo8PT1t47cNBgOurq6kpKQwadIk+vfvT0hICOfPn+eNN97A39+fxx57zBY7YsQIJk6ciJ+fH76+vrz88ss89NBDttlVateuTffu3Rk5cqRthclRo0bRs2fPYplJpay71wqelStXLvQqn0KI0s1ktjB28UH2/x2Pl4uOucObE2LIPU3q7pjdbLy4Ea2i5fUWrxdoylMhhCiryn0xPmPGDADat2+fY//s2bMZNmwYWq2Ww4cPM2/ePBISEggJCaFDhw58//33eHp62uKnTp2KTqdjwIABpKen06lTJ+bMmWObYxxg4cKFjBs3zjbrSu/evZk+fXrxv0khhCgD/rP6OOuPXcFJp2HmsGbUCPLMFWOymPh4r3XtgAE1B1DNu1pJpymEECVKUaX7sdRISkrCYDCQmJiYa/x4RkYG586do0qVKri4uNzlCqKsk++zKK9+PRzDCwsPAPDN043pXi8kz7gfTv7A+7vex8vJi9WPrZax4kKIUie/eu1+lPsx40IIIRzr4o00Xv3pEAAvtK9210I8yZjE9IPWTxNfbPiiFOJCiAeCFONCCCGKTZbZwrglB0nOMNGokjcTutS4a+y3f35LfGY8VQ1VGVBzQAlmKYQQjiPFuBBCiGLz2fq/OHghAU8XHV8+0Qi9Nu9fO+cTz7Po+CIAXm32KnpN7hlWhBCiPJJiXAghRLHY8tdVZmw6A8Dk/vUJ83W7a+yn+z7FpJpoU6ENrSu0LqkUhRDC4aQYF0IIUeTikjOY8EMUAE+3rESPh/IeJw6wI3oHmy9tRqfoeKXZKyWUoRBClA5SjAshhChSFovKhO//5FqKkVrBnrz5aJ27xposJibvnQzAE7WeoIrh3guzCSFEeSLFuBBCiCI1a/s5tp2+hqtey/RBjXItdX+7FadXcCbxDN7O3jzf4PkSzFIIIUoHKcZFiVBVlcmTJ1O1alVcXV1p0KABP/74IwCbNm1CURR+++03GjVqhKurKx07diQuLo5ff/2V2rVr4+XlxZNPPklaWprtmu3bt2fMmDGMGTMGb29v/Pz8ePPNN2XlTiEc6HJCOp+t/wuAN3vWpnpg7oV9smWYMvj6z68BGPnQSAzOhhLJUQghSpNyvwJneaaqKiajxSFt65w0hVqi+s0332TZsmXMmDGDiIgItmzZwtNPP01AQIAtZtKkSUyfPh03NzcGDBjAgAEDcHZ2ZtGiRaSkpPDYY48xbdo0XnvtNds5c+fOZcSIEezevZt9+/YxatQowsPDGTlyZJG+XyFEwUxaeZQ0o5mm4T482axSvrGLTywmLi2OEPcQBtYaWEIZCiFE6SLFeBlmMlr470ubHdL2qC/aoXe++0fPt0tNTeWzzz5j48aNREZGAlC1alW2bdvGt99+y6hRowD4z3/+Q+vW1lkURowYweuvv86ZM2eoWrUqAI8//jh//PFHjmI8LCyMqVOnoigKNWvW5PDhw0ydOlWKcSEcYP2xK6w7dgWdRuH/HnsIjebuf7AnGZP43+H/AdYFfpy1ziWVphBClCoyTEUUu2PHjpGRkUGXLl3w8PCwbfPmzePMmTO2uPr169ueBwUF4ebmZivEs/fFxcXluHbLli1z9NBHRkZy6tQpzGZzMb4jIcSdUjNNvLPiCADPtqlKzeC7D08BmHV4FknGJKp7V6dX1V4lkaIQQpRK0jNehumcNIz6op3D2i4oi8U6lGb16tVUqFAhxzFnZ2dbQa7X31rkQ1GUHK+z92VfSwhRunzx+ykuJ2ZQ0ceVlzpF5BsblxbHwuMLARjXaBxaTcE+ZRNCiPJIivEyTFGUAg8VcaQ6derg7OzMhQsXaNcu9x8Pt/eOF9auXbtyvY6IiECrLf1fFyHKi+MxSczcdg6A9/vUw9Up//9/3/z5DRnmDBoFNqJ9WPsSyFAIIUovKcZFsfP09OTll1/mn//8JxaLhYcffpikpCR27NiBh4cH4eHh933tixcvMmHCBJ577jkOHDjAtGnTmDJlShFmL4TIj8Wi8sbyw5gtKj3qBdOhVmC+8ecTz7Ps1DIAxjceX6gbwYUQojySYlyUiPfff5/AwEA+/PBDzp49i7e3N40bN+aNN96wa+jJkCFDSE9Pp3nz5mi1WsaOHWu7IVQIUfwW773AwQsJuDtpeadX3XvGTzs4DbNqpm3FtjQOalwCGQohROmmqDIpc6mRlJSEwWAgMTERLy+vHMcyMjI4d+4cVapUwcXFxUEZli7t27enYcOGfP75545OpcjI91mUJTdSjbT/5A+SMky83bMOwx/Of/XMo9eO8sTqJ1BQ+LH3j9TwqVFCmQohRNHJr167HzKbihBCiPsyfeNpkjJM1AnxYkjkvYebfXHgCwB6Vu0phbgQQtwkxbgQQohCu3gjjQW7/gbg9UdqodPm/+vkwJUD7IzZiU7R8WLDF0siRSGEKBNkzLgoszZt2uToFIR4YE1d/xdGs4XW1f1oExFwz/ivo6zL3veN6EtFz4rFnZ4QQpQZ0jMuhBCiUI7HJLE8KhqA17rXumf83ti97I7djU6jY9RDcoO1EELcTopxIYQQhTJ57QlUFR6tH0L9it75xqqqyldRXwHQP6I/IR4hJZChEEKUHVKMCyGEKLBdZ6/zx8mr6DQKL3etec/4PbF72H9lP3qNnmcferYEMhRCiLJFinEhhBAFoqoqH/16AoAnmodRxd/9nvHZY8Ufr/E4we7BxZ6jEEKUNVKMCyGEKJDfjsYSdTEBV72WcZ0i7hm/M2YnB+IO4KRxkl5xIYS4CynGhRBC3JPJbGHybycBeLZNFQI981+U6vZe8QE1BxDoFljsOQohRFkkxbgoFypXrpxjJU5FUfj5558BOH/+PIqiEBUV5ZDchCgPlu6/xNmrqfi46RnVtuo947df3s6fV//EWevM8HrDSyBDIYQom6QYF+XC3r17GTWqYFOmbdq0CUVRSEhIKNIcJk2aRMOGDQEYO3YsERF5f4wfHR2NVqtl2bJlRdq+EMXFaLLw5e+nABjTMQJPF32+8bf3ig+sOZAAt3vPQy6EEA8qKcZFuRAQEICbm5uj07AZMWIEp0+fZuvWrbmOzZkzBz8/P3r16uWAzIQovGUHLhGTmEGgpzNPtah0z/it0Vs5fO0wLloXnqn3TAlkKIQQZZcU46JEJCcn89RTT+Hu7k5ISAhTp06lffv2jB8/HoCvv/6aiIgIXFxcCAoK4vHHH7ed2759e8aMGcOYMWPw9vbGz8+PN998E1VVbTF3DlO5m/Pnz9OhQwcAfHx8UBSFYcOGAdbevMmTJ1O1alVcXV1p0KABP/74Y47zjx49yqOPPoqXlxeenp60adOGM2fO5GqnYcOGNG7cmFmzZuU6NmfOHIYMGYJen3/vohClgclsYcZm67/xUW2r4qLX5huvqirf/vktAE/UegJ/V/9iz1EIIcoynaMTEPdPVVVMmZkOaVvn7IyiKAWOnzBhAtu3b2flypUEBQXx9ttvc+DAARo2bMi+ffsYN24c8+fPp1WrVty4cSNXj/LcuXMZMWIEu3fvZt++fYwaNYrw8HBGjhxZqLzDwsL46aef6N+/PydPnsTLywtXV1cA3nzzTZYtW8aMGTOIiIhgy5YtPP300wQEBNCuXTuio6Np27Yt7du3Z+PGjXh5ebF9+3ZMJlOebY0YMYJXX32VadOm4eHhAcDmzZs5ffo0w4fLGFpRNqw5Esvf19PwcdPzZPN794rvitnFoWuHcNY6M7Tu0BLIUAghyjYpxsswU2YmXw59/N6BxWDc3B/Ru+Q/m0K25ORk5s6dy6JFi+jUqRMAs2fPJjQ0FIALFy7g7u5Oz5498fT0JDw8nEaNGuW4RlhYGFOnTkVRFGrWrMnhw4eZOnVqoYtxrVaLr68vAIGBgXh7ewOQmprKZ599xsaNG4mMjASgatWqbNu2jW+//ZZ27drx1VdfYTAYWLJkia1Xu0aNGndta9CgQUycOJGlS5fyzDPWj+pnzZpFZGQkderUKVTeQjiCqqp8/cdpAJ5pXQV353v/yvj2kLVX/PEaj0uvuBBCFIAMUxHF7uzZs2RlZdG8eXPbPoPBQM2a1tX7unTpQnh4OFWrVmXw4MEsXLiQtLS0HNdo2bJljp74yMhITp06hdlsLpIcjx07RkZGBl26dMHDw8O2zZs3zzYMJSoqijZt2hR4eIm3tzf9+vWzDVVJTk7mp59+kl5xUWb8fjyOE7HJuDtpGRpZ+Z7x+2L32VbbHFZ3WLHnJ4QQ5YH0jJdhOmdnxs398d6BxdR2QWWP7b5zWEv2fk9PTw4cOMCmTZtYt24db7/9NpMmTWLv3r22nuviZrFYAFi9ejUVKlTIccz55nvNHs5SGCNGjKBTp06cOnWKzZs3AzBw4EA7sxWi+KmqyvSbveJPR4ZjcLv3H6H/PfRfAPpW7yurbQohRAFJMV6GKYpS4KEijlStWjX0ej179uwhLCwMgKSkJE6dOkW7du0A0Ol0dO7cmc6dO/POO+/g7e3Nxo0b6devHwC7du3Kcc1du3YRERGBVpv/zWR5cXJyAsjRq16nTh2cnZ25cOGCLac71a9fn7lz55KVlVXg3vEOHTpQtWpV5syZwx9//MGAAQPw9PQsdM5ClLSdZ68TdTEBJ52GEQ9XuWf8oauH2BmzE62iZcRDI0ogQyGEKB+kGBfFztPTk6FDh/LKK6/g6+tLYGAg77zzDhqNBkVR+OWXXzh79ixt27bFx8eHNWvWYLFYbMNYAC5evMiECRN47rnnOHDgANOmTWPKlCn3lU94eLit3UceeQRXV1c8PT15+eWX+ec//4nFYuHhhx8mKSmJHTt24OHhwdChQxkzZgzTpk3jiSee4PXXX8dgMLBr1y6aN2+eI9fbKYrCM888w2effUZ8fDyffPLJfeUsREn76mav+BPNwu652ibc6hXvWbUnFTwq3CNaCCFENhkzLkrEZ599RmRkJD179qRz5860bt2a2rVr4+Ligre3N8uWLaNjx47Url2bb775hsWLF1O3bl3b+UOGDCE9PZ3mzZszevRoxo4dW+BFfu5UoUIF3n33Xf71r38RFBTEmDFjAHj//fd5++23+fDDD6lduzbdunVj1apVVKli7RX08/Nj48aNpKSk0K5dO5o0acJ33313z17yYcOGkZiYSM2aNWnduvV95SxESTp4IZ7tp6+j0ygFWm3z+PXjbL60GY2i4dmHni2BDIUQovxQ1NsnaxYOlZSUhMFgIDExES8vrxzHMjIyOHfuHFWqVMGlDAxNuZfU1FQqVKjAlClTGDEi/4+027dvT8OGDQs0j3hZV96+z6JsGjlvH+uPXaF/44pMGdDgnvETNk1g/d/r6VGlB5PbTi6BDIUQwnHyq9fuhwxTESXi4MGDnDhxgubNm5OYmMh7770HQJ8+fRycmRDididjk1l/7AqKAi+0v3ev+On406z/ez0Aox66v0+rhBDiQSbFuCgxn376KSdPnsTJyYkmTZqwdetW/P1lHmIhSpNvt1in8uxeN5jqgfe+2fi7w98B0LlSZ6r7VC/W3IQQojySYlyUiEaNGrF///77OnfTpk1Fm4wQIk9xyRn88mcMQIHGil9IusDa82ut8fWlV1wIIe6H3MAphBACgIW7LmA0W2hUyZtGlXzuGT/ryCwsqoU2FdpQ2692CWQohBDljxTjQgghyDSZWbj7bwCGt773vOJXUq+w4swKAEbWH1msuQkhRHlW7MNUVq1axQ8//MC1a9eoUqUKI0eOpFGjRsXdrBBCiEJY9WcM11KMBHu50L3evVfPnHdsHiaLicaBjWkUKD/ThRDiftnVM/7HH38QGBhIpUqVSEhIyHX8rbfeom/fvixatIh169bx7bff0qJFCxYuXGhPs0IIIYqQqqrM2nYOgCGtwtFr8//VkJCRwNK/lgLSKy6EEPayqxhfs2YN165do2XLlnh7e+c4dujQIT744ANUVUVVVby9vVFVFZPJxKhRo/j777/taVoIIUQR2XPuBsdiknDRa3iyWaV7xi86sYh0Uzq1fWvTOlQWshJCCHvYVYxv27YNRVHo0qVLrmMzZsxAVVV8fHzYv38/169fZ8+ePfj6+pKRkcE333xjT9NCCCGKyKzt1l7xxxpVxMfdKd/Y1KxUFh63fro5/KHhKIpS7PkJIUR5ZlcxHhsbC0CtWrVyHfvll19QFIXRo0fbxog3bdqUMWPGoKoqGzZssKdpIXKoXLlyjhU6FUXh559/BuD8+fMoikJUVNQ9r1OYWCHKg4s30lh/7AoAw1tXvmf8j3/9SJIxiXCvcLpUyt0RI4QQonDsKsbj4uIAMBgMOfafOXOG6OhoAPr165fjWJs2bQA4ffq0PU0LkcPevXsZNcr+eY7DwsKIiYmhXr16BYqfNGkSDRs2BGDs2LFERETkGRcdHY1Wq2XZsmV25yhEUZq74zwWFdpE+BMRlP8iP0azkXlH5wEwvN5wtBptSaQohBDlml3FuKqqACQmJubYv3XrVsBapGcXKtn8/PwASEtLs6dpIXIICAjAzc3N7utotVqCg4PR6Qo/0dCIESM4ffq07d//7ebMmYOfnx+9evWyO0chikpKponv910ECjad4cozK4lLjyPQLZBeVeXfshBCFAW7ivHgYOv0V8ePH8+x/7fffgOgdevcN/akpqYC4ONz7wUlRPmRnJzMU089hbu7OyEhIUydOpX27dszfvx4AL7++msiIiJwcXEhKCiIxx9/3HZu+/btGTNmDGPGjMHb2xs/Pz/efPNN2x+DkHuYSn7i4+N56qmnCAgIwNXVlYiICGbPng3kPUzl6NGjPProo3h5eeHp6UmbNm04c+ZMrus2bNiQxo0bM2vWrFzH5syZw5AhQ9Dr9QXKUYiS8NP+SyRnmKjq7067GgH5xposJmYdsf7bHlpnKHqt/FsWQoiiYFcx3rJlS1RVZcaMGbae7rNnz7JixYq73tj5119/AbcKeXH/VFXFYjQ7ZLu9EC6ICRMmsH37dlauXMn69evZunUrBw4cAGDfvn2MGzeO9957j5MnT7J27Vratm2b4/y5c+ei0+nYvXs3X375JVOnTuV///vffX3d3nrrLY4dO8avv/7K8ePHmTFjBv7+/nnGRkdH07ZtW1xcXNi4cSP79+9n+PDhmEymPONHjBjB0qVLSUlJse3bvHkzp0+fZvjw4feVrxDFwWJRmbPjPADPtK6MRpP/jZjr/17PxeSLeDt783iNx/ONFUIIUXB2Lfrz7LPPsmTJEg4dOkS9evVo3LgxW7ZsISMjAzc3NwYNGpTrnC1btgBQp04de5oWgJpl4fLbOxzSduh7rVCcCjZeNDk5mblz57Jo0SI6deoEwOzZswkNDQXgwoULuLu707NnTzw9PQkPD8+1MFRYWBhTp05FURRq1qzJ4cOHmTp1KiNHFn6O4wsXLtCoUSOaNm0KWHvV7+arr77CYDCwZMkSW692jRo17ho/aNAgJk6cyNKlS3nmmWcAmDVrFpGRkfJvXpQqm09d5dy1VDxddPRrXDHfWFVVmXl4JgCDag/CTW//kDAhhBBWdvWMd+zYkfHjx6OqKufPn2f58uVcu3YNgE8++SRXb2NGRka+veaifDp79ixZWVk0b97cts9gMFCzZk0AunTpQnh4OFWrVmXw4MEsXLgw1z0FLVu2zDGFWmRkJKdOncJsNhc6nxdeeIElS5bQsGFDXn31VXbsuPsfNFFRUbRp06bAw0u8vb3p16+fbahKcnIyP/30k/SKi1Jn4a4LAPyjSRjuzvn3y2yL3sbJ+JO46lwZVCt3J4sQQoj7Z1fPOMBnn31Gx44dWbp0KbGxsYSEhDBkyBA6duyYK3blypV4eXlhMBikGC8Cil5D6HutHNZ2QWUPablzPuLs/Z6enhw4cIBNmzaxbt063n77bSZNmsTevXtzLSZVFHr06MHff//N6tWr2bBhA506dWL06NF8+umnuWJdXV0Lff0RI0bQqVMnTp06xebNmwEYOHCg3XkLUVQuJ6Sz8YR1OsOnWt57kZ/sseL/qPEPDM6Ge0QLIYQoDLuLcYCePXvSs2fPe8YNGDCAAQMGFEWTAmtxW9ChIo5UrVo19Ho9e/bsISwsDICkpCROnTpFu3btANDpdHTu3JnOnTvzzjvv4O3tzcaNG21TY+7atSvHNXft2kVERARa7f29/4CAAIYNG8awYcNo06YNr7zySp7FeP369Zk7dy5ZWVkF7h3v0KEDVatWZc6cOfzxxx8MGDAAT8/8p4wToiQt2XMBiwqRVf2oFuCRb+yfV/9k35V96DQ6BtcZXEIZCiHEg6NIinEh8uPp6cnQoUN55ZVX8PX1JTAwkHfeeQeNRoOiKPzyyy+cPXuWtm3b4uPjw5o1a7BYLLZhLAAXL15kwoQJPPfccxw4cIBp06YxZcqU+8rn7bffpkmTJtStW5fMzEx++eUXateunWfsmDFjmDZtGk888QSvv/46BoOBXbt20bx58xz53U5RFJ555hk+++wz4uPj+eSTT+4rTyGKQ5bZwpK91ukMC9QrftjaK/5olUcJdpcb74UQoqjZNWZco9Gg0+k4duxYgc85c+aM7Tzx4Pjss8+IjIykZ8+edO7cmdatW1O7dm1cXFzw9vZm2bJldOzYkdq1a/PNN9+wePFi6tatazt/yJAhpKen07x5c0aPHs3YsWPve5EfJycnXn/9derXr0/btm3RarUsWbIkz1g/Pz82btxISkoK7dq1o0mTJnz33Xf37CUfNmwYiYmJ1KxZM88pPoVwlN+PXyEuORN/D2e61sm/uD6bcJaNFzcC1kV+hBBCFD1FLewcdbfJ7tk8fPhwgWeKOHPmDBERESiKcl8335VnSUlJGAwGEhMT8fLyynEsIyODc+fOUaVKFVxcXByUYdFJTU2lQoUKTJkyhREjRuQb2759exo2bFjgecTLsvL2fRalz+CZu9l66hqjO1TjlW618o19a/tb/Hz6ZzqEdeDLjl+WUIZCCFG65Vev3Y8S756+2818onw7ePAgJ06coHnz5iQmJvLee+8B0KdPHwdnJsSD49y1VLaeuoaiwBPN8h+iEpsayy9nfwFgxEP5/8EshBDi/pV4MX79+nUA3N3dS7pp4WCffvopJ0+exMnJiSZNmrB169a7LrYjhCh6i/dYpzNsXyOAMN/85wqff2w+JouJJkFNaBDQoCTSE0KIB1KRFOMF7eVOTU1l2rRpgHWGDfHgaNSoEfv377+vczdt2lS0yQjxAMrIMrN0380bN1uE5xubmJnIj3/9CMCIetIrLoQQxalQN3BWrVo1x5ata9euuY7duVWoUAFvb28WL16Moij06tWryN9MXj788EOaNWuGp6cngYGB9O3bl5MnT+aIUVWVSZMmERoaiqurK+3bt+fo0aM5YjIzMxk7diz+/v64u7vTu3dvLl26lCMmPj6ewYMHYzAYMBgMDB48mISEhOJ+i0IIcU9rj8QSn5ZFqMGFDrUC841dcmIJaaY0avjU4OEKD5dQhkII8WAqVM/4+fPnc+1TVZXo6OhCNdqyZUteffXVQp1zvzZv3szo0aNp1qwZJpOJf//733Tt2pVjx47ZhspMnjyZzz77jDlz5lCjRg3+85//0KVLF06ePGmbH3r8+PGsWrWKJUuW4Ofnx8SJE+nZsyf79++3zXU9aNAgLl26xNq1awEYNWoUgwcPZtWqVSXyXoUQ4m4W7PobgCebV0KrufunmemmdBadWARYZ1CR+3uEEKJ4FaoYHzp0aI7Xc+fORVEUevfune9KiYqi4OLiQkhICK1ataJjx44l9gM+uzDONnv2bAIDA9m/fz9t27ZFVVU+//xz/v3vf9sWmJk7dy5BQUEsWrSI5557jsTERGbOnMn8+fPp3LkzAAsWLCAsLIwNGzbQrVs3jh8/ztq1a9m1axctWrQA4LvvviMyMpKTJ0/edU7qwrJj8htRBsj3VxSHE7FJ7Ps7Hq1GYWCzsHxjfz79MzcyblDBowLdKncroQyFEOLBVahifPbs2Tlez507F4D/+7//K/DUho6WmJgIgK+vLwDnzp0jNjaWrl272mKcnZ1p164dO3bs4LnnnmP//v1kZWXliAkNDaVevXrs2LGDbt26sXPnTgwGg60QB+snAAaDgR07duRZjGdmZpKZmWl7nZSUdNe8s+e1TktLu68l2kXZkJaWBlDg1T6FKIhFu603bnatE0Sg192nzDRZTMw9av25PqTOEHQaWQ9CCCGKm10/ad955x0AAgPzH39YWqiqyoQJE3j44YepV68eALGxsQAEBQXliA0KCuLvv/+2xTg5OeHj45MrJvv82NjYPL8OgYGBtpg7ffjhh7z77rsFyl2r1eLt7U1cXBwAbm5u8vFxOaKqKmlpacTFxeHt7W0b+iSEvdKMJpYfsA4lvNeNm+v/Xk90SjQ+zj48FvFYSaQnhBAPvCIpxsuKMWPGcOjQIbZt25br2J2Fraqq9yx274zJKz6/67z++utMmDDB9jopKYmwsLt/hBwcbF0tL7sgF+WPt7e37fssRFFYcziW5EwTlXzdaFXN765xqqoy+4j1088naz+Jq04+gRNCiJLwwHwGOXbsWFauXMmWLVuoWLGibX924RMbG0tISIhtf1xcnK23PDg4GKPRSHx8fI7e8bi4OFq1amWLuXLlSq52r169mqvXPZuzszPOzs4Ffg+KohASEkJgYCBZWVkFPk+UDXq9XnrERZH7Ya91OsMBTSuiyefGzZ0xOzl+4ziuOleerPlkSaUnhBAPvCIrxi0WC8eOHePs2bMkJycXaKn7IUOGFFXzd6WqKmPHjmX58uVs2rSJKlWq5DhepUoVgoODWb9+PY0aNQLAaDSyefNmPv74YwCaNGmCXq9n/fr1DBgwAICYmBiOHDnC5MmTAYiMjCQxMZE9e/bQvHlzAHbv3k1iYqKtYC8qWq1WijYhxD2dvZrCnvM30CjweJP8b9zM7hXvF9EPbxfvEshOCCEEFEExnp6ezn/+8x++++472+qaBaEoSokU46NHj2bRokWsWLECT09P2/htg8GAq6sriqIwfvx4PvjgAyIiIoiIiOCDDz7Azc2NQYMG2WJHjBjBxIkT8fPzw9fXl5dffpmHHnrINrtK7dq16d69OyNHjuTbb78FrFMb9uzZs8hmUhFCiML4YZ91LYR2NQIINtz9xs2j14+yK2YXWkXL4DqDSyo9IYQQ2FmMp6en07FjR/bs2VNqp2SbMWMGAO3bt8+xf/bs2QwbNgyAV199lfT0dF588UXi4+Np0aIF69ats80xDjB16lR0Oh0DBgwgPT2dTp06MWfOnBw91AsXLmTcuHG2WVd69+7N9OnTi/cNCiFEHrLMFn7cby3GBzarlG/snCNzAOhWuRsVPCoUd2pCCCFuo6h2VNEffPABb775JgD16tVjzJgxNGnSBF9fXzSaey/uGR6e/539D5qkpCQMBgOJiYl4eXk5Oh0hRBm27mgso+bvx9/DiZ2vd0Kvzftn8sWki/T8uScW1cKPvX6kpq98kieEEPkp6nrNrp7x77//HoBWrVqxceNGnJyc7E5ICCGE/X7YZ71xs1/jinctxAHmHpuLRbXQOrS1FOJCCOEA9+6+zseZM2dQFIVXX31VCnEhhCglriRlsPGEdQrUAU3vfuPm9fTr/Hz6ZwCG1xteEqkJIYS4g13FeHYBXqlS/uMRhRBClJwf91/CokKTcB+qB3rcNW7xicVkmjOp61eXZsHNSjBDIYQQ2ewqxmvVqgVw1xUmhRBClCxVVVl6c4jKwGZ37xVPy0pj8YnFgLVXXFb0FUIIx7CrGB82bJj1B//SpUWVjxBCCDvsPneD89fTcHfS8uhDIXeNW3ZqGUnGJMI8w+hUqVMJZiiEEOJ2dhXjI0eOpEOHDsybN4/FixcXVU5CCCHu0/c3V9zs1SAUd+e879HPsmQx79g8AIbVHYZWI4uICSGEo9g1m8rFixeZNm0ao0aN4umnn2b58uUMGjSIWrVq4ebmds/zZay5EEIUncT0LNYcjgFgQD5DVNadX0dMagy+Lr70rta7pNITQgiRB7uK8cqVK9vGGaqqyk8//cRPP/1UoHMVRcFkMtnTvBBCiNus/PMymSYLNYI8aBTmnWeMqqrMPjIbgEG1BuGiu/vKnEIIIYqfXcU4kGPlzdK6CqcQQjwIfrx54+aApmF3vSFz5+WdnIw/iavOlSdqPVGS6QkhhMiDXcX47NmziyoPIYQQdvjrSjJ/XkpEp1Ho2+juS9rPPmr9ud0/oj8GZ0NJpSeEEOIu7CrGhw4dWlR5CCGEsMNP+y8B0KFWIP4eznnGHLt+jF0xu9AqWgbXGVyS6dlFVVUsSUmYrl/HdPUa5oQE1MwMLBkZqJlG6/PMTDCbAQWUm5tGQVEUFL0excnJuulvPjo7oXF1RePqiuLiisbNFY2LC4qrKxp3d+s5Mt2jEKIE2D1MRQghhGOZzBaWHYwG4PEmFe8aN+fIHAC6V+lOqEdoSaRWYKqqYrp6lcxTp2yb8fQZsq7GYb56DTUrq2QT0umsxbqb261HNzcUN1c0bu637XNFcXFB42p9rnF1tRb0rjeLexdXNK4u1hhnZxQXFxRnZyn2hRA2UowLIUQZt/XUNa4mZ+Lr7kSHmoF5xlxKvsRvf/8GwDN1nynJ9PKkqirGM2dI2bqN1G3byDh6FHNCQr7naDw90fn5ofX1tRa6zs4oLs5onG8+12oB1Xr/kqqCClgsqFlZqEYjapYRi9F4szc9E0tmBmpaOpaMDCzp6ajp6ahGo7UxkwlLcjKW5OTi+QIoijVnZ2c02b32uTa9tWjXO9181KPodNZNrwOdDkV3ax86LYpWh6LTgtb6HK0GRaO17tNoUbSaHI9oFOvXTaNB0Wjg5qZoNKBorMez96OgaLI/dbC+zv70Icc+xTpJAxqNdf/t+7K3m18D23Gyn95x/PZHlJwPuY7ndU7ur3vhvk15xBfFtYviDzH5Yy4XjYuL9f9CGVNkGVssFjZt2sTOnTuJjY0lLS2N//znP4SE3Fp0wmg0YjKZ0Gq1ODvn/TGqEEKIwvnx5hCVPg1DcdLlvXzE/GPzsagWWoW2oqZvzZJMz8aSmkrKtu2kbttKytZtmO5cvVmjwalSJZwjInCuUQPniOroQ0LQ+fuj9fdHUwK/N1STCUt6Opa0dCxpqVjS0lDT0m7uS8OSetvztFTUdGshb93SrMV9erq10E+/OZQmu9jPzLT+kQCgqqjZx4r9XQnxYKg0exbukZGOTqPQiqQYX716NePGjeP8+fM59k+cODFHMT5z5kzGjBmDh4cHly9fxt3dvSiaF0KIB1ZCmpH1x64Adx+iEp8Rz7JTywB4pl7J94pnnjpF/OIlJK5YgSU11bZfcXLCrXlz3B9ujVvTZjhXr4bGxbFTLSo6HVpPT7SenkV+bVVVrb30mZnWIvzmeHfVaLRttp57oxHVZLqtVz/r1j5TFphMqCbzba/NqGYzqtl067kpC8wWVIvZ+mg25Xx98xGLBdVy81G1gEUFs9n63Gy5+SnDzU8cbHGqbT8WCyqq9Tw156Za33jO7eY+27E7Y27br9764uX/eOdzIcoQu4vx//3vfzz33HO2aQ39/f25du1anh/tjBgxgjfffJOEhASWL1/O008/bW/zQgjxQFv152WMZgu1Q7yoG5r37ChLTi4hw5xBbd/atAhuUSJ5qVlZJP/+O/GLFpO2Z49tvz4sDM+OHXB/+GHcmjZF4+paIvmUBoqioDg5gZMTFEOxL3JTC1KsF2Z/YQv+IvgDoUz+ieGgP4zK4hAVsLMYP336NKNHjwagY8eOTJ8+nVq1aqHR5P0xqZOTE/379+d///sf69atk2JcCCHslD1E5W694hmmDBYfXwzA8HrDi/2mQdVkImHpUq59PQPT1avWnRoNnp064vPkk7i1bGkdgyxECVDyGk9expTNrEVh2FWMf/7552RlZVGvXj3WrFmDk5PTPc9p06YN//vf/4iKirKnaSGEeODdPrd4n4Z5z46y4vQK4jPjqeBRgc7hnYs1n9QdO7jy4UdknjoFgNbfH+9/PI7PgAHobxuyKIQQ4ha7ivHff/8dRVEYP358gQpxgGrVqgFw4cIFe5oWQogH3r3mFjdbzMw9NheAwXUGo9MUz0e4xvPnuTL5E1I2bgRAYzAQMGYMPgMHWIdlCCGEuCu7fjJfvGhderlhw4YFPif7ps20tDR7mhZCiAdaQeYW33hxIxeTL2JwNvBY9ceKPAeL0ci1L7/k+tx5kJUFWi0+Tz5JwJjRaL29i7w9IYQoj+wqxrPHYqmFGKh/9eYYQi8vL3uaFkKIB9q95hZXVdW2yM/AmgNx07sVafvGCxeIHv9PMo4dA8D94YcJ+tdrOFevXqTtCCFEeWfXXTShodYxin/99VeBz9m8eTMAlStXtqdpIYR4oN1rbvEDcQc4dO0QThonnqz1ZJG2nbRuHef69Sfj2DG03t5UnD6NsO/+K4W4EELcB7uK8bZt26KqKosWLSpQ/LVr1/j2229RFIWOHTva07QQQjywCjK3eHaveJ/qffB39S+SdlWjkdj/+4DocS9hSUnBtVEjqixfhmfnzrK0uxBC3Ce7ivFRo0YBsGbNGmbPnp1v7KVLl3jkkUe4du0aWq3Wdq4QQojCWXUoBqPZQq1gzzznFj+bcJZNlzahoDC07tAiadN4KZrzTz1N/Pz5APiOGE74vLkyS4oQQtjJrjHjzZo14/nnn+ebb77h2WefZc2aNfzjH/+wHT906BCHDx9m3bp1LFmyhIyMDBRFYeLEiVSXjzOFEOK+LDuQ/9zi2TOodKzUkXCvcLvbyzh5kgvDR2C+fh2NwUDohx/i2bGD3dcVQggBilqYuy/zYDabGT58OPPnz8/3Y8rsZoYNG8bMmTPlI808JCUlYTAYSExMlBtchRB5Ons1hY5TNqNRYNcbnQj0zLl8/NW0q3T7qRtZlizm95hPw8CGdrWX/uefXBg5CktSEs61axM2fRr6ChXsuqYQQpRlRV2v2b0MmlarZe7cuSxdupRGjRqhqmqeW506dVi0aBGzZs2SQlwIIe7TzzenM2xbIyBXIQ6w8PhCsixZNApsZHchnrp7DxeeGY4lKQnXhg0JnztHCnEhhChiRbYCRP/+/enfvz+XL19m3759xMXFYTab8fPzo1GjRrbFfoQQQtwfi0W1zS3er3HuISqpWan8cPIHAIbVHWZXWymbN3Np3EuomZm4RbYkbPp0NDfXiRBCCFF0inw5ttDQUHr37l3UlxVCiAfe3vM3uBSfjqezjq51gnIdX3ZqGclZyVT2qkz7sPb33U7S2rVEv/wKmEx4dOhAhc+nonHOvcKnEEII+9k9TEUIIUTJWHbA2iv+yEMhuOi1OY6ZLCbmH7POdDKk7hA0yv39eE9cvZroCRPBZMLrkUeo+OUXUogLIUQxkmJcCCHKgIwsM2sOxwDQr3Hucdvr/15PTGoMvi6+9K52f59Opu7ZQ8y/XgeLBcPj/Qn9ZDKKXm9X3kIIIfJXoGEq8+bNsz0fMmRInvvvx+3XEkIIcXfrj10hOdNEBW9XmlX2zXFMVVVmH7Gu9fBkrSdx1ha+JzvzzBkujRmLmpWFZ9euhLz3HopG+muEEKK4FWhqQ41Gg6IoKIqCyWTKtf++Gr7jWkKmNhRC3N0zs/fwx8mrjO1YnYlda+Y4tjd2L8N/G46L1oV1j6/Dx8WnUNc2XbvG+YFPkBUdjWvDhlSaMxuNS+6ZWoQQQhR9vVbgGzjvVrPbOU25EEKIe4hLzmDLqWsAPNYo9xCVOUfnANCnep9CF+KWtDQuPv8CWdHR6MMrUXHG11KICyFECSpQMX7u3LlC7RdCCFF0VkZdxmxRaVTJm6oBHjmOnUk4w5ZLW1BQGFKncEP/VLOZ6JdfIePIEbTe3lT673/R+RSumBdCCGGfAhXj4eF5L6d8t/1CCCGKTvYsKnnNLT7vmPXenU6VOlHJq1KBr6mqKlc+/IiUjRtRnJyo+PXXOMnPdCGEKHFyd44QQpRiJ2KTOBaThF6r0Kt+SI5j19KvserMKgCG1h1aqOsmLltG/IIFoCiETp6MW+NGRZazEEKIgpNiXAghSrHlN3vFO9YKxNvNKcexRccXkWXJokFAAxoGNizwNTNPnSL2/f8AEPDSS3h171Zk+QohhCgcu1bgTE5OZurUqQCMGjWK4ODgfONjYmL47rvvAHjllVdwdXW1p3khhCjXzBaVn6PyHqKSlpXGD3/9AMCwusMKfE1LejqX/vlP1IwM3Fu3xm/UyCLLVwghROHZ1TP+888/M2nSJBYuXHjPQhwgODiYhQsX8u6777Jq1Sp7mhZCiHJvx5lrXEnKxNtNT4eagTmOrTizgsTMRMI8w+gQ1qHA14z9v//DePoM2gB/Qid/LHOJCyGEg9n1U3jZsmUoisKAAQMKFK8oCk888QSqqrJ06VJ7mhZCiHJv+UFrr3jP+iE46W79uDZbzMw7ar1xc0idIWg12gJdL3HVKhJ//Ak0Gip88ik6P7+iT1oIIUSh2FWMnzhxAoBWrVoV+JzIyEgAjh07Zk/TQghRrqUZTfx2JBbIPbf4xosbuZRyCW9nb/pU71Og62WeO0fsO5MA8H/xRdxbtijSfIUQQtwfu4rxS5cuARASEnKPyFuyh7NER0fb07QQQpRr649dIdVoppKvG40r5Zz7O7tX/B81/oGr7t733lgyM4n+5wQsaWm4NW+O/wvPF0vOQgghCs+uYlxzc6xhWlpagc/JjjWZTPY0LYQQ5Vr2EJW+jSqgKIpt/59X/yTqahR6jZ5BtQcV6FpxH08m88QJtL6+hH7yCYq2YMNahBBCFD+7ivHsHvF9+/YV+Jzs2ILc8CmEEA+iq8mZbD11Dcg9RGX+sfkAPFLlEfxd/e95rdSdO4lftAiA0I8/Qh8UeI8zhBBClCS7ivE2bdqgqipff/01WVlZ94zPysri66+/RlEUHn74YXuaFkKIcmvVn5cxW1QahnlTxd/dtj86JZr1f68HYHCdwfe8jiUtjZi33gbAZ9CTeLRpUzwJCyGEuG92FePPPPMMAKdOnWLQoEH5DldJS0vjySef5K+//spxrhBCiJyy5xa/s1d80fFFWFQLLUNaUtO35j2vc/WLL8i6dAldaAgBEyYWS65CCCHsY9eiP61ateKJJ55gyZIlLFu2jN27dzNy5Ejatm1LSEgIiqJw+fJltmzZwv/+9z8uXbqEoig8/vjjtGvXrqjegxBClBun41I4dCkRnUahZ/1bN8enGFP46dRPgHU6w3tJO3iQG/OsQ1pC3n0PrYf7Pc4QQgjhCHYV4wCzZs3i2rVrbNiwgejoaCZNmpRnnKqqAHTp0oW5c+fa26wQQpRLP9+8cbNdjQD8PJxt+5edWkZqVipVDVVpXaF1vtewGI3EvPkWqCqGvn3xaCPDAoUQorSye+k1FxcXfvvtN6ZOnUpoaCiqqua5hYWF8eWXX7J27VpcXFyKInchhChXLBbVNkSl721DVEwWEwuPLwSsY8U1Sv4/uq99/TXGM2fQ+vsT9K/Xii9hIYQQdrO7ZxysK2u+9NJLjBs3jqioKA4ePMi1a9aZAPz9/WncuDENGjTIMT2XEEKInPZfiOdSfDoezjo61w6y7f/9wu9cTr2Mj7MPPav2zPcaGcePc/1/MwEIfusttN7exZmyEEIIOxVJMZ5NURQaNWpEo0aNivKyQgjxQFh2wNor3r1eMK5Ot+YCn3fMusjPwFoDcdHd/ZNF1WQi5t9vgsmEZ9eueHXrWrwJCyGEsJvdw1SEEELYL9NkZvWhywD0u22ISlRcFIeuHkKv0TOw5sB8r3Fjzhwyjh1DYzAQ/NabxZqvEEKIoiHFuBBClAJ/nLhKUoaJYC8XWlT1s+3P7hXvWbVnvov8ZF25wtWvZwAQ9Npr6AICijdhIYQQRaJAw1TmzZtnez5kyJA899+P268lhBAPsuxZVPo0DEWrsd5fcznlMr9f+B2Ap+s8ne/5cZ9OQU1Lw7VhQwyP9S3WXIUQQhSdAhXjw4YNQ1EUFEXJUUBn778fd15LCCEeVIlpWWw8EQfknEVl8YnFtkV+avjUuOv5aQcOkLRqFSgKQW++KTfLCyFEGVLgGziz5wkv6H4hhBAF8+uRGIxmCzWDPKkd4gVAWlYaP/1lXeTn6dp37xVXzWau/Of/APB+vD+u9eoWf8JCCCGKTIGK8XPnzhVqvxBCiILLnlu8T6NQ276VZ1aSnJVMJc9KtKnY5q7nJvz0k/WmTU9PAsaPL+5UhRBCFLECFePh4eGF2i+EEKJgLieks/vcDQB6N7AW4xbVYlvkZ1DtQXdd5MecmMjVqZ8DEDBmNDo/vzzjhBBClF4Fmk2lcePGNGnSJFdP+IULF7hw4QJms7lYkhNCiPJu5Z+XUVVoXtmXij5uAGyP3s75pPN46D3oW73vXc+9Ov0rzPHxOFWrhs+gQSWUsRBCiKJUoGI8KiqKqKgo0tPTc+yvXLkyVatW5eTJk8WSXFHZsmULvXr1IjQ0FEVR+Pnnn3Mcv/0G1eytZcuWOWIyMzMZO3Ys/v7+uLu707t3by5dupQjJj4+nsGDB2MwGDAYDAwePJiEhIRifndCiLIsexaV22/czO4VfyziMdz17nmel3nqFPGLFgEQ9MbrKHp9MWcqhBCiOBSoGM++M99iseQ6VhZu4ExNTaVBgwZMnz79rjHdu3cnJibGtq1ZsybH8fHjx7N8+XKWLFnCtm3bSElJoWfPnjk+FRg0aBBRUVGsXbuWtWvXEhUVxeDBg4vtfQkhyraTscmciE1Gr1V45KFgAM4mnGX75e0oKDxZ68k8z1NVldgPPgCzGY/OnfBo3bok0xZCCFGECjRm3GAwkJiYyMWLF6lXr15x51TkevToQY8ePfKNcXZ2Jjg4OM9jiYmJzJw5k/nz59O5c2cAFixYQFhYGBs2bKBbt24cP36ctWvXsmvXLlq0aAHAd999R2RkJCdPnqRmzZpF+6aEEGVe9o2b7WsG4u3mBNzqFW8f1p4wz7A8z0vZuJG0nbtQnJwIeu21kklWCCFEsShQz/hDDz0EwH/+8x9OnDiRa4x4eZjTdtOmTQQGBlKjRg1GjhxJXFyc7dj+/fvJysqia9eutn2hoaHUq1ePHTt2ALBz504MBoOtEAdo2bIlBoPBFnOnzMxMkpKScmxCiAeDxaKyMuoyAH0bWoeoJGYmsursKgAG18n7UzXVZCLus6kA+A4dilNY3gW7EEKIsqFAxfizzz6Lqqrs2rWLunXr4uTkhFarBawfl9arVw+tVluoTacr8BTnxa5Hjx4sXLiQjRs3MmXKFPbu3UvHjh3JzMwEIDY2FicnJ3x8fHKcFxQURGxsrC0mMDAw17UDAwNtMXf68MMPbePLDQYDYfJLVYgHxr6/44lOSMfDWUen2tafHctOLSPdlE4Nnxo0DWqa53mJK1ZiPHMGjcGA38hnSzJlIYQQxaBAxfjgwYN5+eWX0Wg0qKpq27Ldvq8wW2kxcOBAHn30UerVq0evXr349ddf+euvv1i9enW+56mqmuNTgbw+Ibgz5navv/46iYmJtu3ixYv2vREhRJmRPUSle71gXPRaTBYTi05Yb8h8uvbTef7csGRmcvXmvS/+o0ah9fIquYSFEEIUiwJ3T0+ePJlx48bxxx9/EB0dTWZmJu+++y6KovD888/n2StcVoWEhBAeHs6pU6cACA4Oxmg0Eh8fn6N3PC4ujlatWtlirly5kutaV69eJSgoKM92nJ2dcXZ2LoZ3IIQozYwmC6sPxQC3hqhsvLCR2NRYfF18eaTqI3meF79wEaaYGHTBwfg8/VSJ5SuEEKL4FGqsSMWKFXPMDvLuu+8CMHr0aOrUqVO0mTnQ9evXuXjxIiEhIQA0adIEvV7P+vXrGTBgAAAxMTEcOXKEyZMnAxAZGUliYiJ79uyhefPmAOzevZvExERbwS6EEACbTsaRmJ5FoKczkdWsC/Vk37j5eI3Hcdbm/iPdnJzM9W+/BSBg7Bg08oe8EEKUCwUqxrNvLPS64yPRSpUqodFocHJyKvrMilBKSgqnT5+2vT537hxRUVH4+vri6+vLpEmT6N+/PyEhIZw/f5433ngDf39/HnvsMcA6m8yIESOYOHEifn5++Pr68vLLL/PQQw/ZZlepXbs23bt3Z+TIkXx78xfmqFGj6Nmzp8ykIoTIYcXNGzd7NwhFq1E4eeMkB+IOoFN0DKw5MM9zrv9vJubERJyqVcPQp09JpiuEEKIYFagY9/b2RqPRcOjQoRw94NnDVEr7EJV9+/bRoUMH2+sJEyYAMHToUGbMmMHhw4eZN28eCQkJhISE0KFDB77//ns8PT1t50ydOhWdTseAAQNIT0+nU6dOzJkzx3YjK8DChQsZN26cbdaV3r175zu3uRDiwZOckcWG49YhbdkL/Sw+sRiATuGdCHTL/fM0Ky6OG3PnAhAw/iWUUnQDvBBCCPsoagHupNRoNCiKwuHDh3MU4xqNJs8iXdyfpKQk25zud34KIYQoH5buu8grPx6iWoA7Gya0I8mYROelnckwZzC3+1waBzXOdU7MpEkkLPke1wYNCF+yuFxMJyuEEGVVUddrBZpNJbv312g05jpWmmZFEUKI0m7ln9YhKn0aVkBRFH4+/TMZ5gxq+tSkUWCjXPHG8+dJWPojAAETJ0ghLoQQ5UyBinF/f38Ajh07VqzJCCFEeRaXnMH209cA6NMwFLPFzJITSwB4staTeRbacV98AWYz7m3b4H7z5nAhhBDlR4EGHkZGRvLzzz/z2muvkZiYSI0aNdDr9bbje/fu5dq1a4VuvG3btoU+RwghyqrVh2KwqNAwzJtwP3e2XNrCpZRLeDp55jmdYcbJkyT/uhYUhcCb97oIIYQoXwpUjE+cOJFVq1Zx+fJlxowZk+OYqqoMHz680A0rioLJZCr0eUIIUVb9fHMWlb4NQwFsi/z0q94PV51rrvhrX30NgGf3brjUqlVCWQohhChJBRqm0rp1a5YtW0a1atXK5QqcQghR3M5fS+XPiwloFHi0fih/J/3N9ujtKCh5TmeYcfIvktetA8D/hRdKOl0hhBAlpMDzY/Xq1YtevXpx8eJFoqOjycjIoGPHjiiKwsyZM6lSpUpx5imEEGVa9o2brav7E+DpzMd7rGPF21RsQ5hXWK74azNmAODZrRsuNWqUXKJCCCFKVKEnqw0LCyMsLOcvjubNm8vUhkIIcReqqvJzVDRgnUUlLSuNFadXANYbN++UeeoUyb/9BoD/iy+WXKJCCCFKnF0rRwwZMgRFUfDx8SmqfIQQotw5ejmJs1dTcdJp6FY3iF/OLic5K5lKnpVoFdoqV/y1GTNAVfHs0gWXmtIrLoQQ5ZldxficOXOKKA0hhCi/VtzsFe9cOxAPZ51txc0naj2BRsl5607m6dMk/boWAP/R0isuhBDlXZGvqRwdHU1sbCxpaWk0bdoUV9fcMwQIIcSDwmxRcyz0s+/KPk4nnMZV50qf6n1yxV+b8c3NXvHOMoOKEEI8AAo0m8q9JCcn88477xAWFkalSpVo3rw57du359y5cznilixZwoABAxg5cmRRNCuEEKXennM3uJKUiaeLjvY1A/j+5PcAPFr1Ubycci6jnHn2LElr1gAyVlwIIR4UdveMnz59mh49enD27Nkc0xXmtZJcZGQkgwcPxmKxMHToUB5++GF7mxdCiFJt5Z/WISqP1AshOSue3//+HSDP6Qyze8U9OnXCpXbtEs1TCCGEY9jVM56Zmcmjjz7KmTNncHNz49VXX+WXX365a3x4eDgdOnQAYOXKlfY0LYQQpV6mycyaw7EA9GkYyvJTyzGpJuoH1KeWb84hKJnnzpG0ejUA/i/KvOJCCPGgsKtn/JtvvuHUqVO4u7uzdetWGjZseM9zevTowYYNG9i5c6c9TQshRKm3+eRVEtOzCPR0pmllbyb9vBTIu1f8+jffgsWCR4cOuNatW9KpCiGEcBC7esaXLVuGoii89NJLBSrEAerXrw/AqVOn7GlaCCFKvRU3b9zs1SCUXbE7iEmNwcvJi67hXXPEZUVHkyi94kII8UCyqxg/duwYAF27dr1H5C1+fn4AJCQk2NO0EEKUaimZJjYcuwJYh6hk37jZt3pfXHQuOWKvz5kLJhNukS1xfeihEs9VCCGE49hVjCcnJwNgMBgKfE5GRgYAer3enqaFEKJUW3c0lkyThSr+7vgaUth6aSsA/6jxjxxxpvh4En78EQB/mWlKCCEeOHYV49m93FeuXCnwOYcPHwYgKCjInqaFEKJUy55bvHeDUJadWoaKSouQFlQ2VM4RF79gIWp6Oi516uAWGemATIUQQjiSXcV49jjx33//vcDnzJo1C0VRaNGihT1NCyFEqXU9JZOtp64B0KN+AD+d+gnIfeOmJS2N+AULAPAbNTLPKWGFEEKUb3YV4/369UNVVb799lv+/vvve8a/++677N69G4CBA3PPJiCEEOXBmiOxmC0q9Sp4cT59NzcybhDgGkD7sPY54hJ+/BFzYiL68Ep4dunimGSFEEI4lF3F+LBhw6hduzbJycm0a9eO1atX51r4x2KxsHXrVnr16sV7772Hoig0a9aM3r172528EEKURquibg1R+eHkDwD0r9EfvebWvTJqVhbXZ88BwG/4CBSttsTzFEII4Xh2zTOu1WpZuXIlrVu35sKFC/Tu3Rs3Nzfb8V69enHlyhXS0tIAUFWV0NBQli5dal/WQghRSkUnpLPn/A0UBRpUNTLt971oFA39I/rniEtcvRpTTAxaf38Mffs4KFshhBCOZlfPOEC1atWIiori0UcfRVVVUlNTAWvhffbsWVJTU1FVFVVV6dq1K3v37iUsLMzuxIUQojT65eaNm80r+7LpsnWl4XYV2xHsHmyLUS0WbsycCYDvkCFonJ1LPlEhhBClgl0949mCg4NZtWoVR48eZcWKFezbt4+4uDjMZjN+fn40atSIPn360LRp06JoTgghSq0VN4eo9Kjvy3/PrABy37iZsmkzmadOo/HwwOfJJ0o8RyGEEKVHkRTj2erWrUtdWcZZCPGAOh2XzLGYJHQaBSevIyQbk6ngUYHI0JxTFl7/3/8A8HliIFpPT0ekKoQQopSwe5iKEEIIq5U3e8Xb1ghg7QVrr3j/iP5olFs/atMOHCD9wAEUvR6fIUMckqcQQojSo1iKcZPJxNWrV7l69Somk6k4mhBCiFJFVVXbQj+RtbI4GHcQraKlb/W+OeJuzJ4NgKFvH/SBgSWdphBCiFKmyIrx48ePM3bsWGrXro2LiwvBwcEEBwfj4uJC7dq1GTduHMeOHSuq5oQQolQ5HJ3I+etpuOg1xKlbAOuNmwFuAbYY44ULJG+wLpLmO2yYI9IUQghRyhRJMf76669Tv359vv76a06ePInFYrHNoGKxWDh58iRfffUVDRo04I033iiKJoUQolTJvnGzY21ffv37F8A6t/jtbsxfAKqKe9s2OFerVuI5CiGEKH3svoFz7NixfP3117bFfmrXrk2LFi0IDg5GVVWuXLnCnj17OHbsGGazmY8//pjU1FS++OILu5MXQojSwGxR+eWQtRivFHaGrWcTCXYPpnVo61sxSUkk/PQTAL5DhzokTyGEEKWPXcX49u3b+eqrr1AUhTp16vDf//6XVq1a5Rm7c+dOnn/+eQ4fPsz06dMZOHDgXWOFEKIs2X3uOleSMjG46jmRuh6AftX7odXcWlUzYemPqGlpOEdE4C4/+4QQQtxk1zCVb7/9FoAqVaqwffv2fIvryMhItmzZQtWqVQH45ptv7GlaCCFKjVU3b9xsU0dl/5V9aBQNj0U8ZjuuZmVxY8ECAHyHDUVRFIfkKYQQovSxqxjfunUriqLwr3/9C4PBcM94g8HAa6+9hqqqbN261Z6mhRCiVDCaLKw5HAuAs89eAB6u8HCOFTeT1q3DFBOD1s8Pr549HZKnEEKI0smuYjw21voLqFGjRgU+p3HjxgBcuXLFnqaFEKJU2PLXVRLTswjw1LL32jrAOrd4NlVVuTFnLgA+Tz6JxtnZIXkKIYQonewqxl1cXABITU0t8DkpKSkAOMsvJCFEOZA9t3jDWtHEZ8YT4BpA24ptbcfTDx4k4/BhFCcnfJ58wlFpCiGEKKXsKsarVKkCwMqVKwt8zqpVqwBsY8eFEKKsSjOaWH/M+ilfin47AH2r90WnuXVvfHavuFfvXuj8/Eo+SSGEEKWaXcX4I488gqqqTJ8+nd9///2e8b///jvTpk1DURQeeeQRe5oWQgiHW3/sCulZZioGpHEk3jpevF9EP9tx46VLJG/YAIDvkCEOyVEIIUTpZlcxPn78eLy8vMjKyqJHjx6MHj2a/fv3Y7FYbDEWi4X9+/fz4osv0qNHD7KysvDy8mL8+PH25i6EEA6VPYtKePgRAFqFtqKiZ0Xb8fj588Fiwb11a1xq1HBIjkIIIUo3u+YZ9/f354cffqB3794YjUa++eYbvvnmG5ycnPD19UVRFK5fv47RaASsNzI5OTmxdOlS/OTjWiFEGZaQZmTzX1cBMxdNW4CcN26ak5NJWPojYJ3OUAghhMiLXT3jAF27dmXXrl00bdoUVVVRVZXMzExiYmK4fPkymZmZtv3NmjVj9+7ddO7cuShyF0IIh1l7JJYss0p42EXiM6/h4+xDh7AOtuOJy5djSUvDqWpV3B9+2IGZCiGEKM3s6hnP1rBhQ/bs2cPevXvZsGEDR44c4caNGwD4+vpSr149OnfuTLNmzYqiOSGEcLgVUdYhKoaAg9xIh57VeqLX6gFQLRZuLFwIgM/TT8kiP0IIIe6qSIrxbM2aNZOCWwhR7l1JymDXueso2mQuZOwDoF/1Wzdupm7bRtbfF9B4eODdp4+j0hRCCFEG2D1MRQghHjS/HIpBVaFy5ROYVTP1/etT3ae67fiNBQsAMPR7DI27u6PSFEIIUQZIMS6EEIVkXehHxeK+G4DHIh6zHTP+/TepW7aCouD71FMOylAIIURZYVcxfvDgQbRaLa6urkRHR98zPjo6GhcXF3Q6HceOHbOnaSGEcIjz11L582ICOrcL3Mi6hKvOle6Vu9uOxy9aBIB72zY4hYc7Kk0hhBBlhF3F+Pfff4+qqvTs2ZMKFSrcM75ChQr07t0bi8XCkiVL7GlaCCEcIntu8QphhwHoEt4FDycPACypqST8tAwA36efdkyCQgghyhS7ivFNmzahKAo9evQo8DmPPvooABturkonhBBlhaqqrPjzMiiZJOtu3rh524qbiStXYklJwSk8HPfWrR2VphBCiDLErmL84sWLANSpU6fA59SsWROAS5cu2dO0EEKUuOMxyZyOS8HF5whZlgzCvcJpHNgYsBbqtukMn3oKRSO35AghhLg3u35bXL9+HQAXF5cCn+Ps7AxAXFycPU0LIUSJW3lziIpv0EEA+lbva5tDPG3XLoynz6Bxc8PQ77G7XkMIIYS4nV3FuI+PDwAXLlwo8DnZPeJeXl72NC2EECXKYlFZ9edlNE5xJHMaraKlT7Vbc4jfWGDtFTf07YvWw8NRaQohhChj7CrGs4enrFy5ssDnLF++HLg1XEUIIcqCAxfiiU5Ix813PwBtKrQhwC0AAOOlaFL++AOwrrgphBBCFJRdxfgjjzyCqqrMmzePrVu33jN+y5YtzJ8/H0VR6Nmzpz1NCyFEiVoRdRkw4+xjHaJy+9zi8YsXgcWCe+vWOFet6qAMhRBClEV2FePPPfcc/v7+mM1mHnnkEaZNm0ZGRkauuIyMDL788kseffRRzGYzPj4+vPDCC/Y0LYQQJSbLbGHN4Rh0HicwkoSfix9tKrYBwJKRQeKPPwHWGzeFEEKIwtDZc7KHhweLFi3ikUceIS0tjfHjx/PGG2/QtGlTQkJCUBSFy5cvs2/fPtLS0lBVFb1ez+LFi2XMuBCizNh++hrXU414hR9ABXpX641eowcg6de1mBMT0VeogEe7to5NVAghRJljVzEO0LlzZ3777TeefvppYmJiSE1NZcuWLTliVFUFrIv+zJ8/n/bt29vbrBBClJiVUZdRtMngdhywzqKSLX7xYgC8nxiIotU6Ij0hhBBlmN3FOECHDh04c+YM8+bNY/Xq1Rw8eJBr164B4O/vT+PGjenVqxdPP/20bWpDIYQoC9KNZn47GovOcBAVC/UD6lPV2zouPP3wETIOHULR6/Hu39/BmQohhCiLiqQYB+tc46NGjWLUqFFFdUkhhHC4jSfiSDWaMPjtx0LeveKePbqj8/V1TIJCCCHKNFkiTggh8rEiKhqNy0Usuiu4aF3oXrk7AOaEBJJWrwbA58knHZmiEEKIMkyKcSGEuIvE9Cw2nbyK3ts6t3jn8M54OnkCkLD8Z9TMTJxr18a1YUMHZimEEKIsk2JcCCHu4rcjsRgtGTgb/gRuDVFRLRbil1iHqPg8+QSKojgqRSGEEGWcFONCCHEXK/6MRud5FFWTQQWPCjQLbgZA6o6dZP19AY2HBwZZwEwIIYQdHohifMuWLfTq1YvQ0FAUReHnn3/OcVxVVSZNmkRoaCiurq60b9+eo0eP5ojJzMxk7Nix+Pv74+7uTu/evbl06VKOmPj4eAYPHozBYMBgMDB48GASEhKK+d0JIYpDXFIGO85ctw1R6VOtDxrF+iMz+8ZNw2OPoXFzc1iOQgghyr4HohhPTU2lQYMGTJ8+Pc/jkydP5rPPPmP69Ons3buX4OBgunTpQnJysi1m/PjxLF++nCVLlrBt2zZSUlLo2bMnZrPZFjNo0CCioqJYu3Yta9euJSoqisGDBxf7+xNCFL1fDsWA7gY6tzMA9K7eG4Csy5dJ+eMPwDpERQghhLBHkU1tWJr16NGDHj165HlMVVU+//xz/v3vf9OvXz8A5s6dS1BQEIsWLeK5554jMTGRmTNnMn/+fDp37gzAggULCAsLY8OGDXTr1o3jx4+zdu1adu3aRYsWLQD47rvviIyM5OTJk9SsWbNk3qwQokis+PMyesMBUFRaBLeggkcFAOJ/+AEsFtxatsS5alUHZymEEKKseyB6xvNz7tw5YmNj6dq1q22fs7Mz7dq1Y8eOHQDs37+frKysHDGhoaHUq1fPFrNz504MBoOtEAdo2bIlBoPBFnOnzMxMkpKScmxCCMc7fy2VPy/eQG+wDlHpG9EXANVoJGHpj4BMZyiEEKJoPPDFeGxsLABBQUE59gcFBdmOxcbG4uTkhI+PT74xgYGBua4fGBhoi7nThx9+aBtfbjAYCAsLs/v9CCHstyLqMlq3c2ic4vHQe9CpUicAktavx3z9OrrAQDw7dnBwlkIIIcqDB2KYSkHcOTWZqqr3nK7szpi84vO7zuuvv86ECRNsr5OSkqQgF8LBVFVlRVQ0eu99AHSv0h1XnSsACYuXAOD9j3+g6PUOy1GUTaqqgkVFNalgtqCaVFSzBdV887VZBbOKarn52qKChTte37bPot68JqDePE/FGqOqqKp1v+149muVm9vNfdyK4eZLNTuOnPtzHL993+1xd9mv5nUsv/Pu/oXMN/6ulyjIte+XWpwXFwXl1SUcp1APR6dRaA98MR4cHAxYe7ZDQkJs++Pi4my95cHBwRiNRuLj43P0jsfFxdGqVStbzJUrV3Jd/+rVq7l63bM5Ozvj7OxcZO9FCGG/Q5cSOXvjOh4RR4Bbc4tnnjpF2r59oNXiPeAfDsxQFDfVoqJmmLCkm7BkmLGkm1AzTVgyzaiZZtujajSjGi1YsqyP6u2PppvFdpbFupksYLYUb0EoxAPOo3Woo1O4LwUqxi9cuFAsjVeqVKlYrlsYVapUITg4mPXr19OoUSMAjEYjmzdv5uOPPwagSZMm6PV61q9fz4ABAwCIiYnhyJEjTJ48GYDIyEgSExPZs2cPzZs3B2D37t0kJibaCnYhROn3c1Q0eq9DKJosqhqqUt+/PgDx3/8AgEeH9ujv8ge2KJ1Ui4olJQtzshFzshFLkhFzihFLahaWNBPm1CwsaVm212qm+d4XLQoKoNWgaBUUrQJaBUWrsT5qbu7TWDfljkfbc4Xcz5Wbz5Xb9t98jUKO4zn330wqOx6yd6Lc8fpWjGKLtZ1/28Ot53nsv+O17amS1867n5dnUFGtw1VM63kpxXXhB5w+oGxONVugYrxKlSpF3rCiKJhMpiK/bl5SUlI4ffq07fW5c+eIiorC19eXSpUqMX78eD744AMiIiKIiIjggw8+wM3NjUGDBgFgMBgYMWIEEydOxM/PD19fX15++WUeeugh2+wqtWvXpnv37owcOZJvv/0WgFGjRtGzZ0+ZSUWIMsJktrDqzxj0ftYhKn2r90VRFCxpaSTeXJ/A5wm5cbO0UU0WTNfTMd3IwJyQiSkhE3P8reeWZON99Ugreg2Kqw6Niw6NixbFWYvGyfqoOGvROGtR9FoUJy2KkwZFr0HjpAW9BkVnfW171GY/KqC7+VorBZkQooDFuFrGx0Lt27ePDh1u3WyVPU576NChzJkzh1dffZX09HRefPFF4uPjadGiBevWrcPT09N2ztSpU9HpdAwYMID09HQ6derEnDlz0Gq1tpiFCxcybtw426wrvXv3vuvc5kKI0mf7mevcMF7E3e0CWkVLr2q9AEhaswZLSgr6SpVwbxXp4CwfXJZ0E1kxKWRdScN0NZ2sa+mYrqVjjs+4d7GtgMZDj9bDCa2XExoPJ7QeejTuejRuejTuOutzV511c9Gh6B74OQ6EECVAUQtQac+dOzff419//TV79+5Fr9fTtWtXmjdvTlBQEKqqEhcXx969e1m3bh1ZWVk0a9aMF154AbAWw+KWpKQkDAYDiYmJeHl5OTodIR44E76P4pdL/8PZfzPtK7ZnWqdpAJzr/zgZR48S+MrL+I0Y4eAsHwzmJCPGC0kYL6eQFZNKVkwq5oTMu8Yrzlp0vi5ofVzQeTuj9XFG6+2CzscZrZczGne99EQLIYpEUddrBeoZz69ofvbZZ9m3bx9du3Zl5syZVKhQIc+46OhoRo4cyW+//cZDDz3Ed999d38ZCyFEMUg3mvntaDT68APArRs30w8fJuPoURQnJww3FwYTRUu1qGTFpmL8Ownj30lkXkjGfCMjz1ittzP6EHd0AW7o/V3R+buiC3BF46G/5wxYQghRGtk1m8qPP/7IrFmzaNasGatXr84xZONOFSpUYNWqVURGRjJr1iy6dOliuxlSCCEcbf3xK2Q4HcdNl4yPsw9tK7YFIH6JdTpDz+7d0N2x1oC4f+bETDL+irdupxJQM+64h0gBfZAb+gqe6EPdcQpxRx/sjsZNppQUQpQvdhXj3377LYqiMGHChHwL8WxarZaJEyfy5JNP8t///leKcSFEqfHzwWj0BuuNmz2r9USv1WNOTCRp9RoAfJ54wpHplXmqRcV4IYn0YzfI/OsGWbFpOY4rTlqcKnniFO6Fc7gXTpU80bg88LPvCiEeAHb9pDt06BAANWrUKPA52bGHDx+2p2khhCgyN1KNbDlzHpdqJ4BbQ1QSV6xEzcjAuUYNXG9OfSoKTlVVsi6nkvbnVdIPXc055lsBp4qeONfwwaWGD04VPWVMtxDigWRXMZ6cnAxYF78pqOzY7HOFEMLRVh+6jOJxAEUxU8evDjV8aqCqKvHffw+A9xMDZTxyIZjiM0jdd4X0P69iupZu2684a3Gt44dLLR+cq/ugdZchJ0IIYVcxHh4ezl9//cW8efPo1q1bgc6ZN28eUDoW/BFCCIBlBy+h9741tzhA2t69GM+cQXFzw9C7twOzKxtUVSXzTAIpO2LIOH791lSDOg2utX1xaxCAS00fFP29hzQKIcSDxK5ivE+fPkyePJklS5bQoEEDXn311XzjP/30UxYvXoyiKDz22GP2NC2EEEXiwvU0/ow7hnuVWPQaJx6p8ggACTdv3DT06oXWw8ORKZZqlkwTaQfiSNlxGdPVW73gztUMuDcNxqWOLxpnGfsthBB3U6B5xu8mISGBOnXqcOXKFQDq16/P0KFDadasGYGBgSiKwpUrV9i7dy/z588nKioKVVUJCQnh6NGjeHt7F9X7KBdknnEhSt60308x/dAnOPnupHvl7nzS7hNM165xqkNHyMqiyvJluNSu7eg0Sx1LhomUbdEkb4tGzbAuH684aXFrEohHZCj6wLK5LLUQQtyLQ+YZvxtvb282bNhAt27diI6O5tChQ0ycOPGu8aqqUrFiRdauXSuFuBDC4VRVZXnUefQ+UcCtISoJPy2DrCxcGzSQQvwOlkwzKTsuk7zlEmq6dTpCXYArHpGhuDUOlBlQhBCikOz+qVmnTh2OHj3Ku+++y5w5c4iPj88zzsfHh2eeeYa3335ben2FEKXCn5cSuZCxD1dtOoGuQbQMaYlqNpOQfePmkzKdYTaL0UzqrhiSN1/EknqrCPfqEo5rPX8UjdzgKoQQ96NIujC8vLyYMmUKH374Ifv37+fw4cPEx8ejqiq+vr489NBDNGnSBCcnp6JoTgghisSyA7du3OxTvTdajZbkTZvIunwZjcGAV/fuDs7Q8VRVJePodRJWncWcaJ2aUOfngmfncNwaBEgRLoQQdirSzxOdnJyIjIwkMjKyKC8rhBBFzmiysOLIUbQVTgG3DVFZbL1x0/uxx9C4uDgqvVLBdD2dhFVnyThxA7AuRe/VqRJujYNkTnAhhCgiMrhPCPFA+uNkHOnOu3FWVJoENaWSVyWMl6JJ2bIFAJ8nBjo4Q8dRTRaSN18i6Y+LYLKAVsGzbUU8O4ShcZKpCYUQoigVaTF+9uxZdu7cSWxsLGlpabzwwgv4+/sXZRNCCFEkftx3Ab3BOkSlf0Q/ABJ++AFUFfdWkThVruzA7Bwn83wi8T+dsk1T6FzVgHff6jI7ihBCFJMiKcYPHjzI+PHj2bZtW479/fv3z1GMf/XVV7z77rsYDAaOHTuGXi+rrwkhSt6NVCNbLu7CKSweN507ncM7oxqNJPz0EwDeTzx4N26qZpXkPy6Q9PsFUEHjoce7Z1VcGwTI6qNCCFGMNPZeYPXq1bRq1Ypt27ahqqpty8vQoUNJT0/n7Nmz/PLLL/Y2LYQQ9+WXQ5dRvPYA8P/s3Xd8VFX+xvHPnZreQwq9F0F6VxEQFEXFhljB7qprQd1dXddVd9V1rfvTVXftdVWsWJCigPQSBek9CSWF9J5p9/fHQCASICGBScLzfr3Gmblz7r3fISZ55uTcc8Z3OI9gWzDFc+bgzc3F1qIF4aNGBbjCE8uTX8He//5K0Rx/EA/p24LEewcQ0qeFgriIyHFWrzCemZnJFVdcQWVlJT169GDGjBkUFxcftn1YWBgTJkwAYMaMGfU5tYjIMfvk583YwtcBcPG+ISr5+y/cvOwyDNvJczlN2eq9ZP3rZ1xpRRhOKzGXdyXm8q5Ygk+efwMRkUCq10/b559/npKSEtq2bcuCBQtqtZDPmWeeyQcffEBKSkp9Ti0icky2ZpewqWQ+QWEeOkR2pkdsDyq3bqVsxQqwWomaeFmgSzwhTLeX/C+3UZbiX0HZ0SacmMu7YosNDnBlIiInl3qF8ZkzZ2IYBvfee2+tV9Ts2rUrAKmpqfU5tYjIMfHPLb4CgIldL8EwDPI/8i/yEzbyTOwJCQGs7sTwFlWS8+563LtKwIDwka2JGN0Gw1rvkYsiIlJH9QrjO3bsAGDQoEG13ic8PByAkpKS+pxaRKTOfD6TT9cuxRqfgdWwc1778/CVlVH45ZcARE+6IrAFngCuXcXkvLseX5ELS4iNmKu6E9QxKtBliYictOoVxt1uN0CdZkUpKCgAIDQ0tD6nFhGpsyXbcym0LsIBjGo9iqigKPKnTcNXUoK9TRtChzXvBcvKft1L/rTNmG4fthYhxE3uoWEpIiIBVq+/SSYmJgIHeshrY8mSJQC0atWqPqcWEamzT1K2Y49cBcClXS/GNM2qFTejL78cw9I8h2mYPpPC2WnkfbgR0+0jqGs0LW7rrSAuItII1Os3z/DhwwH44osvatW+rKyMV199FcMwOOOMM+pzahGROimt9DAnbQ6GtYJYZyJDkoZQsXo1FevXYzgcRF58UaBLPC5Mj4+8jzdR/EM6AGGntSR28ilYgjRbiohIY1CvMD558mRM0+R///sfs2bNOmLbkpISJk6cSHq6/xfCDTfcUJ9Ti4jUycx1mfjClgEwsdtFWAwLeR9+CEDEuedii44OZHnHhc/lJfe99ZSv3gtWg+hLOhM1vgOGRXOHi4g0FvUK42eddRYTJkzA5/NxwQUXcP/997N8+fKq1/Py8li2bBl/+9vf6Nq1KzNmzMAwDK699lr69u1b7+JFRGrr/ZUp2EK3AwYXdboIT24uxTO+ByD6qqsCW9xx4KvwkPPmWio25WPYLcRNPoXQgYmBLktERH7DMA+3XGYtlZWVMX78eObNm3fEldr2n2b06NF88803OJ3O+py2WSoqKiIyMpLCwkIiIiICXY5Is5GWW8qYtx/AGTeXfvGDeefc18n5z3/Z+/zzBJ16Ku0/+TjQJTYob6mbnDfX4t5dguG0EnfdKTjbRQa6LBGRZqGh81q9r1YKCQlhzpw5PP300yQmJmKaZo23mJgYnnjiCWbOnKkgLiIn1EcrdmCPWgnAlT0uw/R4yP9o34WbVzav6Qy9RZXs/c+vuHeXYAm1EX/zqQriIiKNWINcwWOxWLj33nu56667WL58OStXriQ7Oxuv10tsbCx9+/bltNNOUwgXkRPO4/XxyfpZWGKLCbNFMar1KErmzsOTkYE1OpqIceMCXWKD8eRVsPf1NXjzKrBGOIi7sRf2FiGBLktERI6gQS+nt9lsDBs2jGHDhjXkYUVEjtlPW/ZS5lyIDbi0y8XYrXYy9l24GXXpJViaSSeBp6CSvf/9FW9BJdbYIOJv6IUtJijQZYmIyFFobisRadbeWZ6CLWwLABO7XUrl9u2ULl4ChkHU5ZMCXF3D8Ba7yHl9Dd6CSmxxwcTffCrWCEegyxIRkVqoVxi//vrrAWjXrh1//vOfsVqtR91nz549PPTQQxiGwRtvvFGf04uIHNHe4kqW5czAHgu9YwfROrw1mf96HICwM8/E0aplgCusP2+pm72vr8GTU441ykncjb0UxEVEmpB6hfG33367agaV+fPn8+mnnxJ9lLl68/Pzq/ZTGBeR4+nTn3dgjfRfuDml1xX4Sksp/PJLoHlMZ+gr909f6MkqwxLhIP6mXtiimsewGxGRk0WDrP1smibz5s1j8ODBbNy4sSEOKSJSL6Zp8v6vM7DYSgi1RjOi9QgKv/4aX0kJjrZtCR02NNAl1ouv0kvOW2v3zZpiJ/7GXlreXkSkCWqQMH799ddjGAZbt25lyJAhfP/99w1xWBGRY5aSlk+e5ScALu16MTbDRv4H/gs3o6+8AsPSID/+AsJ0e8l9Zx2u9GKMYBtxN/TUrCkiIk1Ug/w2mjp1Kl9++SXh4eEUFRVx/vnn89xzzzXEoUVEjsmby1ZgC9sKGFzZfSJlK1ZQuWULRnAwkRddFOjyjpnpM8n9aBOV2wsxnFbir++JIzks0GWJiMgxarCuofHjx7No0SLatm2L1+vl/vvv54YbbsDtdjfUKUREaqWk0sP8jG8A6BUziOSw5Kpe8cjx47E20RVuTdOk8JvtVKzLBatB7LU9cLQOD3RZIiJSDw36d9qePXuycuVKTjvtNEzT5O2332bUqFHs3bu3IU8jInJEX61Kg/AVANx46pW4d++mePZsAKKvvjqQpdVLyYLdlCzeA0DMxK4EdYwKbEEiIlJvDT5oMjY2lh9++IHrrrsO0zRZvHgxgwYNYs2aNQ19KhGRGr216hsstlJCrbGc0foM8j74EHw+QoYOIahrl0CXd0zKVmdT+N0OACLPa09I7/gAVyQiIg3huFzBZLfbeeONN3jmmWewWCykpaUxfPhwvvrqq+NxOhGRKpuzitnt/RGAiztfhKW8koJp0wCIufbaQJZ2zCq2FZD3yWYAwoYnE3Za058fXURE/I7rdAJTp05l+vTpREREUFJSwiWXXMLTTz99PE8pIie5VxctwRa6HTC4tuflFHz1Fb7iYuxt2xA2YkSgy6szd2Ypue+tB69JcK84Is/rULW+g4iINH3HfW6vcePGsWTJEjp06IDP5+O999473qcUkZNUSaWHWbu+AKBX9FASgluQ/67/Z07MNdc2uekMvcUuct5ah1nhxdEugpiJXTEsCuIiIs3JCfnN1L17d5YvX86IESMwTfNEnFJETkIfp2yBMP+Fm7f3m0zpggW4UlOxhIUROWFCYIurI9PtJffd9XgLK7HFBxN3bQ8Me9P6MCEiIkdnq8/Oc+fOBaB9+/ZHbRsTE8Ps2bP529/+Rnp6en1OKyJyCNM0eWv1NIyQSmLsrRjWcig7H74RgKhLL8UaFhrgCmvPNE3yPt2Ca2cxlhAbcZNPwRJiD3RZIiJyHNQrjI+o4/hLm83Go48+Wp9TiojUaHlqLvnWuViAKb2uxrV1K6WLF4PFQvTVVx3Xc3u9PsoKXdidVhxBVizW+vVgF/+4k/LVe8FiEHNVd2xxWuZeRKS5qlcYFxFpLP5v8bdYnDnYCObybheR97enAAgfPRpHq1YNfj7TNMlOK2bTsky2rMiiouTAAmc2hwVHkA1HsI0WbcMZcG47ohOP0jPvdUNFIWVbPRTNTgMgakJHzSUuItLMKYyLSJOXXVzBLwXfYA2Ds1qPx1FSSeG+qVRjJjfsdIbFeRVsWpbJ5mWZ5GeWVW03LAamz39NjMflw+NyUVbkoiCrjC0rsug6OJEB57UnMsqA1IWQsxnyth+4FaTj8rYj3/UUEERYyA+E/fwAbIiG6PbQ/gxodzqEJzTo+xERkcCqVRh/7LHHqh4//PDDNW4/FgcfS0TkWL22eDmW0E1gGvx+wBQK/jcNs7KSoB49CO7fv0HO4XX7WPLVNlb/sBP2XYdutVvo0CeeroMTad09GtMEd4UXV4UHV4WH8mI3a+btYsfqHDYuzWTzsj10D5vHgKAPCLPmVT++GUOu6y+YBBFkWUmk91+Q6fO/uOMn+Pkd/+P4btB+hD+cdzgTnGEN8v5ERCQwDLMW05tYLJaqeW29Xm+N24/FwccSKCoqIjIyksLCQiIiIgJdjkiT4PH6GPTqnbjD5tM5fCCfnf8ftp41Bk9WFkn/eJKoBphFJT+zlFlvrCNnZwkALbtE0XVIIh37tsARfIQ+jdxtsOpDspYvZVnGKHa6+gJgNdwM77ySXqe6IaYDZkR7sr9z4t5TiS3OSYsro7H4iqA8H8pyIfNXfyDPXEPVJwEAZwT0vgIG3gDxXev9PkVE5OgaOq/VepjK4TK7pioUkUD6bl0aruClGMAd/adQ9P33eLKysMbFEXHuufU6tmmabFicwYKPN+Nx+QgKtTPq2m60P9pS9GV5MO9JWPEGmF4SgAsSV7Mn8QaW7hlBxi47P20eiq93Z07t24r8T7fg3pOFEWwj7rpeWGJ/c8Fm78sPHDd1oT+Yb5kFBWmw/D/+W7vTYeCN0O08sGrmFRGRpqJWYdzn89Vpu4jIifLKyo8xrJWEW5MY0Xo4aXdeAkDM1VdhcTiO+biVZW7mfbCJrSnZALTsGs1ZU3oQFu08/E4+L6S8BT8+DuX7hqF0HAV9roKu55LsCOEi02TZV9tJ+T6NhdO24NxZRNjGPDAg9spu2H4bxA8WEgM9LvDffD7YPtcf+DfPgNQF/lt4Epw2FfpPAduxv38RETkxdAGniDRZ27KLSfPMxmqFq7pfSfnCRVRu2oQlJIToK6445uMWZJcx/V+rKM6twGIxGHRBe/qObYvlSKtfpi6CGX+ErDX+5/HdYdw//OO6D2IYBoMv7ODfZXY6IRtywTCIHNeeoM7RtS/SYoFOo/23gp2Q8rZ/XHlxBsy4H5a8BKMegp6X+tuKiEijpJ/QItJkPbfwW6zObCymk8m9LiX3tdcBiJo4EWtk5DEds3BvGV89/wvFuRVExAVx8f396X9Ou8MHcXc5fHU7vH2uP4gHRcK4f8KtCw8J4vsZhkH/EckMjXZgMQx2unxsd9XjL41RrWH0X+Ce9XDecxCW4B/C8vlN8J/TYfMs0JBCEZFGST3jItIklVR6+CnzCwiF05LOxbp+K2UrVoDdTsyUycd0zMK9ZXz53C+U5FcSnRjChKn9CIk4wlCP/FT4+Br/BZYY/qEho/4CobFHPI/p9pL3/kasHh+uEDurC8rwfroVDIPeo1sfU+2Af1jKwBug9yRY9ios/BdkrYUPL4O2p8G5/4SEU479+CIi0uDUMy4iTdKri5ZghmwA4N5BU8h53d8rHnn++dgTE+t8vMK95dWC+IX39D1yEN/6A/z3TH8QD4mFa7+C8184ehA3TfI/34p7dwmWUBut7+hNn3PaArBw2paqMer14giF0++Fu1bB8LvAFgRpC+HV02HWQ1BZUv9ziIhIg6hVz3iHDh0a/MSGYbBt27YGP66INH8er48PN72HEWLSKWwwLfNh+w8/AhB7w/V1Pp4/iP9cLYiHRh7mQk2fDxY+Bz/+HTAhuR9c/h5E1m6Vz5JFeyj7JRssEHNld+wxwQy+sAMel4/VP+5k/oebSO4cdeQPArUVEgNjHoNBN8P3f4INX8PiF2Ht53DOP6D7+VCP6WlFRKT+ahXGU1NTa3Ww/XOO/3a6w5q212d+chE5uX38yzpcQcsxgD8NvZXcf78BpknY6NE4O3as07GKcsr58nl/EI9KOEoQryyGL26Fjd/4n/eb7B8fbg+q1bkqtxdQ+N12ACLP7VC11L1hGAy9uCO7NuWTu7uEeR9sZNytvRru52RkK7j8ff/Y8e/u848n/+Qa6DQGzn0aYto3zHlERKTOahXGJ08+8vjLVatWsXr1akzTJCoqir59+5KQkIBpmmRnZ7Nq1Sry8/MxDIPevXvTu3fvBileRE4+pmny8s9vYzi9JDi60c/amq3TvwYg9sYb6nSsihI3X73wCyV5/iA+YeoRgnhZHrx/Cez5GawOOPcZ6F/7semewkpyP9wIPgjpE0/Y8ORqr1ttFs66rjvTnlzJjtU5bF6eRdfBdR9uc0RdxkL7ZbDgWVj4AmydDS8P9V/8OfhWsFgb9nwiInJUtQrjb7311hFf+/DDD2nVqhXPPvssF110ETZb9cN6vV4+//xz7r//ftavX88dd9zB9dfX/U/JIiJzt6RTaJuPAdw14Bby3nkX3G6CB/QnpG/fWh/H6/Ex4z9rKMrxz5oy4Ug94qU58O4E/2wpwTFw1TRoNaDW5zI9PvLe34CvxI09KZSoizvX2Osd1yqcgee1Y9n0HSz4eDMtu0QfeV7zY2EP9k95eOrl8M09/rnJZz7oH7py4UvQonvDnk9ERI6oXhdwrly5kltuuYW4uDiWLl3KZZdddkgQB7BarVx22WUsWbKEmJgYfve737Fy5cr6nFpETlJPL37Hv8iPpSXntBhAwccfAxB74421PoZpmvz08Wb2bCnA7rRy7m2nEhp1mNBbnAlv7Zu2MLQFTPm2TkEcoGD6Nlw7izGCbcRe3R2L4/A90P3ObkuLtuFUlnmY+/7G47fKcVxnmPw1nP8vcEbA7pX+Czzn/xM8ruNzThEROUS9wvjzzz+P1+vlwQcfJDk5+ajtk5KSePDBB3G73Tz33HP1ObWInITWZ+SS7vkegOt7XkfhR5/gKy3F2bkzYSNG1Po4v87dxfoFe8CAsTecQmxyWM0NC3bCW+MgZxOEJ8N1MyChR51qLl2eSenyTP8Km5O6HnmFTcBitTB6Sg+sNgvp63LZsDijTuerE2PfdIy3LYUu54DPDXMfh9dGwp5fjt95RUSkSr3C+IIFCwAYPHhwrfcZMmQIAAsXLqzPqRvUI488gmEY1W6JB02NZpomjzzyCMnJyQQHB3PmmWeybt26aseorKzk97//PXFxcYSGhnLBBRewa9euE/1WRJq1v8/7AIu9GAfRXNN5HHnvvgv4x4rX9mLH9PW5LJq2BYBhF3Wi3alxNTfM2+7vEc/bDlFt4LrvIK5Tnep17Swm/6utAESMaUtQ15ha7ReTFMrgC/yzWC2ctoWi3PI6nbfOIlvCFR/BJW/4p2nMWguvjYY5j4C74vieW0TkJFevML53717AH0Rra3/b/fs2FqeccgoZGRlVtzVr1lS99s9//pPnnnuOl156iRUrVpCYmMiYMWMoLi6uanP33XfzxRdf8NFHH7Fw4UJKSkoYP348Xq83EG9HpNnJLCpjdcmXAFzU8QpKPvkMb24u9pYtiTj33FodIz+zlJmvrcM0odvQRPqMOcwCO/mp/iBemA4xHf094nWcccRb4iL3/Q3gNQnqEUv4mXVbzKf3Wa1J6hiJu8LLj+8ex+Eq+xkG9LoUbl8OPS8B0wsLn4dXT4P0Zcf33CIiJ7F6hfH4+HgAZsyYUet9vvvuOwDi4g7TGxUgNpuNxMTEqtv+92aaJi+88AJ//vOfufjii+nZsyfvvPMOZWVlfPjhhwAUFhbyxhtv8Oyzz3LWWWfRt29f3n//fdasWcOcOXMC+bZEmo0n5n6GxbEXixnMXb0uI3ffIj9xt92GYbcfdf+KUjff/vtXXOUekjpGcuaV3WruTS/JhvcuguIMiO/mD+K1nEN8P9NrkvfhRryFldjigomZ2AXDUrdpCi0Wg1GTu2OzW9i9KZ/UNbl12v+YhcbBpW/C5R9AWALkboE3z4bvHwBX6YmpQUTkJFKvMD5y5EhM0+S5555j0aJFR22/ePFinn/+eQzDYPTo0fU5dYPbsmULycnJtG/fnkmTJrF9u38u4B07dpCZmcnYsWOr2jqdTkaMGMHixYsBSElJwe12V2uTnJxMz549q9rUpLKykqKiomo3ETlUaaWbuZn+CzXPTLoQ17Qv8eblYW/ThsgLLzjq/qbPZPab6yncW05YjJNzbumF1V7Dj7+KIv/0hfuHplzzJYQn1Lnewu93ULm9EMNhJfaa7liCajVx1SGiWoRw6ij/B4FlX23H9B3n3vGDdR8Pty+DPlcBJix9GV4ZBtvmnrgaREROAvUK43/6059wOBxUVlYyevRo7r77blatWoXP56tqY5omq1at4p577mHUqFFUVFTgcDj405/+VO/iG8rgwYN59913mTlzJq+99hqZmZkMGzaM3NxcMjMzAUhIqP4LOSEhoeq1zMxMHA4H0dHRh21TkyeffJLIyMiqW+vWdfsztsjJ4vkF30NQGpg2Huh7NbmvvwFA3G2/w6hhBqff+nlWGunrcrHaLZz7u1NrXt3SXQEfXblvefs4fxCPSKpzrWWr91KyYDcA0Zd1wZ4QWudjHKzv2LY4gqzk7i5h68/Z9TpWnQVHw4SX4arPIKKVf/jOexPg81v80z2KiEi91SuMd+/enbfffhur1YrL5eLFF1+kf//+hIaG0rJlS1q1akVISAj9+/fn//7v/3C5XNhsNt566y26devWUO+h3saNG8cll1xCr169OOuss/j2228BeOedd6ra/PbP2aZpHvWCsaO1eeCBBygsLKy67dy5sx7vQqR5qvR4+XSb/3uxd9RobJ9/j7egAEe7dkSOH3/U/XdvzmfZV/6/dJ0xqQvxrcMPbeTzwuc3+efcdoTB1Z9CbN1W8gRwZ5aS/+lmAMJHtCKkV/2H4wWF2ukzpg0Ay7/egc/rO8oex0Hns+D2pf6FgTDg14/gpYGw6n9wvMeyi4g0c/UK4wCTJk1iwYIF9OvXD9M0MU2TyspKMjIy2LNnD5WVlVXb+/Xrx8KFC5k0aVJD1H7chIaG0qtXL7Zs2VI1q8pve7izs7OressTExNxuVzk5+cftk1NnE4nERER1W4iUt2zP32PN2gTmBb+2v86cvctQhZ3+21H7RUvK3Ix63X/BZtdhyTSfVgNPd2mCd/eCxum+1fWnPQhJNd+8aD9fOUect9bj+n24ewURcTZ7ep8jMPpPbo1QWF2CrLK2Lj08H9tO66c4TDuKbhxDiT0hPI8+PJWePdCyN0WmJpERJqBeodx8A/zWLlyJcuWLePxxx9n0qRJnH322YwdO5ZJkybx+OOPs2zZMlauXMmgQYMa4pTHVWVlJRs2bCApKYn27duTmJjI7Nmzq153uVzMnz+fYcOGAdC/f3/sdnu1NhkZGaxdu7aqjYjUXbnLw8dbXwOgd9QYor6ej6+wEEeHDkedQcXnM5n1xjrKilxEJ4Uy4oquNf+lat4/IOUtwICLX4MOtZ+vfD/TZ5L38SY8uRVYo5zEXNGtzhdsHokjyEa/s9sCsOLbHXjdAegd36/VALh5Hpz1CNiCYMd8eHkI/PCYLvAUETkGx3ZV0T7p6ekAhIWFERMTw8CBAxk4cGCDFHYi3XfffZx//vm0adOG7Oxs/v73v1NUVMTkyZMxDIO7776bJ554gs6dO9O5c2eeeOIJQkJCuPLKKwGIjIzkhhtu4N577yU2NpaYmBjuu+++qmEvInJs/jn/G3zObWBa+fuAm8i76BoA4u+4HcN6+FUswR9ad2/Kx+awcM7NPbE7a2i/+iOY/w//4/OegVMmHFOdxT+mU7ExD2wWYq/pgTX06LO71FWvES1ZPSedkrxK1i3cw6kj6zbDS4Oy2uG0e6DHhf6/Kmz7ERY86x+2MvZv/qkRaznvu4jIya5ePePt2rWjffv2fPTRRw1VT0Ds2rWLK664gq5du3LxxRfjcDhYunQpbdv6e6L+8Ic/cPfdd3PbbbcxYMAAdu/ezaxZswgPPzD29Pnnn2fChAlMnDiR4cOHExISwtdff431KIFBRGpW7vLw2Q7/hZr9oscR+sVsfMXFODt3Ivycc464b/r6XFZ+lwrAmVd1Iyaphoso05bA9N/7H592Dwy88djqXJ9L0Rx/x0T0hE44Wh5mNc96sjmsDDi3HQApM1JxuxrBGgYxHeDqz/1De6LaQvEe+OwG/xztmWuOvr+IiGCY9VhJIjQ0lIqKCpYuXdoke8Qbm6KiIiIjIyksLNT4cTnpPTRzGl9lPgamnRlnfUzphCvxlZTQ8oUXiDjn7MPuV1pQyUd/X05FiZsepycz8qoaLhbP2+5fYbI8D7pfAJe9A5a69024s8vI/vcqzEovoUOTiL6wbit01pXX4+ODvy6lOLeCoRd1rBq60ii4K2Dxi/4eck85GBbocyWM+BNEaaYoEWk+Gjqv1atnvGXLlgBaZVJEGlRppZvp6W8CMDh2PI5p3+IrKcHZtSvhY8ccdj+fz2T2m+uoKHET2yqM0y/rfGij8gL48HJ/EE/qAxf955iCuK/CQ+676zErvTjaRxA1vkOdj1FXVpuFQeP9K4H+PCuNynLPcT9nrdmDYMT98PuVcMrFYPrgl/fhxX7w/YOaClFE5DDqFcb3L3KzcOHCBilGRATgsR8/wXTsAp+Dv/W4nLy33wYg/s7fYxwhOKfMSGX35gJsTivn3NQTm+M3w8S8bpg2GXI2Q0RLuOIjcITUuT7TZ5L30SY8OeVYIx3EXtUdw1q7H6emz0dJfh7FeTmUFRZQUVqCu6ICr8dTqyXvuwxOJDoxhMpSD6v3DY9pVCJbwWVvwQ2zod3p4HXB0n/Dv3rD3Cf9CyuJiEiVeg1T2bJlC3379iUsLIyUlJSqnnI5NhqmIgLFFS6Gv3cepiOT4XGX85f5RRRN/5qQAQNo8967h527f8+WAr587mdME86a0p2uQ34zjaFpwjf3+GdOsYfC9d9D0qnHVGPhrFSKf9wJNoMWt/bG0erQuctN0yRv906ydmwjf88u8vbsJn/PLvIzM/C4Kms8rtVuJ7ZlG+LbtiOuTTvi27Qnvm07QiKjqrXbmpLNzNfW4gy1MeXJ4Yd+6GgsTNN/cecPj0HGKv+24BgYfAsMvAlCYwNanojIsWjovFav2VQ6d+7Mhx9+yNVXX82QIUN46qmnuPTSS3E4aljdTkSkFv76w/8wHZkYvmAeaTGK/Ok3ANDiT386bBCvKHEz+80D84kfEsQBlr16YArDS9845iBevjbHH8SB6Is6VwvipmmSnbqdLcsWsXnpIvIzdtd4DMNiwTAs+LzVh5l43W6yU7eRnVp93u7IFgl0HDCEzgOHktytOx36xhMeG0RxbgWbl2fR47TkY3ovx51hQKfR0HEUrP8Kfvw75G6BeU/Cwheg3zUw9HaIbhfoSkVEAqZePeOjRo0CIC0tjR07dmAYBg6Hg86dOxMdHX3EmUQMw+CHH3441lM3S+oZl5NdQXkFp39wLtj3cmb8Vdz7wVrKU1KIvPBCkp/6R437mKbJd6+sIfXXHCJbBDPxwYE4gn7Tz7D1B/jgUv845rF/h2G/P6b63FmlZP97NabLS9jwZKLO96/SmbtrJ+vmz2HzskUUZh1YlMdqt5PUqSsxya2ITm5ZdR8Zn4DFasU0TXxeLz6PB6/HQ0VJMXt3ppKTlsre9B3kpKeSn5lRbZXL4PAIOg4YjMXWkU0rHMS2jGTSXwYddUXgRsHn9YfyRS9Axmr/NsMCp1zkD+XJ/TQloog0eg2d1+oVxi0WS9UvgNoexjCMqmXideFndQrjcrK79cuXWVT4CoYvhJlxD1Bw/wMYQUF0/H4G9n2r4f7W6h93svCTLVhsBpf+YQDxbX4zZCRnK7w+CioKoc/VcOFLxxT4vKVusv+9Cm9eBc4OkcTd0JPc3eks+fxjNi9dWBWYbQ4n7fv0p/OQ4XToOxBnSN3HpB/MVV5G2trVbF2+hO0py6koLTnwohGM1dmTcb+7gq5DutTrPCeUacKOn/yhfNuPB7Yn9oJ+k+HUiRAUGbDyRESOpFGF8TPPPLNevTFz58495n2bI4VxOZltys7mkq8vxLCVcH6LG7j+ue9w79xJ3G23EX9nzT3Ze9OL+fSfK/F5TE6/vDOnjvzNFHrlBfD6Wf6hEa0GwZRvwOasc22m10fOG2up3F6INdqJ9cIYln73CVuWLa5q06H/IHqcPor2ffvjCAqu8zlqw+f1smvDOrauWMKWZYsoyc/b94pBh/4D6Tv2PNqe2veIF7k2Ohm/+qdEXP+l/2JPAFuwv7e8/2RoPVi95SLSqDSqMC4NS2FcTmbj3vsDu3wzcPgSmOG9nNxnnsMWH0/H72dgCT100R5XhYdPnlhBYXY57U6N49zf9areOeDzwocTYesc/8wpN82F8IQ612WaJgVfbqV0WSbYDdaFLmftLwd6c7sMHs7giy+nRbvjP7XhwXxeL6vn/MS8d6fh8xyYVSU6KZkB4y+mx4jR2OwNvxLocVOW518R9ed3YO/GA9uj2/lX+uwxAZL7KpiLSMApjDdjCuNysvp6/WoeWD4Zw/DyYIe/0m/qP/EVF5P0+ONEXXLxIe1N02T2m+vZsiKLsGgnl/95EEFhvwmesx7y97jagv0zpyT3OabaSpbsoeCrbZiYLM75il3Fm8Aw6DrkNIZcfDlxbdod03Ebytf/t4rUNVuJbrGN/N0puMrLAAiLjWPg+ZfQa/RY7I66/zUgYEwTdq2AlHdg3efgLjvwWlQbfzDvfiG07AeWRjqLjIg0awrjzZjCuJyMfD6TIW9eSbl9LfHWPny4oyv5H3yAs3t32n86DaOGC8HXLdjNvA82YVgMLrq3H0kdfzO+eNWH8OXv/I8vfQt6Hhroa6NiSz45b64FE1bnzWVj4XLantqXkZNvIrZVm2M6ZkNLW5vLNy+txhFk5YpH+rFp0Y+s/PqzqiEsIZFRDBh/Eb3HjMMRXL/x6yecqxS2zPYPYdk8s3owD46BDmf6Z2rpONI/v7mIyAmgMN6MKYzLyeiJuZ/zv/S/YppW3uv6LEE33gVeL23efovQIUMOaZ+zq4RPn1qJ1+2reUn4ncvh7fP844/PuB9GPXRMdZXtzmfvv1dj9VlJLV7L6soFjJx8E91Pq9+1Mg3N9Jl8+OgyCrLKOP3yLpw6shUel4t18+ew/KtPKdqbDUBQeASDLryUPmef17R6yvdzlcHW2f7ZWLbMhsrfLB4U1wXaj4DWg6DVQP/wlkb0dRKR5qPRh/HU1FRycnIoLy8/6gwrZ5xxRkOeuslTGJeTTX5ZOSM+PA/Tvpe+YRfy2OeplKekEDZyJK1fefmQ9q4KD9OeXElBVhltToll/O2nYlgOClwFO+G1kVC6F7qNh4nvHdNS97vXrKP4ve2EWaLIqdhNRsc9jJh8AyERjXOGj1/n7mLBx5uJbBHMVY8Mqfo38Xo8bFg4j+VffkJ+xh4AQqNjGHLR5fQaPRarrQmNKT+Y1w27U/wzsWz70f/Y9FVvExrvv2i39UBI6gMJPSEsPiDlikjz0ijD+KZNm3jiiSeYPn06RUW1W+rYMAw8Hs/RG55EFMblZDPls6dJKXkXwxfOd9abKH7inxghIXT8ejr236zoa5omc95az+blWYRGObn8oYEEhx20wFhlCbx5DmStgYRe/nHizrA61WOaJinTv8Ayt5wWQW0o95VivyiOdkMHNMTbPW5cFR7e+dMiXBVezrv9VNr1iqv2us/rZf2CuSz59MOqnvKI+ASGXnoFPU4fieUIa0I0CeX5/qkS05f6/zKSsRp87kPbhbaAxJ6QcAq0OAViO0FsRwiJOfE1i0iT1ejC+JdffslVV11FRUVFrecaBzTPeA0UxuVk8mvGTq6ccTGGtYJro27mgr+9i6+khIQH/kTM5MmHtF+/aA9z39uIYTGYMLUvyZ2iDrzo88En18DGb/w9ojf96L/Yrw7Ki4v4/t/PE7+rBe3CeuI1PMTd3IvQ9nFH37kRWDhtC6t/2EmbHjGcf2efGtt43G7W/DiTZZ9/TGlBPgAxya0YfvnVdB48vFENv6kXd4U/kO9a7r8YNHMt5G0HDvM7KijKH8pjOkJMe//484iWENkaIluC49DZfETk5NWowvjOnTvp3r07ZWVltGzZkvvvv5+QkBBuvvlmDMNgzpw55Ofns3LlSt5991327NnDaaedxiOPPILVamXEiBH1fgPNicK4nEzOevd2ssyfCPa15bOf21Myew5BvXrR7qP/HXLRZu7uEqb9wz9OfMiEDvQ/p131g/3wN1jwDFgdMPkbaDO4TrXs2byBb174J229XTklejimYRI3pSfBXZtOj2nh3nLef3gJmHDFXwcTk3T4AOmurGDVzG9Z/tWnVJQUA5DQoROnTbrWP095cwnlB3OVQvZGyFoLWesgez3kboPiPUffNzgawpMgrAWEJfg/8IUl+J8Hx/h71oOj/ffOyGMaGiUiTUejCuP3338/zz77LOHh4WzYsIHk5GTWrVtHr169Dun5Li8v54YbbuDjjz9m0qRJfPDBB/UuvrlRGJeTxX+XzebFjVMBeN68hZb/+DdYrbT/7FOCunWr1ray3MO0J/3zibc5JYbxt/euPk7812nw+Y3+xxNegT5X1roO0zT5+buv+OmDt2gb3INB8ecCEH1xZ0IH1bziZ2P27cu/kvprDr1Htea0iZ2P2r6yrJSV33xByjdf4q6sAKB1j16cdsVkkrt0O8rezYSrzN9rnrfNH84L0qFw14Gbq7huxzMs/tVDnREQFOEP50ER/ufOMH8vuz3Uf+8IAUcY2IP9U3Dag8EeBPYQsAX5F6iyOv33NidYbLooVaQRaOi8ZqvPznPmzMEwDG677TaSk5OP2DY4OJj333+fzZs389FHH3HxxRdzySWX1Of0ItIEZRYV8dLaJ8AG3Tmdtm9OwwPE3nDDIUHc9PnHiRdmlxMW4+SsKT2qB/FdKfDV7f7Hw+6sUxD3uFzMef3frJv/AwlBbRkYfw4A4SNbN8kgDnDK6cmk/prDpuWZDL24I1bbkXtonSGhDJ94NX3PHs+yL6exeta37Fy/hv/95T469BvIsMuuIqFDpxNUfYA4QvzjyBN71vx6RaE/lBdn+i8MLsmCkmz/rTTbP169LB/K88BV4r+QtDzff2tohsX/1x+rA6z2A/cW+4F7i3XfY9tBN+uBx4bF/9ywHnRvOej5vseGZV9bC2AceG5Y/B8IjIO3G/seG795bDnw+JB7qm+DA9sPef23jzmwT7X2B22vVVtqblNtc03b69K2rk7wh63m9uGu3enHtLhboNUrjKempgIwbNiwqm0H/3nT4/Fgsx04hcVi4c4772TKlCm8+eabCuMiJ6Ebvn4U05aDxRvFUzuiqcjOxt62DXG3/e6QtitnpJL6aw5Wm4Vxt/QiOPygCzYL0uGjK8BbCV3OgbMeqXUNpQX5fPXM38nYsokoZwvOaHU5htcguHc8EWPaHv0AjVSbHjGERDooK3SRuiaHjn1b1Gq/kMgoRk6+if7nXciSTz9i3fw5bP95Bdt/XkGngUMYdtlVxLdtf5yrb6SCIv23hFOO3tZTCeUF/iBeWeS/VRx07yrxD5epdisBTwW4y/03T4V/PnV3hf//bd9BEx2YPv/rnorj9nZFmrRrvzr5wnhpaSkArVu3rtoWEnJgUYnCwkJiY2Or7XPKKf4faKtXr67PqUWkCXptxRzSPbMA+EvolVR8+iIASY8+hiUoqFrb1DU5LP9mBwAjruxCi7YH/SmwvAA+mOjvpWzRAy55vdarMWZt38qXz/ydktwcosITGNvmOowyE0e7CGIu61K9572JsVgtdBuSyM8z09mwOKPWYXy/iLgWnH3rnQy68BKWfPYRGxbOY+uKpWxdsZQuQ05j2GVXNprFjholm9MfBBoyDPi8/pDvrdx37/JP7eh1H/TY5Z89xuv2t6967PE/N70HHvs8/lBftX3/vc+/3fTuuz+ojWn6b5jVX2PfdtN30Gsc2F7r+9/sAwdtp4bn5pG316otNbc57DmPpe3hNMCM0loipmZBUYGu4JjUK4xHRkaSl5dHRcWBT+kHh+9t27YdEsb3T32Yk5NTn1OLSBOTVVLEi7/+HWzQ2TKCvu9+i8s0ibzkYkKHVL/gsiC7jNlvrgcTep7Rku7DDhoG53H5Z07ZuwHCEuGqaeAMr1UNm5Ys5PuXn8fjqiQhuSMjE67ALHBjiw8m7toeGEcZ1tEUdBuaxM8z00lfm0tpQSWhUXVf4Cc6qSXn3nEvgydMZPGnH7J5yQI2L13I5mWL6DxoKIMnTGz+w1caC4vVP5SGJrZ6qojUWr1+83Tt2hWA7du3V20LDw+nbVv/n3lnzZp1yD5z5swBICoqqj6nFpEm5sbpj2LacrF4o3l2UySubduwxsWRcP/91dq5K73MeHUNrnIPiR0iql+IaJrw9V3+OaUdYXDVJ7VaBt00TZZ8+j++eeEfeFyVdOw9iLPaXYNZ4MYa5STuxl5YQproAji/EZ0YSlLHSEwTNi3LrNexYlu15vy7/8i1T79E50HDwDTZsmwx7z9wN58+/hd2rl9TpyltRUTkUPUK40OHDgVg6dKl1baPHz8e0zR5+umn+fHHH6u2f/rpp7zwwgsYhsHw4cPrc2oRaULeWDGHVLf/w/mjledR8ennACQ/9Q+sB30wN02TH9/bQN6eUkIiHJxzc6/qFyHO/yes/tB/gdllb0NS76Oe2+N28/2/n2PxNP8MTgPGXcSw2AvxZJRjCbUTd0NPbJFNcHn4I+g2LAmADYszGiQsx7dpxwX3PsjkZ/5N99NHYlgspP36C588+gAfPfwHtq5chs+ndSNERI5FvaY2nDt3LqNHjyY5OZm0tDSs++YGTk9Pp0ePHpSXlwMQExNDZWUlpaWlmKaJ1WplwYIFDBkypGHeRTOhqQ2lOdpbWszoj87HtOXSt2wof35zFb7iYmJvuokW906t1nbljFSWfbUdi8Xgwt8u7LPqf/Dlrf7H45+HAdcf9dzlJcVMf/Zxdq1fi2GxcNb1t5G8pzUVG/IwnFbibz4VR8u6rdLZFLgqPLz1h4V4XD4uvq8fSQf/OzaAwuxMVkz/nLXzZuN1+1e6jExIpO/Z4+k5cgzOEC2SIyLNV0PntXr1jJ955pn89a9/5brrrmP37t1V29u0acO0adOIjIzENE1yc3MpKSnBNE2cTievvfaagrjISWLKl3/BtOVic0Xyl7n5+IqLCe7dm/g7f1+t3eYVmSz7yj/k7fTLO1cP4jt+gun72g+/u1ZBvCAzg/89dB+71q/FERzMxX/8K63y21OxIQ9sBnGTezTLIA7gCLLRqb//4s0NSzIa/PiRLRI568bbuPHFNxh4wSUEhYZRmJXJvHdf5z+3TuaHN18hb8+uBj+viEhzVK+e8aPJy8tj2rRprFu3Do/HQ+fOnZk4cSItW7Y8Xqds0tQzLs3Nw3Pe5YvdTwPw7LozaT19DpbwcNp/8TmOVgfGeu/ZWsBXL/yCz2PS+6zWnHbpQePEd6fAOxf4p4A75SK45M2jrnC4e+N6vnrm75QXFxEeF89Ff3gY2yovpUsywAKxV/cguEfsEY/R1O3ZUsAXz/6M3WllylPDcQTV63r9I3JXVLBh4Tx+njGd3F3pVdtbn3IqvUaOodPgYdgdzWsokIicvBrVCpzSsBTGpTn5blMKf1h8E4bFzUVZp3PFW/PANGn5wvNEnHNOVbuCrDI+/edKKks9dOgTzzk39zwwvWD2RnjrHP+8ze3PgCun+VcoPIKNi+bz/Ssv4HW7SejQmQvvfwjPgnx/EDcg+tIuhPZvevPQ1pVpmnzw8FIK95Yz6trudN83jvx4nzN97Wp++f5rtqUsr5p+zRkSSrfhZ9DzzDEkdOxcbT0KEZGmplGtwCkiUpNdBXk8sPB+DJubliVduHr6GrymSdTEidWCeHmJi29eWk1lqYcW7SI46/qDVtjMT4X3JviDeMv+MOnDIwZx0zRZ9sUnLPr4PQA6DhjCuXfcS+ms3QeC+MWdT4ogDv4F2LoNS2LZV9vZuCTjhIRxwzBo26sPbXv1oWhvNuvm/8DaeXMo2pvF6tkzWD17BrGt2tB12Ol0GXIasS1bH/2gIiLNnHrGGxH1jEtz4PF6GfX+deTzCzZXNB/Na40v5WecnTvR7pNPsAQH+9u5vXz1/CoytxcSHhvEpX8cQEjEvhU2izPhzbP9gTy+O1z3HYTEHPacXo+b2f/9N+vm+6dO7T/+Ik6/cjJF36RWD+IDm+Yy98eqJL+Cdx9cjGnCVY8OISrhxM9Vbfp87Fy/hrVzZ7Nl2WI8blfVa3Gt29JlyGn+YN5KwVxEmoaA9Iz/9NNP9T5RTc4444zjclwRCZxbvn6GfH7B9Fp4ZVU3fCkLsISE0PK556qCuM9n8sM7G8jcXogj2Mb423sfCOJlefDeRf4gHt0OrvniiEG8oqSE6c89wc51v2IYFkZdfyu9x4yjYPq2kzqIA4RFB9G6Ryzp63LZuCSDIRM6nvAaDIuFNj1706ZnbyquL2HriqVsXrKAtDWryNmZRs7ONBZP+4CY5Fa079ufdn0G0Kp7T2z25jHvu4jI0dSqZ9xisTT4GD/DMPB4PA16zKZOPePS1L2+YiYvrLsfwzD545oh9P9mIVgstH7lZcJGjADA9JnM/WAjGxZlYLEYnH9nb1p12xe2K4vh3Qmwe6V/dc0bZvoD+WEUZGXyxT8eIW/PLuxBwZx/9x9p17s/BV8riO+3NSWbma+tJTTKybVPDMNiaRzjtStKSti6cimbly4k7ddV+LwHfh/YnE7anHIq7fr0p80pvYlp2UrjzEWk0QjYmHGNZhGRI/l5z3b+teavGFaT8zZ18wdxIOHPDx4I4qbJgo83s2FRBoYBZ13f40AQLy+ADy71B/HgaLj2yyMG8V3r1zL9uScoLy4iLDaOi/7wMPGt25H/6WbKfs5WEN+n/alxOENtlBZUsntTPq27H/6vDCdSUFgYPc88i55nnkVFaQlpv65ix6qVpK7+mdL8PLb/vILtP68AIDgikpZde9Cqe09adT+F+HbtsVisAX4HIiINo1ZhfO7cuYd9zeVy8dBDD7FixQri4+OZOHEigwYNIiEhAdM0yc7OZsWKFXzyySdkZ2czaNAg/v73v2PXnyBFmo3teVlcP+MmsJXSPT2eyd9sBiBm8rXEXHUV4A/iiz7bypr5u8GA0VN60HnAvospy/L8F2tmrPYH8Wu+gBbdD3u+X3+YyQ9vvILP6yGhQycm3P8XQsKiyH13PRWb8sEC0ZecHLOmHI3VbqFT/wTW/bSbzSuyGk0YP1hQaBhdh55G16GnYZome9N2sGNVCmm//kLG5o2UFxWydcUStq5YAoA9KJiE9h1J6NCRhPadaNGhEzFJLTGOMuWliEhjVK8LOE3T5LzzzmPmzJlcf/31vPDCC4SG1rzyWllZGXfffTevv/4655xzDt99990xF91caZiKNEVZJYWc+8mVuKzpJOSE83/TfBgFhYSNGkWrF/8PY9/KvEu/2kbKjDQARl7djR6nJfsPUJIN714I2eshJA6u/QoSe9Z4Lp/Xy/z33uDnGdMB6DL0dM753V1YPFZy31mHK70Yw24h5spuBHdv3vOI18X+OccdQVaue/o0bPam06vscbvJ2r6V3RvXsWvDWnZvXI+rvOyQdo7gYOLatCeuVRtiW7UmtlVbYlu1JjQ6RkNcRKRBNap5xl9//XVuvvlmxowZw8yZM2u1z9lnn82cOXN45ZVXuPnmm4/11M2Swrg0NUWVZZz94WRKLBsJLQ3htS/Cse3cTVCPHrR9710s+z6cr/xuB8um7wDgjEld6HXmvgV/ivb4F/TJ3eIfIz55OsR3rfFcFSUlfPOvp0j79RcAhk+8msEXX4630EXOm2vwZJdjBNuIm3IKzrb6/jmY6TN596HFlORVcs7NPenYr0WgSzpmPp+XvF07ydqxjcxtW8jasZW9qTvwuCprbO8MDSU6MZnIhCSiEpKITEjw37dIJCwmRsNdRKTOGlUYP+2001iyZAmff/45F154Ya32mT59OhMmTGDo0KEsWrToWE/dLCmMS1NS6XFz9oc3kmv+TGipg1e+iSdoexq2xETaffwx9oQWmKbJzzPTWPqlf5n7YZd0ou+YNv4DFKTDO+f7Z02JbO3vEY+tebaPvD27+PKffyM/Yzc2p5Nzb7+XzoOH4c4qJefNtXgLXVgjHcRd3xN7Qs1/nTvZLfliKz/PTKdD33jG3dIr0OU0KJ/XS+7uneSkp5K7aye5u9LI3bWTgswMTNN32P0Mi4XQ6BjCY+MIj4kjPDaWsOhYQqKiCYmMIjQyipDIKIIjIhTaRaRKo1r0Z+PGjQC0adOm1vu0bt262r4i0vT4fD4u+mQquebPhJVYePGraILS07DGxND6v//xB3GfyaJPt7L6x50ADL6g/YEgnrkWPpwIRbv9F2lO/hqiav45smnJQma++i/cFeWEx8Yz4Q9/oUW7DlRsyiP3fxsxK7zYWgQTd30vbFFacv1wOg9M5OeZ6aSuyaGyzI0zpPlct2OxWolv0474Nu2qbfe4XORn7KYgM4OC7EwKszIoyMqkICuDor3ZmD4fJbk5lOTmkHGE4xuGhaCwMILCwgkKDyc4LNz/OCwcZ0gIzpBQHPvuncGhOEKCcQSFYA8KwhEUjD3IidXWfP69RaRh1SuMV1RUALBz50769u1bq3127vT/Yq6srPlPiiLSuJmmyVWfP8xO9zzCS+D5z6MJ3b0ba1wcbd9+C2enTnjdPn54Zz1bVmYDMPzSTvQ5a1/Y3jIbpk0BVwnEdfH3iEckH3Ier8fNT++/VTU+vFWPnoy/64+EREZRPH8nhd+nggmOthHEXtsDa6jCzpHEtQojJjmUvD2lbPtlLz2GH/pv3tzYHA7i27Ynvm37Q17z+byUFRRQnJtDcZ4/kBfl5lCan0dZYQFlhQWUFhZQXlyEafooLy6ivLiII6b2I7BYbdiDnNgdTmwOJzbn/scOrA4HNrsdq92Bze7Aardhtdux2vbfbFhsNv+91YbVZsWwWPc9t2Kx2rDYrFgsVv9UxNZ9j60WjP3b9t0OfmwY+58bVc8NY/9jAwzD/9ywgMG+16najmFgcHA7w9/uoG0icnT1CuOdOnVizZo1vPrqq1xwwQW12ufVV18FoGPHE7/4hIjUj9vj4fLP/sSWiplElpr8c1oUEZlZ2OLjafPO2zg7dMBV4WHGq2vYtTEfi9Vg9OTudBm0b3rB5a/BjD+A6YN2p8Pl7/lnT/mN4twcvn7hH2Rs9v8FbeCFl3La5ddg+CDv402Ur9oLQOigRKIu6Ihh0ywatdFlUAJLv9zOlhVZJ0UYPxKLxUpYTCxhMbEkUfN1CuAfAlNeXER5USEVJSWUlxRRXlxMRYn/VllWSmVZGa5995VlpbjKy3FXVuCuKMe7bz0Nn9dDZamHytLSE/UWG4+q0L7/qcH+J/vDPRwI8RyU4Y1qT4yD7w46Rg2h/yjHqKnGGjfX3LpOx6i5qT6oHA/n3/MnWnWveQKAxqxeYfyyyy7j119/ZebMmdx2220899xzBAUF1di2srKSe++9l++//x7DMJg0aVJ9Ti0iJ1ipq4ILPr6NbN8KIovhn5+EE52di61FC38Qb9+esiIX37y0mr3pxdidVsbd0ovWPWLA54WZf4Zlr/gP1udqGP882ByHnCft11V8+3//pLy4CGdIKOfcPpVOAwbjKagg970NuHeXgMUg6oIOhA5O0i+1Oug8wB/Gd23Kp7SgklAN6zkqi9VKaFQ0oVGHfmisDa/HjbuiEldFOR5XJe7KSjwuF57KStyuSjyVFXg9HjwuF163C4/bjdftxuN24fN68Xr8z70eD163G5/Xu+/mqbr3eryYPh8+n/810+vF5/P5H/t8+Hw+TJ8P0+c98Ng0990feB3T3LfdPOJY+zozTUxM2HeFmlYtkePF20QXk6zXBZwVFRX07duXTZs2YRgGCQkJTJw4kYEDB9KiRQsMwyArK4sVK1Ywbdo0MjMzMU2Tbt268csvv+B06hfBwXQBpzRWWcX5TPjsZkqMjSTmGjz+ZTjh2XnYEhNp+87bONq2JT+zlG/+/StFe8sJDrcz/o7etGgbAZUl8NkNsPl7/8FGPwynTT2kF8nrcbN42ocs/+pTME3i23XggnseICoxiYptBeT9byO+EjeWUBuxV3XH2SHqxP9DNAOf/TOFzO2F1YcOidTANPeFchNM0+cP6ZiwL6z704M/wFe1OThS7Av3+49VFcr9ux3U9qCgfvA2/yGo/oADx/jtdvNAi8O9n5pfOOy/QO2PcdhD1P+jhz681F5kiwQcQcHH/TyN6gLOoKAgfvzxR8477zxWrVpFZmYmL774Yo1t9/8P3LdvX7755hsFcZEmYkvuHiZ9dSMu6056bbPyp28s2MvysCUn0fadd3C0bs2WlVnMfW8j7kovEXFBnH9nH6JahEDWevj0Oti7EWxBcNGrcMpFh5wjZ2ca3730LHtT/bOu9Bw5llHX34LNaqfw+1SK5+8EE+xJocRe2wNbdM1/gZOj6zIogczthWxZkaUwLkfkHwe+fxYZzSYjcrzUK4wDJCUlsWLFCl5++WVeffVVNmzYUGO77t2787vf/Y7f/e53WK36phZpCpbt3MjNs2/FZ8nh3GV2Js+rxPCZBPfuTauXXsSIiWXhJ1uqZkxp2TWKsTf0JCTcDilvw4w/gqcCwhLg8g+g9cBqxzd9Pn75/mt++vBtvG43QeERjLnpdroMHo47p5zsj1bj3lUCQMiABKIu6IjFoZ8f9dGpfwsWfLKF7LRiCrLKiEoICXRJIiIntXoNU6lJRkYGa9asIT8/H9M0iYmJoVevXiQlJTXkaZolDVORxuSFJZ/zxsYnsfnKufF7B6PWlAMQedFFJD76CGWlJrNeX0vGtkIA+p3dlsEXtMfiLoFv7oa1n/kP1HE0XPQfCIuvdvzivBxmvvKvqkV82vXpz9m33kVoVDRlK7Mo+HobpsuHEWwj+uLOhPSKO2Hvvbn7+sXVpK/LZeB57Rh0fodAlyMi0qQ0qmEqNUlKSlLwFmnCylwVXPvlw2wqn0Fkucn9n9rpsqccLBZa/OF+YiZPZvfmAma9vpbyYjeOYBujJ3enQ5942PMLTLsO8neAYfWPDx92J1gOzHbi83n5dc5MFv7vHSrLSrE5nIy4+np6jz0XX6mbvA83Ur4mBwBnh0iiL++KLVLD2hpSl0EJpK/LZfOKLAaOb6+LYEVEAqjBw7iINF0pu7dyy8y7qLSmM3Czj999byestAJLeDgtn3sO56ChLPxkC7/O2wUmxLYM45xbehIVY4V5T8FPT4PP7V9R89I3ofWgasfP2rGNOa//m8ytmwFI7NiZcXfcS3RiS0pXZFI4IxWz3AMWg4ixbQk/oxWGRUGxobXvHYfNbqEwu5zstGIS2ukvcSIigaIwLiIAPL/4U97c9BQh7nJu/9bCiHU+oBJn5060/L//I7syknl/W0ZRjn+xr+7Dkzj98i7YM5bDq3dBzib/gbqNhwtfqjZ/uKu8jEWffMAvM77GNH04goM5bdK19B57Lt7sCvb+51dcaUWA/yLN6Es642gVfqL/CU4ajiAb7XvHsWVlNluWZymMi4gEUIOEcY/Hw7fffsuCBQvYvn07xcXFeL3eI+5jGAY//PBDQ5xeROohNS+L333/CLvcCzk11cdt31qIKXGDYRB7w/WE3/g7Fn+dzvpFOwAIi3Ey8qputOlghZlT/RdqAoTGwzn/gJ6XVE1baPp8bFyygJ/ef5OSvFwAug49nTOvvZGQsCiKvk+nZOEu8IHhsBIxti1hQ5MxrOoNP966DEr0h/GULIZd2gmL/gIhIhIQ9Q7jCxcu5JprriE9Pb1q25GuCTUMA9M0NUZRJMC8Pi9//uF1vt31BsGuMm6cZzL2Fx/gw962DUlPPEmm0YrvnvyF0kIXAD1HtGTohA44tn4F/34QSrL8B+t7DYx5DEJiAP/PgLTVP7Pgf++SnboNgMiERM66/ne07dWPsp+zyJqzEu++4wafEkvkBR01NvwEat0jBmeIjbJCFxlbC2jZ5dgWtRERkfqpVxjfuHEj55xzDuXl5ZimicPhoHPnzsTExGCxaHlqkcZq5paVPLTwUdzmDsauMpm4ACLK/SvuRV99Nd6LbuT7b3eSuX0tAJHxwYy6thvJ1tXw3q2w52f/gWI7w/n/gnbDq46duXUzC/73NulrfwXAERzMwPMvod/4CXg2F5P1Qgqevf6ZWaxRTqIu7Ehw99gT+O4FwGqz0L5PPBsXZ7B1ZbbCuIhIgNQrjD/xxBOUlZVhtVp59NFHufPOOwkLC2uo2kSkgaXmZXHfnGfZWDqDAdt8XP2jScs8/1+yHB06EHTnn/l5Rxg7XlwHgM1uoc+YNvQ7NR/7T1Ng+zz/geyhMPwuOO1usPl7s7NTt7Psi0/YvHQhAFabjd5jz2PQhMuwZpnk/3ffUvaAJdRG+JltCBuShGHXB/dA6dy/BRsXZ7Dtl2xOv7wzFqu+FiIiJ1q9wviPP/6IYRjcddddPPjggw1Vk4g0sNS8LB748f9YWzyD9lku/vqjj57p/hBujYkh5KY72Wo/lfXTMzF9FRgGdB+ezKAhHkJ/fgTe/sp/IIsdBt4Ap98HYfGYpkn6r6tY8fVnVfOFYxj0OH0kQy+5EsdeGyUfplVdnGk4rISd3pLw01tiCdL144HWsls0QaF2yovd7N5cQOvuMYEuSUTkpFOv34Y5Of65gC+66NDlrUUk8KpCeNF39Ep38eAykz47/CHccDoxLr+VHbFD2LYiH9OXAUC7U+MY2i+bmC0PwPs/7juSAb0nwZkPQHRbfF4vmxbOY8XXn1ctYW8YFroMGc6g8ZcRnOWk5N2dlOT5Z17BahA2JInwka2xhjlO9D+DHIbVaqFDv3jWL9jD1pVZCuMiIgFQrzAeHx/Pnj17CA4Obqh6RKQBLExdx/PL32Fb0WyGb3Txz+U+2mX7XzMtVsrPmUJa/HAy0sohPQ+All0iGNRlE8mpf4Lv901TaFj8UxWe+QAk9KAwO5O1n7zP2nlzKMn1fxi3OZ30GjmWvsPGYWzzUvp+Bq4K/2xKlhAboUOSCBuajDVcIbwx6ty/BesX7GHbqr2ccWVXrBqqIiJyQtUrjJ922ml88sknrF27ln79+jVUTSJyDEpdFfzf0s/5avtnBBdt4oy1JlN/9hHjH6ZNZWQSBWdey05rR4ry3JBWjsVi0OkUG31i5xOf9gas9AdzHOHQ7xoYfAvu0CS2Ll/C2v++X3VRJkBwRCT9z7qALkmDcK8rovSttKrXbHHBhJ3WkpB+LbA4rCfyn0HqKLlLNMERDsqLXOzakE/bnrqYVkTkRDLMI81DeBQrVqxg+PDh9OrVi2XLlmGzaQxofRQVFREZGUlhYSEREVqEQ47ONE3mbl/Na798yra9sxm8tYQz1phV48E9Vie57U9nb/ezyS4JgX3f7Y4gC6d0yOBU7xuEFa44cMCoNjD4Vry9riB96w62LFvE5mWLqCwt9b9uGLTr2ZdTe4wmujyeig354PHtew2CukQTOjiJoG4xWjmzCfnpf5tYM3833YYkMnpKj0CXIyLSqDV0XqtXeh44cCDPPfccd911FxdffDFvvvkmcXFx9S5KRA7P5/Px/ZYUPlj7NduzF3BKejYjN5n8cbOJ0wNuWwgZCadQ2OVMshxt8XoNKPbv2zKhlK6hC+hY8g6O/eO5bUHQ9Vzc3S8htTiSLSuXse0/t+IqL6s6Z0x8K/r2PIdER1u8qeWYy71U4F/Ex5YQQmi/BEL6tsAaoaEoTVGnAQmsmb+b7atzONPtw6oZbkRETph6hfHHHnsMgMGDB/PNN9/Qtm1bxowZQ7du3QgJCTnq/g8//HB9Ti9y0sgvL2Haup+Ys+MnCtIX0GdHARdsNemRbmL1QVlIAplJvchtOYDCoFaY7OuV9kJkaCndQn6iC18QYeyFMsACZtvTyU4YQ3pFHGkbNrJ7+lt4XJUAGBgkx3Wha/shxNtaYWR7IRU87JuaMNxBcM9YQvsnYG8ZpkW8mrikjpGERjkpLagkfX0u7XvHB7okEZGTRr2GqVgslmq/hOu6sqbX6z3WUzdLGqYi+1V4XHy9cRkztswnO3UZbXal0mOXl+7pJkn5UBaSSEFUJwoiO1EQ25VKW/X/X2JD9tLWupj29kUk2LdgGOCzOMiJGc4eR3d2FthJ37yViuJ9Uw5iEOmIo01cT1rH9SCsMhxc1X802BJCCO4RS3CPWH8A1zCUZmXhJ1tY/eNOOg9MYOwNpwS6HBGRRqtRDVMBfwA/0nMROTKfz8fK3VuYvW0l69JTsKSuJSkrg657vFy/yySsMoLisK4Uh7cmp2UbtnXvgNsRXu0YFsNDS8da2jmW0865knDrXorcTrJoySbOI7MslMyMvH0932sIs0XRwtmSuPghJEa1J8wXieHbF66LAUwMpxVn+0icHaMI7hGDLVazJjVnnQa0YPWPO0n9NQePy4tNF96KiJwQ9QrjPp+voeoQOSnsLNzLgtS1rNq5lvwda7Ht2U58Tjbt9roZnO3kLHcLSkOSKAvpQ0lYSzb0bI3LGXXIcWxGJQn2TSTZ1xJr2YDNzCDf5SCrogVrS3uRW2zicnkIsoYRaY8h0hFHn4jeRDlbEOmIw4b9wME8/jvDYcXRLoKgjpE4O0RhTw7DsKr3+2SR0D6CsBgnJXmVpK3NpWO/FoEuSUTkpKDpT0QaUKXHzdbcPfyasYNtuzZRmLoJb1Y6wbnZRBcW06LISUxZDGd7YnE5Y6kIOp3y4HhKoxPYlBhV4zFNs4JwthFu2YaTPVh8e3F5KigsC2G3Jxy70ZEQW1/CbFGE2aLoYI+iV1wUYfZo7JbDXFBpM3AkhWFvFYajVTiOVmHY4kM09OQkZhgGnfonsGp2OltWZiuMi4icIArjx8HLL7/M008/TUZGBqeccgovvPACp59+eqDLkmPk8/nILi0kNT+b9L27yMlIp2jvHipyszDzcrAVFBNS6iO03EqoK4ggTyih3nB6WaOodLak0tEDlyOCytBIisPtFAOm6QGzHNNXgWmWg5mJWbENm5mHkzwchgsbBoYRhM0IJcgaitNIJsjamWB7GCG2cIKt4TisQUcu3gK22GDsLUKwJYRgTwjBnhCKLS4Yw6YZM6S6zgNasGp2OmlrcnBXerE7NVRFROR4UxhvYB9//DF33303L7/8MsOHD+c///kP48aNY/369bRp0ybQ5Z0UTNOkwuOiqLKcovIyikuLKCrIo7gon7LiQsqKiqgsKcVdUoq7tByztAIqXBgVXqxuHzY32NwW7D47Vq8dq+nAYjoxcOK1OAmx2nHYYvFYE3BbrZiGidXqgTAv5XhxGV6seLHuu3cauYSQgw0Dmw/sFgt2qx27xYndcGK3OHFYE3BYgnFYnNgO15t9GIbTijXSiS0mCFtsELaYIKyxwf7nMUEK3VJr8W3CiYgLoiingtQ1OXQekBDokkREmr16zaYihxo8eDD9+vXjlVdeqdrWvXt3JkyYwJNPPnnEfU/0bCoLP/yYnHU7/BfdmoAJpv8/GPhDbdX/HaYJGP4Zc8z9z6l67n9mgOk76FgGhmmCaezbsG8IhOlva+w7pv/xvv0BTMP/yPTP8mEYlqrH+7YAFgyDA4/3bzcMDPbN8mPs22rs28846DFgwf+neQOwGAeOZNl3DIthYDEsWLBgNSxYDBsWw4rVsGIxrFgMG1bDitVo+M+0JiY4DSyhdmwRQdgig7CG2bGE2bGGO7FGOrBG+u8tTn2mloaz5Mtt/Px9Gh36xDPu1l6BLkdEpNFpdLOpALhcLj744AO+/PJLVq9eTU5ODuXl5UfcxzAMPB5PQ5y+0XC5XKSkpPCnP/2p2vaxY8eyePHiQ9pXVlZSWVlZ9byoqOi413iw0oUZDIgYfugLxm/updZ8eDENH6bFxLQZ/u8wuwXDbsHitGENtmMLdWILD8YeFoQlyIYlyIolxI4l2Oa/D7FhOKwavy0B0XlAC37+Po20tbm4yj04gvVhT0TkeKr3T9nNmzczYcIENm3adNJPa5iTk4PX6yUhofqfdhMSEsjMzDyk/ZNPPsmjjz56oso7hNfqocRdeNR2Jma1Zwf+e+gDs8Z2+3vRD25h/uaReaBn/jfbzapnB/YAMA3zwPGM/fdgWMAwwGIxMKwWLFYwrBYMm8W/zbbvsc2KxW7DsNkw7HYsdrv/3unAYrdhtVuxOO1YHXYsDhs2pwNbsGPfPhbYdxxjX9g2bBYFaGnyYluGEZUQQkFWGTt+zaHr4MRAlyQi0qzVK4yXlpYybtw4duzYgcVi4cILLyQ+Pp7XXnsNwzB46KGHyM/PZ+XKlSxduhTDMBg6dChjxoxpqPobpd8ufHS4xZAeeOABpk6dWvW8qKiI1q1bH/f69jv3xftO2LlEpGnwz6rSgpXfpbI1JVthXETkOKtXGH/11VfZsWMHVquVmTNnMmrUKNatW8drr70GUK3Xd9WqVVx99dUsXbqUSZMmcccdd9Sv8kYoLi4Oq9V6SC94dnb2Ib3lAE6nE6fTeaLKExGplU4D/GE8fV0ulWVunCH2o+8kIiLHpF7TLHz99dcYhsHEiRMZNWrUEdv26dOHuXPn0qJFC6ZOnUpKSkp9Tt0oORwO+vfvz+zZs6ttnz17NsOGDQtQVSIidRObHEZMcig+r8mO1TmBLkdEpFmrVxhfv349ABdddFGNr/92DHl8fDxTp07F4/Hw0ksv1efUjdbUqVN5/fXXefPNN9mwYQP33HMP6enp3HrrrYEuTUSk1jr19y/6s2VldoArERFp3uo1TKWgoACAtm3bVm07eNhFSUkJ4eHh1fYZPtw/e8f8+fPrc+pG6/LLLyc3N5fHHnuMjIwMevbsyXfffVft30hEpLHr1L8Fy7/ewa4NeVSUuAkK01AVEZHjoV494yEhIUD1CxajoqKqHqenpx+yz/62Nc0u0lzcdtttpKamUllZSUpKCmeccUagSxIRqZPoxFBiW4Xh85lsX7030OWIiDRb9Qrj7du3B2DPnj1V2+Li4oiJiQFg0aJFh+yzf6y4w1G3VQZFROTE2j9UZevKrABXIiLSfNUrjA8YMACAlStXVts+evRoTNPk6aefJjc3t2p7amoqTz31FIZh0KdPn/qcWkREjrPOA/xhfNemAsqLXQGuRkSkeapXGB8zZgymaTJ9+vRq2++8804Atm/fTpcuXbjssss477zz6N27N7t27QLg5ptvrs+pRUTkOIuMDyG+TTimz2TbLxqqIiJyPNQrjI8fP54zzjiD8PBwtm3bVrV9+PDhPPzww5imSX5+Pp9//jnff/89xcXFAFx33XVceeWV9atcRESOu6qhKikaqiIicjwY5nFcw/6HH37g9ddfZ926dXg8Hjp37sy1117LJZdccrxO2aQVFRURGRlJYWEhERERgS5HRISinHLee2gJhgGT/zGc0EgtVCYiJ7eGzmv1mtrwaEaPHs3o0aOP5ylEROQ4iogLpkW7CLJTi9j2815OHdkq0CWJiDQr9RqmIiIizd/+Czk1VEVEpOHVe2rDjh07snXr1lrvk56eTocOHejYsWN9Ti0iIidIx37+MJ6xrZCS/IoAVyMi0rzUK4ynpaWRmpqKy1X7Ka/cbjepqamkpqbW59QiInKChMcEkdQxEkzYmpId6HJERJoVDVMREZGj6jwwAYAtKzRURUSkIZ3wMF5YWAhASEjIiT61iIgco479WmBYDLLTiinIKgt0OSIizcYJD+Pvv/8+AG3btj3RpxYRkWMUEuGgVbdoALasVO+4iEhDqdPUhqNGjapx+3XXXUdoaOgR962srGT79u1kZ2djGAZjx46ty6lFRCTAOg9IYOf6PLasyGLAue0wDCPQJYmINHl1CuPz5s3DMAwOXifINE1WrFhRp5N26NCBBx54oE77iIhIYHXoG8/8DzeRn1lG7u4S4lqFB7okEZEmr05h/IwzzqjWEzJ//nwMw6B///5H7Bk3DIOgoCCSkpIYNmwYkyZNOmpPuoiINC7OYBtte8ayfdVetqzIUhgXEWkAde4ZP5jF4h9y/vbbb9OjR48GK0pERBqnzgMT9oXxbIZM6KihKiIi9VSnMP5b1157LYZhEB0d3VD1iIhII9auVyx2p5XivAoytxf55x8XEZFjVq8w/vbbbzdQGSIi0hTYHFba94lj87IstqzMUhgXEamn4z61YW5uLvn5+cf7NCIicoJ0HuBfAGhrSjY+ry/A1YiING3HJYxnZWVx8803ExcXR4sWLYiLiyM6OpopU6aQnp5+PE4pIiInSOseMQSF2ikvcrF7c0GgyxERadJqHcYzMzNJTk4mOTmZV1555bDttm/fTv/+/XnjjTfIy8vDNE1M06SwsJD33nuPvn37smrVqoaoXUREAsBqtdCxXzwAW1ZoASARkfqodRifP38+mZmZ5OXlMXHixMO2mzRpEnv27Kmai7x169YMHjyY8PBwTNMkPz+fK664Ao/HU//qRUQkIDoP9A9V2fbLXrxuDVURETlWtQ7j+6c1HDlyJLGxsTW2+eabb1i5ciWGYRATE8P3339PWloaS5YsITMzk+uuuw6AzZs389lnn9W/ehERCYjkTlGERjlxlXtIW5cb6HJERJqsWofx1atXYxgGY8aMOWybDz74oOrxs88+W23J++DgYF5//XV69eoFwFdffXUs9YqISCNgWAw6DWgBwJaVGqoiInKsah3Gs7L8P2x79+592Db7e88jIyO58sorD3ndMAyuv/56TNNk9erVdSxVREQaky77hqrsWJ2Dq1xDD0VEjkWtw3h2djYAcXFxNb6+fft2srKyMAyD008/HbvdXmO7vn37ArBnz5661ioiIo1IfJtwohND8Lp9bPslO9DliIg0SbUO4/svuHS5XDW+vmzZsqrH/fv3P+xxoqKiACgtLa3tqUVEpBEyDIOuQxIB2LQ0M8DViIg0TbUO4/t7xDdv3lzj60uWLKl6PGDAgMMep7i4GICgoKDanlpERBqpLoMSwYDdmwsoyi0PdDkiIk1OrcP4/rHiNc2CYpomX3/9tf+AFgvDhw8/7HHS0tIASEhIqFOhIiLS+ITHBNGySzQAm5fpQk4RkbqqdRi/8MILMU2Tr776infffbfaa08//TRpaWkYhsHo0aOJjIw87HH296B37dr1GEsWEZHGpNv+oSrLMqvWmBARkdqpdRi/6qqraNu2LQDXXXcdgwcP5qqrrqJfv3488MADVe2mTp162GOYpsmXX36JYRgMGTKkHmWLiEhj0aFvPDaHhYKsMrJSiwJdjohIk1LrMB4SEsLHH39ctZLmypUr+eijj1i9enVVT8j1119fbW7x3/ruu+/YvXs3AGeddVY9SxcRkcbAEWSjQ994QBdyiojUVa3DOMCgQYNISUnhsssuIzg4GNM0MU2Ttm3b8swzz/Df//73iPv/7W9/AyAxMVE94yIizUi3wUmAfwEgr9sX4GpERJoOW1136NixIx9//DE+n4+9e/ficDiIjo6u1b4//PCD/6S2Op9WREQasZbdogmNdFBa6CJtbW5VT7mIiBxZnXrGq+1osZCQkFDrIA4QGhpKaGgoTqfzWE8rIiKNkMVi0GWw/0LOjUszAlyNiEjTccxhXERE5GBd94XxtLW5lJfUvECciIhUpzAuIiINIrZlGPFtwvF5TbauzA50OSIiTYLCuIiINJj9veOblmlWFRGR2lAYFxGRBtN5YAKGxSBrRxH5maWBLkdEpNFTGBcRkQYTEuGg7SkxAGzUnOMiIkelMC4iIg2q6xD/nOMbl2Tg9WrOcRGRI1EYFxGRBtW+dxzB4XbKCl2krckNdDkiIo2awriIiDQoq81C92H+3vH1C/cEuBoRkcZNYVxERBpc9+HJAKSty6U4ryLA1YiINF4K4yIi0uCiWoTQsms0mLB+kXrHRUQOR2FcRESOi1NO9/eOb1iUgU8XcoqI1EhhXEREjosOveMJCrNTWlBJ+rq8QJcjItIoKYyLiMhxYbVb6DbEvyLnOl3IKSJSI4VxERE5bnqctu9CzjU5lOTrQk4Rkd9SGBcRkeMmOjGU5M5RmCZsWJwR6HJERBodhXERETmu9veOr1+0B5/PDHA1IiKNi8K4iIgcVx37xeMMsVGSV8nO9bqQU0TkYArjIiJyXNnsVroN8a/IuW7B7gBXIyLSuCiMi4jIcbd/qErqmlxKCyoDXI2ISOOhMC4iIsddTHIoSZ0iMX0ma39S77iIyH4K4yIickKcOrI1AGt/2o3H7Q1wNSIijYPCuIiInBAd+sQRFuOkosTN5uVZgS5HRKRRUBgXEZETwmK1cOqZ/t7xX3/ciWlqmkMREYVxERE5YXqcloTNaSV3dym7NuUHuhwRkYBTGBcRkRPGGWKn+1D/NIerf9gZ4GpERAJPYRxo164dhmFUu/3pT3+q1iY9PZ3zzz+f0NBQ4uLiuPPOO3G5XNXarFmzhhEjRhAcHEzLli157LHH9GdYEZHfOHVkKzAgbU0uBVllgS5HRCSgbIEuoLF47LHHuOmmm6qeh4WFVT32er2cd955xMfHs3DhQnJzc5k8eTKmafLiiy8CUFRUxJgxYxg5ciQrVqxg8+bNTJkyhdDQUO69994T/n5ERBqrqIQQ2vWKI/XXHFb/uJMRV3QNdEkiIgGjML5PeHg4iYmJNb42a9Ys1q9fz86dO0lO9i9c8eyzzzJlyhQef/xxIiIi+OCDD6ioqODtt9/G6XTSs2dPNm/ezHPPPcfUqVMxDONEvh0RkUat96hWpP6aw8YlGQy+oANBofZAlyQiEhAaprLPU089RWxsLH369OHxxx+vNgRlyZIl9OzZsyqIA5x99tlUVlaSkpJS1WbEiBE4nc5qbfbs2UNqamqN56ysrKSoqKjaTUTkZNCyazSxLcPwuHysX7Qn0OWIiASMwjhw11138dFHHzF37lzuuOMOXnjhBW677baq1zMzM0lISKi2T3R0NA6Hg8zMzMO22f98f5vfevLJJ4mMjKy6tW7duiHflohIo2UYBr1HtwJgzdxd+Ly+AFckIhIYzTaMP/LII4dclPnb28qVKwG45557GDFiBKeeeio33ngjr776Km+88Qa5ublVx6tpmIlpmtW2/7bN/os3DzdE5YEHHqCwsLDqtnOnZhYQkZNH54EJBIfbKcmvZNsvewNdjohIQDTbMeN33HEHkyZNOmKbdu3a1bh9yJAhAGzdupXY2FgSExNZtmxZtTb5+fm43e6q3u/ExMRDesCzs7MBDukx38/pdFYb1iIicjKx2a30PKMlK75NZfUPO+nUv4WurxGRk06zDeNxcXHExcUd076//PILAElJ/rlwhw4dyuOPP05GRkbVtlmzZuF0Ounfv39VmwcffBCXy4XD4ahqk5ycfNjQLyJysus5ohU/z0wna0cRuzfl06pbTKBLEhE5oZrtMJXaWrJkCc8//zyrVq1ix44dfPLJJ9xyyy1ccMEFtGnTBoCxY8fSo0cPrrnmGn755Rd++OEH7rvvPm666SYiIiIAuPLKK3E6nUyZMoW1a9fyxRdf8MQTT2gmFRGRIwiJcNDjNP/F8Su+TQ1sMSIiAXDSh3Gn08nHH3/MmWeeSY8ePXj44Ye56aab+N///lfVxmq18u233xIUFMTw4cOZOHEiEyZM4JlnnqlqExkZyezZs9m1axcDBgzgtttuY+rUqUydOjUQb0tEpMnod3YbLDaDPVsK2L05P9DliIicUIapJSIbjaKiIiIjIyksLKzqcRcRORnM/3ATa3/aTcuuUUy4p1+gyxEROayGzmsnfc+4iIgEXr9z2mKxGuzeVMCeLQWBLkdE5IRRGBcRkYALjwmi+zD/BfIrvt0R4GpERE4chXEREWkU+p3dFovFYNfGfDK2FQa6HBGRE0JhXEREGoWIuGC6DU0EYKV6x0XkJKEwLiIijUb/ce0wLAbp6/PI3KHecRFp/hTGRUSk0YiIC6brEH/v+IpvUgNbjIjICaAwLiIijcqAcW39vePrcslKLQp0OSIix5XCuIiINCqR8SF0HZQAwLLp29FyGCLSnCmMi4hIozPgvPZYbAY71+eRtjY30OWIiBw3CuMiItLoRMYHc+rI1gAs/mwrXq8vwBWJiBwfCuMiItIoDTi3HcHhdvIzy1j3055AlyMiclwojIuISKPkDLYx6PwOACz/ZjsVpe4AVyQi0vAUxkVEpNHqMTyJmORQKks9rPw2NdDliIg0OIVxERFptCxWC8Mv7QTAmnm7KMgqC3BFIiINS2FcREQatTY9YmnbKxafz2TRZ1sDXY6ISINSGBcRkUZv+CWdsFgMUn/NYefGvECXIyLSYBTGRUSk0YtODOWUES0BWDRtKz6fFgISkeZBYVxERJqEQee1xxliI3d3CWvm7Qp0OSIiDUJhXEREmoSgMDtDLvRPdbj0y20U7i0PcEUiIvWnMC4iIk3GKae3JLlzFB6Xj7nvb8DUcBURaeIUxkVEpMkwLAYjr+mGzW5h96YC1i3Uypwi0rQpjIuISJMS1SKEIRM6ArD4860U51UEuCIRkWOnMC4iIk1Or5GtSOwQibvCy9z3N2KaGq4iIk2TwriIiDQ5FovBqGu7YbVb2Lk+jw2LMwJdkojIMVEYFxGRJik6MZRB57cHYNGnWynJrwxwRSIidacwLiIiTVafs9rQol0ErnIP8z7QcBURaXoUxkVEpMmyWAxGX9sdi80gbW0uq+bsDHRJIiJ1ojAuIiJNWkxyKKdd2hmAJV9sY/fm/ABXJCJSewrjIiLS5PUc0ZIugxMwfSYzX19HaYHGj4tI06AwLiIiTZ5hGJx5VTdiW4ZSXuRi5mtr8Xp9gS5LROSoFMZFRKRZsDusnHNzLxxBVjK2FbL4s62BLklE5KgUxkVEpNmISghh9JQeAPz64y62rMgKcEUiIkemMC4iIs1Khz7x9Du7LQA/vr+RvD2lAa5IROTwFMZFRKTZGXxBe1p2jcZT6eWbf6/WBZ0i0mgpjIuISLNjsVo4+8ZTiIgPpji3gun/t4qKUnegyxIROYTCuIiINEvB4Q4uvKsPIZEO8vaU8s1Lq3FXegNdlohINQrjIiLSbEXEBXPBnX1whtjI2lHEjP+swevWlIci0ngojIuISLMW2zKM8Xf0xua0snN9HrPfWo/PZwa6LBERQGFcREROAokdIjn3ll5YrAbbfs5m/v82YZoK5CISeArjIiJyUmjdI4Yx15+CYcD6BXuY/+Em9ZCLSMApjIuIyEmjU/8WnHl1NzBg3YI9fP+fNXhcuqhTRAJHYVxERE4qPYYnc87NPbHaLOxYncP0f2naQxEJHIVxERE56XTs24IL7vLPspKxrZDPn06hOK8i0GWJyElIYVxERE5KyZ2juOjefoRGOcnPLOOzf6aQu7sk0GWJyElGYVxERE5asS3DuOQP/YlOCqW0oJLPn05ha0p2oMsSkZOIwriIiJzUwmOCuPi+fiR3jsJV4WXma2uZ98FGXdgpIieEwriIiJz0gkLtXHB3H/qf07ZqppVPn1pJfmZpoEsTkWZOYVxERASwWi0MmdCRC37fh+BwO7m7S/nkiRVsWJyhBYJE5LhRGBcRETlI6x4xXP7QIFp1i8bj8vHjuxuY+do6SgsqA12aiDRDCuMiIiK/ERrp5Pw7+zD4wg4YFoNtP2fzwV+XsmpOOl6vL9DliUgzYpj621ujUVRURGRkJIWFhURERAS6HBERAfamFzP/f5vI2lEEQGzLUM6Y1JXkzlGBLUxEAqKh85rCeCOiMC4i0jiZPpMNizNY8sW2qtU6uw5JZMiFHQiLDgpwdSJyIimMN2MK4yIijVtFiZslX25j/aI9YILFZtB9WDL9zm5DRGxwoMsTkRNAYbwZUxgXEWkaMncUsuTzbezZUgCAxWLQbWgi/c5pR2S8QrlIc6Yw3owpjIuINC27N+ez8rtUdm3MB8CwGHQZmEDPM1uS0C4CwzACXKGINDSF8WZMYVxEpGnK2FbIyu92kL4ur2pbbMswTjk9mS6DEnCG2ANYnYg0JIXxZkxhXESkacvaUcSa+bvYmpKN1+2fAtFmt9Cpfwu6DU0iqXMUFot6y0WaMoXxZkxhXESkeagodbN5eSbrFuwhb09p1fbgcDvtT42jQ78WtOoajdWm5T5EmhqF8WZMYVxEpHkxTZOsHUWsW7iHHav2UlnmqXrNEWSl3alxtDkllpZdogmLdgawUhGpLYXxZkxhXESk+fJ6fezZXMD2X/ayfdVeyopc1V6PSgihVddoWnaNpmWXKILDHQGqVESORGG8GVMYFxE5OZg+k8wdRexYvZfdm/LJTi+G3/w2jogLokXbCP+tXTjxbcJxBNkCU7CIVGnovNbsv6sff/xxvv32W1atWoXD4aCgoOCQNunp6dx+++38+OOPBAcHc+WVV/LMM8/gcBzolVizZg133HEHy5cvJyYmhltuuYW//OUv1aatmj9/PlOnTmXdunUkJyfzhz/8gVtvvfVEvE0REWlCDItBUsdIkjpGAv4x5nu2FLB7Uz67NuWTt6eUopwKinIq2JqSvW8niGoRQkxSKNFJ++9DiU4MwWa3BvDdiEh9NPsw7nK5uOyyyxg6dChvvPHGIa97vV7OO+884uPjWbhwIbm5uUyePBnTNHnxxRcB/yegMWPGMHLkSFasWMHmzZuZMmUKoaGh3HvvvQDs2LGDc889l5tuuon333+fRYsWcdtttxEfH88ll1xyQt+ziIg0LUGhdjr0iadDn3jAH873pheTnVZEdlox2alFlORXUpBVRkFWGaw6aGcDwqKdRMYFEx4XTGRcEBFxwUTEBRMa5SQk0oHVqgtFRRqrk2aYyttvv83dd999SM/4jBkzGD9+PDt37iQ5ORmAjz76iClTppCdnU1ERASvvPIKDzzwAFlZWTid/gts/vGPf/Diiy+ya9cuDMPgj3/8I9OnT2fDhg1Vx7711ltZvXo1S5YsqVWNGqYiIiKHU1bkInd3CXkZpeRllJKfUUrentJqF4XWyIDgcAdhUc6qcB4cZic43EFwuJ3gMAfB4Q6CQm04Q+zYHBYtViRyBBqm0sCWLFlCz549q4I4wNlnn01lZSUpKSmMHDmSJUuWMGLEiKogvr/NAw88QGpqKu3bt2fJkiWMHTu22rHPPvts3njjDdxuN3b7oQs+VFZWUllZWfW8qKjoOLxDERFpDkIiHIRExNC6e0zVNtM0KS92U5RTTuHecopy9t8qKMotp6zAhc9nUl7korzIxd704qOex2I1cIb4g7kzxIYj2IYjyIo9yH/vCLJhd1qxOazYnZZ99/7nNocFm92CzW7Favc/ttotWG0WLFZDIV+kBid9GM/MzCQhIaHatujoaBwOB5mZmVVt2rVrV63N/n0yMzNp3759jcdJSEjA4/GQk5NDUlLSIed+8sknefTRRxvw3YiIyMnEMIx9Id1BYofIQ143fSblJW5KCyopLaikpKCSsiIXFcUuyordVJS4KC9xU17sorLUg89n4vP6A355sbvB67XYDKw2S9XNYjWq7v03C1argWExDmyz7HtuMTD2Bfr9jy0GYDGwGP42huEfj1/12DiwjYOfG4BxoA0GGBgYlgP/rsC+5/52/u37/3PQ84O+FlR7XvXooH0P3lLDQQ63+eDj1niQo2uID0IN8lnqOH4eS+oYRUhE05uFqEmG8UceeeSoIXbFihUMGDCgVser6X9Q0zSrbf9tm/2je+ra5mAPPPAAU6dOrXpeVFRE69ata1WziIjI0RiWA2E9vk34Eduapom70ktlmWffzU1lmQd3hQdXhRfXvnt3uQdXpRePy4u70ofHtf+xF4/bh8ftw+v24XF78Xmqj4T1eUx8Hi9uvMfzbctJ6oK7+xASEXP0ho1Mkwzjd9xxB5MmTTpim9/2ZB9OYmIiy5Ytq7YtPz8ft9td1dOdmJhY1Uu+X3a2/+r2o7Wx2WzExsbWeG6n01lt6IuIiEigGIaBI8iGI8hGeAPlGdNn4vH4w7nPa+Ld99jr9eHzmP57r4nPs+91r4npNff10Puqeup9XhPTZ2Ka+x9z4LG5/zXw+Q48Nk0T9rUzTX8tJvu2mzXdg4n/2FB9u//BvtknzYNeZ//r+9/wvm1Vzw/ebFbf9tt9DnMFX83bD9e4Lsc4XNumeymhM7hJxtqmGcbj4uKIi4trkGMNHTqUxx9/nIyMjKqhJLNmzcLpdNK/f/+qNg8++CAul6tqusNZs2aRnJxcFfqHDh3K119/Xe3Ys2bNYsCAATWOFxcREWnuDIuB3WHF7tDUiyKH0+znOkpPT2fVqlWkp6fj9XpZtWoVq1atoqSkBICxY8fSo0cPrrnmGn755Rd++OEH7rvvPm666aaqK2SvvPJKnE4nU6ZMYe3atXzxxRc88cQTTJ06tWoIyq233kpaWhpTp05lw4YNvPnmm7zxxhvcd999AXvvIiIiItK4NfupDadMmcI777xzyPa5c+dy5plnAv7Afttttx2y6M/BQ0jWrFnD7bffzvLly4mOjubWW2/l4YcfPmTRn3vuuadq0Z8//vGPdVr0R1MbioiIiDRuDZ3Xmn0Yb0oUxkVEREQat4bOa81+mIqIiIiISGOlMC4iIiIiEiAK4yIiIiIiAaIwLiIiIiISIArjIiIiIiIBojAuIiIiIhIgCuMiIiIiIgGiMC4iIiIiEiAK4yIiIiIiAaIwLiIiIiISIArjIiIiIiIBojAuIiIiIhIgCuMiIiIiIgGiMC4iIiIiEiAK4yIiIiIiAaIwLiIiIiISIArjIiIiIiIBojAuIiIiIhIgCuMiIiIiIgGiMC4iIiIiEiAK4yIiIiIiAaIwLiIiIiISILZAFyAHmKYJQFFRUYArEREREZGa7M9p+3NbfSmMNyLFxcUAtG7dOsCViIiIiMiR5ObmEhkZWe/jGGZDxXqpN5/Px549ewgPD8cwjENeLyoqonXr1uzcuZOIiIgAVCgnir7WJwd9nU8O+jqfPPS1PjkUFhbSpk0b8vPziYqKqvfx1DPeiFgsFlq1anXUdhEREfomP0noa31y0Nf55KCv88lDX+uTg8XSMJde6gJOEREREZEAURgXEREREQkQhfEmxOl08te//hWn0xnoUuQ409f65KCv88lBX+eTh77WJ4eG/jrrAk4RERERkQBRz7iIiIiISIAojIuIiIiIBIjCuIiIiIhIgCiMi4iIiIgEiMJ4E/H4448zbNgwQkJCDrvaU3p6Oueffz6hoaHExcVx55134nK5Tmyh0uDatWuHYRjVbn/6058CXZbU08svv0z79u0JCgqif//+LFiwINAlSQN75JFHDvneTUxMDHRZUk8//fQT559/PsnJyRiGwZdfflntddM0eeSRR0hOTiY4OJgzzzyTdevWBaZYqZejfa2nTJlyyPf4kCFD6nwehfEmwuVycdlll/G73/2uxte9Xi/nnXcepaWlLFy4kI8++ojPPvuMe++99wRXKsfDY489RkZGRtXtoYceCnRJUg8ff/wxd999N3/+85/55ZdfOP300xk3bhzp6emBLk0a2CmnnFLte3fNmjWBLknqqbS0lN69e/PSSy/V+Po///lPnnvuOV566SVWrFhBYmIiY8aMobi4+ARXKvV1tK81wDnnnFPte/y7776r83ls9SlSTpxHH30UgLfffrvG12fNmsX69evZuXMnycnJADz77LNMmTKFxx9/XMvyNnHh4eHqUWtGnnvuOW644QZuvPFGAF544QVmzpzJK6+8wpNPPhng6qQh2Ww2fe82M+PGjWPcuHE1vmaaJi+88AJ//vOfufjiiwF45513SEhI4MMPP+SWW245kaVKPR3pa72f0+ms9/e4esabiSVLltCzZ8+qIA5w9tlnU1lZSUpKSgArk4bw1FNPERsbS58+fXj88cc1/KgJc7lcpKSkMHbs2Grbx44dy+LFiwNUlRwvW7ZsITk5mfbt2zNp0iS2b98e6JLkONqxYweZmZnVvr+dTicjRozQ93czNW/ePFq0aEGXLl246aabyM7OrvMx1DPeTGRmZpKQkFBtW3R0NA6Hg8zMzABVJQ3hrrvuol+/fkRHR7N8+XIeeOABduzYweuvvx7o0uQY5OTk4PV6D/l+TUhI0PdqMzN48GDeffddunTpQlZWFn//+98ZNmwY69atIzY2NtDlyXGw/3u4pu/vtLS0QJQkx9G4ceO47LLLaNu2LTt27OAvf/kLo0aNIiUlpU6rc6pnPIBqurjnt7eVK1fW+niGYRyyzTTNGrdLYNXla3/PPfcwYsQITj31VG688UZeffVV3njjDXJzcwP8LqQ+fvt9qe/V5mfcuHFccskl9OrVi7POOotvv/0W8A9bkOZN398nh8svv5zzzjuPnj17cv755zNjxgw2b95c9b1eW+oZD6A77riDSZMmHbFNu3btanWsxMREli1bVm1bfn4+brf7kE/oEnj1+drvv1J769at6l1rguLi4rBarYf0gmdnZ+t7tZkLDQ2lV69ebNmyJdClyHGyf+xwZmYmSUlJVdv1/X1ySEpKom3btnX+HlcYD6C4uDji4uIa5FhDhw7l8ccfJyMjo+oHwKxZs3A6nfTv379BziENpz5f+19++QWg2g96aTocDgf9+/dn9uzZXHTRRVXbZ8+ezYUXXhjAyuR4q6ysZMOGDZx++umBLkWOk/bt25OYmMjs2bPp27cv4L9OZP78+Tz11FMBrk6Ot9zcXHbu3Fnn388K401Eeno6eXl5pKen4/V6WbVqFQCdOnUiLCyMsWPH0qNHD6655hqefvpp8vLyuO+++7jppps0k0oTtmTJEpYuXcrIkSOJjIxkxYoV3HPPPVxwwQW0adMm0OXJMZo6dSrXXHMNAwYMYOjQofz3v/8lPT2dW2+9NdClSQO67777OP/882nTpg3Z2dn8/e9/p6ioiMmTJwe6NKmHkpIStm7dWvV8x44drFq1ipiYGNq0acPdd9/NE088QefOnencuTNPPPEEISEhXHnllQGsWo7Fkb7WMTExPPLII1xyySUkJSWRmprKgw8+SFxcXLWOlloxpUmYPHmyCRxymzt3blWbtLQ087zzzjODg4PNmJgY84477jArKioCV7TUW0pKijl48GAzMjLSDAoKMrt27Wr+9a9/NUtLSwNdmtTTv//9b7Nt27amw+Ew+/XrZ86fPz/QJUkDu/zyy82kpCTTbrebycnJ5sUXX2yuW7cu0GVJPc2dO7fG38eTJ082TdM0fT6f+de//tVMTEw0nU6necYZZ5hr1qwJbNFyTI70tS4rKzPHjh1rxsfHm3a73WzTpo05efJkMz09vc7nMUzTNOv7yUFEREREROpOs6mIiIiIiASIwriIiIiISIAojIuIiIiIBIjCuIiIiIhIgCiMi4iIiIgEiMK4iIiIiEiAKIyLiIiIiASIwriIiIiISIAojIuIiIiIBIjCuIjISeztt9/GMAwMwyA1NTXQ5dSK2+2ma9euGIbBxx9/fNh2pmkSERGBxWIhISGBiRMnkpaWdtTj33bbbRiGweTJkxuybBGRGimMi4hIk/Liiy+yefNmunfvzmWXXXbYdtu2baO4uBjTNMnOzmbatGmce+65Rz3+Aw88gMPh4L333mPFihUNWbqIyCEUxkVEpMkoKSnhySefBODhhx/GYjn8r7GkpCTWrFnD999/T/v27QFYv349KSkpRzxH69atmTx5MqZp8tBDDzVc8SIiNVAYFxGRJuOVV14hJyeH1q1bM3HixCO2DQ0NpWfPnpx99tn87W9/q9q+atWqo57n3nvvBWDWrFnqHReR40phXEREmgSv18tLL70EwBVXXHHEXvHfGjZsWNXjtWvXHrV9165d6devHwD/+te/6lipiEjtKYyLiEiTMHv2bNLT0wG4+uqr67Rvu3btCA8PB2oXxgGuuuoqAD777DMKCwvrdD4RkdpSGBcRkSNyuVy8/PLLjBw5kvj4eBwOB4mJiZx77rm8//77+Hy+ox4jJyeH+++/ny5duhAcHExCQgJjxozhiy++AGo3q8snn3wCQOfOnenVq1ed3oNhGHTu3BmofRi/5JJLAKioqOCrr76q0/lERGpLYVxERA4rLS2NPn36cPvttzNv3jxycnJwu91kZWUxY8YMrrnmGkaMGEFeXt5hj7F69Wp69OjBM888w5YtW6ioqCA7O5s5c+Zw8cUXc8stt9Sqlrlz5wIwZMiQOr+PlJSUqrHimZmZ5ObmHnWftm3bkpSUBMC8efPqfE4RkdpQGBcRkRqVlJQwatQoNmzYAMCECROYPn06K1euZNq0aYwYMQKAhQsXMn78eLxe7yHHyM/P55xzzmHv3r2Af+jHjBkzWLlyJR999BFDhw7lv//9L6+++uoRa9m1a1dVj/nAgQPr9D68Xi8333xztR78devW1Wrf/edasGBBnc4pIlJbCuMiIlKjRx99lO3btwPw0EMP8cUXX3D++efTv39/Lr30UubOnVs1rnrJkiX897//PeQYjzzyCJmZmQA888wzvP/++5xzzjn079+fyy+/nAULFnDhhReybNmyI9ayePHiqsd9+/at0/t48cUX+fnnn6ttq+1Qlf79+wOwdetWsrOz63ReEZHaUBgXEZFDVFZW8vrrrwPQo0cPHnnkkUPaGIbByy+/TGxsLEDVTCf7VVRU8M477wDQr18/pk6desgxrFYr//nPfwgKCjpiPbt27ap63KJFi1q/j127dvGXv/wFqPuMKr891+7du2t9XhGR2lIYFxGRQ6SkpFBQUADAlClTsFqtNbaLiIiomu97/fr1ZGRkVDvG/llIrr32WgzDqPEYCQkJnH322UesZ/8wF4Do6Ohav4/f//73lJSUEB4ezscff0xUVBRQ+zAeExNTYw0iIg1FYVxEpJHzeDxVM43U5/b222/X+pwHh9XBgwcfse3Brx+838GP9w/3OJwBAwYc8fWDLxCtbRifPn06X375JQBPPPEErVq1qpqFpbZh/OBz1eaiTxGRulIYFxGRQxwcfhMSEo7YNjExscb98vPzqx4fbWhJfHz8EV8/eBhLeXn5EdsClJaW8vvf/x7wf1i47bbbAKrCeH5+Pnv27DnqcQ4+V3Bw8FHbi4jUlS3QBYiIyJHZbLaqGU3qY/80fXV1uOEl+5mmeUzHrYuDw3peXl7VAj6H8/DDD5Oeno7dbue1116rWq3z4PnJ165dS3Jy8hGPc/CHi6N9YBARORYK4yIiTUC3bt1O6PkOHiudmZlJly5d/r+9+wtp6o3jOP75tcwgSnCjLswSJYL+6MWGKSKRRRgSQgSKZHkhiigo5J1giVB0oVdCQYSZ5UUyxkBIiSC8iLAYKKIoRmkXYQaRElNks4uxw9Z2Nn/hOj/4vV9XB85znvPs7uPj93wf07HLy8txn4ss8fj69WvCOZLVY0cG4e/fv+vo0aOmYycnJ40j7Nvb26MCeH5+vnE9PT2tixcvJnxv5O4+YRxAKlCmAgCIcerUKeM6WdvBiYmJuM+dPHnSuH7//n3COZLdjwzU8/PzpuOCwaAaGhoUCASUl5dndFKJt77t1I2H37Vv3z7l5uYmHQ8A/xZhHAAQw+l0Gp1HBgYG4h7oI0lra2vGMfUnTpyIKoVxuVzKyMiQJA0ODpqWsywvL2tsbCzhelwul1Gz/e7dO9Nx9+/fN/44ePDgQUyd94EDB4xd9e2E8fC7ioqKtHs3/0wGsPMI4wCAGOnp6aqvr5cUOq2yq6srZszW1pZaWlr07ds3SVJLS0vU/b179+r69euSJJ/Pp97e3pg5gsGgGhsbtb6+nnA9e/bsUWFhoaTonfhIX758UUdHh6RQK8ULFy7EHRfeZZ+ZmUlY776xsaGpqSlJUmlpacL1AcCfIowDAOLq7Ow0SjO6u7t15coVjYyMyOfzye12q6ysTE+ePJEkFRcXq6GhIWaO27dvG91W2tvbde3aNY2Njcnn8+n58+cqLS2V1+s1grZk/sFoRUWFpFAYX1tbi7nf2tqqHz9+yOFwqKenx/R3hevGf/78qY8fP5qOGx8f1+bmZtS7AWCnEcYBAHHt379fr169Mj4e9Xg8unz5spxOp65evarXr19LkkpKOtTf+gAAAn9JREFUSjQyMhL3YKDMzEyNjo4aHz8+e/ZM5eXlcjqdqqqq0ps3b1RXV6fGxkbjGbPTOGtqamSz2bS+vi6PxxN178WLFxoeHpYk9fT0yOFwmP6u3zuqmBkaGpIkHT9+PGkfdAD4U4RxAICpnJwcTU5Oqq+vT2fPnpXdbldaWpoOHTqk8vJyDQ4Oanx8PKqLyu8KCgo0MzOjmzdv6tixY0pPT5fD4dC5c+c0NDSk/v5+ra6uGuPDdea/y8rKUmVlpaRQqA/z+/1qbm6WJJ0/f94ojTGznTAeGfjDPcoBIBX+2fobDWIBAEigvr5ejx490uHDh/X582fTcW/fvlVxcbFsNpsWFhaUk5OTkvU8ffpUtbW1yszM1KdPn5L2NQeAP8XOOADAUn6/X16vV1Koa0kiRUVFunTpkgKBgO7evZuS9QSDQd25c0dSqM6dIA4glQjjAICU+vDhg2nXkkAgoKamJqMjy40bN5LOd+/ePdlsNvX392tpaWlH1ypJw8PDmp2dVXZ2ttra2nZ8fgCIRNNUAEBKdXd3a2JiQtXV1Tpz5owOHjwov9+vqakpPXz4UD6fT1Ko3ns7XUtOnz6tx48fa2FhQUtLSzpy5MiOrjcQCOjWrVsqKyuL6VMOADuNmnEAQErV1dVpYGAg4ZiSkhJ5vV7Z7fa/tCoA+G8gjAMAUmpubk5ut1svX77U4uKiVlZWtLm5KbvdLpfLpaqqKlVXV2vXLionAfz/EMYBAAAAi7ANAQAAAFiEMA4AAABYhDAOAAAAWIQwDgAAAFiEMA4AAABYhDAOAAAAWIQwDgAAAFiEMA4AAABYhDAOAAAAWIQwDgAAAFiEMA4AAABY5BcVv8Bv58dSOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_fig, ax = plt.subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# save figure\n",
    "plt.savefig('ridge_coef.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba1608",
   "metadata": {},
   "source": [
    "## Cross-Validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baaa9c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.280e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.284e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.962e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.648e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.280e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.284e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.962e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.648e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.280e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.284e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.962e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.648e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.280e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.284e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.962e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.647e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.252e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.279e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.283e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.647e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.252e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.279e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.283e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.961e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.647e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.251e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.278e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.282e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.960e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.646e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.251e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.277e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.281e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.959e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.645e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.250e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.276e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.280e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.958e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.644e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.249e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.275e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.279e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.957e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.643e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.248e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.274e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.277e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.955e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.642e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.246e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.272e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.275e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.954e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.640e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.244e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.269e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.273e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.951e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.638e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.241e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.266e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.269e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.948e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.635e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.238e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.262e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.265e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.944e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.631e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.234e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.257e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.939e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.626e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.229e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.251e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.253e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.933e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.621e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.223e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.243e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.245e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.925e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.613e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.215e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.233e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.235e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.916e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.604e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.205e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.221e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.222e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.904e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.593e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.193e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.206e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.206e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.889e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.579e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.177e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.187e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.186e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.870e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.561e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.158e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.164e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.162e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.848e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.540e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.134e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.135e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.132e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.820e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.514e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.106e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.100e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.096e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.786e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.481e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.070e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.059e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.052e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.745e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.028e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.009e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.999e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.696e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.396e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.977e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.949e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.937e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.638e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.341e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.917e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.881e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.865e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.570e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.277e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.847e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.802e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.783e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.493e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.204e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.767e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.713e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.690e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.406e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.121e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.677e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.615e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.588e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.311e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.030e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.578e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.511e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.478e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.208e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.932e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.472e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.400e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.363e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.101e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.829e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.360e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.288e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.246e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.991e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.245e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.175e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.129e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.617e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.131e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.066e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.015e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.775e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.514e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.020e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.961e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.907e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.674e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.915e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.865e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.807e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.581e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.816e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.776e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.716e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.240e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.727e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.697e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.420e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.627e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.563e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.353e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.575e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.565e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.501e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.513e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.511e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.447e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.244e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.459e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.400e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.944e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.164e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.327e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.846e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.823e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.256e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.293e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.736e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.191e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.995e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.729e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.221e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.724e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.217e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.177e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.987e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.720e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+11, tolerance: 1.272e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528048475261.1476, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528028712201.3733, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528003779853.7872, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527972328039.55585, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527932655056.85516, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527882616831.1157, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527819512981.9874, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527739944177.0779, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527639633910.0223, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527513206424.5934, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527353910938.11755, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527153280678.2027, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526900713704.603, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526582961350.8442, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526183509915.6277, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525681842822.88916, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525052575198.462, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524264462701.0988, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523279304385.8978, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522050789253.4084, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520523382711.05853, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518631417403.5329, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 516298646118.0012, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513438630856.7906, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509956468325.56885, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505752454308.95483, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500728304455.72943, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494796381283.3492, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 487891915369.5767, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479987377783.83386, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 471107016026.51575, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461338397088.52405, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 450837160573.1199, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439821717136.91034, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 417326936099.3899, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 406407524612.5433, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 396035711204.03156, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 386391292764.2608, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 377588098700.65955, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 369676463825.0127, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362653792964.12823, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 356479216265.63873, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 351088600855.63184, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 346407288032.43677, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 342359261871.9338, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 338872611254.71686, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335881927139.9824, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333328639476.0495, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 331160300375.7219, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329329583248.9241, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327793440420.2401, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326512572078.0876, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325451174874.47906, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324576862390.36115, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323860647324.16943, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323276906376.4171, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322803285794.5584, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322420535620.9771, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322112280500.87134, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321864745283.44434, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321666456792.3605, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321507941514.9297, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321381434805.5795, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321280612257.72455, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321200349295.871, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321136511341.2756, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321085774261.93506, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321045473139.6256, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321013476475.1238, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320988082590.7134, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320967934986.65875, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320951953618.0829, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320939279376.3065, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320929229415.40283, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320921261318.5991, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320914944427.2076, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320909936945.90155, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320905967689.3468, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320902821547.4742, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320900327923.1949, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320898351542.0104, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320896785151.3644, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320895543724.1404, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320894559858.11847, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320893780125.83386, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320893162179.1743, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320892672453.09174, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320892284344.8071, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891976770.1332, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891733019.01526, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891539848.2595, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891386762.37573, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891265443.57776, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891169299.97174, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891093107.4647, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891032725.99866, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890984874.5831, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890946953.0737, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890916900.8732, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528448074352.54614, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528427023256.1906, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528400466065.59424, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528366964687.70917, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528324706651.54724, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528271408371.6509, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528204193844.44086, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528119442792.71893, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528012600967.84644, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527877943822.0877, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527708283110.30475, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527494604261.1042, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527225620756.6161, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526887230612.8238, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526461859931.5777, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525927680333.1277, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525257692328.4793, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524418677530.03534, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523370042128.7293, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522062606437.23, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520437445565.906, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518424959647.8668, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 515944451803.10645, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512904615742.24835, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509205467494.5067, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504742360129.68207, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 499412727894.03046, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493126014503.15674, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 485816732477.45984, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 477459702431.0634, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 468085302972.58325, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457791345455.77686, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 446747567214.08594, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435189385358.10583, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 423399794749.2046, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 411681660508.6588, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 400325849048.0717, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 389582146651.75653, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 379638947727.57336, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 370614745393.61316, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362560947768.7969, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 355472911563.22363, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 349305034756.4879, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 343986135882.7355, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 339432547580.5959, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335557722893.6556, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 332278275616.3448, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329517082133.2898, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327204364078.7225, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325277641555.0235, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323681226790.79376, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322365650129.9352, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321287174041.3808, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320407403965.502, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319692945566.3684, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319115056836.1172, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318649267087.4754, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318274959992.5095, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317974934182.2276, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317734961259.1421, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317543360222.10895, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317390602770.00543, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317268958412.0658, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317172183309.6808, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317095252950.44354, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317034136200.00104, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316985606809.7544, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316947087808.3547, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316916524109.341, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316892278911.45886, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316873049894.24744, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316857801712.4983, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316845711802.20276, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316836126989.19214, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316828528820.82166, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316822505913.719, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316817731927.2277, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316813948036.62396, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316810948998.599, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316808572080.1489, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316806688267.1168, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316805195285.9068, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316804012066.2944, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316803074348.8467, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316802331201.10596, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316801742254.9727, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316801275516.2793, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800905628.2238, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800612494.7311, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800380189.23584, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800196089.70905, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800050193.1062, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799934571.9959, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799842943.897, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799770329.9782, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799712784.55383, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799667180.7024, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799631040.3784, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799602399.75616, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799579702.53064, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496224986600.4098, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496205483430.9915, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496180879003.4755, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496149840938.634, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496110689986.6315, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496061310390.4424, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495999037480.6409, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495920516952.20795, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495821529061.52606, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495696769590.49835, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495539577886.29675, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495341600682.1602, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495092378905.4019, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494778843591.30176, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493889735660.7309, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493268900421.383, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 492491401369.00775, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491519592349.6038, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490307852466.6972, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 488801501847.7192, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 486935925685.1531, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484636163024.79175, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 481817331611.39325, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 478386383823.75006, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 474245787459.3388, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469299735751.27167, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463463318789.9132, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456674624265.47943, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 448908907999.72864, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440192844084.81586, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 430615723679.48834, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 420333869024.571, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 409565095646.47437, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 398572108690.56445, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 387636845872.0059, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 377030784428.063, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 366987691583.3198, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 357684456234.36755, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 349232917263.3622, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 341682315799.8093, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335029500154.01135, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329232987932.6658, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324227325738.5381, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319935310083.5559, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316276934726.40247, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 313175002174.43695, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 310558007527.37164, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 308361167670.9948, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 306526422077.2857, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 305002010165.59644, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 303741968330.2816, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 302705680357.5455, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301857493473.99133, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301166367399.2714, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 300605523846.75024, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 300152079950.3474, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299786664561.76294, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299493025795.1905, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299257641896.0869, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299069347401.27576, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298918984384.26514, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298799085551.5159, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298703592851.4171, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298627612546.65295, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298567205612.6084, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298519210909.2883, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298481097764.13794, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298450844272.167, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298426837629.19275, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298407793039.61993, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298392688084.16736, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298380709825.3931, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298371212324.08496, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298363682610.52576, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298357713487.4239, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298352981830.81036, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298349231301.4792, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298346258585.6534, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298343902454.12134, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298342035068.4843, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298340555076.8617, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298339382133.15106, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298338452547.7907, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298337715837.4362, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298337131988.3244, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336669286.10706, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336302595.1166, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336011994.1294, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335781694.89966, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335599184.86536, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335454547.6318, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335339924.37463, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335249086.95166, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335177099.56274, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335120050.60815, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335074840.17255, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335039011.6082, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335010618.03815, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298334988116.5913, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464810599896.45087, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464793248944.3599, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464771359282.1457, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464743745231.46216, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464708912299.84937, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464664977298.2786, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464609568120.04706, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464539698195.30743, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464451609528.50726, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464340576943.7874, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464200664725.12036, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464024425303.44995, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463802528141.80493, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463523305745.13684, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462731118530.4985, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462177624567.70447, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461484070627.82294, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 460616576597.31976, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 459533956330.6653, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 458186645907.33246, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456515769959.38416, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 454452559853.37103, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451918441172.80896, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 448826226105.8657, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 445082953945.3695, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435275733091.1002, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 429056514965.6001, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 421899449009.2604, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 413811417944.61755, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 404856105040.9058, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 395160735594.5588, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 384914151700.114, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 374354369600.08203, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 363746523291.4193, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 353355170277.946, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 343416943320.09064, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 334119451052.6597, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325590184068.5088, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317896029116.958, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 311051219264.0425, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 305030072647.7236, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299780828765.7506, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 295237831707.45074, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 291330618478.0862, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 287989659762.0391, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 285149306799.46454, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 282748856213.29626, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 280732627601.9095, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 279049706436.2962, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 277653700527.2541, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 276502612203.1458, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 275558789940.3094, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 274788881594.73468, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 274163727141.08127, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273658163603.8505, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273250745449.65015, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272923401877.61023, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272661058881.1206, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272451252538.23804, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272283754610.40863, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272150224978.3077, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272043899344.33215, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271959315727.6475, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271892079752.48126, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271838666460.20462, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271796255107.8321, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271762592874.1055, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271735883335.48764, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271714695806.80237, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271697892027.12494, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271684567118.49487, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271674002195.96768, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271665626429.67087, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271658986737.10428, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271653723611.2165, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271649551867.52618, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271646245325.2189, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271643624628.56247, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271641547571.22223, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271639901413.12088, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271638596782.02603, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271637562834.60263, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271636743417.80136, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271636094024.43405, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271635579379.00717, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271635171523.58615, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634848300.29248, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634592148.3221, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634389150.34995, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634228276.72906, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634100786.3789, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633999751.98294, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633919683.6565, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633856230.68973, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633805945.17883, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633766094.70123, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633734513.83478, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633709486.50262, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525301147711.6022, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525281103704.23224, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525255816918.97766, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525223917979.2684, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525183680985.7675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525132931373.5039, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525068930356.60675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524988230250.0947, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524886493707.8172, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524758268479.14954, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524596707695.49316, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524393224033.7979, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524137064538.2692, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523814791725.0607, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523409656382.71454, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522900849085.8078, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522262622225.7143, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 521463284378.8002, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520464087008.6203, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 519218053774.1407, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 517668849923.09174, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 515749858422.83264, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513383724044.3558, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 510482744685.9554, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 506950617287.7621, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502686149632.5061, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497589565085.6275, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491571857999.71405, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484567190093.43774, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 476547476356.84705, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 467537148423.8904, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457624897377.5301, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 446968545937.3337, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435789736487.481, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 424357193694.17426, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 412960531183.66077, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 401879706287.51886, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 391356808311.0815, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 381576070446.9056, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 372655219803.94714, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 364647866256.41864, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 357553995189.92377, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 351334510979.3379, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 345926085410.1604, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 341253718481.08026, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 337239788704.96814, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333809526177.61835, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 330893588419.35187, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 328428730675.2572, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326357518668.32117, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324627772068.48505, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323192104831.1432, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322007663582.5347, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321036012065.2327, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320243061470.459, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319598963566.3247, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319077924137.76794, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318657931428.97186, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318320417550.4367, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318049880283.31525, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317833492658.004, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317660722649.13605, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317522978640.6069, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317413289921.7065, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317326026279.58093, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317256656993.96765, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317201547103.4444, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317157787435.8938, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317123054289.6646, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317095494558.7486, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317073632313.3988, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317056293231.3007, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317042543726.2156, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317031642080.089, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317022999316.27167, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317016147938.5145, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317010716996.50073, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317006412224.1675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317003000235.5371, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317000295959.7698, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316998152658.17615, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316996453996.834, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316995107754.17334, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316994040828.00507, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316993195274.67456, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316992525167.6508, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991994106.4376, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991573241.433, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991239708.0435, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990975385.35114, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990765912.1365, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990599906.94055, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990468349.87195, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990364092.65015, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990281470.27295, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990215993.257, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990164103.718, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990122982.0697, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990090393.818, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990064568.1554, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+11, tolerance: 1.272e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.129e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.881e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.617e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.131e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.066e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.015e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.775e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.514e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.020e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.961e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.907e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.674e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.915e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.865e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.807e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.581e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.324e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.816e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.776e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.716e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.496e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.240e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.727e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.697e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.635e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.420e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.165e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.647e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.627e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.563e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.353e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.575e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.565e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.501e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.295e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.513e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.511e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.447e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.244e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.459e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.400e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.201e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.944e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.164e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.327e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.846e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.823e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.256e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.293e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.736e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.191e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.995e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.729e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.221e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.724e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.217e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.177e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.987e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.720e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+11, tolerance: 1.272e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528048475261.1476, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528028712201.3733, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528003779853.7872, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527972328039.55585, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527932655056.85516, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527882616831.1157, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527819512981.9874, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527739944177.0779, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527639633910.0223, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527513206424.5934, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527353910938.11755, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527153280678.2027, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526900713704.603, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526582961350.8442, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526183509915.6277, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525681842822.88916, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525052575198.462, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524264462701.0988, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523279304385.8978, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522050789253.4084, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520523382711.05853, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518631417403.5329, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 516298646118.0012, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513438630856.7906, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509956468325.56885, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505752454308.95483, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500728304455.72943, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494796381283.3492, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 487891915369.5767, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479987377783.83386, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 471107016026.51575, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461338397088.52405, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 450837160573.1199, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439821717136.91034, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 417326936099.3899, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 406407524612.5433, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 396035711204.03156, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 386391292764.2608, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 377588098700.65955, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 369676463825.0127, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362653792964.12823, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 356479216265.63873, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 351088600855.63184, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 346407288032.43677, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 342359261871.9338, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 338872611254.71686, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335881927139.9824, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333328639476.0495, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 331160300375.7219, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329329583248.9241, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327793440420.2401, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326512572078.0876, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325451174874.47906, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324576862390.36115, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323860647324.16943, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323276906376.4171, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322803285794.5584, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322420535620.9771, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322112280500.87134, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321864745283.44434, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321666456792.3605, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321507941514.9297, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321381434805.5795, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321280612257.72455, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321200349295.871, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321136511341.2756, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321085774261.93506, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321045473139.6256, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321013476475.1238, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320988082590.7134, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320967934986.65875, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320951953618.0829, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320939279376.3065, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320929229415.40283, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320921261318.5991, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320914944427.2076, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320909936945.90155, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320905967689.3468, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320902821547.4742, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320900327923.1949, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320898351542.0104, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320896785151.3644, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320895543724.1404, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320894559858.11847, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320893780125.83386, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320893162179.1743, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320892672453.09174, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320892284344.8071, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891976770.1332, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891733019.01526, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891539848.2595, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891386762.37573, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891265443.57776, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891169299.97174, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891093107.4647, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891032725.99866, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890984874.5831, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890946953.0737, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890916900.8732, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528448074352.54614, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528427023256.1906, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528400466065.59424, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528366964687.70917, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528324706651.54724, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528271408371.6509, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528204193844.44086, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528119442792.71893, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528012600967.84644, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527877943822.0877, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527708283110.30475, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527494604261.1042, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527225620756.6161, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526887230612.8238, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526461859931.5777, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525927680333.1277, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525257692328.4793, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524418677530.03534, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523370042128.7293, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522062606437.23, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520437445565.906, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518424959647.8668, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 515944451803.10645, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512904615742.24835, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509205467494.5067, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504742360129.68207, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 499412727894.03046, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493126014503.15674, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 485816732477.45984, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 477459702431.0634, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 468085302972.58325, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457791345455.77686, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 446747567214.08594, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435189385358.10583, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 423399794749.2046, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 411681660508.6588, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 400325849048.0717, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 389582146651.75653, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 379638947727.57336, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 370614745393.61316, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362560947768.7969, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 355472911563.22363, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 349305034756.4879, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 343986135882.7355, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 339432547580.5959, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335557722893.6556, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 332278275616.3448, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329517082133.2898, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327204364078.7225, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325277641555.0235, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323681226790.79376, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322365650129.9352, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321287174041.3808, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320407403965.502, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319692945566.3684, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319115056836.1172, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318649267087.4754, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318274959992.5095, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317974934182.2276, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317734961259.1421, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317543360222.10895, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317390602770.00543, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317268958412.0658, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317172183309.6808, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317095252950.44354, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317034136200.00104, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316985606809.7544, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316947087808.3547, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316916524109.341, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316892278911.45886, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316873049894.24744, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316857801712.4983, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316845711802.20276, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316836126989.19214, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316828528820.82166, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316822505913.719, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316817731927.2277, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316813948036.62396, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316810948998.599, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316808572080.1489, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316806688267.1168, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316805195285.9068, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316804012066.2944, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316803074348.8467, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316802331201.10596, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316801742254.9727, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316801275516.2793, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800905628.2238, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800612494.7311, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800380189.23584, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800196089.70905, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800050193.1062, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799934571.9959, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799842943.897, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799770329.9782, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799712784.55383, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799667180.7024, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799631040.3784, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799602399.75616, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799579702.53064, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496224986600.4098, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496205483430.9915, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496180879003.4755, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496149840938.634, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496110689986.6315, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496061310390.4424, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495999037480.6409, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495920516952.20795, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495821529061.52606, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495696769590.49835, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495539577886.29675, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495341600682.1602, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495092378905.4019, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494778843591.30176, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493889735660.7309, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493268900421.383, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 492491401369.00775, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491519592349.6038, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490307852466.6972, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 488801501847.7192, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 486935925685.1531, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484636163024.79175, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 481817331611.39325, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 478386383823.75006, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 474245787459.3388, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469299735751.27167, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463463318789.9132, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456674624265.47943, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 448908907999.72864, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440192844084.81586, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 430615723679.48834, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 420333869024.571, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 409565095646.47437, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 398572108690.56445, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 387636845872.0059, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 377030784428.063, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 366987691583.3198, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 357684456234.36755, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 349232917263.3622, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 341682315799.8093, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335029500154.01135, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329232987932.6658, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324227325738.5381, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319935310083.5559, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316276934726.40247, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 313175002174.43695, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 310558007527.37164, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 308361167670.9948, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 306526422077.2857, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 305002010165.59644, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 303741968330.2816, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 302705680357.5455, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301857493473.99133, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301166367399.2714, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 300605523846.75024, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 300152079950.3474, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299786664561.76294, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299493025795.1905, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299257641896.0869, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299069347401.27576, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298918984384.26514, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298799085551.5159, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298703592851.4171, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298627612546.65295, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298567205612.6084, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298519210909.2883, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298481097764.13794, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298450844272.167, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298426837629.19275, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298407793039.61993, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298392688084.16736, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298380709825.3931, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298371212324.08496, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298363682610.52576, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298357713487.4239, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298352981830.81036, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298349231301.4792, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298346258585.6534, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298343902454.12134, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298342035068.4843, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298340555076.8617, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298339382133.15106, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298338452547.7907, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298337715837.4362, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298337131988.3244, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336669286.10706, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336302595.1166, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336011994.1294, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335781694.89966, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335599184.86536, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335454547.6318, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335339924.37463, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335249086.95166, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335177099.56274, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335120050.60815, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335074840.17255, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335039011.6082, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335010618.03815, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298334988116.5913, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464810599896.45087, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464793248944.3599, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464771359282.1457, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464743745231.46216, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464708912299.84937, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464664977298.2786, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464609568120.04706, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464539698195.30743, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464451609528.50726, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464340576943.7874, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464200664725.12036, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464024425303.44995, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463802528141.80493, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463523305745.13684, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462731118530.4985, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462177624567.70447, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461484070627.82294, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 460616576597.31976, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 459533956330.6653, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 458186645907.33246, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456515769959.38416, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 454452559853.37103, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451918441172.80896, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 448826226105.8657, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 445082953945.3695, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435275733091.1002, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 429056514965.6001, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 421899449009.2604, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 413811417944.61755, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 404856105040.9058, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 395160735594.5588, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 384914151700.114, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 374354369600.08203, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 363746523291.4193, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 353355170277.946, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 343416943320.09064, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 334119451052.6597, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325590184068.5088, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317896029116.958, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 311051219264.0425, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 305030072647.7236, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299780828765.7506, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 295237831707.45074, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 291330618478.0862, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 287989659762.0391, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 285149306799.46454, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 282748856213.29626, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 280732627601.9095, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 279049706436.2962, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 277653700527.2541, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 276502612203.1458, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 275558789940.3094, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 274788881594.73468, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 274163727141.08127, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273658163603.8505, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273250745449.65015, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272923401877.61023, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272661058881.1206, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272451252538.23804, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272283754610.40863, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272150224978.3077, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272043899344.33215, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271959315727.6475, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271892079752.48126, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271838666460.20462, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271796255107.8321, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271762592874.1055, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271735883335.48764, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271714695806.80237, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271697892027.12494, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271684567118.49487, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271674002195.96768, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271665626429.67087, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271658986737.10428, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271653723611.2165, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271649551867.52618, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271646245325.2189, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271643624628.56247, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271641547571.22223, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271639901413.12088, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271638596782.02603, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271637562834.60263, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271636743417.80136, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271636094024.43405, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271635579379.00717, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271635171523.58615, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634848300.29248, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634592148.3221, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634389150.34995, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634228276.72906, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634100786.3789, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633999751.98294, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633919683.6565, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633856230.68973, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633805945.17883, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633766094.70123, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633734513.83478, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633709486.50262, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525301147711.6022, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525281103704.23224, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525255816918.97766, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525223917979.2684, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525183680985.7675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525132931373.5039, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525068930356.60675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524988230250.0947, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524886493707.8172, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524758268479.14954, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524596707695.49316, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524393224033.7979, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524137064538.2692, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523814791725.0607, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523409656382.71454, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522900849085.8078, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522262622225.7143, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 521463284378.8002, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520464087008.6203, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 519218053774.1407, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 517668849923.09174, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 515749858422.83264, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513383724044.3558, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 510482744685.9554, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 506950617287.7621, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502686149632.5061, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497589565085.6275, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491571857999.71405, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484567190093.43774, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 476547476356.84705, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 467537148423.8904, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457624897377.5301, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 446968545937.3337, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435789736487.481, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 424357193694.17426, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 412960531183.66077, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 401879706287.51886, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 391356808311.0815, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 381576070446.9056, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 372655219803.94714, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 364647866256.41864, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 357553995189.92377, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 351334510979.3379, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 345926085410.1604, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 341253718481.08026, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 337239788704.96814, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333809526177.61835, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 330893588419.35187, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 328428730675.2572, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326357518668.32117, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324627772068.48505, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323192104831.1432, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322007663582.5347, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321036012065.2327, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320243061470.459, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319598963566.3247, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319077924137.76794, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318657931428.97186, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318320417550.4367, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318049880283.31525, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317833492658.004, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317660722649.13605, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317522978640.6069, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317413289921.7065, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317326026279.58093, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317256656993.96765, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317201547103.4444, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317157787435.8938, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317123054289.6646, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317095494558.7486, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317073632313.3988, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317056293231.3007, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317042543726.2156, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317031642080.089, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317022999316.27167, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317016147938.5145, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317010716996.50073, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317006412224.1675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317003000235.5371, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317000295959.7698, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316998152658.17615, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316996453996.834, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316995107754.17334, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316994040828.00507, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316993195274.67456, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316992525167.6508, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991994106.4376, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991573241.433, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991239708.0435, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990975385.35114, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990765912.1365, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990599906.94055, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990468349.87195, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990364092.65015, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990281470.27295, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990215993.257, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990164103.718, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990122982.0697, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990090393.818, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990064568.1554, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+11, tolerance: 1.272e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Using K-fold CV w/ K=5\n",
    "K = 5\n",
    "kfold = skm.KFold(K,\n",
    "                  random_state=0,\n",
    "                  shuffle=True)\n",
    "\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "ridge = lm.ElasticNet(alpha=lambdas[59], l1_ratio=0)\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {'ridge__alpha': lambdas}\n",
    "\n",
    "grid = skm.GridSearchCV(pipe,\n",
    "                        param_grid,\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error') \n",
    "grid.fit(X, Y)\n",
    "grid.best_params_['ridge__alpha']\n",
    "grid.best_estimator_\n",
    "\n",
    "ridgeCV = lm.ElasticNetCV(alphas=lambdas,\n",
    "                           l1_ratio=0,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('ridge', ridgeCV)])\n",
    "pipeCV.fit(X, Y)\n",
    "\n",
    "tuned_ridge = pipeCV.named_steps['ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75bc1025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results\n",
    "ridge_fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(lambdas),\n",
    "            -grid.cv_results_['mean_test_score'],\n",
    "            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\n",
    "#ax.set_ylim([80000000,220000000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('ridge_cv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490865a",
   "metadata": {},
   "source": [
    "# LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b2be136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 6.15456520763878$"
      ],
      "text/plain": [
       "6.154565207638776"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoCV = lm.ElasticNetCV(n_alphas=100,\n",
    "                           l1_ratio=1,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('lasso', lassoCV)])\n",
    "pipeCV.fit(X, Y)\n",
    "tuned_lasso = pipeCV.named_steps['lasso']\n",
    "tuned_lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47176817",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, soln_array = lm.Lasso.path(Xs,\n",
    "                                    Y,\n",
    "                                    l1_ratio=1,\n",
    "                                    n_alphas=100)[:2]\n",
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2afef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_fig, ax = plt.subplots(figsize=(10,5))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficiients', fontsize=20)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('lasso_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5180e80",
   "metadata": {},
   "source": [
    "## Cross-Validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a1f1fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAALICAYAAACevi28AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8IklEQVR4nO3dd3hUZfrG8fvMpAcSCIHQAgFFpCgiIE0UEEGMLFhAV6WJuorouii78tMV0N21dwV0FSk2REVUEMSVaqOjFEEgIZRQQk+dzOT8/ggZEpNAkpnkzGS+n+vKdZ28c8qTbFbuvHnmfQ3TNE0BAAAAAcRmdQEAAABAVSMEAwAAIOAQggEAABBwCMEAAAAIOIRgAAAABBxCMAAAAAIOIRgAAAABhxAMAACAgEMIBgAAQMAhBAMAACDgEIIrYPny5RowYIAaNmwowzD0+eefl/seixYtUpcuXVSzZk3VrVtXN954o5KSkrxfLAAAAIohBFdARkaG2rVrp9dff71C1+/atUsDBw5U7969tWHDBi1atEhpaWm64YYbvFwpAAAASmKYpmlaXYQ/MwxDc+fO1aBBg9xjDodDjz32mN5//30dP35cbdu21TPPPKOePXtKkj755BP9+c9/Vk5Ojmy2/N9DvvzySw0cOFA5OTkKDg624CsBAAAIHMwEV4KRI0fq+++/10cffaRffvlFgwcP1jXXXKPff/9dktSxY0fZ7Xa9++67crlcOnHihGbNmqW+ffsSgAEAAKoAM8Ee+uNM8M6dO9WiRQvt3btXDRs2dJ/Xp08fXXbZZfrPf/4jKb+vePDgwTpy5IhcLpe6du2qBQsWqFatWhZ8FQAAAIGFmWAvW7dunUzT1AUXXKAaNWq4P5YtW6adO3dKkg4cOKA777xTw4cP1+rVq7Vs2TKFhITopptuEr+TAAAAVL4gqwuobvLy8mS327V27VrZ7fYir9WoUUOS9MYbbygqKkrPPvus+7X33ntP8fHx+vnnn9WlS5cqrRkAACDQEIK9rH379nK5XDp06JB69OhR4jmZmZnFAnLB53l5eZVeIwAAQKCjHaIC0tPTtWHDBm3YsEGSlJSUpA0bNiglJUUXXHCBbrvtNg0bNkyfffaZkpKStHr1aj3zzDNasGCBJCkxMVGrV6/WE088od9//13r1q3TyJEj1bRpU7Vv397CrwwAACAw8Ma4Cli6dKl69epVbHz48OGaPn26cnNz9a9//UszZ87Uvn37VKdOHXXt2lWTJk3SRRddJEn66KOP9Oyzz2r79u2KiIhQ165d9cwzz+jCCy+s6i8HAAAg4BCCAQAAEHBohwAAAEDAIQQDAAAg4LA6RBnl5eVp//79qlmzpgzDsLocAAAA/IFpmjp16pQaNmwom+3sc72E4DLav3+/4uPjrS4DAAAA57Bnzx41btz4rOcQgsuoZs2akvK/qVFRURZXAwAA4NsyMjLUsGFDSfmTiZGRkZX+zJMnTyo+Pt6d286GEFxGBS0QUVFRhGAAAIBzKLwxWFRUVJWE4AJlaV31yzfGLV++XAMGDFDDhg1lGIY+//zzs57/2Wef6eqrr1bdunUVFRWlrl27atGiRVVTLAAAAHyOX4bgjIwMtWvXTq+//nqZzl++fLmuvvpqLViwQGvXrlWvXr00YMAArV+/vpIrBQAAgC/y+80yDMPQ3LlzNWjQoHJd16ZNG9188816/PHHy3T+yZMnFR0drRMnTtAOAQAAcA4ZGRmqUaOGJCk9Pb3KeoLLmtcCsic4Ly9Pp06dUkxMTKnn5OTkKCcnx/35yZMnq6I0AAAAVAG/bIfw1AsvvKCMjAwNGTKk1HOeeuopRUdHuz9YHg0AAKD6CLgQ/OGHH2rixImaPXu26tWrV+p548eP14kTJ9wfe/bsqcIqAQAA/Jvdbte1116ra6+9tshKEb4ioNohZs+erVGjRmnOnDnq06fPWc8NDQ1VaGhoFVUGAABQvYSFhWn+/PlWl1GqgJkJ/vDDDzVixAh98MEHSkxMtLocAAAAWMgvZ4LT09O1Y8cO9+dJSUnasGGDYmJi1KRJE40fP1779u3TzJkzJeUH4GHDhumVV15Rly5ddODAAUlSeHi4oqOjLfkaAAAAYB2/nAles2aN2rdvr/bt20uSxo4dq/bt27uXO0tNTVVKSor7/DfffFNOp1P33XefGjRo4P7461//akn9AAAA1V1GRoYiIyMVGRmpjIwMq8spxu/XCa4qrBMMAABQdr6+TrBfzgQDAAAAniAEAwAAIOAQggEAABBwCMEAAAAIOIRgAAAABBy/XCcYAAAAvs1ms+nKK690H/saQjAAAAC8Ljw8XEuXLrW6jFL5XiwHAAAAKhkhGAAAAAGHEAwAAACvy8jIUN26dVW3bl2f3DaZnmAAAABUirS0NKtLKBUzwQAAAAg4hGAAAAAEHEIwAAAAAg4h2EdlOpxKeGS+Eh6Zr0yH0+pyAAAAqhVCMAAAAAIOq0MAAADA62w2mzp27Og+9jWEYAAAAHhdeHi4Vq9ebXUZpfK9WA4AAABUMkIwAAAAAg4hGAAAAF6XmZmphIQEJSQkKDMz0+pyiqEnGAAAAF5nmqZ2797tPvY1zAQDAAAg4BCCAQAAEHAIwQAAAAg4hGAAAAAEHEIwAAAAAg6rQwAAAMDrDMNQ69at3ce+hhAMAAAAr4uIiNDmzZutLqNUtEMAAAAg4BCCAQAAEHAIwQAAAPC6zMxMtWnTRm3atGHbZAAAAAQG0zS1ZcsW97GvYSYYAAAAAYcQDAAAgIBDCAYAAEDAIQQDAAAg4BCCAQAAEHBYHQIAAABeZxiGmjZt6j72NYRgAAAAeF1ERISSk5OtLqNUtEMAAAAg4BCCAQAAEHAIwQAAAPC6rKwsderUSZ06dVJWVpbV5RRDTzAAAAC8Li8vT2vWrHEf+xpmggEAABBwCMEAAAAIOIRgAAAABBxCMAAAAAIOIdiPZDqcSnhkvhIema9Mh9PqcgAAAPwWq0MAAACgUsTGxlpdQqkIwQAAAPC6yMhIHT582OoySkU7BAAAAAIOIRgAAAABhxAMAAAAr8vKylLPnj3Vs2dPtk0GAABAYMjLy9OyZcvcx76GmWAAAAAEHEIwAAAAAg4hGAAAAAGHEAwAAICAQwgGAABAwGF1CAAAAFSKiIgIq0soFSEYAAAAXhcZGamMjAyryygV7RAAAAAIOIRgAAAABBxCMAAAALwuOztbiYmJSkxMVHZ2ttXlFENPMAAAALzO5XJpwYIF7mNfw0wwAAAAAg4hGAAAAAGHEAwAAICAQwgGAABAwCEEAwAAIOAQggEAABBwWCINAAAAXhcZGSnTNK0uo1TMBAMAACDgEIIBAAAQcAjBAAAA8Lrs7GwNHjxYgwcP9sltkwnBAAAA8DqXy6VPPvlEn3zyCdsmAwAAAL6AEAwAAICAQwgGAABAwCEEAwAAIOAQggEAABBwCMEAAAAIOGybDAAAAK+LiIhQenq6+9jXEIIBAADgdYZhKDIy0uoySuWX7RDLly/XgAED1LBhQxmGoc8///ys56empurWW29Vy5YtZbPZ9OCDD1ZJnQAAAPBNfhmCMzIy1K5dO73++utlOj8nJ0d169bVo48+qnbt2lVydQAAAMjJydGIESM0YsQI5eTkWF1OMX7ZDtG/f3/179+/zOcnJCTolVdekSRNmzatssoCAADAaU6nUzNmzJAkvfHGGwoNDbW4oqL8MgRXhZycnCK/tZw8edLCagAAAOBNftkOURWeeuopRUdHuz/i4+OtLgkAAABeQgguxfjx43XixAn3x549e6wuCQAAAF5CO0QpQkNDfa53BQAAAN7BTHA1kOlwKuGR+Up4ZL4yHU6rywEAAPB5fjkTnJ6erh07drg/T0pK0oYNGxQTE6MmTZpo/Pjx2rdvn2bOnOk+Z8OGDe5rDx8+rA0bNigkJEStW7eu6vIBAABgMb8MwWvWrFGvXr3cn48dO1aSNHz4cE2fPl2pqalKSUkpck379u3dx2vXrtUHH3ygpk2bKjk5uUpqBgAACCQRERE6dOiQ+9jX+GUI7tmzp0zTLPX16dOnFxs72/kAAADwLsMwVLduXavLKBU9wQAAAAg4hGAAAAB4XU5Oju677z7dd999PrltMiEYAAAAXud0OjV58mRNnjxZTqfvrV5FCAYAAEDAIQQDAAAg4BCCAQAAEHAIwQAAAAg4hGAAAAAEHEIwAAAAAo5f7hgHAAAA3xYeHq6kpCT3sa8hBAMAAMDrbDabEhISrC6jVLRD+KiPVqVYXQIAAEC1RQj2Uc480+oSAAAAKszhcGjcuHEaN26cHA6H1eUUQwj2Uc1iI60uAQAAoMJyc3P1/PPP6/nnn1dubq7V5RRDCPZRCYVCsNOVZ2ElAAAA1Q8h2Ec1iApzH+8/kW1hJQAAANUPIdhH2WyG+zgpLcPCSgAAAKofQrAfSCYEAwAAeBUh2A8wEwwAAOBdhGA/wEwwAACAd7FjnB9IPpJpdQkAAADlEh4erk2bNrmPfQ0h2A8cOpWj9BynCr1XDgAAwKfZbDa1adPG6jJKRTuEn0g6TEsEAACAtxCC/cSutHSrSwAAACgzh8OhiRMnauLEiT65bTLtEH5iFzPBAADAj+Tm5mrSpEmSpHHjxikkJMTiiopiJthP7GKFCAAAAK8hBPuJXYdphwAAAPAWQrCfSErLkGmaVpcBAABQLRCC/YDdZijT4dLBkzlWlwIAAFAtEIL9QOPa+QtMJx+hLxgAAMAbCMF+oFmdSElsnwwAAOAtLJHmBxJiI6Xth5VECAYAAH4iLCxMq1atch/7GkKwH2gWGyFJSkrLtLgSAACAsrHb7erUqZPVZZSKdgg/kHC6HWI3PcEAAABewUywH0iIzQ/B+45nWVwJAABA2TgcDr3yyiuSpL/+9a8+t2McIdgPxNYIUc3QIJ3KcVpdCgAAQJnk5ubq73//uyRp9OjRPheCaYfwA4ZhqHndSKvLAAAAqDYIwX6iWWz5Q3Cmw6mER+Yr4ZH5ynQwiwwAAFCAEOwnmtetYXUJAAAA1QYh2E/QDgEAAOA9hGA/0TyWmWAAAABvIQT7iYTTG2YAAADAcyyR5iciQoJUPzpMB05kW10KAADAOYWFhWnJkiXuY19DCPYjzepEEoIBAIBfsNvt6tmzp9VllIp2CD/SjJYIAAAAr2Am2I8kVGCtYAAAACvk5ubqrbfekiTdfffdCg4OtriiogjBfiShDiEYAAD4B4fDoTFjxkiSRowY4XMhmHYIP1J41zhXnmlhJQAAAP6NEOxHGkSfeWflvuNZFlYCAADg3wjBfsRmM9zHSWkZFlYCAADg3wjBfirlaKbVJQAAAPgtQrCf2kMIBgAAqDBCsJ/ac5SeYAAAgIpiiTQ/RTsEAADwZaGhofrqq6/cx76GEOyn9h7LlCvPlL3Qm+UAAAB8RVBQkBITE60uo1S0Q/ipXJepAyezrS4DAADALxGC/dhulkkDAAA+Kjc3V9OnT9f06dOVm5trdTnFEIL92G76ggEAgI9yOBwaOXKkRo4cKYfDYXU5xRCC/djuI4RgAACAiiAE+7HdR2iHAAAAqIgyh+AbbrhBN954o/bu3Vvi65mZmVq+fLmWL19+1vv89ttviomJUZ06dcpXKYphJhgAAKBiyrxE2ueffy7DMPTkk0+W+HpSUpJ69uwpm80mp9NZ6n1cLpeOHz8uw2BpL0+lHM2UaZpWlwEAAOB3vN4OQSirGoYhpec4dSTD9xrNAQAAfB09wX4qLipMEi0RAAAAFcGOcX6qSe1wHTiRrZSjGWrVoKbV5QAAABQRGhqqjz/+2H3sawjBfio+JkKrko8xEwwAAHxSUFCQBg8ebHUZpaIdwk81iYmQRDsEAABARTAT7Kfi3SGYtYIBAIDvcTqdmjt3riTp+uuvV1CQb8VO36oGZVYwE5zC1skAAMAH5eTkaMiQIZKk9PR0nwvBtEP4qfiYcElSWrpDGTmlr8sMAACA4sodyR977DHVqlWr2Pjx48fdx3fccUep1xc+DxVXMyxYMZEhOprhYDYYAACgnModgufNm1fqawW7wM2YMaPiFaHMmsRE6GiGQ3sIwQAAAOVSrnYI0zS98gHvSKhDXzAAAEBFlHkmOCkpqTLrQAU0qRMpSdpzNMviSgAAAPxLmUNw06ZNK7MOVEDTCqwQkelwqvXjiyRJW57op4gQ33qnJgAAQFUgAfmxhFjaIQAAgG8KCQnRu+++6z72NYRgP9YkJr8d4sDJbIsrAQAAKCo4OFgjRoywuoxSVVoITklJ0dy5c7Vjxw7ZbDY1a9ZMAwYM0HnnnVdZjww4sTVCFBFiV6bDZXUpAAAAfqXMIdjpdGratGmSpIsuukhdu3Yt9dwnnnhC//73v+V0Ft3EYdy4cXrggQf0wgsvVLBcFGYYhprWidTW1JNWlwIAAFCE0+nUokX570Pq16+fz+0YV+Zq1qxZo3vuuUeGYeibb74p9bznnntOEydOLPE1l8ull19+WTabTc8991y5i0VxTWMiCMEAAMDn5OTk6LrrrpPk59smL1u2TJLUpEkTXXXVVSWes3//fk2YMMH9effu3fXOO+/o66+/1hNPPKHo6GiZpqmXX35Zv//+u4elQ5Kanl4rGAAAAGVX5ki+YsUKGYahgQMHlnrOtGnTlJ2d7T7v008/de8i169fPw0YMEBdunSRw+HQzJkz9eSTT3r+FQS4pqfXCgYAAEDZlXkmOCUlRZLO2gv85Zdfuo+fffZZdwAu0K5dOw0bNkymaWrlypXlrRUlYCYYAACg/Mocgg8dOiRJSkhIKPH1zMxMrV+/XoZh6KKLLtL5559f4nnXXHONJGnbtm3lLBUlaRJDCAYAACivMofgY8eOSZLCw8NLfH3NmjXu1SC6d+9e6n0Kdp47fvx4WR9dzPLlyzVgwAA1bNhQhmHo888/P+c1y5YtU4cOHRQWFqbmzZtr6tSpFX6+L2lYK1xBduPcJwIAAMCtzCE4IiJ/xvHw4cMlvv7zzz+7jy+55JJS71PQIuFyVXxt24yMDLVr106vv/56mc5PSkrStddeqx49emj9+vX6v//7Pz3wwAP69NNPK1yDr7DbDDWuVfIvJgAAAChZmd8Yl5CQoF9++UU//vhjiatDLF261H18tr7hghAdHR1djjKL6t+/v/r371/m86dOnaomTZro5ZdfliS1atVKa9as0fPPP68bb7yxwnX4iviYCCUfYetkAADgO0JCQtwTlr64bXKZZ4Ivv/xymaapqVOn6tSpU0Ve2717txYvXizDMNSwYUO1bdu21Pts2LBBktSsWbOKVVwBP/74o/r27VtkrF+/flqzZo1yc3NLvCYnJ0cnT54s8uGr4ukLBgAAPiY4OFj33Xef7rvvPgUHB1tdTjFlDsGjRo2SYRhKTU1Vz549tXDhQv3+++/64osvdM0117j7gYcPH37W+/zvf/+TYRhq166dZ5WXw4EDBxQXF1dkLC4uTk6nU2lpaSVe89RTTyk6Otr9ER8fXxWlVkhTQjAAAEC5lDkEX3LJJbr33ntlmqY2bNigxMREXXjhhbr++uu1fft2SVK9evX00EMPlXqP1NRUfffdd5KkK664wsPSy+ePy7WZplnieIHx48frxIkT7o89e/ZUeo0VFR9DTzAAAPAtLpdLS5cu1dKlSz16L1hlKdf+da+++qq7JaIgRBaoX7++5s2bp9q1a5d6/csvvyyXy6WgoKBy9fR6qn79+jpw4ECRsUOHDikoKEh16tQp8ZrQ0FCFhoZWRXkeK9wO8cf/XQAAAKyQnZ2tXr16ScrfNjky0rc2+CpXCLbZbHrjjTd033336YsvvtDu3bsVEhKi9u3ba/Dgwef84iIiIvTQQw+pQYMGpYbPytC1a9ciG3lI0jfffKOOHTv6ZI9KecXXPhOC09Idigz1/68JAACgMpUrBBdo3bq1WrduXe7rJkyYUJHHFZOenq4dO3a4P09KStKGDRsUExOjJk2aaPz48dq3b59mzpwpSbrnnnv0+uuva+zYsbrrrrv0448/6p133tGHH37olXqsFhJ0pqtl95EMtlIGAAA4hzL3BPuSNWvWqH379mrfvr0kaezYsWrfvr0ef/xxSfm9xwXbPEv5K1EsWLBAS5cu1SWXXKInn3xSr776arVYHu2PWCoNAADg3Co0E2y1nj17nrX3dfr06cXGrrzySq1bt64Sq/INyWkZVpcAAADg8/xyJhilSzpCCAYAADiXMs8E9+7d26sPNgxD//vf/7x6T0i702iHAAAAOJcyh+ClS5e619Q1TbPU9XXLwtPrUbqUY5lyuvIUZGeSHwAAWCc4OFjPPvus+9jXlLsnOCwsTPXq1auMWuAFTpepfcezWCECAABYKiQkROPGjbO6jFKVOwRnZ2erQYMGGjp0qG6++WbFxMRURl3wwK7DLJMGAABwNmX+m/mTTz6pli1byjRN/fTTTxozZowaNmyoG264QXPnzlVubm5l1oly2MUKEQAAwGIul0urV6/W6tWrfXLb5DKH4EcffVRbtmzRqlWrNGbMGMXGxsrhcOjzzz/XTTfdpPr16+vee+/V999/X5n1ogyS0tKtLgEAAAS47OxsXXbZZbrsssuUnZ1tdTnFlPvdUx07dtSrr76q/fv364svvtBNN92k0NBQHTt2TG+99ZauuOIKnXfeeZo4caJ+//33yqgZ55DETDAAAMBZVXgJAbvdruuuu04ff/yxDhw4oP/+97/q0aOHpPxtjJ988kldeOGF6tq1q6ZMmaKjR496rWic3a7DhGAAAICz8co6WlFRURo1apSWLl2qpKQkPfHEE2rRooVM03S3TzRv3twbj0IZpJ7IVqbDaXUZAAAAPsvri8k2adJEjz32mH777Te99tprCg0NlWmacjgc3n4USlArIn8dvuRybpqR6XAq4ZH5SnhkPgEaAABUe+VeIu1cUlJS9P7772vWrFnatm2bezwkJMTbj0IJEupEakPmcSWlZSghNsLqcgAAAHySV0LwyZMnNWfOHL333ntasWKFTNOUaZqSpK5du7rXFEbZRYQEKfnpxHJflxAboQ17jmvX4XT1urBuJVQGAADg/yocgl0ul77++mvNmjVLX375pXJyctzBt3nz5rr99ts1dOhQnXfeeV4rFueWcHqTDFaIAAAAVgoODtaECRPcx76m3CF49erVmjVrlj766CMdOXJEkmSapmrVqqUhQ4Zo6NCh6t69u9cLRdkk1MlvgWDDDAAAYKWQkBBNnDjR6jJKVeYQ/O9//1uzZs1yr/1rmqaCg4PVv39/DR06VAMGDKDv1wckxObPBO86nO6emQcAAEBRZQ7B//znP2UYhkzTVOfOnTVs2DDdcsstql27dmXWh3JqEhMhw5BOZjt1LJOtrAEAgDXy8vK0detWSVKrVq1ks3l9UTKPlLsdIjw8XAcPHtRzzz2n5557rsIPNgxDO3furPD1KFlYsF0No8O173iWkmmJAAAAFsnKylLbtm0lSenp6YqMjLS4oqLKHYKzsrKUnJzs8YMNw/D4HihZ87qR+SH4SPnWCgYAAAgUZQ7BV1xxBcHVTzSPjdSK39OYCQYAAChFmUPw0qVLK7EMeFOz02+OSz5CCAYAACiJb3Uowyua1a0hSbRDAAAAlIIQXA01Pz0TnHKUEAwAAFASQnA11LBWuEKCbHI486wuBQAAwCdVeNtk+C67zVBCnQhtP5hudSkAACBABQcH6+GHH3Yf+xpCcDXVLDaSEAwAACwTEhLi0Z4SlY12iGqq+ek3xwEAAKA4ZoKrqYJl0gAAAKyQl5enlJQUSVKTJk38f9tk+IfmhGAAAGChrKwsNWvWTJJvbpvsW5EcXsNMMAAAQOkIwdVUTGSIosKZ6AcAACgJIbiaMgxDCXWYDQYAACgJIbgaIwQDAACUrEx/Ly94Z5+3NWnSpFLui3wJsRFWlwAAAOCTyhSCC97Z502GYcjpdHr9vjiDN8cBAACUrEwh2DTNyq4DleC8Qhtm8L8hAACoSkFBQRo9erT72NeUqaJ33333rK9PnjxZq1evVnBwsPr27avLLrtMcXFxMk1Thw4d0urVq/XNN98oNzdXnTp10r333uuV4nF2TeucaYdIPZGt8+v53r7dAACgegoNDdUbb7xhdRmlKlMIHj58eKmv3XnnnVqzZo369u2rd955R40aNSrxvH379umuu+7SokWLdNFFF+m///1vxSpGmQXbz7zvccehdJ1fr6aF1QAAAPgOj1aH+OSTTzRt2jR17NhR8+fPLzUAS1KjRo305ZdfqkOHDpo2bZo+/vhjTx6NctpxKL1C12U6nEp4ZL4SHpmvTAc93AAAoGxM09Thw4d1+PBhn2zL9CgEv/nmmzIMQ2PHjpXdbj/n+Xa7XQ899JBM09Rbb73lyaNRTjsOVywEAwAAVERmZqbq1aunevXqKTMz0+pyivEoBP/yyy+SpAsuuKDM1xSc++uvv3ryaJTTzgrOBAMAAFRHHoXgU6dOSZIOHTpU5msKzi24FlVjx+EM5eX53p8iAAAArOBRCG7atKkkaebMmWW+puBcNsqoWlkOl/afyLK6DAAAAJ/gUQgeOHCgTNPURx99pGefffac5z///PP68MMPZRiGrr/+ek8ejQr4/SAtEQAAAFIZl0grzSOPPKKZM2fq4MGDGj9+vD788EMNHz5cnTp1Ur169WQYhg4ePKjVq1dr1qxZ2rBhgySpfv36+sc//uGN+lEO2w+eUufmMVaXAQAAYDmPQnCtWrX07bffql+/ftq3b59++eUXPfTQQ6Web5qmGjdurIULF6pWrVqePBoVsJ2ZYAAAAEketkNIUuvWrbV582b97W9/U61atWSaZokftWrV0tixY7Vp0ya1bt3aG7WjnH4/xJsRAQBA1QgKCtLw4cM1fPhw/902+VyioqL0wgsv6KmnntLatWv166+/6tixYzJNUzExMbrooovUoUMHhYSEeONxqKDfD6azQgQAAKgSoaGhmj59utVllMqrsTwkJERdu3ZV165dvXlbeEGw3VBWLitEAAAASF5oh4B/aBYbKani2ycDAACUh2maysjIUEZGhk9um+zVmeBdu3bpxx9/1IEDB5SZmal7771XsbGx3nwEKuj8ejW0/WA6IRgAAFSJzMxM1ahRQ5KUnp6uyMhIiysqyisheP369XrwwQe1cuXKIuM33nhjkRD8xhtvaNKkSYqOjtaWLVsUHBzsjcejDM6vm/9DuPNwhsWVAAAAWM/jdoj58+erW7duWrlyZZHVIEoyfPhwZWVladeuXfrqq688fXTAiQgJUvLTiUp+OlERIeX7/eX8evkhmJlgAAAAD0PwgQMH9Oc//1k5OTlq3bq1vv76a506VfoyXDVq1NCgQYMkSV9//bUnj0Y5FYTgXcwEAwAAeBaCX3rpJaWnp6tp06ZasWKF+vXrd85+j549e8o0Ta1du9aTR6Oc4mMiFBJkU1auy+pSAAAALOdRCF60aJEMw9BDDz1U5h3gWrZsKUlKTk725NEoJ7vN0Hmn+4IBAAACnUchOCkpSZJ02WWXlfmamjVrSsp/lyCqVot6hGAAAADJw9UhcnNzJalcqzwcP35cknxumYxAcEEcIRgAAFQNu92um266yX3sazwKwfXr19fu3buVlJSk9u3bl+maH3/8UZLUuHFjTx6NCmgRV9PqEgAAQIAICwvTnDlzrC6jVB61Q3Tv3l2SNHfu3DKdn5mZqalTp8owDF1xxRWePBoVcAEhGAAAQJKHIXj48OEyTVMffvihvvnmm7Oem56eriFDhiglJUWSNGrUKE8ejQpoEhOh0CB2ygYAAPAoEfXp00eDBg1SXl6e/vSnP2ncuHFatWqV+/WjR4/q559/1pNPPqmWLVvq66+/lmEYGjZsWJnbJ+A9dpuhZrH0YgMAgMqXkZEhwzBkGIYyMnxvnwKPt01+7733dN1112np0qV68cUX9eKLL8owDEnSlVde6T6vYBe5q666SlOnTvX0saig8+vV0G8HSt/QpCwyHU61fnyRJGnLE/3KvXsdAACA1Tz+23hERIS+/fZbPffcc6pfv36RrZMLf8TExOg///mPFi1apNDQUG/UjgpgrWAAAAAvzARLks1m00MPPaS//vWvWrVqldasWaNDhw7J5XKpTp06at++vS6//HLCrw84n7WCAQAAvBOC3TcLClK3bt3UrVs3b94WXnR+vTM9wa4808JKAAAArONRO8Ty5cu1fPlyZWVllfma7Oxs93Woeo1rR7iP9xzLtLASAAAA63g0E9yzZ0/ZbDb98ssvat26dZmu2bdvn/s6p9PpyeNRAXab4T7ecShdrRtEW1gNAACANTxuhyhY9aGqroP3bD9wSmpndRUAAKA6stvtuvbaa93HvqbK17bKy8uT5JvfjECz1cOl0gAAAEoTFham+fPnW11Gqap8+7Dk5GRJUnQ0f4a32pb9J60uAQAAwBLlmgku2PL4j1JTU1WjxtmX3srJydHOnTv1z3/+U4ZhqE2bNuV5NCpB6olsHctwqHZkiNWlAAAAVKlyheBmzZoVGzNNU3379i33g4cNG1bua+B9W1JPqvv5sVaXAQAAqpmMjAzVq1dPknTo0CFFRkae44qqVa52iD/uAlfa+Nk+QkNDNW7cON1xxx1e/2JQfpv2nbC6BAAAUE1lZmYqM9M3l2Qt10zwu+++W+TzkSNHyjAMPfnkk2rUqFGp1xmGobCwMDVo0EDt27c/Z+sEqs5m+oIBAEAAKlcIHj58eJHPR44cKUkaNGhQmdcJhm/ZvJ+ZYAAAEHg8WiJtyZIlkkruFYZ/2JWWoUwHm5YAAIDA4lEIvvLKK71VByxQt2aoDp/K0dbUk2rVIMrqcgAAAKpMla8TDN/Rqn5NSfQFAwCAwOO1HeNM09SGDRu0ceNGpaWlKSsr65xbIz/++OPeejwqoFWDKC3/PU2b953UTR2srgYAAFQnNpvN3TVgs/nevKtXQvCMGTM0adIk7d69u1zXEYKt1bphfgvEJt4cBwAAvCw8PFxLly61uoxSeRyCH330UT399NPnnPWV8pdKK8t5qBqtGuS3Q2w/eEoOZ57F1QAAAFQdj+amf/75Zz311FOSpKuvvlobNmzQunXrJOUHXpfLpbS0NC1cuFADBw6UaZq6/PLLlZqaqrw8QpfVGtUKV1RYkHJdpnYeTre6HAAAgCrjUQieMmWKJKlp06aaP3++Lr74YgUHB7tfNwxDMTEx6tu3r+bOnas33nhDK1eu1DXXXCOHw+FZ5fCYYRhq0zBakrQ19ZTH98t0OJXwyHwlPDKfZdcAAAhwGRkZqlu3rurWrauMjAyryynGoxD8ww8/yDAMPfDAAwoKOndnxb333qsbb7xRv/zyiyZPnuzJo+ElbU73BW9JZYUIAADgXWlpaUpLS7O6jBJ5FIJTU1MlSW3atDlzw0Lv/svNzS12zdChQ2WapmbPnu3JozV58mQ1a9ZMYWFh6tChg1asWHHW89944w21atVK4eHhatmypWbOnOnR86uLNo3yQ/BvhGAAABBAPArBBSG3Xr167rEaNWq4jw8fPlzsmvj4eEnSjh07Kvzc2bNn68EHH9Sjjz6q9evXq0ePHurfv79SUlJKPH/KlCkaP368Jk6cqM2bN2vSpEm677779OWXX1a4huqibUE7xAHP2yEAAAD8hUchuG7dupKkkyfPzCLGxcXJbrdLkrZu3VrsmoLZ41OnKh66XnzxRY0aNUp33nmnWrVqpZdfflnx8fHuHuU/mjVrlv7yl7/o5ptvVvPmzXXLLbdo1KhReuaZZypcQ3XRvG4NhQXblOVwWV0KAABAlfEoBBe0Qfz222/usZCQEPd4SS0P77//viSpYcOGFXqmw+HQ2rVr1bdv3yLjffv21Q8//FDiNTk5OQoLCysyFh4erlWrVpXYslFwzcmTJ4t8VEd2m6EL67NlMgAACCweheAePXrINE0tWbKkyPjNN98s0zQ1bdo0Pf7449q8ebNWr16tMWPG6MMPP5RhGOrfv3+FnpmWliaXy6W4uLgi43FxcTpw4ECJ1/Tr109vv/221q5dK9M0tWbNGk2bNk25ubmlNms/9dRTio6Odn8UtHFURwVvjgMAAAgUHoXgQYMGSZK++uqrIjOlf/3rX5WQkKC8vDz9+9//1sUXX6wuXbq42xVq166t8ePHe/JoGYZR5HPTNIuNFfjnP/+p/v37q0uXLgoODtbAgQM1YsQISXK3bvzR+PHjdeLECffHnj17PKrXl7VtFG11CQAAoJqx2Wzq2LGjOnbsWP22TW7Tpo2WLFkip9Mpp/PMurARERFasmSJbr/9dn3//fdFrmnbtq1mzZqlxo0bV+iZsbGxstvtxWZ9Dx06VGx2uEB4eLimTZumN998UwcPHlSDBg301ltvqWbNmoqNjS3xmtDQUIWGhlaoxqoWERKk5KcTK3w9M8EAAMDbwsPDtXr1aqvLKJXH2yZfeeWVJY43bdpUK1as0LZt27R582Y5nU61aNFC7du39+h5ISEh6tChgxYvXqzrr7/ePb548WINHDjwrNcGBwe7w/dHH32k6667zid/M6lqF8TVlN1myJXHltYAACAweByCz6Vly5Zq2bKlV+85duxYDR06VB07dlTXrl311ltvKSUlRffcc4+k/FaGffv2udcC3r59u1atWqXOnTvr2LFjevHFF7Vp0ybNmDHDq3X5q7Bgu86vG6ltB9k6GQAABIZKD8GV4eabb9aRI0f0xBNPKDU1VW3bttWCBQvUtGlTSfnLsBVeM9jlcumFF17Qtm3bFBwcrF69eumHH35QQkKCRV+B72nVIIoQDAAAvCYzM1OtW7eWJG3ZskUREREWV1SUX4ZgSRo9erRGjx5d4mvTp08v8nmrVq20fv36KqjKf7VqEKXPN+y3ugwAAFBNmKap3bt3u499TZlC8BNPPFEpD3/88ccr5b4ov1YNeHMcAAAIHGUKwRMnTix1+TFPEIJ9x4X1a7qPj2Y4FBHit38kAAAAOKcyL41gmuZZPypyDnxHjbAzofeXvcetKwQAAKAKlCkE5+Xllfqxa9cuderUSaZpqn///pozZ452796t7OxsZWdna/fu3frkk0/Uv39/maapTp06KSkpSXl5eZX9taGCNuw5YXUJAAAAlcqjv3mfOHFCffv2VVJSkmbOnKnbb7+92Dnx8fGKj4/XDTfcoPfff1/Dhw9Xnz59tGbNGkVHs1OZL9q457jVJQAAAFQqj3aKeOmll7Rjxw7dddddJQbgP7rtttt01113aefOnXrhhRc8eTQq0S/7TrBxBgAA8IhhGGrdurVat25dKe8t85RHIfjTTz+VYRgaPHhwma8ZMmSIJOmzzz7z5NGoRFkOl7YdOOW1+2U6nEp4ZL4SHpmvTIfz3BcAAAC/FxERoc2bN2vz5s0+t0aw5GEITk5OlqRytTUUnFuwbhx807qUY1aXAAAAUGk8CsHBwcGSpF9//bXM1xScW3AtfNP6lONWlwAAAFBpPArB7dq1k2maeuaZZ5SZmXnO8zMzM/XMM8/IMAxdfPHFnjwalWw9M8EAAMADmZmZatOmjdq0aVOmnFjVPArBd955pyRp27Zt6tmzpzZs2FDquRs3blSvXr3022+/SZLuvvtuTx6NSrYrLUPHMhxWlwEAAPyUaZrasmWLtmzZ4pP7Q3i0RNptt92muXPn6rPPPtPatWvVoUMHXXTRRerUqZPq1asnwzB08OBBrV69ukjLxA033KBbb73V4+JROZrFRiopLUPr9xxTl+Z1rC4HAADA6zzeG3f27Nl68MEHNWXKFOXl5emXX34psUfYNE0ZhqExY8boxRdf9PSxqETtGkfnh+CU44RgAABQLXnUDiFJdrtdr732mtavX697771XLVq0kFR0e+Tzzz9f9957r9avX69XX31VQUEeZ29UonbxtSSxQgQAAKi+vJZGL7roIr3xxhuSpJycHB0/flymaap27doKDQ311mNQBS45HYI3pBxn0wwAAFAtVcqUbGhoqOLi4irj1qgC59erocgQuzIcLu08nG51OQAAAF7ncTsEqh+7zXC3RGzYc9zSWgAAgH8yDENNmzZV06ZNfXLbZJpzUaJLm9TWDzuPEIIBAECFREREuHcX9kVlCsG9e/eWlJ/o//e//xUbr4g/3gu+pX2TWpKkX/acsLYQAACASlCmELx06VJJKjaVvXTpUhmGUa4FkAvO98VpcZzRvkltSfmbZgAAAFQ3ZQrBV1xxRYmhtbRx+L+YyBD3phkAAADllZWVpSuuuEKStHz5coWHh1tcUVHlmgku6ziqh/bxtQjBAACgQvLy8rRmzRr3sa9hdQiUqn3T2pV6/0yHUwmPzFfCI/OV6XBW6rMAAAAKIwSjVJeefnMcAABAdUMIRqlaxtVUeIjd6jIAAAC8jhCMUgXZbbqoYZTVZQAAAHhdmd4YZ7d7fzbQMAw5nfSB+rp28bW0KvmY1WUAAAB4VZlCcHnWAUb1csnp7ZMBAADKKzY21uoSSlWmEDxhwoTKrgM+6tJCK0QcOpmthNgaFlYDAAD8RWRkpA4fPmx1GaUiBOOsosOD3cc/Jx0lBAMAgGqhTCEY/ikiJEjJTyd67X6rko7q5k5NvHY/AAAAq7A6BMpsVdJRq0sAAAB+IisrSz179lTPnj2VlZVldTnFMBOMMttzLEv7jmepdkTwuU8GAAABLS8vT8uWLXMf+xqvhuBjx45p48aNSktLU1ZW1jlXlRg2bJg3H48q8OPOI7r2ovpWlwEAAOARr4TgpUuXasKECVq5cmWZrzEMgxDsh37YmUYIBgAAfs/jEDxlyhTdf//9Mk2T9YQDwE87j/C/MwAA8HsevTFu69ateuCBB2Sapi666CJ9/vnnmj9/vqT8md6dO3dqzZo1mjp1qi699FJJ0uWXX67Nmzdr165dnlePKhVkN7T/RLb2HPO95nYAAIDy8CgEv/baa3K5XIqNjdWKFSv0pz/9SU2anFlCq1mzZrr00kt19913a/Xq1Ro3bpxWrlyp+++/X02bNvW4eFStixtFS6rcVSIyHU4lPDJfCY/MV6aDbbUBAEDl8CgEL1u2TIZh6IEHHlDNmjXPeq5hGHrmmWfUu3dvLVmyRNOmTfPk0bBA52YxkvI3zQAAADiXiIgIRUREWF1GiTwKwXv37pUkd6uDlB92C+Tm5ha75u6775Zpmnrvvfc8eTQscNnpEMx6wQAA4FwiIyOVkZGhjIwMRUZGWl1OMR6F4OzsbElSw4YN3WOFv8hjx44Vu+b888+XJG3ZssWTR8MCl8TXUkiQTYdP5VhdCgAAgEc8CsExMfkzgxkZGe6xunXrumeDt2/fXuyatLQ0SdLx48c9eTQsEBpsV4cmta0uAwAAwGMeheALL7xQkvT777+7xyIiItSiRQtJ0hdffFHsmoKxunXrevJoWKTreXWsLgEAAPiB7OxsJSYmKjEx0d094Es8CsGXX365TNPU8uXLi4zfcMMNMk1Tr776qqZNm6aMjAwdPnxYzz//vN566y0ZhqHevXt7VDisQQgGAABl4XK5tGDBAi1YsEAul8vqcorxKARfd911kqR58+YVSfgPPfSQYmJilJubq7vuuktRUVGqX7++/vGPf8jpdCosLEyPPPKIZ5XDEu0a11J4sN3qMgAAADziUQju3Lmz3n33XT3zzDNF3gRXp04dLVq0SAkJCe6d5Ao+6tWrp7lz56pVq1YeF4+qFxJkU/smtawuAwAAwCMeb5s8fPjwEsc7dOig3377Td999502b94sp9OpFi1aqF+/fj67XhzKpnOzGP2w84jVZQAAAFSYxyH4bIKDg9WvXz/169evMh+DKlawXrAk5eWZFlYCAABQMR61QyAwtWkY5T7+7eApCysBAACoGI9CcKdOnfTKK6/owIED3qoHfiDIfubH5udd7B4HAAD8j0cheO3atRo7dqzi4+PVt29fzZgxQ6dOMTMYSJZvP1wlz8l0OJXwyHwlPDJfmQ5nlTwTAABUXGRkpHthhGq3bXKrVq1kmqZcLpf+97//6Y477lD9+vV1880364svvpDTSVip7tbuPqZT2blWlwEAAFAuHoXgzZs3a/369Xr44YfVqFEjmaaprKwsffLJJ7r++usVFxene++9VytWrPBWvfAxzjxTK39Ps7oMAACAcvH4jXHt2rXTs88+q5SUFC1ZskR33XWXatWqJdM0dezYMb311lvq2bOnmjZtqv/7v//Tpk2bvFE3fMh3vx2yugQAAOBjsrOzNXjwYA0ePLj6bZv8R1deeaXefPNNHThwQHPnztXgwYMVGhoq0zS1Z88ePfPMM2rXrp0uvvhiPfvss958NCy0dPthlkoDAABFuFwuffLJJ/rkk0+q37bJpQkODtbAgQM1e/ZsHTp0SO+++6769Okjm80m0zS1adMmjR8/vjIejSoWEWLX4VM52rz/pNWlAAAAlFmlrxNco0YNDR8+XIsWLdKMGTNUq1atyn4kqlC38+pIoiUCAAD4l0oPwevWrdPDDz+s+Ph4DR06VCdOnKjsR6IKXXlBXUnSd9sIwQAAwH9UyrbJO3fu1AcffKAPPvhA27dvlySZZn7PaM2aNXX99dfrtttuq4xHo4pdcToE/7L3uI6k51hcDQAAQNl4LQQfOnRIs2fP1gcffKBVq1ZJOhN8g4OD1a9fP912220aOHCgwsLCvPVYWKxuzVC1bRSlTftOagVLpQEAAD/hUQjOyMjQZ599pvfff1/fffed+51/BeG3W7duuv322zVkyBDFxMR4Xi18Uu+W9bRp30ktq6Ld4wAAADzlUQiOi4tTVlaWpDPBt1WrVrrtttt06623KiEhweMC4ft6XVhPr363Q9/vOGJ1KQAAwEdEREQoPT3dfexrPArBmZmZkqSGDRvqlltu0W233ab27dt7pTBUjoiQICU/nejVe7ZrXEt1IkN0JMPh1fuWRabDqdaPL5IkbXminyJCKqXNHQAAlJNhGIqMjLS6jFJ5lBhGjBih22+/Xb169ZJhGN6qCX7GZjN0Zcu6+mzdPqtLAQAAKBOPlkibNm2aevfuTQCGerWsZ3UJAADAh+Tk5GjEiBEaMWKEcnJ8bwWpSlknODk5Wb1799ZVV11VGbeHD7rigrqy2/hlCAAA5HM6nZoxY4ZmzJghp9NpdTnFVEoDZUZGhpYuXcoMcQCJDg9W+/haWrP7mNWlAAAAnFOl7xiHwFGwexwAAICvIwTDa65oeSYEZzp8788eAAAABQjB8Jrz655ZBmUlu8cBAAAfRgiG1xTuAV+0+aCFlQAAAJwdIRiVYun2w8pyuKwuAwAAoESVsjpEvXr1NGHChMq4NfxElsOlpdsOqf9FDar82ewiBwCA9SIiInTo0CH3sa+plHRQt25dQjA0/9dUS0IwAACwnmEYqlvXd1eOoh0CleZ/Ww/REgEAAHxSpYfgL7/8UkOHDlX//v01evRorV+/vrIfCR/QqFa4snLzWyIAAEDgycnJ0X333af77ruv+m2bvGTJEtWrV09NmjTR8ePHi73+z3/+U4MGDdIHH3ygb775Rm+++aY6d+6s999/35PHwg/0axMnSfrq11SLKwEAAFZwOp2aPHmyJk+e7JPbJnsUghcsWKC0tDR16dJFtWrVKvLaL7/8ov/85z8yTVOmaapWrVoyTVNOp1N33323du/e7cmj4eOuaVtfkvQdLREAAMAHeRSCV65cKcMwdPXVVxd7bcqUKTJNU7Vr19batWt15MgRrVq1SjExMcrOztbUqVM9eTR8XJuGUWpcO78lYvnvh60uBwAAoAiPQvCBAwckSRdeeGGx17766isZhqH77rtP7du3lyR17NhRY8aMkWma+vbbbz15NHycYRhKvDh/ZQg2zgAAAL7GoxBcsPZbdHR0kfGdO3dq3759kqQbbrihyGs9evSQJO3YscOTR8MPXHdRQ0nSsm3MBAMAAN/iUQg2TVOSdOLEiSLjK1askJQfji+55JIir9WpU0eSlJmZ6cmj4QfaNopSfEx+S4QvyHQ4lfDIfCU8Ml+ZDt9r0AcAAFXHoxBcv37+m5+2bt1aZHzRovzdurp3717smoyMDElS7dq1PXk0/IBhGEo8PRsMAADgSzwKwV26dJFpmpoyZYp7ZnfXrl2aN29eqW+Y2759u6QzARrVWyI7xgEAEJDCw8OVlJSkpKQkhYeHW11OMR6F4DvvvFNS/nJobdu21U033aQuXbooOztb4eHhuvXWW4tds3z5cklS69atPXm0Jk+erGbNmiksLEwdOnRwt2CU5v3331e7du0UERGhBg0aaOTIkTpy5IhHNeDc2jaKUnxt3/vBBwAAlctmsykhIUEJCQmy2Xxvk2KPKurdu7cefPBBmaap5ORkzZ07V2lpaZKk5557TrGxsUXOz87OPusscVnNnj1bDz74oB599FGtX79ePXr0UP/+/ZWSklLi+StXrtSwYcM0atQobd68WXPmzNHq1avdIR6VxzAM9WvDrD8AAPAtHsfyF198UV988YWGDh2qPn36aNiwYfr222917733Fjv3iy++UFRUlJo0aeJRCH7xxRc1atQo3XnnnWrVqpVefvllxcfHa8qUKSWe/9NPPykhIUEPPPCAmjVrpssvv1x/+ctftGbNmgrXgLIr2DhDkk5m5VpYCQAAqCoOh0Pjxo3TuHHj5HA4rC6nGK/MTV933XWaMWOGFi1apOnTp6t3794lnjdkyBAlJycrKSlJjRs3rtCzHA6H1q5dq759+xYZ79u3r3744YcSr+nWrZv27t2rBQsWyDRNHTx4UJ988okSExMrVAPKp1WDmu7jBZsOWFgJAACoKrm5uXr++ef1/PPPKzfX9ybBfK9B4xzS0tLkcrkUFxdXZDwuLs69eccfdevWTe+//75uvvlmhYSEqH79+qpVq5Zee+21Up+Tk5OjkydPFvlAxRiG4T6eu26fhZWUjKXTAAAIPFUSgnfu3Kmff/5ZBw96b+ewwsFKyl+z+I9jBbZs2aIHHnhAjz/+uNauXauFCxcqKSlJ99xzT6n3f+qppxQdHe3+iI+P91rtvigiJEjJTycq+elERYQEVdpzft13QtsPnqq0+wMAAJSFRyH48OHDmjx5siZPnlxswwwpf1e4Dh066IILLlC3bt3UqFEj3XTTTTp+/HiFnxkbGyu73V5s1vfQoUPFZocLPPXUU+revbvGjRuniy++WP369dPkyZM1bdo0paamlnjN+PHjdeLECffHnj17Klwzipqzhu8lAACwlkch+NNPP9WYMWP02muvFds6OScnR/3799eGDRtkmqZM01ReXp7mzp2rQYMGVfiZISEh6tChgxYvXlxkfPHixerWrVuJ12RmZhZbmsNut0s6s+vdH4WGhioqKqrIB7xj7vp9ynXlWV0GAAAIYB6F4G+++UaGYejGG28s9tr06dO1c+dOSdKf/vQnvfLKKxowYIBM09SKFSv08ccfV/i5Y8eO1dtvv61p06Zp69at+tvf/qaUlBR3e8P48eM1bNgw9/kDBgzQZ599pilTpmjXrl36/vvv9cADD+iyyy5Tw4bsaFaV6kSGKC3doaXbDltdCgAACGAeheBt27ZJki677LJir3344YeS8tcS/vzzz3X//fdr3rx56tOnj0zTdL9eETfffLNefvllPfHEE7rkkku0fPlyLViwQE2bNpUkpaamFlkzeMSIEXrxxRf1+uuvq23btho8eLBatmypzz77rMI1oGIGtMv/peNjWiIAAICFPHoH1OHD+bN5f5xNzcrK0o8//ijDMHT33XcXee2OO+7Qt99+q3Xr1nnyaI0ePVqjR48u8bXp06cXG7v//vt1//33e/RMeO76Sxtp+g/JWvLbIR1Jz7G6HAAAUEnCw8O1adMm97Gv8SgEF7zB7Y/9tj/99JNyc3Nls9nUp0+fIq81a9ZMUv4b2RB4WtSroXbxtbRxz3F9ubHkNyX6gkyHU60fXyRJ2vJEv0pdMQMAgOrIZrOpTZs2VpdRKo/aIWrUqCFJxVZqWLp0qSSpdevWql27dpHXgoODJUlBQYSKQDW4Q/5GKZ+t9701gwEAQGDwKARfeOGFkqSFCxcWGf/0009lGIauvPLKYtcUBObSljND9TegXUOFBtm041C61aUAAIBK4nA4NHHiRE2cOLH6bZucmJgo0zT11ltvacqUKdq0aZMefvhhbdmyRZJ0ww03FLumoBe4otsmw/9FhwerX5v6VpcBAAAqUW5uriZNmqRJkyb55LbJHvUkjBkzRpMnT1ZqaqrGjBlT5LWuXbuqV69exa758ssvZRiGevTo4cmj4ecGd2ysLzbut7oMAAAQoDyaCY6Ojta3336rSy+91L0hhmma6tGjR4nrAG/cuFGrV6+WJF199dWePBp+rtt5sWoQHWZ1GQAAIEB5/O60Vq1aac2aNUpKStKBAwfUoEEDJSQklHr+u+++K0ml7u6GwGC3GRp0SUNNWbbL6lLKhVUjAACoHrz2L3izZs3cy5+Vpl27dmrXrp23Hgk/d2OHxu4QvOtwuto2qmVtQQAAIGB41A4BeKJhrTMLZ89Zs9fCSgAAQKDx6t9yDx48qKVLl2rTpk06evSoJCkmJkZt27ZVz549WRYNpZq7YZ/GX9vK6jIAAECA8EoITk1N1dixY/XZZ5/J6XSWeI7dbtdNN92kF154QQ0aNPDGY1GNnMxyav4vqep/EUunAQBQHYSFhWnVqlXuY1/jcTvExo0bdfHFF+vjjz9Wbm5ukVUiCn84nU7Nnj1b7dq106+//uqN2lHNvP/zbqtLqLBMh1MJj8xXwiPzleko+RdBAAACid1uV6dOndSpUyfZ7XaryynGoxCckZGhxMREHTlyRKZpqk+fPpo9e7aSk5OVnZ2t7OxsJScn6+OPP1bfvn1lmqbS0tKUmJiozMxMb30NqAaCbIbWpRzXbwdOWl0KAAAIAB6F4Ndff1379++XzWbTf//7X33zzTcaPHiwmjRpopCQEIWEhKhJkya66aabtHDhQr399tsyDEP79u3TG2+84a2vAdXAVa3qSZI+Xs0b5AAAqA4cDoeee+45Pffcc9Vv2+R58+bJMAyNGDFCo0aNOuf5d9xxh0aOHCnTNDV37lxPHo1q5uZO8ZLELnIAAFQTubm5+vvf/66///3vPrltskchePv27ZKkW265pczX/PnPfy5yLSBJnZvFqHlspDIdLqtL8Rr6hAEA8F0eheD09HRJ+cuglVXt2rUl5fcTAwUMw9CtnZtYXQYAAAgQHoXgunXrSpK2bt1a5msKzo2NjfXk0aiGburQWCFB7N8CAAAqn0eJo0uXLjJNUy+++GKp6wMXlpubqxdeeEGGYahLly6ePBpVJCIkSMlPJyr56URFhHh1b5ViakWEqH9b1gkGAACVz6MQPGzYMEnShg0blJiYqP37S39T0759+3Tddddpw4YNkqQRI0Z48mhUUzd3jHcfH8/0vXeSegO9wgAAWM+jqb0BAwZo0KBB+vzzz/Xtt9+qefPmuvrqq9W5c2fFxcXJMAwdOHBAP//8sxYvXux+Z+D111+vxMREr3wBqF7axUe7jz9es1cP9rnAwmoAAEB15fHftz/88EMNGzZMc+bMkcPh0IIFC7RgwYJi55mmKUkaPHiwZs6c6eljUU0ZhuE+nvXTbt1z5XkKC/a9XWYAAMDZhYWFacmSJe5jX+Pxu5BCQ0M1e/Zsffnll+rfv7/Cw8OLbZkcHh6u/v3766uvvtLs2bMVGhrqjdpRzR1Jd+izdfusLqPK0CYBAKhO7Ha7evbsqZ49e/rktslee6dTYmKiEhMT5XK5tGvXLh09elRS/vJpzZs398kvHr7vreU73RtpAAAAeItHIbh3796SpKFDh2rkyJGS8lN/ixYtPK8MAS8qPEjJRzL1zeYDurJlXavLAQAA5ZCbm6u33npLknT33XcrODjY4oqK8qgdYsWKFVq2bJkSEhK8VA5wxq2X5W+eMXXZTndPeSCiTQIA4I8cDofGjBmjMWPGyOHwvRWfPArB9erVkyTVqlXLG7UARdzWuYlCg2zauPeEVicfs7ocAABQjXgUgtu1aydJ2r59u1eKAQqrUyNUQ06vG/z2iiSLqwEAANWJRyH4zjvvlGmamjp1qrfqAYq4q0dz2Qxp5Y40q0vxKbRIAADgGY9C8A033KDbb79dy5Yt0x133KGMjAxv1QVIkprUidC1FzWwugwAAFDNeLQ6xMyZM3XVVVfpl19+0YwZMzRv3jwNGDBAF198sWrXrn3OZdEKtl0GzuaeK8/TV7+kWl0GAACoRjwKwSNGjCiyw9exY8c0a9asMl1rGAYhGGXStlG0ujavox93HbG6FJ+X6XCq9eOLJElbnuiniBCvLQUOAEC14vG/kH9cuiqQl7JC5RnVo5k7BB86ma2E2BoWVwQAAM4mNDRUX331lfvY13gUgpOSeMc+qkbX5jHu4ynLdumZGy+2sBoAAHAuQUFBSkxMtLqMUnkUgps2beqtOoCzKtx288navbrnyvPULDbSwor8C20SAAAU5dHqEIAVXHmmXvhmm9VlAACAs8jNzdX06dM1ffp05ebmWl1OMYRg+B3DkL76JVW/7j1hdSkAAKAUDodDI0eO1MiRI/1/2+Svv/5al156qS699FJ98MEH5XrQ+++/777222+/Lde1QGHXXZy/bvCzi36zuBL/xoYbAIBAVuYQbJqm/va3v2njxo2qU6eObr311nI96NZbb1WdOnW0YcMGPfTQQ+UuFL4jIiRIyU8nKvnpREt6S+/vfb6C7YZW/J6mn1g2DQAAVECZQ/B3332n7du3y2az6eWXXy73gwzD0CuvvCK73a5NmzZp6dKl5b4HIEmNa0fots75b8p8afF2i6upfpghBgAEgjKH4E8//VSSdPXVV6tNmzYVeljr1q3Vr1+/IvcDKuK+XucrIsSuX/edtLoUAADgh8ocgletWiXDMDRgwACPHnjdddfJNE399NNPHt0Hga1uzVDd2aO51WUEFGaIAQDVSZlD8O7duyVJLVu29OiBF1xwgSQpOTnZo/sAd/VoploRwVaXAQAA/FCZQ/CJE/nLUcXExJzjzLMruP7kSf6MDc/UDAvWPVecmQ0+nul7y69Ud8wOAwBKExoaqo8//lgff/yxT26bXOYQHBUVJUk6fvy4Rw8suL5mzZoe3QeQpD93buI+funb3y2sBIURjgEAQUFBGjx4sAYPHqygIN/bqbTMIbhevXqSpC1btnj0wK1btxa5H+CJYPuZH+E5a/ZqXcoxC6vBuRCOAQC+oswh+LLLLpNpmvriiy88euC8efNkGIY6derk0X2Akjw6d5OcrjyrywAAIOA5nU7NmTNHc+bMkdPpexMfZQ7B/fv3lyQtXrxYy5cvr9DDli9frm+++abI/QBviQoP0tbUk/rg5xSrS0E5MDsMANVTTk6OhgwZoiFDhignJ8fqcoopcwi+8cYb1bx5c5mmqSFDhmjbtm3letD27ds1ZMgQGYahhIQE3XTTTeUuFjibh67OX3nk1e92WFwJvKG0cExoBgB4Q5lDcFBQkF544QUZhqHDhw+rY8eOeumll5Senn7W69LT0/Xyyy+rY8eOOnTokCTphRde8MkGafi3Gy9trEub1FKmw2V1KbAA4RgAUB7lSqIDBw7Uv/71Lz366KPKzMzUww8/rAkTJqhHjx669NJLFRcXp8jISGVkZOjgwYNat26dVqxYoYyMDJmmKUmaNGmSBg0aVBlfCwKczWboX4Mu0oDXV8qVZ1pdDnxApsOp1o8vkiRteaKfIkKCzjoOAAgc5f4v//jx49W4cWONHj1aGRkZSk9P18KFC7Vw4cISzy8IvxEREXr99dc1YsQIjwoGzqZ1wyjd3qWJZvyQv7lLdq6LgIMyKykcE5gBoHoqcztEYUOHDtX27dv10EMPqW7dujJNs9SP2NhYPfzww9q+fTsBGFViTK/z3ccvs3YwKkl5epZp1QAA31PhKY0GDRroueee03PPPactW7Zo48aNSktL06lTp1SzZk3FxsaqXbt2at26tTfrBc4pMvTMj/XMH3fr6tZx6tGiroUVASUrT7tGeVs7vHFvAKjOvPJfutatWxN24bMenrNRC/96hUKDK/SHDyBgEaQBeCIkJETvvvuu+9jXkApQrTWPjdTBkzka/9mv7v50AFXHG20jLJcH+Kfg4GCNGDFCI0aMUHBwsNXlFEMIhtdEhAQp+elEJT+d6DOzQM/edLGC7YYWbj6guev3W10OgErmrSDtjZAOwLcRglGttW4YpbFXt5Qk/WfBVourARBIKmu225eCfnW/h6/X5yv3KG38ZGa26g2eqHqDJ+pkZrZ8DSEY1d7dVzTXZc1i2EQDAIAqlJOTo8OfTNLhTyb597bJgL+y2wy9dPMlqhnmGy0aAADAeoRgBIRGtcL1+HVnVjBZ+XuahdUAAACrEYIRMBIvbuA+fnjORiWlZVhYDQAAsBIhGAHpZLZTd81co1PZuVaXAgAALEAIRkCqVzNUOw6l62+zNygvj/WDAQAINIRgBKRX/3yJQoJs+nbrIb2xZIfV5QAAgCpGCEZAurhxLT11/UWSpCnLdllcDQAA1U9ISIhirr5HMVff45PbJrNmFALWjR0aa0vqSb2zMsnqUgAAqHaCg4NV89Lr3Me+hplgBLTx/S9U1+Z13J/vPZZpYTUAAKCqEIIR0ILsNr0w5GL356NmrNGhk763tSMAAP7G5XIpO+UXZaf8IpfL93ZtJQQj4NWKONOntOdoloZNW6XjmQ4LKwIAwP9lZ2fr4If/p4Mf/p+ys31vgokQDBQSWyNEvx04pXvfW2d1KQAAoBIRgoFC3hneUdHhwdq494TVpQAAgErE6hCodBEhQUp+OtHqMsqkRVxNTR/ZSbe+/bOyHPn9S05XnsVVAQAAb2MmGPiD9k1q6/U/t3d//tCcjcpx+l5DPwAAqDhCMFCCruedWTZt8ZZDumP6amXkOC2sCAAAeBMhGDiH8BC7vt9xRLe9/TOrRgAAUE0QgoFzeHdER9WKCNaGPcc1bNpqq8sBAMAvBAcHq1bPkarVcyQ7xgH+6OLGtTTnL10VFxWqHYfSrS4HAAC/EBISoujONyq6840KCQk59wVVjBAMlEGLuJr65J5uahIT4R5bn3LMwooAAIAnCMFAGcXHROi9Oy9zfz783dX6cFWKhRUBAOC7XC6XclK3Kyd1O9smA/4utkao+9jpMjX+s1/1z883yeFkLWEAAArLzs7WgZljdWDmWLZNBqqTB646X4Yhzfppt0bNWGN1OQAAoBwIwUAF3XPleXp7WEfVCA3S2t30BwMA4E8IwYAHrmoVp8/v666EOmfeMPfW8l1y5ZkWVgUAAM6FEAx46Px6NfTR3V3cn7/87e+65a0fte9YloVVAQCAs/HbEDx58mQ1a9ZMYWFh6tChg1asWFHquSNGjJBhGMU+2rRpU4UV448iQoKU/HSikp9OVERIkNXleCQq/Mwi4BEhdq1OPqZBk7+3sCIAAHA2fhmCZ8+erQcffFCPPvqo1q9frx49eqh///5KSSl5uapXXnlFqamp7o89e/YoJiZGgwcPruLKEQjmju6mDk1rKyPnzHIwxzLYbhkAAF/ilyH4xRdf1KhRo3TnnXeqVatWevnllxUfH68pU6aUeH50dLTq16/v/lizZo2OHTumkSNHVnHlCATxMRGafXcXPXDV+e6xa19dqY9WpSiPXmEAQIAIDg5WdPc/K7r7n9k22RscDofWrl2rvn37Fhnv27evfvjhhzLd45133lGfPn3UtGnTUs/JycnRyZMni3wAZRVkt+meK89zf34iK1ePfParbpz6g7am8rMEAKj+QkJCVOvy21Tr8tvYNtkb0tLS5HK5FBcXV2Q8Li5OBw4cOOf1qamp+vrrr3XnnXee9bynnnpK0dHR7o/4+HiP6kZge+SalooMsWt9ynENnvqj1eUAABDw/C4EFzAMo8jnpmkWGyvJ9OnTVatWLQ0aNOis540fP14nTpxwf+zZs8eTchHghnVL0P8e6qnrLm6gwh0RH65KYbc5AEC1lJeXJ8fh3XIc3q28PN/7t87vQnBsbKzsdnuxWd9Dhw4Vmx3+I9M0NW3aNA0dOvSc0/KhoaGKiooq8gF4on50mF6/9VK9PayDe+zJr7aqz4vLNG/DPvqFAQDVSlZWllKn3afUafcpK8v3lg31uxAcEhKiDh06aPHixUXGFy9erG7dup312mXLlmnHjh0aNWpUZZYInFW382Pdx3VqhCjlaKb++tEG3TilbD3tAADAc34XgiVp7NixevvttzVt2jRt3bpVf/vb35SSkqJ77rlHUn4rw7Bhw4pd984776hz585q27ZtVZcMlGjRgz00rl9L1QwL0raD6e7x9SlswwwAQGXyyx0Kbr75Zh05ckRPPPGEUlNT1bZtWy1YsMC92kNqamqxNYNPnDihTz/9VK+88ooVJQMliggJ0n29ztdtnZvote9+1zsrkyVJt729Sn1axenv17RU49rh1hYJAEA15JchWJJGjx6t0aNHl/ja9OnTi41FR0crMzOzkqsCKqZWRIge6tvSHYLtNkPfbj2o7347qIGXNLK2OAAAqiG/bIcAqrt593XTNW3qK8+U5q7f5x4/fCrHwqoAAKg+CMGAD2pet4amDu2guaO76bKE2u7xfi8v138WbNVRtmEGAMAjftsOgeopIiRIyU8nWl2Gz2jfpLbeHdlJbSZ8I0nKzs3TW8t3adZPuy2uDACAswsODlbUZTe4j30NM8GAjyu8CczU2y/VxY2jleVwucde+fZ3Hc9kZhgA4FtCQkJUu9cdqt3rDrZNBuCZKy6oq3n3ddcbt7Z3j725fJd6PLNEL36zTSeyci2sDgAA/0E7BOBnDMNQrwvruT9vGVdD2w6m69Xvdmja98nWFQYAQCF5eXlynjjoPvY1zAQDfu7Te7tp6u2X6sL6NZWe43SPv/jNdlaTAABYJisrS/umjtK+qaPYNhmA99lshq5p20ALHuihl25u5x5/e2WSuj/znR77/FftPcYa2QAAFEYIBqoJm81Qvzb13Z+3axwthzNP7/2Uov6vrLSwMgAAfA8hGKimPrirsz68q4t6tIiVK890j9/85o/6eM2eIitMAAAQaHhjHFBNGYahrufVUdfz6mhN8lHdNPVHSdKv+07q75/8oqgw/u8PAAhczAQDAaB1wyj38dirWyg+Jlwns8+8ie7mN3/UtJVJOnQq24ryAACockwFAQHmzh7NNaZXC32z5YDueW+dpPzZ4V/3bdG/5m9Rl+Z1LK4QAIDKx0wwEIBsNkNXXFDX/fn/XXuhLomvpTxT+mHnEff43TPXaNaPydp/3PeWtgEA+LagoCDVaJ+oGu0TFRTke/OuvlcRUIKIkCAlP51odRnV1u1dmuruK87T7iMZ+mTtXr323Q5J0sodR7RyxxH9c95mtW5wpqWi8BvtAAAoSWhoqOr0vdd97GuYCQbg1rROpO7teZ7784euvkAdm9aWYUhbUk+6x7s//Z3+MmuNPlyVYkWZAAB4jJlgAKUa1aOZ7r+qhdLSc7Rw0wE99vkmSdLJbKcWbT6oRZsPus99eM5GdW5WRx2a1laTmHCrSgYA+AjTNOXKPOE+9jWEYADnFFsjVDdc2sgdgj+6u7PW7j6u5dsP6+eko5KkBb8e0IJfD0iSIkLs7mu/3XJQHRJi1DA6rOoLBwBYJjMzU3tfuy3/eFKiIkOjLa6oKEIwgHK7uHEtdWkeq5HdE9T68UWSpPt7n6+Ne09o/e5jOpVzZvm1Bz7aIEmKiQxRq/o13eO/H0ov0mcMAEBVIgQD8Ip7e56niJAgufJM/brvuAa98YMkqWX9mtp5KF1HMxz6vtDKEwNf/17BdkPNYiPdY99sPqCW9aOUUCey2P0BAPAmQjAAr7LbDF0Qd2bGd+7obrIZhrYdOKW1Kcf0xJdbJOW3TGQ6XNp+MN197oOzN7qP6xdqn3h7xS41r1tD8bUjFFsjpAq+CgBAdUcIBlDpwoLtahdfSy3iarhD8OpHr9KxzFz9uveE7n0/f9OOdo2jlXwkUyeycnXgxJnd615c/HuJ971rxho1rh2h+tFhqlMoHKel56hRLXuJ1wAAIBGCAVjEMAw1rh2hmMgz4fXDu7soIiRIxzIc2pp6Ure+/bMkaUC7Bko9nq09xzJ18GSO+/z89oojf7y1rnh2qWyGitz7kU9/Vb2aoYqpEaKaoWf+05eclqEG0eGKCg+uhK8SAOCrCMHwa2yiUT3VjgzRJU1quT9/5saLFRGS/5+rYxk5av/kt5Kkf1/fVkfTHUo9ma19x7K0bPthSZJhSHmmlJbucN/ji437S3zWta+udF8TFXYmCP9l1lrViQxRZKHA/Pn6fapTI1RRYcEKCTLc405XnodfMQCgqhGCAfiV0OAzbQ7Xt2/kDseZDqd7pYqNj1+t7Nw8pRzL1E1TfpSUv/HHqRynjqQ7dPhUtpb/niZJigy1KyPHJdOUTmTluu+94vTrhf3f3E0l1nTxpMUKD7arRliQIgstD3ff++tUMyxYocFn9iV6Z0WSoiOCFR5sl912Jkiv3X1MNUKDFGy3yZV3JlTvPZapyNBg5eS63GNHMxzKyc2TzTCU7TyzEkeWwyWbkX9Ph/PMPZyuPHdQLxzYfXHdTgDVR1BQkCLbXuU+9jW+VxEAeCjIblO98BDVCDvzn7hRPZqVGJhXP9pHQTabjmc5lHoiWwNf/16S9K9BbZXlcOlweo7eWr5LknT5+bHKcDh1KtupE1m5OnzqTGtGVq5LWbkuHS5Ux5JthT/L98Li7SXWPPSdVSWO931pRbGxy59ZUuK5Hf71bYnjF09aXOJ4mwnfyDDkDs6S1Onf3yrYblOw3VYkpF8/+QeFBdkUEmQrcv64ORsVERKkIPuZsclLdigyNFghQTZJZ4L217+mKjI0WMF2o8jW2xv3HFeN0GDl5p0J+nuOZqpGWLDshiGH68x4Ro5TdpuhIBsbngK+LjQ0VLGJf3Mf+xpCMICAFxJkU72aYapRqPXhhkvzZ5kzHU53CH5rWIcSg/QPj/SSK09Kz3EqLT3HHWifGNhGuS5TxzMdeu27HZKkQZc0lMOVp0yHS+nZTq3ZfUyS1LROhFx5ppwuUw5Xno5m5LdyhAfblWeayjNN5bq8P3NrmpKr0IxwRo5LkqvYedsOnCrx+vmnN0gp7PUlO0s896E5v5Q4/uf//lxsrN/LxcO/JHX69//cx4WyuLo//Z3Cg+0KDjoTjoe9s0rhIXaFBtlkKxTon/xqi8KDgxQSZJNRKKTP/CFZkadn4/MKfU++3XpQNUKDSgzudpshZ6GZ+/3Hs1QzLFg2w1BuofCenevKv28es++AryAEA4CHakWEFAnHBW7q0NgdpAtC8H9uuKjEIP31X3uUOL72n33c9ygY2zypr8KDg5RnmkrPceqSJ/Jnelc/epXCQ4JkmqYyHU51/s93kqQfx/dW+Ok2kkyHU92ezp9JXvH3ngoNtiszx6mezy87XcflCrbb5HCaSs/J1ZA3f5Ik/XdYh9PBLk+nsp0a90l+oH3kmpbKk3Qq26kpS/PD75COjZVn5rdkZOU6tXjLIUnSZQm1lWdKua485Tjz9NvpYN24drhceaZyXXnuPu7wELvy8sz8XwxKCY6FuzmOZebqmHKLvF7wC8YffbhqT4njTy/cVuL4Ax9uKDZWUnCXpD4vLi9x/NIni8/St5v0jew2Q3bDKBLSr3h2iXtchYL+ta+sUJC96Az4TVN+lN1myGYUnnOXbn/7ZwXZbfnjhV64a8YaBdltMv4wPuaD9QqyGTIL3eVvszecvoehvEJB/x+f/qJgm00yVCTUP/b5JgXbbEV+KZjwxWYFn6658C8RT361RcF2mwxJuYXG/7Ng6+mWoDNjzyz8TcG2/JoL/yL4/KJt7l96XIXGX1y8XcF2W5HWn5e//V3Bp/9a4Sx07qv/+/1MHYXOf+27HSWOv7Ekf7zw2OQlOxRsz///V+FffKYs3VniPaYu26kQu02OEsYkFRl/c9mu039RKdri9NbyXcXq+O/yXe7vR26hc99esavE+grGC4+9syKpxHuUOr4yqcS6C8YdTpfynPn/v9x3LFMt4nxrgyRCMAD4GeN0aLLJcP8DKUmRoUHuIF24lSE6PNg9Xvj8OjVC8wN26Jng3rROZImBvvv5sUXGC0LwsG4J7pBeEIIn/qlNiYF++h2XlTj+zd+uKBb01z7Wp+RfCh7ro5Agm5wuU6dycnXFs0slSfPGdJeh/L7ugpn4F4e0k2FIObl5Ss9x6l/zt0qSRvc8T6by/zHPdDj1welQnHhR/dMh3VR2rksrd+T3hbdrHC1TUo4zzz0jXhDcnXmmnK48HcvM/4c+NMh21uBeWK6r5Nn9wm/oLCz5SGaxsS2pJ0s8d13K8RLHC29YU9h3vx0qNrZo88ESz/1yY2qJ45+t21dsbM6avSWeW9ovIu/9lFJsbMYPu0s8d9r3ySWOv70iqdhYwV9z/mjqspLHC36W/+iNEv7KUdpfPgp+8f2jV/9XfLykMUl65X8lLw/58rfFx18qYUwqfYnJksZLa9cqdfybc4/bgvLfcLx9/zFCMAAAFRUeYneH48JvOGxRr4Y7SBe4pm39IkG6IASP6X1+kfGCEPzc4HYlBu+CpftKCu5/PHf941e7x9Ozc9V24jeS8mfpQ4LsSs/OVffTPd3/e+gKhdjz210ycpzulUrmju6mYLtNpillOHJ1y1v5s84z77hMIUE2ZTmcGjl9jSTpzaGXKsRulylTWQ6X7vtgvSTp5ZvbKfj0vXOcLj18uhXl6RsuUpDdJtM0lePM02Of57/Zc+KfWivYblOOM8+9lvejiRcq2GbL/wUg1+WeKR/Xr6WC7YbM07P9BQHpwT4tTv8VIc8d3u7vfb6CbDaZMuVw5mny6XB575XN3bPaDmee3jwdUu/q0Sz/Hq48d5i98/Jmsp+exc115rnD74huCe46cvPy3GF5eNemsp+ekZ75Y/7Y0C5NTn/dkjMvzx22b+vcREGnf2HMzTP1wc/547deFi/76b5zZ16eO7Tf0ileQXZDTpepj1bnj93cKf70LHr+G08/Ph38B3ds7O5dd+bluX8hGNyhsfsec9bmj93UoVGRcz9Zm/8LxY2XFh3/9PQvGjdc2khBNkPOPNP9y0fBWP65Z8avb190fO76/PFB7Rsq6PT36fP1+4uMFTzPPX5JQ/f3w5WXp883lG082+HQ7A8/kCTVuaO9fA0hGACASlC4xaFglj640BsIG0SHlzjr3rJ+zRLHOybULhb0e7SoW+K5fdsU/QWgIAT/6ZKGRcYLQvCQjvHuexeE4Ns6Ny1ybkEIHtk9och4QQi++4rm7nsUhOCC7dQLzi0Iwfdf1aLIeEEI/tvVF7jvURCCx/a9oMi5BSH479e0LDJeEIL/0f9C9z0KQvD4a1sVObcgBD+aWHS8IAQ/dl3rIuMFIfjxAa3d9y4IwRMGFD23IARP+sNfRApC8KSBbdz3KAjBTwxsW+TcghD85KCi4wUh+F+nxzMdTnfY/dcfzi0Y//f1RccLQvB/rr/IfY+CsFswVnCue/wPbVwFYfdc44ePndDkoS9Lks57b6J8DW+vBQAAQMBhJhjVDhtoAACAc2EmGAAAAAGHEAwAAICAQzsEAAAAvM5utyuiZXf3sa8hBAMAAMDrwsLCVHfQePexr6EdAgAAAAGHEAwAAICAQzsEAgZLpwEAUHUyMjK0+5nr8o//cVwRIdEWV1QUM8EAAAAIOIRgAAAABBxCMAAAAAIOIRgAAAABhxAMAACAgEMIBgAAQMBhiTQEPJZOAwDA++x2u8Kbd3Qf+xpCMAAAALwuLCxM9QZPdB/7GtohAAAAEHAIwQAAAAg4tEMAAADA6zIyMpTy4o35x/846HPbJhOCgRLwZjkAADxn5uZYXUKpaIcAAABAwCEEAwAAIOAQggEAABBwCMEAAAAIOLwxDigH3jAHAED1QAgGAACA19lsNoXGt3Uf+xpCMAAAALwuPDxc9W992n3sa3wvlgMAAACVjJlgwAvoFQYAwL8QggEAAOB1GRkZ2vPqrfnH/0hh22QAAAAEhrysk1aXUCp6ggEAABBwmAkGKgl9wgAA+C5mggEAABBwCMEAAAAIOLRDAFWMNgkAAKxHCAYAAIDX2Ww2hdRv4T72NYRgAAAAeF14eLgaDH/JfexrCMGAj6BNAgCAquN7c9MAAABAJWMmGAAAAF6XmZmpvVPuyD9+5HdFhERZXFFRhGDAx9EmAQDwR6ZpynXykPvY19AOAQAAgIBDCAYAAEDAoR0C8EO0SAAA4BlmggEAABBwmAkGqhFmiAEAKBtCMAAAALzOMAwF12niPvY1hGAAAAB4XUREhBreOdl97GsIwUAAoE0CAICieGMcAAAAAg4zwUCAYnYYAFCZMjMztf/t0fnHj/zKtskAAACo/kzTVO6RFPexr/HbdojJkyerWbNmCgsLU4cOHbRixYqznp+Tk6NHH31UTZs2VWhoqM477zxNmzatiqoF/EfBDHHy04mKCOH3ZABA9eSX/8LNnj1bDz74oCZPnqzu3bvrzTffVP/+/bVlyxY1adKkxGuGDBmigwcP6p133tH555+vQ4cOyel0VnHlgP+ifQIAUJ34ZQh+8cUXNWrUKN15552SpJdfflmLFi3SlClT9NRTTxU7f+HChVq2bJl27dqlmJgYSVJCQkJVlgwAAAAf4nftEA6HQ2vXrlXfvn2LjPft21c//PBDidd88cUX6tixo5599lk1atRIF1xwgR5++GFlZWWV+pycnBydPHmyyAeAomidAAD4K7/7VystLU0ul0txcXFFxuPi4nTgwIESr9m1a5dWrlypsLAwzZ07V2lpaRo9erSOHj1aal/wU089pUmTJnm9fiAQ0DoBAPB1fheCC/xx+z3TNEvdki8vL0+GYej9999XdHS0pPyWiptuuklvvPGGwsPDi10zfvx4jR071v35yZMnFR8f78WvAAg8hGMACByGYcgeVc997Gv8LgTHxsbKbrcXm/U9dOhQsdnhAg0aNFCjRo3cAViSWrVqJdM0tXfvXrVo0aLYNaGhoQoNDfVu8QCKKS0YE5gBwL9FRESo8b3T3Me+xu96gkNCQtShQwctXry4yPjixYvVrVu3Eq/p3r279u/fr/T0dPfY9u3bZbPZ1Lhx40qtF4B3ldaHTH8yAKA8/PJfirFjx2ro0KHq2LGjunbtqrfeekspKSm65557JOW3Muzbt08zZ86UJN1666168sknNXLkSE2aNElpaWkaN26c7rjjjhJbIQBUD8wyAwBK45ch+Oabb9aRI0f0xBNPKDU1VW3bttWCBQvUtGlTSVJqaqpSUlLc59eoUUOLFy/W/fffr44dO6pOnToaMmSI/vWvf1n1JQDwQSWFY4I0AFRMVlaWUmf8Lf94/FpFhNS0uKKi/DIES9Lo0aM1evToEl+bPn16sbELL7ywWAsFAHibN4J0Zd3D1+vzlXtU9r2BQJGXlyfHgd/dx77GMH1xM2cfdPLkSUVHR+vEiROKioqyuhwAAACflpGRoRo1akiS0tPTFRkZWenPLE9e87s3xgEAAACeIgQDAAAg4BCCAQAAEHAIwQAAAAg4frs6BAAAAHxbbGys1SWUihAMAAAAr4uMjNThw4etLqNUtEMAAAAg4BCCAQAAEHAIwQAAAPC6rKws9ezZUz179lRWVpbV5RRDTzAAAAC8Li8vT8uWLXMf+xpmggEAABBwCMEAAAAIOIRgAAAABBxCMAAAAAIOIRgAAAABh9UhAAAAUCkiIiKsLqFUhGAAAAB4XWRkpDIyMqwuo1S0QwAAACDgEIIBAAAQcAjBAAAA8Lrs7GwlJiYqMTFR2dnZVpdTDD3BAAAA8DqXy6UFCxa4j30NM8EAAAAIOIRgAAAABBxCMAAAAAIOIRgAAAABhxAMAACAgMPqEGVkmqYk6eTJkxZXAgAA4PsK7xZ38uTJKlkhoiCnFeS2syEEl9GpU6ckSfHx8RZXAgAA4F8aNmxYpc87deqUoqOjz3qOYZYlKkN5eXnav3+/atasKcMw3OMnT55UfHy89uzZo6ioKAsr9G98Hz3H99A7+D56B99Hz/E99A6+j57zp++haZo6deqUGjZsKJvt7F2/zASXkc1mU+PGjUt9PSoqyud/MPwB30fP8T30Dr6P3sH30XN8D72D76Pn/OV7eK4Z4AK8MQ4AAAABhxAMAACAgEMI9lBoaKgmTJig0NBQq0vxa3wfPcf30Dv4PnoH30fP8T30Dr6Pnquu30PeGAcAAICAw0wwAAAAAg4hGAAAAAGHEAwAAICAQwgGAABAwCEEe9G6det09dVXq1atWqpTp47uvvtupaenW12W39m+fbsGDhyo2NhYRUVFqXv37lqyZInVZfmVpUuXyjCMEj9Wr15tdXl+Zf78+ercubPCw8MVGxurG264weqS/E5CQkKxn8NHHnnE6rL8Uk5Oji655BIZhqENGzZYXY7f+dOf/qQmTZooLCxMDRo00NChQ7V//36ry/IrycnJGjVqlJo1a6bw8HCdd955mjBhghwOh9WllRsh2Ev279+vPn366Pzzz9fPP/+shQsXavPmzRoxYoTVpfmdxMREOZ1Offfdd1q7dq0uueQSXXfddTpw4IDVpfmNbt26KTU1tcjHnXfeqYSEBHXs2NHq8vzGp59+qqFDh2rkyJHauHGjvv/+e916661Wl+WXnnjiiSI/j4899pjVJfmlv//972rYsKHVZfitXr166eOPP9a2bdv06aefaufOnbrpppusLsuv/Pbbb8rLy9Obb76pzZs366WXXtLUqVP1f//3f1aXVn4mvOLNN98069WrZ7pcLvfY+vXrTUnm77//bmFl/uXw4cOmJHP58uXusZMnT5qSzG+//dbCyvybw+Ew69WrZz7xxBNWl+I3cnNzzUaNGplvv/221aX4vaZNm5ovvfSS1WX4vQULFpgXXnihuXnzZlOSuX79eqtL8nvz5s0zDcMwHQ6H1aX4tWeffdZs1qyZ1WWUGzPBXpKTk6OQkBDZbGe+peHh4ZKklStXWlWW36lTp45atWqlmTNnKiMjQ06nU2+++abi4uLUoUMHq8vzW1988YXS0tL4y0Q5rFu3Tvv27ZPNZlP79u3VoEED9e/fX5s3b7a6NL/0zDPPqE6dOrrkkkv073//2y//dGqlgwcP6q677tKsWbMUERFhdTnVwtGjR/X++++rW7duCg4Otrocv3bixAnFxMRYXUa5EYK9pHfv3jpw4ICee+45ORwOHTt2zP2ngdTUVIur8x+GYWjx4sVav369atasqbCwML300ktauHChatWqZXV5fuudd95Rv379FB8fb3UpfmPXrl2SpIkTJ+qxxx7TV199pdq1a+vKK6/U0aNHLa7Ov/z1r3/VRx99pCVLlmjMmDF6+eWXNXr0aKvL8humaWrEiBG65557aGfygn/84x+KjIxUnTp1lJKSonnz5lldkl/buXOnXnvtNd1zzz1Wl1J+Vk9F+7oJEyaYks76sXr1atM0TfP999834+LiTLvdboaEhJgPP/ywGRcXZz7zzDMWfxXWK+v3MS8vz/zTn/5k9u/f31y5cqW5du1a89577zUbNWpk7t+/3+ovw3Ll+XkssGfPHtNms5mffPKJRVX7lrJ+D99//31Tkvnmm2+6r83OzjZjY2PNqVOnWvgV+IaK/CwW+OSTT0xJZlpaWhVX7VvK+j185ZVXzG7duplOp9M0TdNMSkqiHaKQ8v4sHj582Ny2bZv5zTffmN27dzevvfZaMy8vz8KvwDdU5P/T+/btM88//3xz1KhRFlXtGbZNPoe0tDSlpaWd9ZyEhASFhYW5Pz948KAiIyNlGIaioqL00UcfafDgwZVdqk8r6/fx+++/V9++fXXs2DFFRUW5X2vRooVGjRoV8O8or8jP45NPPqnXXntN+/bt409+Kvv38Mcff1Tv3r21YsUKXX755e7XOnfurD59+ujf//53ZZfq0yrys1hg3759aty4sX766Sd17ty5skr0eWX9Ht5yyy368ssvZRiGe9zlcslut+u2227TjBkzKrtUn+bJz+LevXsVHx+vH374QV27dq2sEv1Ceb+P+/fvV69evdS5c2dNnz69SDuovwiyugBfFxsbq9jY2HJdExcXJ0maNm2awsLCdPXVV1dGaX6lrN/HzMxMSSr2fyabzaa8vLxKqc2flPfn0TRNvfvuuxo2bBgB+LSyfg87dOig0NBQbdu2zR2Cc3NzlZycrKZNm1Z2mT6vIv9tLLB+/XpJUoMGDbxZkt8p6/fw1Vdf1b/+9S/35/v371e/fv00e/bsgP4looAnP4sF84A5OTneLMkvlef7uG/fPvXq1UsdOnTQu+++65cBWCIEe9Xrr7+ubt26qUaNGlq8eLHGjRunp59+ml7Wcujatatq166t4cOH6/HHH1d4eLj++9//KikpSYmJiVaX53e+++47JSUladSoUVaX4neioqJ0zz33aMKECYqPj1fTpk313HPPSVLA/2WnPH788Uf99NNP6tWrl6Kjo7V69Wr97W9/c6/XinP74/epRo0akqTzzjtPjRs3tqIkv7Rq1SqtWrVKl19+uWrXrq1du3bp8ccf13nnnRfws8DlsX//fvXs2VNNmjTR888/r8OHD7tfq1+/voWVlR8h2ItWrVqlCRMmKD09XRdeeKHefPNNDR061Oqy/EpsbKwWLlyoRx99VL1791Zubq7atGmjefPmqV27dlaX53feeecddevWTa1atbK6FL/03HPPKSgoSEOHDlVWVpY6d+6s7777TrVr17a6NL8RGhqq2bNna9KkScrJyVHTpk1111136e9//7vVpSHAhIeH67PPPtOECROUkZGhBg0a6JprrtFHH32k0NBQq8vzG99884127NihHTt2FPslzN86bOkJBgAAQMDxzyYOAAAAwAOEYAAAAAQcQjAAAAACDiEYAAAAAYcQDAAAgIBDCAYAAEDAIQQDAAAg4BCCAQAAEHAIwQAAAAg4hGAA8FHTp0+XYRgyDEPJyclWl1Mmubm5atmypQzD0OzZs0s9zzRNRUVFyWazKS4uTkOGDNHu3bvPef/Ro0fLMAwNHz7cm2UDCECEYACA17z22mvavn27WrVqpcGDB5d63s6dO3Xq1CmZpqlDhw5pzpw5uvbaa895//HjxyskJESzZs3S6tWrvVk6gABDCAYAeEV6erqeeuopSdLjjz8um630f2IaNGigX3/9VQsXLlSzZs0kSVu2bNHatWvP+oz4+HgNHz5cpmnqscce817xAAIOIRgA4BVTpkxRWlqa4uPjNWTIkLOeGxkZqbZt26pfv3568skn3eMbNmw453MeeughSdI333zDbDCACiMEAwA85nK59Prrr0uS/vznP591FviPunXr5j7etGnTOc9v2bKlLr30UknSK6+8Us5KASAfIRgA4LHFixcrJSVFknT77beX69qEhATVrFlTUtlCsCTddtttkqRPP/1UJ06cKNfzAEAiBAOAX3M4HJo8ebJ69eqlunXrKiQkRPXr19e1116r9957T3l5eee8R1pamsaNG6cLLrhA4eHhiouL09VXX625c+dKKtsqFR9//LEkqUWLFrrooovK9TUYhqEWLVpIKnsIvvHGGyVJ2dnZmjdvXrmeBwASIRgA/Nbu3bt1ySWX6L777tPSpUuVlpam3NxcHTx4UF9//bWGDh2qK6+8UkePHi31Hhs3blTr1q31/PPP6/fff1d2drYOHTqkb7/9VjfccIP+8pe/lKmWJUuWSJK6dOlS7q9j7dq17l7gAwcO6MiRI+e8pmnTpmrQoIEkaenSpeV+JgAQggHAD6Wnp6t3797aunWrJGnQoEH64osvtGbNGs2ZM0dXXnmlJGnlypW67rrr5HK5it3j2LFjuuaaa3T48GFJ+S0GX3/9tdasWaOPPvpIXbt21VtvvaWpU6eetZa9e/e6Z4g7depUrq/D5XLp7rvvLjJjvXnz5jJdW/CsFStWlOuZACARggHAL02aNEm7du2SJD322GOaO3euBgwYoA4dOuimm27SkiVL3H2zP/74o956661i95g4caIOHDggSXr++ef13nvv6ZprrlGHDh108803a8WKFRo4cKB+/vnns9byww8/uI/bt29frq/jtdde07p164qMlbUlokOHDpKkHTt26NChQ+V6LgAQggHAz+Tk5Ojtt9+WJLVu3VoTJ04sdo5hGJo8ebLq1KkjSe6VGwpkZ2drxowZkqRLL71UY8eOLXYPu92uN998U2FhYWetZ+/eve7jevXqlfnr2Lt3r/75z39KKv8KEX981r59+8r8XACQCMEA4HfWrl2r48ePS5JGjBghu91e4nlRUVHu9Xq3bNmi1NTUIvcoWFVh2LBhMgyjxHvExcWpX79+Z62noJ1CkmrXrl3mr+P+++9Xenq6atasqdmzZ6tWrVqSyh6CY2JiSqwBAMqCEAwAHnA6ne6VEzz5mD59epmfWTgkdu7c+aznFn698HWFjwvaCkrTsWPHs75e+I13ZQ3BX3zxhT7//HNJ0n/+8x81btzYvapEWUNw4WeV5c10AFAYIRgA/Ezh0BkXF3fWc+vXr1/idceOHXMfn6uFoW7dumd9vXC7RFZW1lnPlaSMjAzdf//9kvJD+ujRoyXJHYKPHTum/fv3n/M+hZ8VHh5+zvMBoLAgqwsAAH8WFBTkXqHBEwXLfZVXaW0MBUzTrNB9y6NwSD569Kh744vSPP7440pJSVFwcLD++9//uneXK7y+8KZNm9SwYcOz3qdwqD9XUAeAPyIEA4CHLrzwwip9XuFe2AMHDuiCCy4o9dyDBw+WeF3hVoJDhw6d9R7n6rctHECPHTumpk2blnruxo0b3VsdP/zww0WC78UXX+w+3rRpk/r27XvW5xaezSYEAygv2iEAwM+0bdvWfXyu5ctWrVpV4nVt2rRxH69Zs+as9zjX64WD7Pbt20s9Ly8vT3fffbdcLpfOO+8898oQJdVXlr7ggmdFRkaqefPm5zwfAAojBAOAn+nQoYN7JYUZM2aUuBGGJJ06dcq9nXHr1q2LtFx07NhR0dHRkqRZs2aV2jZx8OBBLVq06Kz1dOzY0d2Tu3r16lLPmzJlijuUT506tVgfb1RUlHsWuSwhuOBZXbp0UVAQf9gEUD6EYADwM6Ghobrzzjsl5e+uNmnSpGLnmKapMWPGKC0tTZI0ZsyYIq+HhYVp2LBhkqR169bpxRdfLHaPvLw8/eUvf1F2dvZZ6wkJCdFll10mqejMc2Gpqal69NFHJeUvydanT58SzyuYVd6yZctZ+5lzcnL0yy+/SJJ69Ohx1voAoCSEYADwQ48//ri7BeDJJ5/UDTfcoK+++krr1q3Tp59+qt69e2vmzJmSpK5du+ruu+8udo+JEye6V494+OGHdfvtt2vRokVat26dPv74Y/Xo0UPz5s1zB1yp9DfiJSYmSsoPwadOnSr2+l//+ledOHFCsbGxeuGFF0r9ugr6gjMyMpSUlFTqecuXL1dubm6RZwNAeRCCAcAP1axZU//73//cb8r747bJS5culSR1795dX331VYkbasTExGjhwoXuN5W9//77RbZN/uGHHzRixAj95S9/cV9T2u5xt956q+x2u7KzszV37twir3399deaM2eOJOmFF15QbGxsqV/XH1eIKM0HH3wgSWrZsuU51zEGgJIQggHATyUkJGjjxo16/fXXdeWVV6pOnToKDg5WXFycrrnmGs2aNUvLly8vsirEH7Vr105btmzRQw89pBYtWig0NFSxsbHq1auXPvjgA7377rs6efKk+/yCPuI/atSokQYOHCgpP0wXyMrK0n333SdJuuqqq9wtGKUpSwguHLQL1hgGgPIyzKpYRBIA4LfuvPNOvfPOO2rcuLH27NlT6nk//fSTunbtKrvdrh07dighIaFS6nnvvfc0dOhQxcTEKDk5+ZzrEgNASZgJBgCUKisrS/PmzZOUvwrD2XTp0kX9+/eXy+XSU089VSn15OXl6T//+Y+k/D5mAjCAiiIEA0AA27lzZ6mrMLhcLt17773uFSaGDx9+zvs988wzstvtevfdd5WSkuLVWiVpzpw52rp1q+Lj4/Xggw96/f4AAgcLKwJAAHvyySe1atUq3XLLLercubPq1aunrKws/fLLL/rvf/+rdevWScrv5y3LKgwXXXSRpk+frh07diglJUVNmjTxar0ul0sTJkxQ7969i60zDADlQU8wAASwESNGaMaMGWc9p3v37po3b57q1KlTRVUBQOUjBANAANu2bZs+/fRTLV68WLt379bhw4eVm5urOnXqqGPHjrr55pt1yy23yGajew5A9UIIBgAAQMDhV3sAAAAEHEIwAAAAAg4hGAAAAAGHEAwAAICAQwgGAABAwCEEAwAAIOAQggEAABBwCMEAAAAIOIRgAAAABBxCMAAAAAIOIRgAAAAB5/8B0IXgnzVIjZwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lassoCV_fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(tuned_lasso.alphas_),\n",
    "            tuned_lasso.mse_path_.mean(1),\n",
    "            yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\n",
    "#ax.set_ylim([50000,250000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('lasso_cv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1b75988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO:  6.154565207638776\n",
      "Ridge:  0.05358981380658186\n"
     ]
    }
   ],
   "source": [
    "# comparing lambdas\n",
    "\n",
    "# tuned alphas\n",
    "print(\"LASSO: \", tuned_lasso.alpha_)\n",
    "print(\"Ridge: \", tuned_ridge.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3baffa1e-9741-4994-945d-015423124e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8171941166276557\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(tuned_lasso.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d9b6b-1ad0-4d57-ac8f-897ca16dfcee",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ffeb132-3aaf-4c21-bd39-1a0010f835f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting sample\n",
    "(X_train,\n",
    " X_test,\n",
    " y_train,\n",
    " y_test) = skm.train_test_split(X,\n",
    "                                df['rmkvaf'],\n",
    "                                test_size=0.3,\n",
    "                                random_state=0)\n",
    "feature_names = list(D.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ada53644-59b1-48d7-a3ae-a28d1735227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAOwCAYAAAAKo+iFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAADn8UlEQVR4nOzdd3zN9//+8etkCLFrRqu1qqWlNT6ttkqCIPaq1ZgxYqT2LqKqVFuKIEaIil1bjASJVXvvrXZRtUfW+f3RX87XamskeZ/xuN9uvX0+OM65HDmeuV6vc94vk9lsNgsAAAAAABvlZHQAAAAAAABeBcUWAAAAAGDTKLYAAAAAAJtGsQUAAAAA2DSKLQAAAADAplFsAQAAAAA2jWILAAAAALBpFFsAAAAAgE2j2AIAAAAAbBrFFgAAAABg0yi2AAAAAACbRrEFAAAAANg0ii0AAAAAwKZRbAEAAAAANo1iCwAAAACwaRRbAAAAAIBNo9gCAAAAAGwaxRYAAAAAYNMotgAAAAAAm0axBQAAAADYNIotAAAAAMCmUWwBAAAAADaNYgsAAAAAsGkUWwAAAACATaPYAgAAAABsGsUWAAAAAGDTKLYAAAAAAJtGsQUAAAAA2DSKLQAAAADAplFsAQAAAAA2jWILAAAAALBpFFsAAAAAgE2j2AIAAAAAbBrFFgAAAABg0yi2AAAAAACbRrEFAAAAANg0ii0AAAAAwKZRbAEAAAAANo1iCwAAAACwaRRbAAAAAIBNo9gCAAAAAGwaxRYAAAAAYNMotgAAAAAAm0axBQAAAADYNIotAAAAAMCmUWwBAAAAADaNYgsAAAAAsGkUWwAAAACATaPYAgAAAABsGsUWAAAAAGDTKLYAAAAAAJtGsQUAAAAA2DSKLQAAAADAplFsAQAAAAA2jWILAAAAALBpFFsAAAAAgE2j2AIAAAAAbBrFFgAAAABg0yi2AAAAAACbRrEFAAAAANg0ii0AAAAAwKZRbAEAAAAANo1iCwAAAACwaRRbAAAAAIBNo9gCAAAAAGwaxRYAAAAAYNMotgAAAAAAm0axBQAAAADYNIotAAAAAMCmUWwBAAAAADaNYgsAAAAAsGkUWwAAAACATaPYAgAAAABsGsUWAAAAAGDTKLYAAAAAAJtGsQUAAAAA2DSKLQAAAADAplFsAQAAAAA2jWILAAAAALBpFFsAAAAAgE2j2AIAAAAAbBrFFgAAAABg0yi2AAAAAACbRrEFAAAAANg0ii0AAAAAwKZRbAEAAAAANo1iCwAAAACwaRRbAAAAAIBNo9gCAAAAAGwaxRYAAAAAYNMotgAAAAAAm0axBQAAAADYNIotAAAAAMCmUWwBAAAAADaNYgsAAAAAsGkUWwAAAACATaPYAgAAAABsGsUWAAAAAGDTKLYAAAAAAJtGsQUAAAAA2DSKLQAAAADAplFsAQAAAAA2jWILAAAAALBpFFsAAAAAgE2j2AIAAAAAbBrFFgAAAABg0yi2AAAAAACb5mJ0AAAAzp49q2vXrhkdAy8pa9asevPNN42OAQBwYBRbAIChzp49q0KFCunevXtGR8FLcnd31+HDhym3AADDUGwBAIa6du2a7t27p7CwMBUqVMjoOHhBhw8flq+vr65du0axBQAYhmILALAKhQoVUvHixY2OAQAAbBDFFgDgcMxms0wm03/+WqtWrTR58uRn3i4hIUHx8fFydXX918fq1q2btm3bpg0bNkiShgwZosuXLytXrlzq06ePJk2apN27d8vd3V0//vijAgMDdfLkSaVNm1ZBQUFycfl7VM+cOVNRUVG6evWqJk6cqOzZs+u7777TpUuXVK5cOX388ccKCAhQlixZ1LBhQ5UrV+5lnx4AAGwOV0UGADiM0NBQ+fv7a9asWWrevLmuXbsmX19fxcTEqHz58vrxxx+1detWNWrUSP3799edO3eeuo9Lly5p6NChatmy5XNd8Oqnn37SO++8I0l6+PChLl68qDFjxujcuXN6+PChtm7dqnHjxiljxoz6/ffflSpVKrm6uipr1qyWUitJjRs31qRJk9SiRQtt3rxZO3bs0L59+xQfHy8PDw9t3rxZ9evX14QJExQaGppkzxkAALaAHVsAgEOpUaOGqlSpok8//VSenp4KCQlRqlSplCVLFnXv3l2jR49Wz549VaRIETVo0OCx3+vn5yd3d3cFBASoYMGCkqS1a9dq7ty5ltt88sknatas2TMf+88//1S2bNkkSdmyZdP169fl5PT3GvMbb7yhixcvqk+fPjKZTAoKCtK6detUtmxZy++PiYnRkiVLNGbMGC1evFgFCxbUwIED5efnp3Hjxqlfv37auXOnbt++naTPGQAA1o4dWwCAQ8mQIYMk6cqVK8qUKZOlBCb+vNlslqurq5ydneXs7PzY7+3YsaNcXV0VFBSkTZs2vfBjv/baa7p69aok6erVq3rttdcUHx8vSbpw4YI8PDwsb4POnj37YwU1NjZWnTp10tdffy13d3d5eHgoV65ccnZ2lpOTk9zd3TVy5Eh98803ypQp0wtnAwDAlrFjCwBwODExMRo9erTWrl2r1q1bq1SpUpZfa9CggQYNGqS8efM+9fuKFSumYsWK6c6dO5o5c6Zy586tcuXK/evnWYcOHapNmzapV69e+v777+Xh4aHOnTvrjTfekJubm+WzsalSpVKePHk0bNgwnT59Wnfu3FFISIhCQkJUtGhRhYeH68iRI/r+++/VpEkTff7555o5c6Y6dOggb29v3bp1SwEBAXr48KH69euXLM8bAADWymQ2m81GhwAAOK5du3apRIkS2rlzJ1dFtkH8/QEArAFvRQYAAAAA2DSKLQDA7vzbm5FatWr1UvfzT/f5Im98Gjp0qAoXLmz58aRJk9S+fXt1795dkvTDDz+oVatW6tSpk8xms0JDQ1WrVi35+/vrjz/+UHR0tMqXLy9/f38dOnRIp06dkr+/v2rVqqWVK1cqPj5eDRs2VKtWrTRkyBBJUocOHeTr66vvvvtO0t/HDQUEBGjo0KGPZTt//rwCAgIUEBCgCxcuaP369apSpYrCwsIst7l+/boKFCig8+fPP/efGQCAlMBnbAEAVmP69On67bff5OHhof79+8vf319Zs2ZVZGSktm3bZjlXNjAwUK1atdKePXu0fv16xcXFacSIEfLz89M777yjUqVKKSoqSrdu3ZKbm5uGDBmitm3bKmvWrNq3b99Tj7tt2zbNmjVLd+/eVZs2bbRs2TLFxMTo008/1ciRI+Xj46PSpUsrODhYLi4uatasmeLj4zV27FhVqFBBbdu2fa4/X58+fXTy5EnLj7du3arJkydr8ODB+v3337V9+3bNnTtX48eP1+bNm+Xk5KQ0adLIzc1NmTJlkslkUrp06WQ2m5UjRw5lyZJFwcHB+uOPPzR27FiVLl1aHh4eGjlypJo3by5JGjt2rCSpbdu2luOGxo4dq/bt2+vhw4dyc3OTJAUFBcnd3V0xMTHKnDmzypQpo4SEhMdK7OjRo1WrVq2X/NsFACD5sGMLALAaly5dUrFixdS+fXvt379fhQsX1pAhQ5QrV65n3t5sNsvJyUm7d+/WlStXlJCQoA4dOihLlizavn270qdPr3Pnzmnfvn0qVKiQvvvuO+XMmfOp+xk/frwyZsyoHDlyaNeuXZIkX19fVatWzXIM0LZt29SlSxdNmDBB06ZNkyR5eXk9Vmr37t0rf39/y38jRoz41z/vk0f9NG3aVAEBAdq6dasuXryoJk2aaNasWfL29tbs2bNVpkwZLV68WAEBAQoKCpIkLVmyRDVr1lSlSpXk7u6uP//8U9WqVVPJkiUlSYcOHVLVqlX17rvvPvO4oUR79uxR27ZtVatWLc2cOfOprBs2bFDRokWVLl26f/0zAQBgBIotAMBq9OzZU8WLF1fz5s2f+RbfxKNw7t27J0maN2+ehg0bpvfff1/37t2Ts7Oz0qZNK7PZrFKlSikwMFDTp0+XJLm6ukqSUqVK9dT9ms1m9e7dW4MHD1abNm0k/d/xP48eA/RkjsRfe1lPHvVTrVo1jRkzRu+++64KFCjw1NE/zzoKqEaNGtq4caMmTZqkXbt26X//+5+WLVum/fv3S5IKFy6s8PBw7dy585nHDSXy8PCQh4eHXnvtNd25c+eprBs3btT69eu1cuVKhYSEvNKfGwCApMZbkQEAVmPixIk6duyYsmXLpqJFi2rcuHHq06ePLly4IEl688039cMPP2jnzp2SpNdff13Dhg2zlLhE77//vkJCQtS9e3fFxcVp5MiRmjp1qkaMGKHTp08/9bjt27eXv7+/MmfOLB8fn2dma9CggXr37i1XV1c1bdrUUkof9cEHHyg4OPhf/3ybNm2Sv7+/goKCnjrqZ+rUqdq8ebNee+01ffjhh5o8ebK2bdumGzduaOzYsVq0aJHCw8N148YNDR48WFu3btW0adN0//591alTR4UKFdKIESN06NAhZciQQTdu3FDfvn0VHx+vwoULK3Xq1E8dN9S2bVtNmDBBAQEBat++veLj4zV8+HDt379fI0aM0N27d/XWW2+pT58+kqTAwED5+fk9318oAAAphON+AACGep7jYhI/Wwvrw3E/AABrwI4tAMDqJXWp3bJli1auXClJSp06tXr37p2k9w8AAFIWxRYA4HBKlSqlUqVKJcl9JeVu8vjx47VlyxbdvXtXU6dOVWRkpH7++Wd99913Kl26tObPn6/IyEidPHlSdevWVaVKldS5c2e5u7uradOm//g2agAA7B3FFgDgkB49WmjAgAH65ptvdO3aNZUpU0b16tXTRx99pHLlyikmJkYZM2bUgQMHNGfOHH3xxRcqVaqUjh8/rp9//tlyf2FhYdqzZ49u3LihESNGqGvXrsqYMaMqVKjw3IVz9+7dmjp1qn766SedPXtWderU0a1btyy/XrduXdWtW1cdO3ZUvXr1tHPnTvn6+srT01PDhw+n2AIAHBZXRQYAOKRHjxZKSEhQfHy8MmXKpAULFkj6+yrBw4YN07Fjx9SvXz8VLlxY58+fl7Ozs3r06KEaNWooMjLScn8hISFKly6d0qRJoyNHjujOnTuqXLmyKlasaLnN+fPnHzsOqF+/fo9l8vHxkY+Pj6Kjo1WwYMFn5n7w4IFu376trFmzqnjx4powYYLq16+vBg0aJMOzBACAbaDYAgAc0qNHC+3atUtZs2bVwIEDdf/+fUmynPeaM2dOubi4yNXVVTExMYqPj1dCQoJiYmIsx+9Ifx/BExgYqDFjxuijjz7S5MmTdePGDfXs2fO5My1cuFCrVq2Sn5+f1q5d+8zbLF68WNWrV5ckTZs2TSNHjtTq1as1ZcqUl30qAACwebwVGQDgkB49Wih//vz64YcfdOfOHbm4/PtodHZ2Vr9+/XTmzBmFhIRoyZIlkqSKFSuqQ4cOMpvN6tOnj4YNGyZnZ2cVLVrU8nvfeOONfz0OqEiRIvL399e1a9c0evRoRUdHa9q0acqYMaNee+01FS5cWEuXLrWcI+vt7a0hQ4bI3d1dZcuWTYJnBQAA28RxPwAAQ9nacTEcPfQ4W/v7AwDYJ96KDADAC6DUAgBgfSi2AAA8IjAwUOfPn0+y+xswYICaN2+uTp06SZImTZqk4sWLWx7j/PnzCggIUEBAgC5cuKD58+fL399f3t7ej71t+ebNm6pdu7aaN2+ukJAQXbx4UXXr1lWbNm20du1anTt3Ti1btlT9+vU1d+7cJMsPAIAtoNgCAByKv7+/Hj58qFOnTmnYsGHauHGjevfurTZt2igmJsZyu1atWkmSQkNDtXHjRm3btk1dunRRmzZttGPHjud+vG+++UahoaF68OCBJKl169aqUaOG5deDgoLk7u4uFxcXZc6cWXXr1lVwcLDeeecd1atXz3K7y5cv67PPPtOUKVP022+/afPmzapfv74mTJig0NBQ5c6dW1OmTNHcuXO1cuXKV32aAACwKVw8CgDgUCpXrqxVq1bp8OHDqlevnq5evaq4uDhdvXpVe/bs+cffN378eL311ltKly6ddu3apZIlS0qS9u7dq/Hjx1tuV7BgQXXt2tXy48uXL6tTp07KlSvXM+93z549GjdunM6dO6eZM2eqVatWjx3pkyh37tzasGGDIiIi1LFjR1WoUEH9+vXTzp07dfv2bcvtJkyYwNE/AACHw44tAMCh+Pj4KCIiQidPnlSBAgU0YcIE/fDDDypdurTu3btnuV3iUT6JP2c2m9W7d28NHjxYbdq0ee7Hy5kzp+bMmaPY2FjduHHjqV/38PCQh4eHXnvtNd25c0fS40f6JFq+fLlatWqliIgIhYeHy93dXSNHjtQ333yjTJkySZJmzZolJycnVapU6UWeEgAAbB47tgAAh+Lm5qbY2FjlzZtX0t9H7AwePFi7d+9WiRIlLLcrUaKEvv32W+3du1dFixZV+/bt5e/vr8yZM8vHx0cVK1aUJH3wwQf/eoRPx44dFR8frzRp0ihTpkyaP3++li1bppMnT+rHH39UQECA2rdvr/j4eA0fPlySHjvSZ9WqVbp3754+/fRTde7cWcuWLVP+/Pl169YtBQQE6OHDh+rXr59OnDih3r17y8fHRzdv3lT37t2T6ykEAMDqcNwPAMBQHBdj2/j7AwBYA96KDAAAAACwaRRbAAAAAIBN4zO2AACrcPjwYaMj4CXw9wYAsAYUWwCAobJmzSp3d3f5+voaHQUvyd3d/bGjiQAASGlcPAoAYLizZ8/q2rVrRsewOHr0qNq1aycPDw+NGzdOGTNmNDTPzZs31a5dO12+fFnBwcEqWLCgoXmelDVrVr355ptGxwAAODCKLQAAj9i9e7cqVKigvHnzKjIyUpkzZzY6kiTp+vXrqlixok6fPq01a9boww8/NDoSAABWg4tHAQDw/+3atUvly5dXvnz5tHr1aqsptZL02muvKTIyUvny5VP58uW1e/duoyMBAGA1KLYAAEjauXOnKlSooAIFCigyMlKZMmUyOtJTMmfOrMjISOXPn1/ly5fXrl27jI4EAIBVoNgCABzejh07VKFCBRUsWNBqS22iTJkyKSIiQm+//bbKly+vnTt3Gh0JAADDUWwBAA5t+/bt8vb21rvvvqtVq1YZfqGo55FYbt955x1VqFBBO3bsMDoSAACGotgCABzWtm3b5O3trUKFCtlMqU2UMWNGrVq1Su+++64qVKig7du3Gx0JAADDUGwBAA5p69at8vb21nvvvaeVK1cqQ4YMRkd6YYnltnDhwqpQoYK2bdtmdCQAAAxBsQUAOJwtW7aoYsWKKlKkiM2W2kQZMmTQqlWr9P7778vb21tbt241OhIAACmOYgsAcCibN29WxYoVVbRoUa1YsULp06c3OtIrS58+vVauXKkiRYrI29tbmzdvNjoSAAApimILAHAYv/32mypWrKgPP/zQbkptovTp02vFihX68MMPValSJf32229GRwIAIMVQbAEADmHTpk2qVKmSihcvruXLlytdunRGR0py6dOn1/Lly1WsWDFVqlRJmzZtMjoSAAApgmILALB7GzZsUKVKlVSyZEm7LbWJ0qVLp/DwcJUoUUKVK1fWxo0bjY4EAECyo9gCAOza+vXr5ePjo48++kjLli1T2rRpjY6U7BLLbcmSJVW5cmVt2LDB6EgAACQrii0AwG6tW7dOVapU0ccff+wwpTZR2rRpFR4ero8++kg+Pj5av3690ZEAAEg2FFsAgF2Kjo5WlSpVVKpUKS1dulTu7u5GR0px7u7uWrZsmT7++GP5+Pho3bp1RkcCACBZUGwBAHYnKipKVapU0WeffeawpTaRu7u7li5dqk8//VRVqlRRdHS00ZEAAEhyFFsAgF1Zu3atqlatqs8//1yLFy9WmjRpjI5kOHd3dy1ZskSfffaZqlSporVr1xodCQCAJEWxBQDYjdWrV6tq1aoqU6aMFi1aRKl9RJo0abR48WJ9/vnnqlatmtasWWN0JAAAkgzFFgBgFyIjI1W9enV5enpSav9BYrktW7asqlWrptWrVxsdCQCAJEGxBQDYvIiICNWoUUPlypXTwoULlTp1aqMjWa3UqVNr4cKF8vLyUvXq1RUZGWl0JAAAXhnFFgBg01atWqUaNWqofPnyWrBgAaX2OaROnVoLFixQuXLlVL16da1atcroSAAAvBKKLQDAZq1cuVI1a9aUt7e35s+fLzc3N6Mj2YzEcluhQgXVrFlTK1euNDoSAAAvjWILALBJy5cvV82aNVWxYkX9+uuvlNqX4Obmpvnz56tixYqqVauWVqxYYXQkAABeCsUWAGBzwsPDVbt2bfn4+FBqX5Gbm5vmzZunSpUqqVatWlq+fLnRkQAAeGEUWwCATVm2bJlq166tKlWqaO7cuUqVKpXRkWxeYrn18fFR7dq1tWzZMqMjAQDwQii2AACbsXTpUtWpU0fVqlWj1CaxVKlSae7cuapatarq1KmjpUuXGh0JAIDnRrEFANiExYsXq27duqpRo4bmzJkjV1dXoyPZnVSpUmnOnDmqXr266tatqyVLlhgdCQCA50KxBQBYvUWLFqlevXqqWbOmZs2aRalNRq6urpo9e7Zq1KihevXqafHixUZHAgDgP1FsAQBWbeHChfriiy9Uu3ZtzZw5k1KbAlxdXTVr1izVqlVL9erV08KFC42OBADAv6LYAgCs1vz581W/fn3VrVuXUpvCXF1dNWPGDNWpU0f169fXggULjI4EAMA/otgCAKzSr7/+qgYNGqhevXoKCwuTi4uL0ZEcTmK5rVevnurXr6/58+cbHQkAgGei2AIArM68efPUsGFDNWjQQNOnT6fUGsjFxUXTp09X/fr11aBBA82bN8/oSAAAPIXvFAAAVmXOnDn68ssv1bBhQ4WGhlJqrYCLi4t++eUXmUwmNWrUSGazWfXr1zc6FgAAFny3AACwGrNnz9aXX36pL7/8UlOnTpWzs7PRkfD/JZZbJycnNW7cWGazWQ0aNDA6FgAAkii2AAArMXPmTDVp0kS+vr6aMmUKpdYKOTs7KzQ0VCaTSY0bN1ZCQoIaNWpkdCwAACi2AADjzZgxQ02bNlWTJk0UEhJCqbVizs7Omjp1qkwmk3x9fWU2m9W4cWOjYwEAHBzFFgBgqLCwMDVr1kzNmjXTpEmTKLU2wNnZWVOmTJGTk5OaNGmihIQE+fr6Gh0LAODAKLYAAMP88ssvat68uVq0aKFJkybJyYmL9dsKZ2dnhYSEyGQyqVmzZjKbzWrSpInRsQAADopiCwAwxLRp09SiRQv5+flpwoQJlFob5OTkpMmTJ8vJyUnNmjVTQkKCmjVrZnQsAIADotgCAFLc1KlT5efnp1atWik4OJhSa8OcnJw0ceJEmUwmtWjRQmazWc2bNzc6FgDAwVBsAQApasqUKWrVqpXatGmjcePGUWrtgJOTk2XXvWXLljKbzWrRooXRsQAADoRiCwBIMZMnT1br1q3l7++vsWPHUmrtiJOTk8aPHy+TySQ/Pz8lJCTIz8/P6FgAAAdBsQUApIhJkyapTZs2at++vYKCgmQymYyOhCTm5ORk2YVv1aqVzGazWrVqZXQsAIADoNgCAJLdxIkT1bZtW3Xo0EFjxoyh1NoxJycnjR07ViaTSa1bt1ZCQoLatGljdCwAgJ2j2AIAklVwcLDatWungIAAjRo1ilLrAEwmk4KCguTk5KS2bdvKbDarbdu2RscCANgxii0AINmMGzdOHTp0UKdOnTRy5EhKrQMxmUwaPXq0nJyc5O/vr4SEBLVr187oWAAAO0WxBQAki7Fjx6pjx47q3LmzRowYQal1QCaTST///LNMJpPat28vs9ms9u3bGx0LAGCHKLYAgCQXFBSkgIAAdenSRT/99BOl1oGZTCbLbn2HDh1kNpvVoUMHo2MBAOwMxRYAkKRGjx6tTp06qVu3bvrhhx8otZDJZNKIESPk5OSkjh07KiEhQQEBAUbHAgDYEYotACDJ/Pzzz+rSpYt69Oih77//nlILC5PJpB9//FFOTk766quvlJCQoE6dOhkdCwBgJyi2AIAkMXLkSHXt2lU9e/bUsGHDKLV4islk0vDhw2UymdS5c2eZzWZ17tzZ6FgAADtAsQUAvLIRI0aoW7du6t27t7777jtKLf6RyWTS999/LycnJ3Xp0kVms1ldunQxOhYAwMZRbAEAr+THH39Ujx491LdvX3377beUWvwnk8mkoUOHysnJSV27dlVCQoK6detmdCwAgA2j2AIAXtrw4cPVq1cv9evXT4MHD6bU4rmZTCYNGTJEJpNJ3bt3V0JCgnr06GF0LACAjaLYAgBeyvfff6/evXurf//+GjRoEKUWL8xkMunbb7+Vk5OTevbsKbPZrJ49exodCwBggyi2AIAXNnToUPXt21cDBgxQYGAgpRYvzWQy6ZtvvpHJZFKvXr2UkJCg3r17Gx0LAGBjKLYAgBcyZMgQff311woMDNTAgQONjgM7kFhunZyc1KdPHyUkJKhv375GxwIA2BCKLQDguX377beWtx4PGDDA6DiwM4m7//369ZPZbFa/fv2MjgQAsBEUWwDAc/nmm280cOBAffPNN+rfv7/RcWCnBg4cKJPJpK+//loJCQl8rQEAngvFFgDwnwIDAzVo0CB9++237KIh2Q0YMEBOTk7q37+/EhISeMs7AOA/UWwBAP/IbDYrMDBQ33zzjb777jv16dPH6EhwEF9//bWcnJwsb0sODAw0OhIAwIpRbAEAz2Q2mzVw4EANHjxYQ4cO5Uq1SHF9+/aVyWRS3759LeWWK3ADAJ6FYgsAeIrZbFb//v01ZMgQff/995wtCsP06dNHTk5O6t27txISEixHAwEA8CiKLQDgMWazWV9//bW+++47/fDDD+revbvRkeDgevXqJScnJ/Xs2VNms1mDBw+m3AIAHkOxBQBYmM1m9e3bV8OGDdOPP/6obt26GR0JkCT16NFDJpNJPXr0UEJCgoYMGUK5BQBYUGwBAJL+LrW9e/fW8OHDNWLECHXp0sXoSMBjunfvLicnJ3Xr1k0JCQkaOnQo5RYAIIliCwDQ36W2V69e+uGHHzRy5Eh17tzZ6EjAM3Xt2lUmk0ldu3aV2WzWsGHDKLcAAIotADg6s9msHj166KefftKoUaP01VdfGR0J+FddunSRk5OTOnfurISEBA0fPpxyCwAOjmILAA7MbDarW7duGjlypMaMGaOOHTsaHQl4Lp06dZKTk5O++uorJSQk6Mcff6TcAoADo9gCgIMym83q2rWrfv75ZwUFBalDhw5GRwJeSEBAgEwmkwICAmQ2m/XTTz9RbgHAQVFsAcABmc1mde7cWaNHj9bYsWPVvn17oyMBL6Vjx44ymUzq2LGjEhISNHLkSMotADggii0AOBiz2axOnTppzJgxGj9+vPz9/Y2OBLySDh06yMnJSe3bt5fZbNbPP/9MuQUAB0OxBQAHYjabFRAQoLFjx2rChAlq06aN0ZGAJNGuXTs5OTnJ399fCQkJGj16NOUWABwIxRYAHERCQoI6duyo8ePHa+LEiWrdurXRkYAk1bZtW5lMJrVt21YJCQkKCgqi3AKAg6DYAoADSEhIUIcOHTRhwgRNnjxZfn5+RkcCkkWbNm3k5OSk1q1by2w2KygoSE5OTkbHAgAkM4otANi5hIQEtW/fXhMnTtTkyZPVsmVLoyMByapVq1YymUyWcjt27FjKLQDYOYotANixhIQE+fv7a/LkyQoJCVGLFi2MjgSkCD8/P5lMJrVq1UoJCQkaP3485RYA7BjFFgDsVEJCgtq2bauQkBBNnTpVzZo1MzoSkKJatmwpJycntWzZUgkJCZowYQLlFgDsFMUWAOxQQkKCWrduralTpyo0NFRNmzY1OhJgiObNm8tkMqlFixYym82aOHEi5RYA7BDFFgDsTHx8vFq1aqVffvlFv/zyi3x9fY2OBBiqWbNmcnJyUrNmzZSQkKDJkydTbgHAzlBsAcCOxMfHy8/PT9OnT9cvv/yiL7/80uhIgFVo0qSJTCaTmjVrJrPZrMmTJ8vZ2dnoWACAJEKxBQA7ER8fr5YtWyosLEzTp09X48aNjY4EWBVfX185OTmpSZMmMpvNCgkJodwCgJ2g2AKAHYiPj1fz5s01c+ZMzZgxQw0bNjQ6EmCVGjduLJPJJF9fXyUkJGjq1KmUWwCwAxRbALBx8fHxatasmWbPnq2ZM2eqQYMGRkcCrFqjRo3k5OSkL7/8UmazWaGhoZRbALBxFFsAsGFxcXFq1qyZ5syZo1mzZumLL74wOhJgExo0aCCTyaTGjRsrISFB06ZNk4sL3xYBgK3iX3AAsFFxcXFq0qSJ5s2bp9mzZ6tevXpGRwJsSv369WUymdSoUSOZzWb98ssvlFsAsFH86w0ANiguLk6+vr6aP3++5syZo7p16xodCbBJX3zxhZycnNSwYUOZzWZNnz6dcgsANshkNpvNRocAADy/2NhY+fr6asGCBZo7d65q165tdCTA5i1YsEANGjRQnTp1NGPGDMotANgYii0A2JDY2Fg1btxYixYtotQCSWzhwoWqX7++ateurRkzZsjV1dXoSACA50SxBQAbERsbq0aNGmnJkiWaN2+eatasaXQkwO4sXrxYX3zxhWrUqKFZs2ZRbgHARlBsAcAGxMTEqGHDhlq2bJnmz5+v6tWrGx0JsFtLlixRvXr1VL16dc2ePZtyCwA2gGILAFYuJiZGDRo00PLlyzV//nxVq1bN6EiA3Vu6dKnq1q2ratWqafbs2UqVKpXRkQAA/4JiCwBWLCYmRvXr19eKFSu0YMECVa1a1ehIgMNYtmyZ6tatKx8fH82dO5dyCwBWjGILAFbq4cOH+uKLL7Rq1SotXLhQVapUMToS4HCWL1+u2rVrq3Llypo3bx7lFgCsFMUWAKzQw4cPVa9ePUVGRmrRokWqXLmy0ZEAh7VixQrVrl1bFStW1Lx58+Tm5mZ0JADAEyi2AGBlHj58qLp162r16tVavHixKlWqZHQkwOGtXLlStWrVUoUKFTR//nzKLQBYGSejAwAA/s+DBw9Up04drVmzRkuWLKHUAlaicuXKWrx4sVavXq06derowYMHRkcCADyCHVsAsBIPHjxQ7dq1FR0drSVLlsjb29voSACeEBERoZo1a8rLy0sLFixQ6tSpjY4EABDFFgCswoMHD1SrVi2tX79eS5cuVfny5Y2OBOAfrF69WtWrV5enp6cWLlxIuQUAK0CxBQCD3b9/X7Vq1dKGDRu0bNkylStXzuhIAP7DmjVrVL16dX3++edatGiR0qRJY3QkAHBoFFsAMND9+/dVs2ZNbdy4UeHh4fLy8jI6EoDntHbtWlWrVk2lS5fW4sWLKbcAYCAuHgUABrl3755q1KihTZs2afny5ZRawMaUK1dOy5cv16ZNm1SjRg3du3fP6EgA4LDYsQUAA9y7d0/Vq1fXli1btHz5cpUtW9boSABe0rp161SlShV98sknWrJkidzd3Y2OBAAOhx1bAEhhd+/eVbVq1bR161atWLGCUgvYuLJly2rFihXasmWLqlevzs4tABiAHVsASEGJpXbHjh1asWKFSpcubXQkAElkw4YN8vHx0UcffaSlS5cqbdq0RkcCAIdBsQWAFHLnzh1VrVpVu3bt0sqVK/XZZ58ZHQlAEtu4caN8fHxUokQJhYeHU24BIIVQbAEgBdy5c0dVqlTRnj17tHLlSn366adGRwKQTDZt2qTKlSurePHiCg8PV7p06YyOBAB2j2ILAMns9u3bqlKlivbu3atVq1bpk08+MToSgGT222+/qXLlyvrwww+1fPlyyi0AJDOKLQAko9u3b8vHx0f79+/XqlWrVKpUKaMjAUghmzdvVqVKlfTBBx9o+fLlSp8+vdGRAMBuUWwBIJncunVLPj4+OnDggCIiIvTxxx8bHQlACtuyZYsqVaqkIkWKaMWKFZRbAEgmFFsASAa3bt1S5cqVdejQIUVEROijjz4yOhIAg2zdulUVK1bU+++/rxUrVihDhgxGRwIAu0OxBYAkdvPmTVWuXFmHDx9WZGSk/ve//xkdCYDBtm3bpooVK6pQoUJauXKlMmbMaHQkALArFFsASEI3b95UpUqVdPToUUVGRqpkyZJGRwJgJXbs2CFvb2+98847WrVqFeUWAJKQk9EBAMBe3LhxQxUrVtSxY8e0evVqSi2Ax5QsWVKrV6/W0aNHVbFiRd24ccPoSABgN9ixBYAk8Ndff6lixYo6efKkVq9ereLFixsdCYCV2rVrlypUqKACBQooIiJCmTJlMjoSANg8dmwB4BX99ddf8vb21qlTp7RmzRpKLYB/Vbx4ca1Zs0YnT56Ut7e3/vrrL6MjAYDNY8cWAF7B9evX5e3trTNnzmjNmjX68MMPjY4EwEbs2bNH5cuXV968eRUZGanMmTMbHQkAbBY7tgDwkq5fv64KFSro999/19q1aym1AF7Ihx9+qLVr1+rMmTOqUKGCrl+/bnQkALBZFFsAeAl//vmnypcvr3Pnzmnt2rX64IMPjI4EwAZ98MEHWrt2rc6ePUu5BYBXwFuRAeAFXbt2TRUqVNCFCxe0du1aFSlSxOhIAGzc/v37Va5cOb3xxhtavXq1smTJYnQkALAp7NgCwAu4du2aypcvr4sXLyoqKopSCyBJFClSRFFRUbpw4YLKly+va9euGR0JAGwKxRYAntPVq1dVrlw5Xb58WVFRUXr//feNjgTAjrz//vuKiorSpUuXKLcA8IIotgDwHK5cuaJy5crpypUrioqK0nvvvWd0JAB26L333lNUVJQuX76scuXK6erVq0ZHAgCbQLEFgP+QWGqvXr2qqKgoFS5c2OhIAOxY4cKFFRUV9diCGgDg33HxKAD4F3/88YfKlSun69evKyoqSu+++67RkQA4iCNHjsjLy0tZsmTR2rVrlT17dqMjAYDVYscWAP7B5cuX5eXlpb/++kvR0dGUWgAp6t1331VUVJT+/PNPeXl56Y8//jA6EgBYLYotADzDpUuX5OXlpZs3byo6OlrvvPOO0ZEAOKB3331X0dHR+uuvv+Tl5aXLly8bHQkArBLFFgCekFhqb9++rejoaBUsWNDoSAAc2DvvvKPo6GjdvHlTXl5eunTpktGRAMDq8BlbAHjExYsX5eXlpbt37yo6OloFChQwOhIASJKOHz8uLy8vpUuXTlFRUfLw8DA6EgBYDXZsAeD/u3Dhgjw9PXXv3j1KLQCr8/bbbys6Olp3796Vp6enLl68aHQkALAaFFsA0N+l1svLSw8ePKDUArBaBQoUUHR0tO7fvy9PT09duHDB6EgAYBUotgAc3vnz5+Xp6amHDx8qOjpa+fPnNzoSAPyj/PnzKzo6Wg8ePJCnp6fOnz9vdCQAMBzFFoBDO3funDw9PRUbG6vo6Gjly5fP6EgA8J/y5cun6OhoxcTEyNPTU+fOnTM6EgAYiotHAXBYZ8+elZeXl+Lj4xUdHa08efIYHQkAXsjp06fl5eUlZ2dnRUdHK3fu3EZHAgBDsGMLwCH9/vvv8vT0VEJCgtatW0epBWCT8ubNq+joaCUkJMjT01Nnz541OhIAGIJiC8DhnDlzRp6enpKk6OhovfXWW8YGAoBXkCdPnsfK7e+//250JABIcRRbAA4lsdQ6OTlRagHYjbfeekvr1q2TJHl6eurMmTPGBgKAFEaxBeAwTp8+LU9PT7m4uCg6Olpvvvmm0ZEAIMm8+eabWrdunZycnCi3ABwOxRaAQzh16tRjpZYLrACwR7lz51Z0dLRcXFxUtmxZnT592uhIAJAiKLYA7N7Jkyfl6ekpNzc3rVu3Tm+88YbRkQAg2SSW21SpUqls2bI6deqU0ZEAINlRbAHYtcRSmzp1akVFRen11183OhIAJLs33nhD0dHRSp06tTw9PXXy5EmjIwFAsqLYArBbJ06cUNmyZeXu7q7o6GhKLQCH8vrrrysqKkpp0qSRp6enTpw4YXQkAEg2FFsAdun48eMqW7as0qVLp+joaOXKlcvoSACQ4hLLbdq0aeXp6anjx48bHQkAkgXFFoDdOXbsmDw9PZUhQwZFR0fLw8PD6EgAYJhcuXIpKipK6dOnp9wCsFsUWwB25ejRo/L09FTGjBkVFRWlnDlzGh0JAAzn4eGhqKgoZciQQWXLltXRo0eNjgQASYpiC8BuHDlyRJ6ensqcOTOlFgCekDNnTkVHRytz5szy8vKi3AKwKxRbAHbh8OHD8vLyUpYsWRQVFaUcOXIYHQkArE6OHDm0du1aZc6cWZ6enjpy5IjRkQAgSVBsAdi8Q4cOycvLS1mzZtXatWuVPXt2oyMBgNXKkSOHoqKilCVLFnl6eurw4cNGRwKAV0axBWDTDh48KC8vL2XPnp1SCwDPKXv27IqKilL27Nnl6empQ4cOGR0JAF4JxRaAzTpw4IC8vLyUM2dOrV27VtmyZTM6EgDYjGzZsmnNmjXKkSOHvLy8dPDgQaMjAcBLo9gCsEkHDhxQuXLllCtXLq1Zs0ZZs2Y1OhIA2Jxs2bJp7dq1ypkzp7y8vHTgwAGjIwHAS6HYArA5+/fvl5eXl15//XVKLQC8osTrE7z++usqV66c9u/fb3QkAHhhFFsANmXv3r3y8vJS7ty5tWbNGmXJksXoSABg87JkyaLVq1frjTfeULly5bRv3z6jIwHAC6HYArAZe/fuVfny5fXWW29p9erVeu2114yOBAB2I7Hc5s6dW+XKldPevXuNjgQAz41iC8Am7NmzR+XKlVOePHkotQCQTF577TWtXr1ab731lsqXL689e/YYHQkAngvFFoDV27Vrl8qVK6d8+fIpMjJSmTNnNjoSANitxHKbJ08elS9fXrt37zY6EgD8J4otAKu2a9cuVahQQQUKFKDUAkAKyZw5s1avXq38+fOrfPny2rVrl9GRAOBfUWwBWK2dO3eqfPnyevvttxUZGalMmTIZHQkAHEamTJkUERGht99+WxUqVNDOnTuNjgQA/4hiC8Aqbd++XRUqVNA777yjiIgIZcyY0ehIAOBwEsttwYIFVaFCBe3YscPoSADwTBRbAFZn27Zt8vb2VqFChSi1AGCwjBkzatWqVXr33XdVoUIFbd++3ehIAPAUii0Aq7J161Z5e3vrvffe08qVK5UhQwajIwGAw0sst4ULF5a3t7e2bdtmdCQAeAzFFoDV2LJliypWrKgiRYpQagHAymTIkEErV67Ue++9J29vb23dutXoSABgQbEFYBU2b96sihUrqmjRolqxYoXSp09vdCQAwBMSy22RIkXk7e2tLVu2GB0JACRRbAFYgd9++02VKlXShx9+SKkFACuXPn16rVixQh988IEqVqyozZs3Gx0JACi2AIy1adMmVapUScWKFdPy5cuVLl06oyMBAP5DYrn98MMPVbFiRW3atMnoSAAcHMUWgGE2btyoSpUqqWTJkpRaALAx6dKl0/Lly1WiRAlVrlxZGzduNDoSAAdGsQVgiA0bNqhy5cr66KOPtGzZMqVNm9boSACAF5QuXTqFh4erZMmSqly5sjZs2GB0JAAOimILIMWtW7dOPj4++vjjjym1AGDj0qZNq2XLlumjjz6Sj4+P1q9fb3QkAA6IYgsgRUVHR6tKlSoqVaqUli5dKnd3d6MjAQBeUWK5/fjjj+Xj46N169YZHQmAg6HYAkgxUVFRqlq1qj799FNKLQDYGXd3dy1dulSffvqpqlSpoujoaKMjAXAgFFsAKWLt2rWqWrWqPvvsMy1ZskRp0qQxOhIAIIm5u7tryZIl+uyzz1SlShWtXbvW6EgAHATFFkCyW7NmjapWraoyZcpo8eLFlFoAsGNp0qTR4sWL9fnnn6tatWpas2aN0ZEAOACKLYBktXr1alWrVk2enp5atGgRpRYAHEBiuS1TpoyqVaum1atXGx0JgJ2j2AJINhEREapevbq8vLy0cOFCpU6d2uhIAIAUkjp1ai1atEienp6qXr26IiMjjY4EwI5RbAEki1WrVqlGjRoqX748pRYAHFTq1Km1cOFClStXTtWrV1dERITRkQDYKYotgCS3cuVK1axZU97e3po/f77c3NyMjgQAMEjq1Km1YMECVahQQTVq1NCqVauMjgTADlFsASSp5cuXq2bNmqpYsaJ+/fVXSi0AQG5ubpo/f768vb1Vs2ZNrVixwuhIAOwMxRZAkgkPD1ft2rXl4+NDqQUAPMbNzU2//vqrKlWqpFq1amn58uVGRwJgRyi2AJLEsmXLVKdOHVWpUkVz585VqlSpjI4EALAybm5umjdvnnx8fFS7dm2Fh4cbHQmAnaDYAnhlS5cuVZ06dVS1alXNmTOHUgsA+EepUqXS3LlzVaVKFdWuXVtLly41OhIAO0CxBfBKFi9erLp166pGjRqUWgDAc0kst9WrV1fdunW1ZMkSoyMBsHEUWwAvbdGiRfriiy9Us2ZNzZo1S66urkZHAgDYCFdXV82ePVs1atRQvXr1tHjxYqMjAbBhFFsAL2XhwoX64osvVKtWLc2cOZNSCwB4Ya6urpo1a5Zq1qypevXqaeHChUZHAmCjKLYAXtj8+fNVv3591a1bl1ILAHglrq6umjlzpurUqaP69etrwYIFRkcCYIMotgBeyK+//qoGDRqoXr16CgsLk4uLi9GRAAA2ztXVVTNmzFDdunXVoEEDzZ8/3+hIAGwMxRbAc5s3b54aNmyo+vXra/r06ZRaAECScXFxUVhYmOrVq6cGDRpo3rx5RkcCYEP4rhTAc5kzZ46+/PJLNWzYUKGhoZRaAECSc3Fx0fTp0+Xk5KRGjRrJbDarfv36RscCYAP4zhTAf5o9e7Z8fX3VqFEjhYaGytnZ2ehIAAA75eLiol9++UUmk0mNGzeW2WxWgwYNjI4FwMpRbAH8q5kzZ6pJkyb68ssvNXXqVEotACDZOTs7a9q0aXJyclLjxo2VkJCgRo0aGR0LgBWj2AJ4ytWrV+Xl5aWWLVuqR48eatKkiUJCQii1AIAU4+zsrKlTp8pkMsnX11eXL19WSEiIoqKilC1bNqPjAbAyJrPZbDY6BADrMn78eAUEBCghIUG+vr7s1AIADBMfH6/mzZtrxowZMplMGjt2rPz9/Y2OBcDKUGwBPKVQoUI6cuSI0qVLp4SEBP31119KlSqV0bEAAA4oJiZGmTNnlpOTk+7cuaNChQrp0KFDRscCYGU47gfAY8xms44eParUqVOrUaNGioiIoNQCAAyTKlUqRUREqFGjRkqdOrWOHj0q9mUAPIkdWwBPuXTpkrJly8aRPgAAqxIbG6urV68qV65cRkcBYGUotgAAAAAAm8Z2DBzK2bNnde3aNaNj4AVkzZpVb775ptExAMDhMDPtDzMV9oxiC4dx9uxZFSpUSPfu3TM6Cl6Au7u7Dh8+zCAGgBTEzLRPzFTYM4otHMa1a9d07949hYWFqVChQkbHwXM4fPiwfH19de3aNYYwAKQgZqb9YabC3lFs4XAKFSqk4sWLGx0DAACrx8wEYCs47gdIQv91Lbbo6GiFhYW91H0nJCQoNjb2P29XunRp+fv7KyQkRJJUs2ZN+fv7a/jw4ZKkUaNGyc/PT19++aXMZrMCAwPVpEkT+fv7Ky4u7rGs5cuXl7+/vw4dOqQbN26oadOmatKkiXbv3i1JCg4OVkBAgOWxAAB4Fd98840kqVWrVo/977Nu86gzZ87o22+/lZ+fn27duiVJmjt3rubNm/ePj/Xw4cP/zPPkzJw4caL8/f1VqlQprVy5UvPnz5e/v7+8vb0VHBwsSbp+/boKFCig8+fP6+bNm6pdu7aaN2/+1Kzs2rWr2rVr99jPjxo1yvJnHjp0qPz9/fXee+/p8OHD/5kVcHTs2AJJIDQ0VFu2bFGZMmW0d+9excbGqkSJEipbtqyCg4N1+fJlde/e3XL7U6dOadSoUYqPj1eVKlX0559/6rfffpOHh4cGDBjw2H1funRJoaGhOnr0qIYOHSoPD49/zZIuXTo9ePDA8jajtGnTKi4uTq+//rokqVOnTpKkLl266P79+0qVKpVcXV2VNWvWx473MZlMSpcuncxms3LkyKGFCxeqbdu2KlmypLp27aoBAwYoPDxcefPmVfbs2ZPkeQQA2JeDBw9q6NChKliwoM6ePasuXbro22+/Va5cudSvXz+1bt1apUqV0vHjx/Xzzz/r7NmzT93HlStX1LFjR7355ptq27at5TYTJ07UwYMH9eabb6pu3bqSpHr16mnBggVq3ry5wsPDNWHChMfu6969e5ozZ47WrVunFi1aqGzZsv+a/8mZ2aZNG0mSr6+vKlSoIBcXF9WtW1cdO3ZUvXr1JEmjR49WrVq1JEmXL1/WZ599pq5du6p169by8/OTJJ0+fVpZsmRRv3795OfnJz8/P504cUKpU6e2PHafPn0kSQ0bNuTt4MBzoNgCSaRGjRqqUqWKoqOjVbVqVVWqVEk3btzQgwcPlC5dOi1ZskQfffSRpL93OtOnTy8XFxft2bNHLi4uKlasmOrUqfPYffr5+cnd3V0BAQEqWLCgJGnt2rWaO3eu5TaffPKJmjVrZvnxihUrZDab1ahRI3l7e2vGjBkymUzy9fVVvXr1ZDKZ1KZNGz18+FBubm7q06ePTCaTgoKCtG7dOsuQL1OmjMqWLasDBw4oKChIrq6u8vLykpubm2JjY3Xq1CllzpxZo0ePVpMmTVS9evXkfooBADYmJCREY8eOVVxcnHr16qWbN28qe/bsatu2rV577TU5OzurR48eWrZsmSIjI595Hw8ePFDq1Knl6+urt99+2/Lzmzdv1tSpUyX9vWMrSRUrVlSzZs1Uo0YNpUuX7rGiGBwcrIiICHXq1EktWrSQJJ0/f17ffvut5TZZsmTRkCFDLD+OiYl5bGZK0oULF5Q9e3bLYvCDBw90+/ZtZc2aVRs2bFDRokW1b98+SVLu3Lm1YcMGRUREqGPHjpb7vXTpkmXB2dnZ2ZLv+++/1/bt2y2327Jliz7++OMXecoBh8VbkYEkkiFDBklSUFCQ3Nzc1K5dO4WFhalZs2Zq0qTJY1eWTEhIkL+/vwIDA9W3b1/17NlTxYsXV/PmzR97O3DHjh3l6uqqoKAgbdq06blymEwmOTk5KVWqVJYfS1LGjBkVExOjVKlSKTQ0VCVKlND+/fstv549e3bdvn37sft59Odz5sypixcv6uHDh3J1dZWHh4dy5colSXJ1dX3Zpw0AYOfMZrPlozqffvqpOnfurJ9++kk7d+5UfHy8EhISFBMTY5k7T3rzzTf1/fffa/78+Vq4cKHl5591e2dnZ+XMmVM///yzGjRo8NivVa9eXcWKFdOMGTP066+/KiYm5j+zPzkzJSksLEyNGze23Gbx4sWWxd2NGzdq/fr1WrlypUJCQrR8+XK1atVKERERCg8Pt/yexJkqSfHx8frjjz907tw5devWTVu2bNGhQ4ckSTNmzFCjRo3+MycAdmyBJNe/f3/Fxsbq7bffVqlSpRQUFKSMGTPK3d3dcpv27durb9++ypYtm0qWLKnbt2/r2LFjypYt22NvBy5WrJiKFSumO3fuaObMmcqdO7fKlSuncuXKPfOxb926pfbt28vFxcWywtu8eXO5uLjIw8ND6dOn18CBA3X16lXFxMToq6++0rBhw3T69GnduXNHISEhCgkJUdGiRXXhwgWFh4frxo0bGjx4sHLkyKHOnTvLZDIpICBAefLkUVxcnDp37qyiRYsm75MKALBJfn5+6tixo/LkyaN06dIpKipKy5Yt061bt5QjRw45OzurX79+OnPmjEJCQrRkyZKn7mP//v2aMmWK/vzzT9WuXdvy85988ok6d+6sPHnyWN76K0mNGzfWF198oUGDBj12P6+//rr69++v+Ph4rVixQps2bZKXl5fls7HP8uTMlKQdO3aoV69eltssXbrU8jnZxLcPBwYGys/PT05OTurcubOWLVum/Pnz69atW/r22281fPhwXb16VQEBAfrkk0+UI0cOzZkzR9LfnysuXLiwYmJi9Ndffylnzpwv+KwDjslk/q+r3QB2YteuXSpRooR27tzJFR5tBH9nAGCMpPr398qVKxozZowuX75suU7Do1q1aqXJkye/alw8B2Yq7B07tgAAAEgW2bNn1+DBg//x1ym1AJIKn7EFXtG/venhWccUPM/9/NN9vugbLB49NkCSZs2apZo1a0r6+zNCbdq0Ue3atXX79m2tX79eVapUsRxHNHPmTLVu3Vq1atXSlStXJP19EY1SpUpp48aN2rVrl/z8/FSnTh2tX79e586dU8uWLVW/fv3HLm4lSZMmTVL79u0tV4b+4Ycf1KpVK3Xq1Elms1nTp09Xs2bN1KZNG929e/eF/owAANthrTNz6NChKly4sOXHHTp0ULNmzdSvXz9JT8+tHTt2qGnTpmratKlu3br11Mzcv3+//P391axZMzVs2NByv3Xq1LHM2SeP+xkwYICaN29uuRLz+PHj1axZM9WrV++xa2A8+dhPHs936tQp+fv7q1atWlq5cuVzPweArWPHFg5r+vTpliN2+vfvL39/f2XNmlWRkZHatm2b5e1RgYGBatWqlfbs2aP169crLi5OI0aMkJ+fn9555x2VKlVKUVFRunXrltzc3DRkyBC1bdtWWbNmtVwV8VHbtm3TrFmzdPfuXbVp00bLli1TTEyMPv30U40cOVI+Pj4qXbq0goOD5eLiombNmik+Pl5jx45VhQoV1LZt2+f68z15bMD169d1+vRpZcuWTdLfRxX4+vpq+PDhunLlisqUKaOEhASdP39e0t+fUWrcuLEWL16szZs3q2bNmho3bpyqVasmSSpevLhCQkJ08+ZNBQYGauTIkZoyZYokWQpuoq1bt2ry5MkaPHiwfv/9d23fvl1z587V+PHjtXnzZi1fvlxhYWGKiIjQ4sWLH7soBwDAePY+M/v06aOTJ09afjx27FhJf88zSU/NrbCwME2ePFlbtmzRwoUL1axZs6dmZnBwsGbOnKm0adNK+nvB+LPPPpP07ON+Es/nTcy8e/duTZ06VT/99JPOnj2r9957T5I0ZcqUxx478fPLicfzZcmSRcHBwfrjjz80duxYVa5c+WX+ygGbw44tHNalS5dUrFgxtW/fXvv371fhwoU1ZMgQy5V+n2Q2m+Xk5KTdu3frypUrSkhIUIcOHZQlSxZt375d6dOn17lz57Rv3z4VKlRI33333TMv+DB+/HhlzJhROXLk0K5duyT9XTKrVaumLFmyqHv37tq2bZu6dOmiCRMmaNq0aZIkLy+vxwb03r175e/vb/lvxIgRjz1OcHDwY6vfI0eOfOyoAUnq1q2b1q5dqxw5cjzzzxwTE6MlS5bI29tbJ06ckKurq954443HbjNy5EjLsQmSNGHChKeuROnk9Pc/NW+88YYuXryopk2bKiAgQFu3btXFixfVqVMnffXVV1q5cqXlKpEAAOth7zPzWXbt2qV8+fJJ0lNzKy4uTqlSpVLu3Lktc+vRmZlo+fLlqlKliq5fv64TJ06oRIkSlufzyeN+Ll++rAYNGlguNunj4yMfHx9FR0dbjvyT9NRjlylTRosXL1ZAQICCgoIkSUuWLFHNmjVVqVKl//xzAvaCYguH9egRO896u1LiMQKJx/TMmzdPw4YN0/vvv6979+7J2dlZadOmldlsVqlSpRQYGKjp06dL+r/jbxKP3HmU2WxW7969NXjwYMtB74lHBSX+76N5EnMk/trzeNaxAQcOHNCAAQO0ZcsWbdy4UZL0008/qVWrVlq9evVT9xEbG6tOnTrp66+/lru7u9avX6/Dhw9r5syZlrdNjRgxQiVLlrRcFXnWrFlycnJ6apDGx8dL+vvsPw8PD1WrVk1jxozRu+++qwIFCqhUqVIaO3asPvnkExUoUOC5/5wAgJRhzzPzWQ4ePKhp06ZZ3or85NxydnZWbGysZa49OTOlv8tr1qxZ5erqqq1bt+rcuXMKCgpSWFiYsmbN+thxP9LfRwDNmTNHsbGxunHjhhYuXKhVq1bJz89Pa9eutWR78rGfdWxfjRo1tHHjRk2aNOmVngfAlvBWZDisiRMnWo7YKVq0qMaNG6c+ffrowoULkv4+N++HH37Qzp07Jf19TMCwYcMs59glev/99xUSEqLu3bsrLi5OI0eO1NSpUzVixAidPn36qcdt3769/P39lTlzZvn4+DwzW4MGDdS7d2+5urqqadOmlqH3qA8++OAfjyh41rEBiWf/tWrVSqVLl9bo0aN1+PBh3blzR6NGjdL+/fs1YsQI3b17V2+99ZbWrFmjI0eO6Pvvv1eTJk3UsmVLtWzZUqGhoSpQoICio6M1ceJEeXp66urVq/r888/Vu3dv+fj46ObNm+revbvatm2rCRMm6OOPP1ZAQIBSpUqlPHnyaOrUqdq8ebNee+01ffjhh1qyZImWLVsmJycny9u/AADWw55nZuKfb9OmTfL391dQUJBq1aqlMmXKqF27dho3bpymTZv22Nxq0aKFWrduLbPZrNGjR2vIkCGPzczPPvvssfNuH919PX/+vAoWLPjYcT/S32fXx8fHK02aNMqUKZOKFCkif39/Xbt2TaNHj1ZgYKD8/f2feuxFixY9djzf1q1bNW3aNN2/f1916tR5jr9dwD5w3A8cxvNe5p6jB6wHRxMAgDGYmfaHmQp7x44t8ISkHtBbtmyxXJUwderU6t27d5LePwAARmFmArAWFFsgmZUqVUqlSpV6pftIyhXxoUOH6vfff9eGDRv066+/atmyZTp69KjSpk2rn3/+WWFhYVq9erXc3Nw0cuRIubm5qX///rp9+7bq16+vMmXKJEkOAACelBQz858k5SydNGmSxo8fryVLluiNN97Q4sWLNWvWLMXExGjUqFFauHCh9u3bpzt37ig0NPSxUwoAJA+KLZCMHj0eYcCAAfrmm2907do1lSlTRvXq1dNHH32kcuXKKSYmRhkzZtSBAwc0Z84cffHFFypVqpSOHz+un3/+2XJ/YWFh2rNnj27cuKERI0aoa9euypgxoypUqPCPnz16Up8+fSRJDRs2VKFChTRw4MB/PXonVapU+vPPP+Xs7CwPD4/keJoAAPhH1jhLW7dubfl8sfT3TvOYMWMUHR2tffv26auvvpL093nyR48e1QcffJCkzwmAp1FsgWSUeDxCnTp1lJCQoPj4eGXKlEkLFixQvXr15OHhoWHDhqlatWpatGiRBg8erPPnz8vZ2Vk9evTQsmXLFBkZabm/kJAQlS1bVg8fPtSRI0d0584dNWjQQOXKlbPc5vz58/r2228tP86SJYuGDBnyWK4tW7bo448/lvR/Rxjcvn1b2bJlsxy94+Lioty5cys2NlalSpVSgwYN1K1bt3+9+AYAAEnNWmfpo2rWrKmGDRsqISFBixYtkiTduHFD+/fvV0BAQNI/KQCewnE/QDJ69HiEXbt2KWvWrBo4cKDu378vScqWLZukvy/x7+LiIldXV8XExCg+Pl4JCQmKiYmxXMZf+vtS/oGBgRozZow++ugjTZ48WTdu3FDPnj1fKNeMGTPUqFEjSU8fYfDk0TseHh7KlSuX3N3dFRcXl0TPDAAAz8daZ+mjJkyYoMjISP3000+aPXu2bt++ra5du2r48OGWs9wBJC92bIFk9OjxCPnz59cPP/ygO3fuyMXl3196zs7O6tevn86cOaOQkBAtWbJEklSxYkV16NBBZrNZffr00bBhw+Ts7Gw5R1aS3njjjX/dVY2JidFff/2lnDlzStJ/Hr1z584dderUSb/++qu+/PLLJHhWAAB4ftY4S+fPn69ly5bp5MmT+vHHH+Xl5aU2bdro1q1b6tevnwICAnT9+nX17dtXnTt31rvvvps0TwaAf8RxP3AYtnSZe45P+Jst/Z0BgD2xh39/maWPs4e/U+Df8N4IwAoxiAEAeDXMUsCxUGyBFBYYGKjz588nyX09fPhQzZs3l6+vr0aPHi1J6tChg5o1a6Z+/fpZbrdhwwaVKFFCkrRixQq1bdtW1apV07lz5yy3uXnzpmrXrq3mzZsrJCREklSoUCH5+/tbLoTx66+/qmPHjho+fHiS5AcA4Hkk5exMNGrUKLVq1cry//38/PTll1/KbDZr4sSJ8vf3V6lSpbRy5UrdunVLXbt2VceOHXXgwAFJ0vXr11WgQIGncj05Ozt06CBfX1999913z3xsAEmDYgskMX9/fz18+FCnTp3SsGHDtHHjRvXu3Vtt2rRRTEyM5XaJAy00NFQbN27Utm3b1KVLF7Vp00Y7dux4rsdyc3NTaGio5egCSRo7dqymTZumS5cuSfr7M7UrVqxQsWLFJEk+Pj6aMGGCmjRpouPHj1vu6/Lly/rss880ZcoU/fbbb5KkdOnS6f79+3rzzTcVHx+vqVOnysXFRVmzZn3l5wkAgEQpOTsl6cSJE4+dLdupUyeFhIQoe/bsun//vtq0aaPg4GAVKFBAFSpU0MSJE+Xs7Cyz2azs2bNLkkaPHq1atWo9dd+Pzk7p77kcFham33///ZmPDSBpUGyBJFa5cmWtWrVK8+bNU7169eTs7Ky4uDhdvXrVUj6fZfz48cqYMaNy5MihXbt2WX5+79698vf3t/w3YsSIp35veHi4PvvsM8uPd+3apXz58kmSgoKC1LZt28duP3z4cAUFBem9996z/Fzu3Lm1YcMGVa5cWTVr1pQkbdu2TRMmTNDIkSN15coV3b59Wz///LO2bdume/fuvdTzAwDAk1J6dgYHBz+2YxoTE6PmzZvr8uXLcnNzkyRduHBB2bNnl4uLi44fPy4fHx/16tVLo0eP1oYNG1S0aFGlS5fuqUyPzk5JOnTokKpWrWq5gNSTjw0gaVBsgSTm4+OjiIgInTx5UgUKFNCECRP0ww8/qHTp0o+VwcSjBxJ/zmw2q3fv3ho8eLDatGnz3I8XHR2tXbt2yc/PT5J08OBBTZs2zfJW5IMHD2rkyJHasmWL5YqQPXv21MiRIzVr1izL/SxfvlytWrVSRESEwsPDLRlTp04tk8mk1157TXnz5pX092r0w4cPX/YpAgDgMSk5O//44w+dO3dO3bp105YtW3To0CGlSpVKoaGhKlGihPbv3y9JCgsLU+PGjSXJcvRd5syZdefOHW3cuFHr16/XypUrLR/feTRj4uyUpMKFCys8PFw7d+585mMDSBoc9wMkMTc3N8XGxlpKYJEiRTR48GDt3r3b8jlXSSpRooS+/fZb7d27V0WLFlX79u3l7++vzJkzy8fHRxUrVpQkffDBB/945MCdO3fUuHFj1ahRQ3369NHQoUNVq1YtlSlTRu3atdO4ceMsA7dVq1aqUaOGwsLC9Ntvv+nmzZv65ptvtGrVKt27d0+ffvqpOnfurGXLlil//vw6fvy4hgwZotjYWNWpU0dubm4qVKiQunTporRp0ypz5szJ/EwCABxFSs7OHDlyaM6cOZL+no2FCxfWwIEDdfXqVcXExOirr76SJO3YsUO9evWSJLVo0UIDBgyQyWRSjx49LO94CgwMlJ+fnw4dOqTVq1fLx8fnsdl548YN9e3bV/Hx8SpcuPAzHxtA0uC4HzgMLnNve/g7AwBj8O+v/eHvFPaOtyIDAAAAAGwaxRYAAAAAYNP4jC0czuHDh42OgOfE3xUAGIt/h+0Hf5ewdxRbOIysWbPK3d1dvr6+RkfBC3B3d+fcXABIYcxM+8RMhT3j4lFwKGfPntW1a9eMjvHcIiIi1KdPH4WGhqpIkSIvfT/79u1TixYtNHToUMsVI21F1qxZLYfcAwBSjq3NzCcxQ5/GTIU9o9gCVio+Pl5FihTRW2+9pRUrVrzy/VWuXFnnzp3Tvn375OzsnAQJAQCwTsxQwPFw8SjASs2dO1eHDx/WoEGDkuT+Bg0apEOHDmnevHlJcn8AAFgrZijgeNixBaxQfHy83n//feXLl0/h4eFJdr9VqlTR6dOndeDAAVacAQB2iRkKOCZ2bAErNGfOHB05ckSBgYFJer+BgYE6cuSI5s6dm6T3CwCAtWCGAo6JHVvAysTHx+u9995TgQIFtGzZsiS//6pVq+rUqVOsOAMA7A4zFHBc7NgCVmbWrFk6evRokq80J0pccZ49e3ay3D8AAEZhhgKOix1bwIrExcWpcOHCevfdd7VkyZJke5zq1avr2LFjOnjwoFxcOM4aAGD7mKGAY2PHFrAiM2fO1PHjx5NtpTlRYGCgjh07plmzZiXr4wAAkFKYoYBjY8cWsBJxcXEqVKiQ3nvvPS1atCjZH69mzZo6fPiwDh06xIozAMCmGTFDDx06pMOHDzNDASvBji1gJWbMmKETJ05o4MCBKfJ4gYGBOn78uGbOnJkijwcAQHIxYoaeOHGCGQpYEXZsASsQFxend999V0WKFNHChQtT7HFr1aqlgwcPsuIMALBZzFAAEju2gFWYPn26Tp48meyfC3pS4opzWFhYij4uAABJhRkKQGLHFjBcbGys3nnnHRUrVkzz589P8cevU6eO9u7dqyNHjsjV1TXFHx8AgJfFDAWQiB1bwGC//PKLTp8+nWKfC3pSYGCgTp06penTpxvy+AAAvCxmKIBE7NgCBoqJidE777yjkiVLat68eYblqFevnnbt2qWjR4+y4gwAsAnMUACPYscWMNAvv/yiM2fOaMCAAYbmGDhwoE6fPq1ffvnF0BwAADwva5mhAwYMYIYCVoAdW8AgMTExKliwoD766CPNnTvX6Dj64osvtGPHDh09elSpUqUyOg4AAP+IGQrgSezYAgYJDQ3V2bNnDftc0JMGDhyo33//XdOmTTM6CgAA/4oZCuBJ7NgCBoiJidHbb7+tTz75RLNnzzY6jkWDBg20ZcsWHT9+nBVnAIBVYoYCeBZ2bAEDTJkyRefOnTP8c0FPGjhwoM6dO6epU6caHQUAgGdihgJ4FnZsgRT28OFDFShQQJ9//rlmzpxpdJynNGrUSJs2bdKJEydYcQYAWBVbmaHHjx+Xm5ub0XEAh8KOLZDCQkJCdOHCBfXv39/oKM/Uv39/nT9/XlOmTDE6CgAAj2GGAvgn7NgCKejhw4fKnz+/ypYtqxkzZhgd5x81btxYGzZs0IkTJ1hxBgBYBWYogH/Dji2QgiZPnqxLly5Z3eeCnjRgwABdvHhRISEhRkcBAEASMxTAv2PHFkghDx48UP78+VWuXDlNnz7d6Dj/ydfXV9HR0Tpx4oRSp05tdBwAgANjhgL4L+zYAilk0qRJunz5stV+LuhJAwYM0KVLlzR58mSjowAAHBwzFMB/YccWSAH3799X/vz55e3tbVOHtzdt2lRr1qzRyZMnWXEGABiCGQrgebBjC6SAiRMn6sqVK/r666+NjvJCvv76a12+fFmTJk0yOgoAwEHZ+gydOHGi0VEAh8COLZDM7t+/r3z58qlSpUoKDQ01Os4La9asmSIjI3Xy5EmlSZPG6DgAAAfCDAXwvNixBZLZhAkTdPXqVZv5XNCT+vfvrytXrrDiDABIccxQAM+LHVsgGd27d0/58uVTlSpVbPqw9hYtWmjlypU6deoUK84AgBTBDAXwItixBZJRcHCw/vzzT5v7XNCTvv76a129elXBwcFGRwEAOAhmKIAXwY4tkEzu3r2rfPnyqXr16nZxuX8/Pz+Fh4fr1KlTcnd3NzoOAMCOMUMBvCh2bIFkMn78eF2/fl39+vUzOkqS6Nevn/78809WnAEAyc5eZ+j48eONjgLYLXZsgWRw9+5d5c2bVzVr1rSro3JatWqlpUuX6tSpU0qbNq3RcQAAdogZCuBlsGMLJINx48bpr7/+spuV5kRff/21rl+/zoozACDZMEMBvAx2bIEkdufOHeXNm1d16tTRhAkTjI6T5Nq0aaNFixbp9OnTrDgDAJIUMxTAy2LHFkhiY8eO1c2bN+1upTlRv379dOPGDY0bN87oKAAAO+MoM3Ts2LFGRwHsDju2QBK6ffu28ubNqy+++MKu32rk7++v+fPn6/Tp00qXLp3RcQAAdoAZCuBVsGMLJKGgoCDdunVLffr0MTpKsurbt69u3rzJijMAIMk42gwNCgoyOgpgV9ixBZLI7du3lSdPHjVo0MAh3qbbrl07zZs3T6dPn1b69OmNjgMAsGHMUACvih1bIImMGTNGd+7cUd++fY2OkiL69u2r27dvs+IMAHhlzFAAr4odWyAJ3Lp1S3ny5FHjxo0dakh16NBBs2fP1unTp5UhQwaj4wAAbBAzlBkKJAV2bIEkMHr0aN27d8/uPxf0pD59+ujOnTsO9Y0IACBpOfoMHTNmjNFRALvAji3wim7evKk8efKoSZMmGj16tNFxUlxAQIBmzJihM2fOsOIMAHghzNC/Z+jp06eVMWNGo+MANo0dW+AVjRo1Svfv31fv3r2NjmKI3r176969ew75DQkA4NUwQ5mhQFJhxxZ4BTdu3FDevHnVtGlTjRo1yug4hvnqq680ffp0nTlzhhVnAMBzYYb+jRkKJA12bIFXMGrUKD148MBhV5oT9e7dWw8ePHDob0wAAC+GGfo3ZiiQNNixBV7SjRs3lCdPHrVo0UIjR440Oo7hOnfurNDQUJ05c0aZMmUyOg4AwIoxQx/HDAVeHTu2wEsaOXKkYmJi1KtXL6OjWIVevXrp4cOHrDgDAP4TM/RxiTP0559/NjoKYLMotsBL+Ouvv/Tzzz+rXbt2ypkzp9FxrIKHh4fatWunkSNH6saNG0bHAQBYKWbo0x6doX/99ZfRcQCbRLEFXsKIESMUGxurnj17Gh3FqvTs2VMxMTG8rQwA8I+Yoc/Ws2dPxcbGMkOBl0SxBV7Q9evXNWrUKLVv3145cuQwOo5VyZkzp9q1a6eff/6ZFWcAwFOYof8scYaOGjVK169fNzoOYHMotsALGjFihOLj41lp/geJK84jRowwOgoAwMowQ/8du7bAy6PYAi/gzz//1KhRo9ShQwdlz57d6DhWKUeOHOrQoQMrzgCAxzBD/xszFHh5FFvgBfz0008ym83q0aOH0VGsWo8ePRQfH8+uLQDAghn6fBJn6E8//WR0FMCmUGyB53Tt2jWNGTNGHTt2VLZs2YyOY9WyZ8+ujh07atSoUfrzzz+NjgMAMBgz9PklztDRo0fr2rVrRscBbAbFFnhOP/74o8xms7p37250FJvQvXt3mc1mVpwBAMzQF8QMBV4cxRZ4DlevXlVQUJACAgKUNWtWo+PYhGzZsqljx44aM2YMK84A4MCYoS+OGQq8OIot8Bx+/PFHmUwmVppfUOLz9eOPPxqcBABgFGboy+nevbtMJhMzFHhOFFvgP1y5ckVBQUH66quvlCVLFqPj2JSsWbMqICBAQUFBunr1qtFxAAApjBn68pihwIuh2AL/4YcffpCzs7O6detmdBSb1K1bNzk5ObHiDAAOiBn6ahJn6A8//GB0FMDqUWyBf/HHH39o7Nix6tSpk1577TWj49ikLFmy6KuvvlJQUJCuXLlidBwAQAphhr66xBk6duxYZijwHyi2wL8YPny4XF1d1aVLF6Oj2LSuXbvK2dmZFWcAcCDM0KSROEOHDx9udBTAqlFsgX9w+fJljR8/npXmJPDaa6+pU6dOGjt2rP744w+j4wAAkhkzNOkkztBx48YxQ4F/QbEF/sHw4cOVKlUqVpqTSNeuXeXq6sqKMwA4AGZo0mKGAv+NYgs8w6VLlzR+/Hh17txZmTNnNjqOXcicObM6d+6s8ePH6/Lly0bHAQAkE2Zo0mOGAv+NYgs8w/fffy83Nzd17tzZ6Ch2pUuXLkqVKhUrzgBgx5ihySNxhn7//fdGRwGsEsUWeMLFixcVHBysLl26KFOmTEbHsSuZMmVSly5dNH78eF26dMnoOACAJMYMTT6ZMmVS586dFRwczAwFnoFiCzxh2LBhSpMmDSvNyaRTp05yc3NjxRkA7BAzNHl17txZbm5uGjZsmNFRAKtDsQUeceHCBU2cOFFdu3ZVxowZjY5jlzJlyqSuXbsqODhYFy9eNDoOACCJMEOTX+IMnTBhAjMUeALFFnjEsGHD5O7urk6dOhkdxa516tRJadKkYcUZAOwIMzRlMEOBZ6PYAv/f+fPnNXHiRHXr1k0ZMmQwOo5dy5gxo7p166aJEyfqwoULRscBALwiZmjKYYYCz0axBf6/oUOHKl26dAoICDA6ikP46quvlDZtWlacAcAOMENT1ldffSV3d3cNHTrU6CiA1aDYApLOnTunyZMns9KcgjJkyGBZcT5//rzRcQAAL4kZmvISZ+ikSZN07tw5o+MAVsFkNpvNRocAjNauXTvNmzdPp0+fVvr06Y2O4zBu3bqlvHnzqmHDhho7dqzRcQAAL4EZaozEGdqgQQONGzfO6DiA4dixhcM7e/asQkJC1L17dwZyCsuQIYO6d++uyZMns+IMADaIGWocZijwOHZs4fD8/f01f/58nT59WunSpTM6jsO5ffu28ubNqy+++ELjx483Og4A4AUwQ43FDAX+Dzu2cGi///67pkyZoh49ejCQDZI+fXr16NFDISEhOnv2rNFxAADPiRlqvEdn6O+//250HMBQ7NjCobVp00YLFy7UmTNnlDZtWqPjOKw7d+4ob968qlu3roKDg42OAwB4DsxQ65A4Q+vUqaMJEyYYHQcwDDu2cFinT5/W1KlT1bNnTwaywdKlS6cePXpoypQprDgDgA1ghlqPR2fomTNnjI4DGIYdWzisVq1aaenSpTp16hRD2QrcvXtXefPmVa1atTRx4kSj4wAA/gUz1LokztCaNWtq0qRJRscBDMGOLRzSqVOnNG3aNFaarUjatGnVs2dPTZ06VadPnzY6DgDgHzBDrU/iDA0NDWWGwmGxYwuH5Ofnp/DwcJ06dUru7u5Gx8H/d/fuXeXLl0/Vq1fX5MmTjY4DAHgGZqh1YobC0bFjC4dz8uRJTZs2Tb169WIgW5m0adOqV69emjZtmk6dOmV0HADAE5ih1itxhoaGhjJD4ZDYsYXDadGihVauXKlTp04pTZo0RsfBE+7du6d8+fKpatWqCgkJMToOAOARzFDrljhDq1SpoilTphgdB0hR7NjCoZw4cULTp09Xr169GMhWyt3d3bJre/LkSaPjAAD+P2ao9Uucob/88otOnDhhdBwgRbFjC4fSrFkzRUZG6uTJkwxlK3b//n3ly5dPlStX1tSpU42OAwAQM9RWJM7QSpUqKTQ01Og4QIphxxYO4/jx4woLC1Pv3r0ZyFYuTZo06t27t6ZPn86KMwBYAWao7UicoWFhYcxQOBR2bOEwmjZtqjVr1ujkyZNKnTq10XHwH+7fv6/8+fPL29tb06ZNMzoOADg0ZqhtYYbCEbFjC4dw9OhRzZgxQ3369GEg24g0adKoT58+CgsL0/Hjx42OAwAOixlqex6doceOHTM6DpAi2LGFQ/D19VV0dLROnDjBULYhDx48UP78+VW+fHn98ssvRscBAIfEDLVNiTO0XLlymj59utFxgGTHji3s3pEjRzRr1ixWmm1Q6tSp1adPH82YMUNHjx41Og4AOBxmqO1KnKEzZ85khsIhsGMLu9e4cWNt2LBBJ06ckJubm9Fx8IIePHigAgUKyNPTU2FhYUbHAQCHwgy1bYkztGzZspoxY4bRcYBkxY4t7Nrhw4c1e/Zs9e3bl4Fso1KnTq2+fftq1qxZOnLkiNFxAMBhMENtHzMUjoQdW9i1Ro0aadOmTTp+/DhD2YY9fPhQb7/9tkqXLq2ZM2caHQcAHAIz1D4wQ+Eo2LGF3Tp48KDmzJmjfv36MZBtnJubm/r27avZs2fr8OHDRscBALvHDLUfj87QQ4cOGR0HSDbs2MJuNWjQQFu2bNHx48eVKlUqo+PgFcXExKhAgQL67LPPNGvWLKPjAIBdY4bal8QZ+umnn2r27NlGxwGSBTu2sEsHDhzQvHnz1K9fPwaynUiVKpX69eunOXPm6ODBg0bHAQC7xQy1P4kzdO7cucxQ2C12bGGX6tevr23btunYsWMMZTsSExOjggUL6uOPP9acOXOMjgMAdokZap+YobB37NjC7uzfv1/z5s3T119/zUC2M4krzvPmzdOBAweMjgMAdocZar+YobB37NjC7tSrV0+7du3S0aNH5erqanQcJLHY2FgVLFhQJUuW1Lx584yOAwB2hRlq35ihsGfs2MKu7N27V/Pnz9fXX3/NQLZTrq6u+vrrr/Xrr79q//79RscBALvBDLV/j87Qffv2GR0HSFLs2MKu1KlTR3v37tWRI0cYynYsNjZW77zzjooXL65ff/3V6DgAYBeYoY4hcYYWK1ZM8+fPNzoOkGTYsYXd2LNnjxYuXMhKswNIXHGeP3++9u7da3QcALB5zFDHkThDFyxYoD179hgdB0gy7NjCbtSuXVv79+/XkSNH5OLiYnQcJLPY2Fi9++67+uCDD7RgwQKj4wCATWOGOhZmKOwRO7awC7t379aiRYvUv39/BrKDcHV1Vf/+/bVw4UJWnAHgFTBDHQ8zFPaIHVvYhZo1a+rw4cM6dOgQQ9mBxMXFqVChQnr//fe1cOFCo+MAgE1ihjqmxBn63nvvadGiRUbHAV4ZO7aweTt37tSSJUtYaXZALi4u6t+/vxYtWqTdu3cbHQcAbA4z1HElztDFixdr165dRscBXhk7trB51atX17Fjx3Tw4EGGsgOKi4tT4cKFVahQIS1evNjoOABgU5ihji1xhr777rtasmSJ0XGAV8KOLWza9u3btWzZMlaaHVjiivOSJUu0c+dOo+MAgM1ghiJxhi5dulQ7duwwOg7wStixhU2rVq2aTpw4oYMHD8rZ2dnoODBIXFyc3nvvPRUsWFBLly41Og4A2ARmKCRmKOwHO7awWdu2bVN4eLgGDBjAQHZwLi4uGjBggJYtW6bt27cbHQcArB4zFImYobAX7NjCZlWpUkVnzpzR/v37GcpQfHy83n//feXPn1/Lli0zOg4AWDVmKB6VOEPz5cun8PBwo+MAL4UdW9ikLVu2aMWKFaw0w8LZ2VkDBgxQeHi4tm3bZnQcALBazFA8KXGGLl++XFu3bjU6DvBS2LGFTapcubLOnTunffv2MZRhER8fryJFiihPnjxavny50XEAwCoxQ/EsiTP0rbfe0ooVK4yOA7wwdmxhczZv3qxVq1ax0oynJK44r1ixQlu2bDE6DgBYHWYo/kniDF25ciUzFDaJHVvYnEqVKunChQvat2+fnJxYm8Hj4uPjVbRoUeXOnVsrV640Og4AWBVmKP4NMxS2jH/RYFN+++03RUREaODAgQxkPJOzs7MGDhyoVatWafPmzUbHAQCrwQzFf2GGwpaxYwub4u3trT/++EN79uxhKOMfJSQk6IMPPlCuXLm0atUqo+MAgFVghuJ5JM5QDw8PRUREGB0HeG78qwabsXHjRq1evZqVZvwnJycnDRw4UBEREfrtt9+MjgMAhmOG4nklztDIyEht2rTJ6DjAc2PHFjajfPnyunbtmnbv3s1Qxn9KSEjQhx9+qBw5cigyMtLoOABgKGYoXkTiDM2ePbtWr15tdBzgufAvG2zC+vXrtXbtWlaa8dwSV5xXr16tjRs3Gh0HAAzDDMWLSpyha9as0YYNG4yOAzwXdmxhE8qVK6fr169r165dDGU8t4SEBBUrVkxZs2bVmjVrjI4DAIZghuJlMENha/jXDVZv3bp1ioqKUmBgIAMZL8TJyUmBgYFau3at1q9fb3QcAEhxzFC8LGYobA07trB6np6eunXrlnbu3CmTyWR0HNgYs9ms4sWLK3PmzFq7dq3RcQAgRTFD8SoSZ2imTJkUFRVldBzgX7F0B6sWFRWldevWKTAwkIGMl2IymRQYGGj5WgIAR8EMxatKnKHR0dGKjo42Og7wr9ixhdUym83y9PTUnTt3tGPHDoYyXprZbFaJEiWUIUMGBjMAh8AMRVJhhsJWsGMLqxUVFaX169ez0oxXlrjinPhZMwCwd8xQJBVmKGwFO7awSmazWWXKlNGDBw+0bds2hjJemdls1v/+9z+5u7tr3bp1fE0BsFvMUCQ1ZihsATu2sEpr1qzRxo0bWWlGkklccd6wYQMXkQJg15ihSGrMUNgCdmxhdcxms0qXLq24uDht2bKFoYwkYzab9fHHH8vNzU3r16/nawuA3WGGIrkkztBUqVJpw4YNfG3B6rBjC6sTGRmp3377jZVmJLnEFeeNGzdy2DwAu8QMRXJJnKGbNm3S6tWrjY4DPIUdW1gVs9mszz77TAkJCdq8eTNDGUnObDarVKlScnFx0caNG/kaA2A3mKFIbsxQWDN2bGFVIiIitHnzZlaakWwSV5x/++03RUZGGh0HAJIMMxTJjRkKa8aOLayG2WzWJ598IpPJpN9++42hjGTD1xoAe8O/a0gpfK3BWrFjC6uxcuVKbd26VYMGDeIfSSQrk8mkQYMGacuWLYqIiDA6DgC8MmYoUsqjM3TVqlVGxwEs2LGFVUi80p6rqyuf2UCK4LNoAOwFMxQpLXGGxsfHc/VtWA12bGEVli9fru3bt7PSjBSTuOK8detWrVy50ug4APDSmKFIaYkzdNu2bVqxYoXRcQBJ7NjCCpjNZn300Udyc3PjXDSkKLPZrM8//1wxMTHaunUrX3sAbA4zFEZJPDM5NjaWGQqrwI4tDBceHq4dO3aw0owUl3h1x+3bt2v58uVGxwGAF8YMhVESd22ZobAW7NjCUGazWf/73//k7u6udevWMZSR4sxms8qUKaMHDx5o27ZtfA0CsBnMUBiNGQprwo4tDLV06VLt3LmTlWYYJnHFeceOHQoPDzc6DgA8N2YojPboDF22bJnRceDg2LGFYcxms0qUKKEMGTIoOjra6DhwYGazWZ6enrp79662b9/ON4gArB4zFNYicYbeuXNHO3bsYIbCMOzYwjBLlizR7t27FRgYaHQUOLjEFeedO3dq6dKlRscBgP/EDIW1SJyhu3btYobCUOzYwhBms1nFixdXpkyZFBUVZXQcQJLk6empW7duaefOnaw4A7BazFBYI2YojMaOLQyxaNEi7dmzR4MGDTI6CmAxaNAg7d69W4sXLzY6CgD8I2YorBEzFEZjxxYpLiEhQcWKFVPWrFm1Zs0ao+MAjylXrpyuX7+uXbt2ycmJtT8A1oUZCmvGDIWR+IpDilu4cKH27dvHSjOs0qBBg7R3715WnAFYJWYorFniDF20aJHRUeCA2LFFikpISNAHH3ygnDlzKjIy0ug4wDNVqFBBV69e1e7du1lxBmA1mKGwBRUqVNCVK1e0Z88eZihSFF9tSFELFizQgQMHuIojrFpgYKD27dunhQsXGh0FACyYobAFgYGB2r9/PzMUKY4dW6SYxJVmDw8PRUREGB0H+Ffe3t66fPmy9u7dy4ozAMMxQ2FLmKEwAl9pSDG//vqrDhw4wOeCYBMGDRqkAwcOaP78+UZHAQBmKGwKMxRGYMcWKSI+Pl5FixZV7ty5tXLlSqPjAM+lUqVKunDhgvbt28eKMwDDMENhi5ihSGl8lSFFzJs3T4cOHWKlGTZl0KBBOnjwoH799VejowBwYMxQ2KLEGTpv3jyjo8BBsGOLZBcfH68iRYooT548Wr58udFxgBfi4+Ojs2fPat++fXJ2djY6DgAHwwyFLWOGIiWxY4tkN3fuXB0+fJirOMImBQYG6tChQ6w4AzAEMxS2jBmKlMSOLZJVfHy83n//feXLl0/h4eFGxwFeSpUqVXT69GkdOHCAFWcAKYYZCnvADEVKYccWyWr27Nk6cuQIK82waYGBgTpy5IjmzJljdBQADoQZCnvADEVKYccWySYuLk7vvfeeChYsqKVLlxodB3gl1apV04kTJ3Tw4EFWnAEkO2Yo7AkzFCmBHVskm1mzZunYsWOsNMMuBAYG6ujRo5o9e7bRUQA4AGYo7EniDJ01a5bRUWDH2LFFsoiLi1PhwoVVqFAhLV682Og4QJKoUaOGjh49qoMHD8rFxcXoOADsFDMU9ogZiuTGji2SxcyZM3X8+HFWmmFXAgMDdezYMVacASQrZijsETMUyY0dWyS5uLg4FSpUSO+9954WLVpkdBwgSdWsWVOHDh3S4cOHWXEGkOSYobBnzFAkJ3ZskeTCwsJ04sQJVpphlwIDA3XixAnNmDHD6CgA7BAzFPaMGYrkxI4tklRsbKzeffddffDBB1qwYIHRcYBkUbt2be3fv19HjhxhxRlAkmGGwhEwQ5Fc2LFFkpo+fbpOnTrFSjPsWmBgoE6ePKmwsDCjowCwI8xQOILEGTp9+nSjo8DOsGOLJBMbG6t33nlHxYsX16+//mp0HCBZ1a1bV3v27NGRI0fk6upqdBwANo4ZCkfCDEVyYMcWSeaXX37R6dOnNXDgQKOjAMlu4MCBOnXqFCvOAJIEMxSOhBmK5MCOLZJETEyM3nnnHZUsWVLz5s0zOg6QIurVq6ddu3bp6NGjrDgDeGnMUDgiZiiSGju2SBLTpk3T77//zkozHMrAgQN1+vRpTZs2zegoAGwYMxSOiBmKpMaOLV5ZTEyM3n77bZUqVUpz5swxOg6QourXr6/t27fr6NGjSpUqldFxANgYZigcWf369bVt2zYdO3aMGYpXxo4tXlloaKjOnTvHSjMc0sCBA/X777+z4gzgpTBD4cgGDhyos2fPMkORJNixxStJXGn+9NNPNWvWLKPjAIZo2LChNm/erOPHj7PiDOC5MUMBZiiSDju2eCVTpkzRuXPnNGDAAKOjAIYZMGCAzp07p6lTpxodBYANYYYCzFAkHXZs8dIePnyoAgUK6PPPP9fMmTONjgMYqlGjRtq0aZOOHz8uNzc3o+MAsHLMUOD/MEORFNixxUsLCQnRxYsXWWkG9PeK8/nz5zVlyhSjowCwAcxQ4P8wQ5EU2LHFS3nw4IEKFCggT09PhYWFGR0HsApffvml1q9frxMnTrDiDOAfMUOBpzFD8arYscVLCQkJ0aVLl1hpBh4xYMAAXbx4USEhIUZHAWDFmKHA05iheFXs2OKFPXjwQPnz51f58uX1yy+/GB0HsCpNmjRRVFSUTpw4odSpUxsdB4CVYYYC/4wZilfBji1e2KRJk3T58mX179/f6CiA1enfv78uXbqkyZMnGx0FgBVihgL/jBmKV8GOLV7I/fv3lT9/fnl7e3OYNvAPmjZtqjVr1ujkyZOsOAOwYIYC/40ZipfFji1eyMSJE3XlyhVWmoF/0b9/f/3xxx+aOHGi0VEAWBFmKPDfmKF4WezY4rndv39f+fLlU+XKlTlEG/gPzZs3V0REhE6ePKk0adIYHQeAwZihwPNjhuJlsGOL5zZhwgRdvXpVX3/9tdFRAKvXv39/XblyhRVnAJKYocCLYIbiZbBji+dy79495cuXT1WrVuUy7MBzatmypVasWKFTp06x4gw4MGYo8OKYoXhR7NjiuQQHB+vPP/9Uv379jI4C2Iyvv/5aV69eVXBwsNFRABiIGQq8OGYoXhQ7tvhPd+/eVb58+VS9enUuvw68ID8/P4WHh+vUqVNyd3c3Og6AFMYMBV4eMxQvgmKLf1W1alXlypVLoaGhOn78uPLkyWN0JMCmnD59WgULFlSLFi104cIFhYeHGx0JQAphhgKvhhmKF+FidABYt/Xr18tsNuuDDz7QsmXL1LFjR6MjATYlPDxcH3zwgWbOnCknJz79ATgSZijwapiheBEUW/yrmJgYxcTEaM+ePcqQIYPRcQCbkyFDBu3Zs0fx8fFyc3MzOg6AFMQMBV4NMxQvgrci41+5uLgoTZo0WrVqlT799FOj4wA2adOmTapUqZIePHiguLg4o+MASCHMUODVMUPxvNixxb+aO3euSpUqpVy5chkdBbBZn332mY4dO6YtW7YYHQVACmKGAq+OGYrnxY4tAAAAAMCm8SlsAAAAAIBN463Iz+ns2bO6du2a0THwD7Jmzao333zT6BgwAK9N28Jr1b7xerQvvF7tD69R+8Jr9HEU2+dw9uxZFSpUSPfu3TM6Cv6Bu7u7Dh8+zIvbwfDatD28Vu0Xr0f7w+vVvvAatT+8Rh9HsX0O165d07179xQWFqZChQoZHQdPOHz4sHx9fXXt2jVe2A6G16Zt4bVq33g92hder/aH16h94TX6NIrtCyhUqJCKFy9udAwAT+C1CVgPXo+AdeM1CnvFxaMM9l8XpY6OjlZYWNhL3XdCQoJiY2P/9Tbnzp1Ty5YtVb9+fc2dO1eSNHToUBUuXNhym/Hjx6tZs2aqV6+ebt++rYkTJ8rf31+lSpXSypUrLbeLi4tTnz591LFjR61fv15XrlxRy5Yt1apVK0nSrVu31LZtW9WvX19Tp059qT8TkBKMfl1KUrdu3fT5559bfjxgwAA1b95cnTp1svzcrFmzVLNmTUnS4sWL1bBhQ9WpU0fnzp1TaGioatWqJX9/f/3xxx+W37Nv3z7Vq1dPfn5+OnDggKS/v8nx9/fXokWLJEmlS5eWv7+/QkJCXurPCBjh3163j/5a4kx6lud9fXbo0EG+vr767rvvLD83atQoy30PHTpU/v7+eu+993T48GENHDhQLVu2VOvWrR+7n8jISDVs2FB+fn66ePGibty4oaZNm6pJkybavXu35XZ16tR56X9zAGuQkq/P/5qfLVu2lL+/v3r16iVJ//h97ZkzZ+Tr66svv/xSZ8+e1f79++Xv769mzZqpYcOG2rVrl/z8/FSnTh2tX7/+P3Mh+bFja5DQ0FBt2bJFZcqU0d69exUbG6sSJUqobNmyCg4O1uXLl9W9e3fL7U+dOqVRo0YpPj5eVapU0Z9//qnffvtNHh4eGjBgwGP3fenSJYWGhuro0aMaOnSoPDw8/jFH7ty5NWXKFEmyFNw+ffro5MmTltvs3r1bU6dO1U8//aSzZ8+qTZs2kiRfX19VqFDBcrtFixbpzz//lLOzszw8PJQ9e3ZNmTLF8o9UhgwZNGHCBMXFxalTp05q0aLFqz+RQBKyltelJP3000+PDfhvvvlGktS2bVtJ0vXr13X69Glly5ZNkrRlyxaNGTNG0dHR2rdvn5ycnJQmTRq5ubkpU6ZMlvuJjIxUjx499P7776tbt24KDg5WunTpdP/+fctbmdKlS6cHDx7w1ibYhEdftxEREfrxxx/VuXNnTZkyRT4+PvLx8VHp0qU1atQoFShQQHfu3HnqPl709Tl27FhJ//d6PHHihFKnTm359T59+kiSGjZsqEKFCmnQoEGSpC5duujmzZvKmDGjJGnJkiUaNWqUrl+/rtDQUHl4eKht27YqWbKkunbtqrFjx2rmzJn67LPPXv2JAgxgxOvzv+anu7u74uLilDNnTkn6x+9rp06dqu+//17x8fGaOnWqBg4cqODgYM2cOVNp06ZV8eLFFRISops3byowMFBlypR5tScLr4xia6AaNWqoSpUqio6OVtWqVVWpUiXduHFDDx48ULp06bRkyRJ99NFHkqTg4GClT59eLi4u2rNnj1xcXFSsWDHVqVPnsfv08/OTu7u7AgICVLBgQUnS2rVrLbuxkvTJJ5+oWbNmj/2+CRMmqEGDBs/MmfgPj4uLizp37ixJunDhgrJnzy4Xl//7Ejp+/LhKlSqlBg0aWL5hftLGjRvVr18/BQQEvPgTBqQAa3pdPury5cvq1KmTcuXKJUkaOXKkevTooa5du0qSatasqYYNGyohIUGLFi1ShgwZ1LRpUy1btkyzZ8+23Levr68GDx6szJkz6+7du5Kkbdu26eHDh2rdurWmT5+uFStWyGw2q1GjRvL29k6iZxZIPomv208//VSenp4KCQlRqlSplCVLFnXv3l2jR49Wz549VaRIkadm3cu8Pg8dOqQePXpYvgkODg7W999/r+3bt1tus2XLFn388ceWH589e1aSLKVWkjp27KhBgwYpa9asun79uiTJy8tLbm5uio2N1fXr13XixAmVKVNG58+fT6qnC0hRKf36fNKT83PMmDEymUzq0aOHzpw5ozx58jzz+9rLly9bfs+lS5csP798+fLH3nk4cuRINmusBG9FNlCGDBkkSUFBQXJzc1O7du0UFhamZs2aqUmTJo9dtS4hIUH+/v4KDAxU37591bNnTxUvXlzNmzdXXFyc5XYdO3aUq6urgoKCtGnTpufKMWvWLDk5OalSpUrP/PWFCxdq1apV8vPz09q1ayVJYWFhaty48WO38/DwUK5cuSwrYc9SunRprVu3zvKWR8DaWMvr8kk5c+bUnDlzFBsbqxs3bujAgQMaMGCAtmzZoo0bN2rChAmKjIzUTz/9pNmzZ8tkMkmSsmfPrtu3b1vuJ0eOHAoKClK7du2UPXt2SZLJZFLq1Kktv8dkMsnJyUmpUqV6qaxASkt83V65ckWZMmWyfM0n/rzZbJarq6ucnZ3l7Oz82O99mddn4cKFFR4erp07d+qPP/7QuXPn1K1bN23ZskWHDh2SJM2YMUONGjWSJF28eFHffPONhg0b9tj9vPPOOxo3bpyqVKmiAgUKKGfOnLp48aIePnwoV1dXbd26VefOnVNQUJDCwsIUExPz8k8SYJCUfn0+6cn5mTjrsmXLZtkhftb3tTly5NClS5d04cIFy+7upUuXlDVrVrm6ukqSRowYoZIlS6po0aIvlQ1Jix1bK9C/f3/Fxsbq7bffVqlSpRQUFKSMGTPK3d3dcpv27durb9++ypYtm0qWLKnbt2/r2LFjypYt22OrS8WKFVOxYsV0584dzZw5U7lz51a5cuVUrly5Zz72iRMn1Lt3b/n4+OjmzZvq3r27Jk6cqE2bNsnf319BQUEqUqSI/P39de3aNY0ePVqStGPHDstnE0JCQlS0aFHVrl1bnTp10q+//qovv/xSDx8+VKdOnbRp0yaFhISobNmyGjFihGJjY1W2bNlkfEaBV2fk61L6+zN6mzZtUq9evfT999+rY8eOio+PV5o0aZQpUyYtXLhQ0t+fRypdurROnTqlNm3a6NatW+rXr58mT56sbdu26caNGxo7dqxWrVqle/fuqUiRIvruu+/04MEDDR8+XMePH9eQIUMUGxurOnXq6NatW2rfvr1cXFwe220CrF1MTIxGjx6ttWvXqnXr1ipVqpTl1xo0aKBBgwYpb968T/2+F3193rhxQ3379lV8fLwKFy6sHDlyaM6cOZL+fj0WLlxYMTEx+uuvvyzfDH/55ZfKlSuXOnXqpMGDB2vJkiUqWrSo4uLiFBISotjYWAUFBSkuLk6dO3eW6f+1d+fhMZ77/8DfWQixRIgQCaViSUpsRSxlprs2B8VB1ZI2kQwSFK04XfhStVW0agliaatUT6so3U/nESERRARBo4pE7GQn6/z+mN+MZBJZZ+Z5npn367pc53IaM3fyyT3v+dz3M89tY4PQ0FD06tULQ4YMgSAISE1N5WITyZa55idQeX7OmTMHWVlZsLGxQZcuXQCU/772zTffRFhYGDQaDT766CMApRtgQRCwceNGKBQK3L59m7u2EmCjqewuKYT4+Hj06tULJ06c4F3kJIj1sV6svbywXpaN9bUsrKflYU0tC+tZFi9FJiIiIiIiIlljYyuiijbLK7rdeUWP87jHrM7G/LBhw6BSqbB8+XIA2uNBQkJCMH36dOTm5iI1NRWhoaEIDQ3FtWvXkJmZiVmzZiEkJARnzpyBIAhQqVQYMWIEwsLCqnS8yOOee9q0aZg0aRLee+89AMCCBQswYcIEqFSqx36Ol6g2pDgv8/Ly4O/vj/Hjx+s/DmB4DBcAHDp0CL169QIAHD16FCNHjsSoUaMQHx8PQHsnZU9PT6SmpuK3337D5MmTMWzYMJw/fx5FRUUYO3YsAgMDsXjxYgDAd999h5CQEP18XLx4MUJDQ7FkyZJS4zM8yuT777+HSqXCCy+8UO5N5IiqQ4pzEih7NN6mTZswderUUndOLzknDbMyLi4OKpUKfn5++qN98vPz4evri+joaABls9JwTlZ2jInOV199hUmTJiEoKAg5OTmVHmNCVFVymZ+G7ycNj8iLiorCK6+8oj9Wa8eOHZg8eTKGDx+OW7duASg9P9PS0jBy5EgEBQXp7z9jeMyQYWa+8847CAoK0t8lXccwrys7PpMej5+xrYGvvvpKf6THBx98AJVKBRcXF/z++++Ii4tDYGAgIiMjsWDBAgQGBiIhIQFRUVEoLCxEeHg4AgIC0KlTJ/j6+kKtViMzMxMODg5YvHgxgoOD4eLigsTExDLPGxcXh507dyInJwdBQUHYv38/8vPz0b9/f6xatUp/y/SIiAjY29tj0qRJKCoqwtq1a/H888/rb3NemQYNGqCwsBDu7u4AtHd7rF+/PurVq4d69ephzZo1cHR0RH5+PpydnbFu3TrY2dlBo9HA1dUVXbp0gUKhwMcff4xXX321SseLPO65dUcqvPXWWwCAunXrok6dOnBxcSn1GUYiS56XDg4O2LZtG4BHc8HwGK4OHTrg559/Ro8ePQAACQkJCAsLAwDExMSgZ8+eWL16NYYPHw4AePHFF/Hiiy/i1KlT+PXXX+Hh4QE3NzesWrUK/v7++uMNOnToABcXF+Tl5SEtLQ1r167F1KlTkZeXBwcHBwAoc5TJyJEjMXLkSISEhGDUqFG1qivJlyXPSQBljsY7evQoIiMjsWjRIly5cgVubm6l5uTGjRtLZaWrqyv69OmDo0eP4uTJk+jRowfWrVsHPz8//WOWzErDOQlUfoyJzk8//YTt27fjt99+w969e3HhwoUKjzEhy2dt89Pw/aThEXmvvvoqiouL9XcfHzduHMaNG4e9e/ciJiYGw4YNKzU/Y2JiMHr0aIwePRqTJk3Cs88+W+qYofIy88qVK/j2228RFhaGa9eu6d/rGub1pUuXKjw+kx6PO7Y1cP36dfTo0QNTp07F6dOn4e3tjcWLF+tvCW5Io9HA1tYWJ0+exK1bt1BcXIxp06ahWbNmOHbsGBo1aoSUlBQkJibCy8sLH3/8cZlQArQ7NE5OTmjRooV+B2b8+PHw8/PT3zI9Li4Ob7/9NjZs2IAvvvgCgPbogJIvBKdOnYJKpdL/CQ8PL/U8X3/9NSIjI/Hzzz8jLy8PR44cwaJFi9CxY0f8+eefSEhIQHBwMIYPH44dO3YgOTkZQ4YMwdy5c/W7Sbrn6datG8aPH4+vvvoKS5cuLXW8yIYNG7Bq1aoKnxvQfobgySefBKB9odqyZQtatmyJgwcPVqtuZNksfV4CwIEDB/TnWeqO4RIEAR07dsSaNWtKPd5zzz2HuXPnYvbs2fDz88OhQ4fg4+ODhg0blvoZREZGYvTo0XB0dMTdu3fh5+eHp59+Grdu3UJWVhY+/fRTxMXF4e7du/ozc5s3b64/mkTH8CiThw8fIisrS/8GnKyPNczJkmxttW+pPDw8kJaWVmZOlpeVW7duxfTp0zFgwABcvHgRderUgYeHh/7flMxKwzmZm5uLGzduYMyYMfqb2n3++eeIiIhAWloaLl++rH+cGTNmYPr06fjll1+QlpamP8akdevWZY4xeeWVVyr8PskyWNv8BEq/n9Qdkbdu3ToMHDiw3K/Pz8/Hvn378MILL5SZn0OGDEFsbCzmzp1b6uQBnfIy85VXXsHMmTPx119/lZp3hnmtOz7zk08+wcqVKyv9vugRbnnVwLvvvovjx4/D399ff8leSbrbiOuOBfnvf/+LL7/8EqGhocjNzYWdnR0aNGgAjUYDX19fvP/++wC0k1R3+/Dy7nyo0WgQFhamPwR+wYIF+lull7xluuE4dP+tqnT/zsnJCfn5+ejcuTMcHR3RtGlTZGdnw83NDW5ubsjJydH/vVWrVnB2dtbfNv3o0aPo3bs3gEfHi6SlpeknqOHxIo977osXL+KLL77Ap59+Wuq/Gx5hQmTp81IQBMTHx+ODDz4A8OgYrt27d+PPP//E2bNncfXqVcTGxmLfvn347bffsGfPHhQWFmLFihVo3Lgxbt68idjYWNjY2GD+/PkICwvDpEmT4ObmhuPHj6N3794IDQ1FcHAwJk+erL9DZcOGDWFjY4Pbt28DAG7fvo2mTZvqx6Y7ykS3Ig5oL/P617/+Va3vkSyLpc9JQ0VFRQC057wrlcoyc7K8rHzzzTfxr3/9Cx999BF8fHxw7tw5/PXXX3B3d8fAgQNLZWXTpk1Lzcm8vDz9MSYhISFIT09HkyZNAJQ+xgQAfH194evri2+++Qb16tVDZmYmrl+/juLi4sceY0KWzdrm59mzZ0u9n9QdkZeQkIBvvvmmzE5wQUEBZsyYgffffx+Ojo6IiooqMz9XrVqFhw8fYsqUKWWer2nTpmUy09/fHwAQGhqKtm3b6r/2008/LZXXnTp1QsuWLSs8PpPKx8a2BjZu3Kg/0sPHxwfr1q3DvHnzcO3aNQBAmzZtsGLFCpw4cQIA4O7ujqVLl+L06dOlHqdLly7YvHkz5syZg8LCQqxatQpbt25FeHg4/vnnnzLPO3XqVKhUKjg7O2PIkCHljm3MmDEICwtDnTp1MHHiRH3QltStW7cKP/fm7+8Pe3t7uLm5oVGjRvrP5RQUFGDdunVo06YNpk6diqKiIixfvhx5eXn48MMP9YddA9rPJuguq7h48WKFx4sA2suoNmzYUOa5hw8fjkGDBmHKlClYt24dli9fjn/++QfZ2dnYvHlzZaUiK2LJ8zI7Oxvjxo3D0KFDMW/ePCxZsqTMMVy6c6gDAwMxdOhQ1KtXD9OmTUNxcTEmTZqEF198EYD2TURAQAC2bduG//3vf8jIyEBaWhqee+45hIeHIykpCY0bN4aDgwO8vLzw9ttvo0GDBvoFrZkzZ8LDwwMODg76eWt4lEnz5s3x448/co5aOUuek7rvr+TReH379kVoaCjq1q2Ltm3b6n//dXOyW7dupbLy559/xo8//oisrCyEhISgb9++eOutt7Bt2zZ4enqWyUrDOens7FzpMSYLFiyASqVCXFwc9u/fD1tbW6xduxbdunWr8BgTsnzWNj8N308qlcpSR+SdPn0a4eHhyMnJwRNPPIH//e9/OH/+PJYtW4YJEybgrbfeKjU/MzMzERoairy8PP3ndg2PGTLMzBUrVuDChQvw8vKCi4uLfn4OHTq0VF737du3wuMzAwICqlZkK8TjfqqgqrfT1n0egcyLtzu3XlWpPeeldHCuWjZmpWXhfLU8zEzLwjlaFndsjcjYLwSxsbH45ZdfAAD16tXT74ASUdVxXhJJC+ckkXRxfpKcsbGVMN1nYmrCmCtuS5YswZUrV3Do0CF89913+Oabb5CSkgI7Ozts2rQJmZmZWLBgAfLz86FSqdCgQQPMnDkTjo6OmDhx4mMvNSGSo9rMS0PmnKdHjx7F8uXLYWNjg//85z9c3SWLYcw5aciYc3TTpk1Yv3499u3bBw8PD6SmpmLZsmUAgLCwMLi7u+PixYsYPHiw/nJQIrkz5fzUMeU83bt3L3bu3In8/Hx89tlnaN26tVGeh0yDja3ElLz9+ocffoiFCxfizp07GDRoEEaNGoU+ffrg2WefRX5+PpycnHDmzBns2rUL//73v+Hr64vk5GT9B+MB7WdmEhISkJ6ejvDwcMyaNQtOTk54/vnnq9xwzps3DwAwduxYeHl5lTnaY9OmTaWOMDh58iTGjx8PhUKB5cuXs7EliyPHeVre8T9ElkqKc3Ty5MmlGlbDo/MA7Ztq3efliSydHOap4bFAbGyljY2txOhuvz5ixAgUFxejqKgITZo0we7duzFq1Ci4ublh6dKl8PPzw549e7Bo0SKkpqbCzs4O77zzDvbv34/ff/9d/3ibN2/G4MGDkZeXh/PnzyM7OxtjxozBs88+q/+a1NRU/c0jAKBZs2Zl7pAXGxuLvn376v9e8miP5ORkjBkzBp6enli9ejVmzJiB119/HevWrcOKFStM9aMiEo0c5+lzzz2HoKAgFBYW4quvvjLVj4ZIEqQ6R0tKSEjAunXrkJKSgh07dsDR0REjR47Exo0bTfNDIZIYOcxT3bFAxcXF2LNnj0l+DmQ8PMdWYt5991307NkT/v7+iI+Ph4uLC+bPn48HDx4AgP5MrJYtW8Le3h516tRBfn4+ioqKUFxcjPz8/FJH6Li6umLBggX4/PPP0adPH0RGRiI9PR3vvvtutcb19ddf4/XXXwfw6GiPpUuXAkCZIwy++OILrFq1Cn/88Qe2bNlijB8LkaTIcZ7qjhPYs2dPhXeSJLIEUp2jJenuNK47Su/kyZPYsWMHYmNjsW3btlp9/0RyIId5qjsWaOXKlfjmm29q9w2TyXHHVmJK3n69ffv2WLFiBbKzs2FvX3Gp7Ozs8N577+Hy5cvYvHkz9u3bBwB48cUXMW3aNGg0GsybNw9Lly6FnZ0dfHx89P/Ww8Ojwje6+fn5uH//vv6sO8OjPd58881SRxgUFhZi8eLFcHR0xODBg43wUyGSFjnOU8PjBIgsmRTn6Pfff4/9+/fj77//xieffILQ0NBSR+fp5m5gYKD+vEsiSyaHeWp4LBBJG4/7qQI53E7bmm/PLof6kGnIrfbWPE8B+dWLqscS6mvtc7QkS6gnlWYpNeU81bKUehoTL0W2EJzgRNLHeUokbZyjRNLHeUqPw8ZWBhYsWIDU1FSjPd6SJUvg7e2t//umTZswdepUzJkzBwCwYsUKBAYGYsaMGdBoNFi9ejUCAwMxduxYPHz4UP/vTp8+DZVKhUmTJmHs2LEoKirC2LFjERgYqP8g/uzZs/HMM88YbexEUmHqefnZZ58hICAAb7zxBjQaDTZu3AiVSgVfX1/88ssv+PnnnxEcHAw/Pz+kpKRAEAQ899xzUKlUSEpKKvXYERERCA0NxebNmwEA06ZNw/jx4/Hxxx8jLy8P/v7+GD9+PFavXm2074dIbMaeox9++CH8/f0xY8YMAMD69esxadIkjBo1CllZWdi7dy/Gjh2LESNGICUlBQBw8eJFuLu7AwB27NiByZMnY/jw4bh165b+cQ2zMy8vDyqVCiqVCp07dy73uYkshbHn6fz58xEQEIAJEyaguLi4zDxNTU1FaGgoQkNDce3aNRw/fhwTJ07ExIkTkZmZicLCQsybNw8hISGIiorSP25mZiaCg4MxevRobN26Vf//v/3226VuRkXiYmMrASqVCnl5ebh06RKWLl2K6OhohIWFISgoCPn5+fqvCwwMBABs27YN0dHRiIuLw9tvv42goCAcP368ys83b9489O/fX//3o0ePYt26dXBycsKVK1dw7NgxREZGonPnzoiJicH06dMRGRmJfv364cKFC/p/17VrV0REROCll17C66+/jgcPHsDNzQ2RkZFITk4GAKxcuRKdOnWq7Y+IyOzEnpczZszA5s2b4erqigcPHiAoKAgRERHw9PTUH12wYcMGTJgwAcnJybCxsUHDhg2h0WjQokUL/ePcvHkTBw4cgI2NDVxdXQEAa9euxfbt23HlyhU4ODhg27Zt+mMSiOTC3HN04cKF2LZtm36B9+TJk9i6dSv69u2Lq1ev6o8Fef3115GYmAig9PE948aNw6ZNm/Dmm28iJiZG/7iG2eng4ICIiAi8//778PPzK/e5ieTC3PP02rVr2Lx5s/6GpobzVHfMlr29PZydnbFlyxZERkYiMDAQP/zwA/bs2YO7d++iqKgIbm5u+sdt3LgxNmzYgB07dujHc+jQIbRr185IPykyBja2EvDyyy/j119/xX//+1+MGjUKdnZ2KCwsxO3btyt8o7l+/Xo4OTmhRYsWiI+P1///p06d0q/2qlQqhIeHV/j8trbaXwMPDw+kpaVh4sSJCA0NxdGjR5GWlgYASE9Px+nTp9G1a9cy//6nn37CK6+8AkdHR9y9exd+fn54+umna/CTIJIOsedlfn4+/P39cePGDTg4OADQBrarq6v+xhrLly/HmjVr8NRTT2HQoEHYu3cvQkNDsWbNGv3jXLp0Cc7Ozli9ejW+/fZbAEBSUhJeffVV/W4QABw4cAADBgyo9s+JSCzmnqM3btzAmDFj4OjoCAAYMmQIhgwZAkEQ0LFjR/2xIOvWrcPAgQOxY8cOjBw5Up+xgHZe79u3Dy+88IL+/3tcdm7fvh1vvPFGuc9NJBfmnqc+Pj7w8/NDeno6GjduXGaeJiQkIDg4GMOHD8eOHTtQWFiIunXronXr1khLS0NycjJ8fX3xySefYOXKlaUeOzo6Gs899xyUSiXy8/Px448/YujQoUb9eVHtsLGVgCFDhuC3337D33//DU9PT2zYsAErVqzAwIEDkZubq/863S3Ndf+fRqNBWFgYFi1ahKCgoBo/f1FREQDtm2Y3Nzf4+fnh888/R+fOneHp6YmsrCzMmjULy5cvLxXQgPYMMhcXF9SpUwfx8fHo3bs39u/fj9OnT9d4PERSIPa8rFu3LrZt24ZevXrp59P27dsxbtw4/de8++67WLVqFXbu3Kkfh6urK7KysvRfozuOCwDq1KkDAPD29saBAwdw4sQJAIAgCIiPj0dAQECNx0tkbuaeoy1btsSuXbtQUFCA9PR0/PDDD/j1118REBCAP//8s8yxIIbH9xQUFGDGjBl4//33SzWoj8vO+Ph49OjRo9znJpILc8/T48ePY//+/ejevTsuXLhQZp4aHrNlZ2eHgoIC/XtgXWY6OjqisLCw1GMPHDgQBw8exJ49e5CUlIQbN25g4cKF+Omnn3Djxg0j/LSotnjcjwQ4ODigoKBAfzlD165dsWjRIpw8eRK9evXSf12vXr3w0Ucf4dSpU/Dx8cHUqVOhUqng7OyMIUOG4MUXXwQAdOvWrcJbmW/cuBGHDx+GSqXCmjVr0LdvX4SGhqJu3bpo27Yttm7dipiYGDRt2hTdu3eHv78/7t27h//85z+YOXMmrly5gtzcXLz22mul3mh7eXkhPDwcSUlJaNy4MQDt5wYPHz6MuXPnYtmyZab6ERIZndjzctGiRbh9+zby8/Mxffp0ANrAnjt3LgBtk3vkyBFkZGRg4cKF2LNnDw4cOID09HQsWrQISUlJ+OOPPzB9+nQUFhZi5syZ8PHxQXp6Ov7zn/+gqKgI3t7eyM7Oxrhx4zB06FDMmzcPS5YsMdWPlMiozD1HQ0JCUFRUhPr166NJkybo2rUrVCoV7ty5g9WrV+PmzZuljgUJDg4G8Oj4ngULFuD8+fNYtmwZJkyYAGdnZ/zxxx8ICAgok53Hjh0rtXtr+NxEcmHueerm5oYpU6bg3r17CA4OLjNPmzdvXuqYratXr2Ly5Mn6e8oA2o8Cfffdd3jjjTf0WfrKK68gPDwcBQUFGDx4MLp3744vv/wSly9fxvbt2/XHdZG4eNxPFfB22tLG+lgv1l5eWC/LxvpaFtbT8rCmloX1LIuXIhMREREREZGssbElIiIiIiIiWeNnbKvh3LlzYg+BysG6EH8H5IF1sg6ss2VgHS0Xa2sZWMey2NhWgYuLCxwdHTF+/Hixh0KP4ejoCBcXF7GHQWbGuSk/nKuWi/PR8nC+WhbOUcvDOVoabx5VRVevXsWdO3fEHka15eXlYfDgwQgNDdWfh2eoqKgIzz77LN54441aHU8iJhcXF7Rp00bsYZAI5Do3dTZs2ICdO3fizz//LHOcls7XX3+NNWvWQBAE/Zm2csW5atnkPh8NWUuGPg7nq+WxtDlqbRlqiHO0NDa2Fk4QBCiVSiQkJKBbt26P/brhw4cjIyMDarXajKMjIoVCAWdnZ/zwww+P/ZqEhAT06NEDgiBg8ODBZhwdkXVjhhJJGzOUSuLNoyycIAho2rQpunbtWuHXKRQKxMTE4OHDh2YaGRE9fPgQsbGxUCgUFX6dj48PnJ2dIQiCWcZFRFrMUCLpYoaSITa2Fk6tVmPw4MGPvTxDR6lUIi8vD7GxsWYaGRHFxMQgLy8PSqWywq+ztbXF4MGDuRtEZGbMUCLpYoaSITa2FuzBgwdVWskCgK5du3I1i8jMdLtBXbp0qfRrFQoFYmNjuSNEZCbMUCJpY4aSITa2Fiw2Nhb5+flVCmXdahZDmch8dJ/3qWw3CNCGMneEiMyHGUokbcxQMsTG1oKp1Wo0a9asSitZgPZSqpiYGDx48MDEIyMi3W5QZZdQ6XTt2hVNmzblpVREZsIMJZIuZiiVh42tBavOShagXc3Kz8/nahaRGcTExFR5NwjgjhCRuTFDiaSLGUrlYWNroXJzc6u1kgUAXbp0QbNmzbiaRWQGarUaLi4ueOqpp6r8b5RKJWJjY7kjRGRizFAiaWOGUnnY2FqomJgYFBQUVHklC+BqFpE5VXc3CHi0IxQTE2PCkRERM5RI2pihVB42thZKt5Ll7e1drX+nu2tcbm6uiUZGRLm5uTh69Gi13jQDwFNPPcUdISIzYIYSSRczlB6Hja2FEgQBCoWiWitZgPYyjYKCAq5mEZnQkSNHUFBQUK3LHAHtjpBCoeCOEJGJMUOJpIsZSo/DxtYC5eTkIC4urtorWQDg7e0NFxcXTnoiExIEAc2bN6/2bhCg3RE6evQod4SITIQZSiRtzFB6HDa2FqimK1nAo9UsXqZBZDpqtRoKhQI2NjbV/re6HaEjR46YYGRExAwlkjZmKD0OG1sLJAgCXF1d4eXlVaN/r1AoEBcXh5ycHCOPjIhqsxsEaHeEmjdvzh0hIhNhhhJJFzOUKsLG1gLVZiUL4GoWkSkdPnwYhYWFNdoNAgAbGxvuCBGZEDOUSLqYoVQRNrYWJjs7G8eOHavxShYAeHl5cTWLyER0u0GdO3eu8WNwR4jINJihRNLGDKWKsLG1MEeOHEFhYWGtQlm3msVQJjI+3d1Wa7obBGhDubCwkDtCREbGDCWSNmYoVYSNrYVRq9Vo0aJFrVayAO2lVHFxccjOzjbSyIhItxtU00uodLy8vODq6spLqYiMjBlKJF3MUKoMG1sLY4yVLICrWUSmoPtsUG12gwDuCBGZCjOUSLqYoVQZNrYWJCsryygrWQDQuXNntGjRgqtZREakVqvRsmVLdOrUqdaPpVQqcezYMe4IERkJM5RI2pihVBk2thbk8OHDKCoqqvVKFsDVLCJTMNZuEPBoR+jw4cNGGBkRMUOJpI0ZSpVhY2tBdCtZHTt2NMrjKRQKHDt2DFlZWUZ5PCJrlpWVhePHjxvlTTMAdOrUCS1btuSOEJGRMEOJpIsZSlXBxtaCCIIApVJplJUsQHuZRlFREVeziIwgOjoaRUVFRrnMEeCOEJGxMUOJpIsZSlXBxtZCZGZm4sSJE0ZbyQKAjh07omXLlpz0REYgCALc3NzQoUMHoz2mQqHA8ePHuSNEVEvMUCJpY4ZSVbCxtRDGXskCtKtZSqWSl2kQGYFarTbqbhDwaEcoOjraaI9JZI2YoUTSxgylqmBjayEEQUCrVq3g6elp1MdVKBQ4ceIEMjMzjfq4RNbEFLtBANChQwe4ublxR4iolpihRNLFDKWqYmNrIUyxkgVwNYvIGA4dOoTi4mKj7gYB3BEiMhZmKJF0MUOpqtjYWoCMjAzEx8cbfSULADw9PdGqVSuuZhHVgiAIcHd3R/v27Y3+2NwRIqodZiiRtDFDqarY2FqA6OhoFBcXmySUedc4otoz5tl7hhQKBYqLi7kjRFRDzFAiaWOGUlWxsbUAarUaHh4eJlnJArSXUp04cQIZGRkmeXwiS6bbDTL2JVQ6np6ecHd356VURDXEDCWSLmYoVQcbWwtgypUsgKtZRLWh+2yQKXaDAO4IEdUWM5RIupihVB1sbGUuPT0dJ0+eNNlKFgC0b98eHh4eXM0iqgG1Wo3WrVvjySefNNlzKJVKxMfHc0eIqJqYoUTSxgyl6mBjK3OmXskCuJpFVBum3g0CHu0IHTp0yGTPQWSJmKFE0sYMpepgYytzupWsdu3amfR5FAoFTp48ifT0dJM+D5EluX//Pk6ePGnSN80A8OSTT3JHiKgGmKFE0sUMpepiYytzgiCY5Ow9Q0qlkqtZRNV06NAhaDQak17mCDw6i487QkTVwwwlki5mKFUXG1sZu3//PhISEky+kgUA7dq1Q+vWrTnpiapBEAS0adMGbdu2NflzcUeIqHqYoUTSxgyl6mJjK2NRUVFmWckCHq1m8TINoqpTq9Vm2Q0CtDtCGo0GUVFRJn8uIkvADCWSNmYoVRcbWxkTBAFPPPGEWVayAO1qVkJCAu7fv2+W5yOSs3v37uHUqVNm2Q0CgLZt26JNmzbcESKqImYokXQxQ6km2NjKmG4ly1y4mkVUdbrdIHOFMneEiKqHGUokXcxQqgk2tjJ17949JCYmmm3CA9rVrCeeeIKrWURVIAgC2rZta7bdIEC7I3Tq1Cncu3fPbM9JJEfMUCJpY4ZSTbCxlSlzr2Tp8Cw+oqrRnb1nTgqFAhqNhndeJaoEM5RI2pihVBNsbGVKrVajXbt2eOKJJ8z6vEqlkqtZRJW4e/cuTp06ZdbLHAHoV7d5KRVRxZihRNLFDKWaYmMrU2KsZAGPVrP4GSGix9PND7HmKHeEiCrGDCWSLmYo1RQbWxm6c+cOEhMTzb6SBQBPPPEE2rVrx9Usogqo1Wo8+eSTaNOmjdmfW7cjdPfuXbM/N5EcMEOJpI0ZSjXFxlaGdCtZgwcPFuX5uZpFVDGxdoOAR68L3BEiKh8zlEjamKFUU2xsZUgQBNFWsgBtKCcmJnI1i6gcd+7cwenTp0ULZd2OEN84E5WPGUokXcxQqg02tjJk7rP3DOlebA4ePCjaGIikSjcvxAplADyLj6gCzFAi6WKGUm2wsZWZ27dv48yZM6JO+DZt2uDJJ5/kahZROQRBQPv27dG6dWvRxqBQKHD69GncuXNHtDEQSREzlEjamKFUG2xsZUYKK1kAV7OIHkfs3SCAO0JEj8MMJZI2ZijVBhtbmREEAZ6envDw8BB1HAqFAmfOnMHt27dFHQeRlNy6dQtnz54V/U1z69at0b59e+4IERlghhJJFzOUaouNrcxIYSUL4GoWUXmkshsEcEeIqDzMUCLpYoZSbbGxlZFbt24hKSlJEhPew8MDnp6eXM0iKkEQBHTo0AHu7u5iDwUKhQJnz57FrVu3xB4KkSQwQ4mkjRlKtcXGVkaktJIF8Cw+IkNinr1nSDcOnsVHpMUMJZI2ZijVFhtbGVGr1ejYsSNatWol9lAAaC/T4GoWkdbNmzeRlJQkicscAcDd3R0dOnTgpVRE/x8zlEi6mKFkDGxsZURKK1kAPyNEVJLUdoMA7ggRlcQMJZIuZigZAxtbmbhx4wbOnTsnmZUsAGjVqhU6duzI1SwiaHeDOnXqBDc3N7GHoqdUKpGUlISbN2+KPRQiUTFDiaSNGUrGwMZWJnQrWYMHDxZ5JKVxNYtIS2q7QcCj1wvuCJG1Y4YSSRszlIyBja1MCIIguZUsQBvK586d42oWWbUbN27g/Pnzkgtl3Y4Q3ziTtWOGEkkXM5SMhY2tTEjl7D1DuhchTnqyZrrff6mFMsCz+IgAZiiRlDFDyVjY2MrA9evXceHCBUlOeDc3N3Tq1ImhTFZNEAR07twZLVu2FHsoZSgUCpw/fx43btwQeyhEomCGEkkbM5SMhY2tDEh5JQvgahaRVHeDAO4IETFDiaSNGUrGwsZWBgRBgJeXF1q0aCH2UMqlUChw4cIFXL9+XeyhEJldWloa/vrrL8m+aW7ZsiU6d+7MUCarxQwlki5mKBkTG1sZkPJKFsDVLLJuUt8NArgjRNaNGUokXcxQMiY2thJ37do1JCcnS3rCt2jRAl5eXgxlskqCIMDb2xuurq5iD+WxFAoF/vrrL6SlpYk9FCKzYoYSSRszlIyJja3ESfXsPUM8i4+slRTP3jPEs/jIWjFDiaSNGUrGxMZW4tRqNZ566ilJr2QB2ss0uJpF1ka3GyTlyxwB7Y6Qt7c3L6Uiq8MMJZIuZigZGxtbiZPDShbwaDWLK85kTXS/71LfDQK4I0TWiRlKJF3MUDI2NrYSlpqaiosXL0p+JQsAXF1d8dRTT3E1i6yKWq1Gly5d0Lx5c7GHUimlUonk5GRcu3ZN7KEQmQUzlEjamKFkbGxsJUy3MjRo0CBxB1JFXM0iayOX3SDg0esI5yhZC2YokbQxQ8nY2NhKmCAIslnJArShfPHiRaSmpoo9FCKTS0lJwd9//y2bUNbtCDGUyVowQ4mkixlKpsDGVsKkfvaeIX5GiKyJnD4bpMOz+MiaMEOJpIsZSqbAxlairl69ikuXLslmJQsAmjdvji5dujCUySoIgoCuXbvCxcVF7KFUmUKhwN9//42UlBSxh0JkUsxQImljhpIpsLGVKDmuZAFczSLrIbfdIIA7QmQ9mKFE0sYMJVNgYytRgiDAx8cHzZo1E3so1aJQKHDp0iVcvXpV7KEQmcyVK1fwzz//yGo3CABcXFzQtWtXhjJZPGYokXQxQ8lU2NhKlFqtlt2EB3jXOLIOgiDAxsZGNndbLUmhUHBHiCweM5RIupihZCpsbCXo8uXLuHz5suwu0QC0q1k+Pj4MZbJoct0NArSXOv7zzz+4cuWK2EMhMglmKJG0MUPJVNjYStDBgwdlu5IF8Cw+snxyOnvP0KBBg2BjY4ODBw+KPRQik2CGEkkbM5RMhY2tBKnVanTr1g1NmzYVeyg1wtUssmRy3g0CgGbNmsHHx4eXUpHFYoYSSRczlEyJja0EyXklC3i0msUVZ7JEcv5skA53hMiSMUOJpIsZSqbExlZidKu0cl3JAoCmTZuiW7duXM0ii6RWq9G9e3c4OzuLPZQaUyqV+lVzIkvCDCWSNmYomRIbW4nRrWQ988wzYg+lVriaRZZIo9HIfjcIAJ555hnuCJFFYoYSSRczlEyNja3ECIIg+5UsQBvKV65c4WoWWZTLly/j6tWrsg9l3Y4QQ5ksDTOUSLqYoWRqbGwlRKPRQK1Wy/oSKh3dZ4R4KRVZErVaLfvPBukolUqo1WpoNBqxh0JkFMxQImljhpKpsbGVkH/++QcpKSmyX8kCAGdnZ3Tv3p2rWWRRBEFAjx490KRJE7GHUmsKhQJXr17ljhBZDGYokbQxQ8nU2NhKiFqthq2trew/G6TD1SyyJJa0GwRwR4gsDzOUSLqYoWQObGwlxJJWsgDtalZKSgr++ecfsYdCVGuXLl1CamqqRewGAUCTJk3Qo0cP7giRxWCGEkkXM5TMgY2tROhWsixlwgPau8bZ2tpyNYssgqXtBgHaN87cESJLwAwlkjZmKJkDG1uJ+Pvvv3Ht2jWLuUQD4GoWWRZBENCzZ084OTmJPRSjUSqVSE1NxaVLl8QeClGtMEOJpI0ZSubAxlYiBEGAra0tBg4cKPZQjEp3Fh9Xs0jOLOXsPUO6HSG+cSa5Y4YSSRczlMyFja1EqNVqi1vJAh6tZv39999iD4Woxi5evIhr165ZXCg7OTmhZ8+evNSRZI8ZSiRdzFAyFza2EqBbybKkS6h0Bg4cyNUskj1BEGBnZ2dRnw3S4Y4QyR0zlEjamKFkLmxsJSA5ORlpaWkWGcpOTk7o1asXV7NI1tRqNXr16oXGjRuLPRSjUyqVuHbtGi5evCj2UIhqhBlKJG3MUDIXNrYSoFvJGjBggNhDMQmuZpGcWepng3S4I0Ryxwwlki5mKJkTG1sJEATBYleyAG0op6WlcTWLZCk5ORnXr1+32FBu3LgxevXqxVAm2WKGEkkXM5TMiY2tyHRn71niJVQ6AwcOhJ2dHS+lIllSq9Wws7OzuLutlqRUKnkWH8kSM5RI2pihZE5sbEX2119/4caNGxa7kgVwNYvkTRAEPP3002jUqJHYQzEZhUKB69evIzk5WeyhEFULM5RI2pihZE5sbEVmDStZAFezSJ6sYTcI4I4QyRczlEi6mKFkbmxsRSYIAnr37o2GDRuKPRSTUigUuHHjBv766y+xh0JUZRcuXMDNmzctejcIABo1aoSnn36aO0IkO8xQIulihpK5sbEVkaXfKa6kAQMGcDWLZEetVsPe3t5i77ZakkKh4I4QyQozlEjamKFkbmxsRXT+/HncvHnT4i/RALSrWb179+ZqFsmKtewGAdpLHW/evIkLFy6IPRSiKmGGEkkbM5TMjY2tiARBgL29Pfr37y/2UMyCZ/GRnFjTbhCg3RGyt7fnG2eSDWYokXQxQ0kMbGxFpFar0adPH6tYyQIerWadP39e7KEQVercuXO4deuWVewGAUDDhg3Ru3dvXupIssEMJZIuZiiJgY2tSKxtJQsA+vfvz9Uskg1BEFCnTh2r2Q0CuCNE8sEMJZI2ZiiJgY2tSJKSknD79m2rWckCtKtZffr04WoWyYJuN6hBgwZiD8VslEolbt26hXPnzok9FKIKMUOJpI0ZSmJgYysS3UpWv379xB6KWXE1i+TAGneDAO4IkXwwQ5mhJF3MUEHsoVgtNrYiscaVLEAbyrdv30ZSUpLYQyF6rLNnz+LOnTtWF8oNGjTgjhDJAjOUGUrSxQxlhoqFja0IiouLcfDgQau6hEqnf//+qFOnDlezSNKs8bNBOkqlkjtCJGnMUGYoSRszlBkqFja2IkhKSrLKlSzg0WoWQ5mkTBAE9O3bF46OjmIPxewUCgXu3LnDHSGSLGYoM5SkjRnKDBULG1sRqNVq1K1b1+o+G6SjW80qLi4WeyhEZRQXF0MQBKvcDQIe7QjxUiqSKmYoM5SkixnKDBUTG1sRWPNKFsDVLJK2s2fP4u7du1a5GwQAjo6O6Nu3L3eESLKYocxQki5mKDNUTGxszczaV7IAoF+/fqhbty5Xs0iSrH03COCOEEkXM5QZStLGDGWGiomNrZmdOXMG9+7ds9qVLICrWSRtgiDA19cX9evXF3soolEoFLh79y7Onj0r9lCISmGGMkNJ2pihzFAxsbE1M0EQULduXfj6+oo9FFEpFAocPHiQq1kkKbq7rVrzm2bg0Y4Q3ziT1DBDtZihJEXMUC1mqHjY2JqZWq1Gv379rHolC9BepnH37l2cOXNG7KEQ6Z0+fRr37t2z6sscAaB+/frw9fXlpY4kOcxQLWYoSREzVIsZKh42tmbElaxHfH19uZpFkiMIAhwcHKx+NwjgjhBJDzP0EWYoSREz9BFmqDjY2JpRYmIi7t+/b/UrWYB2Natfv35czSJJ0e0G1atXT+yhiE6pVOLevXs4ffq02EMhAsAMLYkZSlLEDH2EGSoONrZmpFvJ6tu3r9hDkQSuZpGUFBcXIyoqirtB/5+vry8cHBy4I0SSwQwtjRlKUsIMLY0ZKg42tmbElazSFAoF7t+/j8TERLGHQoRTp07h/v37DOX/r169evyMEEkKM7Q0ZihJCTO0NGaoONjYmklRURGioqJ4CVUJXM0iKREEAfXq1eNuUAlKpRJRUVHcESLRMUPLYoaSlDBDy2KGmh8bWzNJTExEeno6V7JKqFevHvr168dQJkkQBIG7QQa4I0RSwQwtixlKUsIMLYsZan5sbM1ErVZzJascSqUSBw8eRFFRkdhDIStWVFSEgwcPcjfIQN++fVGvXj1eSkWiY4aWjxlKUsAMLR8z1PzY2JqJIAjo378/HBwcxB6KpCgUCqSnp3M1i0R16tQpZGRkcDfIAHeESCqYoeVjhpIUMEPLxww1Pza2ZsDPBj0eV7NICtRqNerXr48+ffqIPRTJ4Y4QiY0Z+njMUJICZujjMUPNi42tGSQkJHAl6zEcHBzQv39/rmaRqLgb9HgKhQIZGRk4deqU2EMhK8UMfTxmKEkBM/TxmKHmxcbWDHQrWb179xZ7KJKkUCgQFRXF1SwSRWFhIc/eq0CfPn24I0SiYoZWjBlKYmKGVowZal5sbM1AEAQMGDCAK1mPoVQqkZGRgYSEBLGHQlYoISEBmZmZvMzxMRwcHDBgwADuCJFomKEVY4aSmJihFWOGmhcbWxMrLCzEoUOHuJJVgd69e6N+/fqc9CQKQRDg6OjI3aAKcEeIxMIMrRwzlMTEDK0cM9R82Nia2MmTJ7mSVQndahYv0yAxqNVqDBgwAHXr1hV7KJKlVCqRmZmJkydPij0UsjLM0MoxQ0lMzNDKMUPNh42tielWsp5++mmxhyJpCoUChw4dQmFhodhDISvC3aCq4Y4QiYUZWjXMUBIDM7RqmKHmw8bWxLiSVTUKhYKrWWR28fHxyMrKYihXom7dutwRIlEwQ6uGGUpiYIZWDTPUfNjYmpBuJYuXUFWud+/ecHR05GoWmRU/G1R1SqWSO0JkVszQqmOGkhiYoVXHDDUPNrYmFB8fj+zsbK5kVYFuNYuhTOYkCAIGDhyIOnXqiD0UyVMoFMjKyuKOEJkNM7TqmKEkBmZo1TFDzYONrQmp1Wo0aNCAnw2qIq5mkTkVFBRwN6gadDtCvJSKzIUZWj3MUDInZmj1MEPNg42tCXElq3p0q1nx8fFiD4WsAHeDqqdOnToYOHAgd4TIbJih1cMMJXNihlYPM9Q82NiaCFeyqu/pp59GgwYNuJpFZqFWq9GwYUP06tVL7KHIhm5HqKCgQOyhkIVjhlYfM5TMiRlafcxQ02NjayInTpxATk4OV7KqgatZZE7cDao+hUKB7Oxs7giRyTFDq48ZSubEDK0+ZqjpsbE1Ed1KVs+ePcUeiqzozuLjahaZUkFBAaKjo/mmuZp69erFHSEyC2ZozTBDyRyYoTXDDDU9NrYmIggCnnnmGa5kVZNSqUROTg5OnDgh9lDIgh0/fhw5OTm8zLGa6tSpg2eeeYY7QmRyzNCaYYaSOTBDa4YZanpsbE2AK1k117NnTzRs2JCTnkxKEAQ0atSIu0E1oFAoEB0dzR0hMhlmaM0xQ8kcmKE1xww1LTa2JnDs2DHk5uZyJasGdKtZvEyDTEmtVuOZZ56Bvb292EORHd2O0PHjx8UeClkoZmjNMUPJHJihNccMNS02tiagW8nq0aOH2EORJa5mkSnl5+fj8OHD3A2qIe4IkakxQ2uHGUqmxAytHWaoabGxNQGuZNWOQqFAbm4ujh07JvZQyALpdoMYyjVjb2/PHSEyKWZo7TBDyZSYobXDDDUtNrZGplvJ4iVUNdezZ080atSIq1lkEoIgoHHjxtwNqgWlUonDhw8jPz9f7KGQhWGG1h4zlEyJGVp7zFDTYWNrZMeOHcODBw+4klULutUshjKZgu5uq9wNqjndjhA/I0TGxgytPWYomRIztPaYoabDxtbI1Go1V7KMgKtZZAp5eXncDTKCHj16oHHjxryUioyOGWoczFAyBWaocTBDTYeNrZEJgoBBgwbBzs5O7KHIGj8jRKbA3SDj4I4QmQoz1DiYoWQKzFDjYIaaDhtbI+JKlvFwNYtMQa1Ww8nJCd27dxd7KLKn2xHKy8sTeyhkIZihxsMMJVNghhoPM9Q02NgaUVxcHB4+fMiVLCOws7PDoEGDuJpFRsXdIONRKBR48OABd4TIaJihxsMMJVNghhoPM9Q02NgakVqtRpMmTdCtWzexh2IRFAoFV7PIaPLy8nDkyBG+aTaS7t27w8nJiTtCZDTMUONihpIxMUONixlqGmxsjYgrWcalVCrx8OFDxMXFiT0UsgBHjx7Fw4cPeZmjkXBHiIyNGWpczFAyJmaocTFDTYONrZE8fPgQMTExXMkyom7duqFJkyac9GQUgiCgSZMm8PHxEXsoFkOhUODIkSPcEaJaY4YaHzOUjIkZanzMUONjY2skXMkyPt1qFi/TIGNQq9UYPHgwd4OMSLcjdPToUbGHQjLHDDU+ZigZEzPU+JihxsfG1kgEQYCzszNXsoxMoVAgJiYGDx8+FHsoJGPcDTINHx8f7giRUTBDTYMZSsbADDUNZqjxsbE1ErVajUGDBsHWlj9SY1IoFFzNolqLjY1FXl4eQ9nIuCNExsIMNQ1mKBkDM9Q0mKHGxwQxgocPHyI2NpaXUJlAt27d4OzszNUsqhXuBpmOUqnkjhDVCjPUdJihZAzMUNNhhhoXG1sjiImJ4UqWidja2nI1i2pN99kg7gYZn0KhQF5eHmJjY8UeCskUM9R0mKFkDMxQ02GGGhd/Q41AEAQ0bdoUXbt2FXsoFkmpVCI2NparWVQjDx484G6QCfn4+HBHiGqFGWpazFCqDWaoaTFDjYuNrREIgsCVLBPiahbVRmxsLPLz87kbZCK2trYYPHgwQ5lqjBlqWsxQqg1mqGkxQ42LKVJLXMkyva5du6Jp06a8lIpqRK1Wo1mzZujSpYvYQ7FYus8IPXjwQOyhkMwwQ02PGUq1wQw1PWao8bCxraWYmBiuZJkYV7OoNrgbZHoKhQL5+fncEaJqY4aaHjOUaoMZanrMUOPhb2kt6VaynnrqKbGHYtEUCgViY2O5mkXVkpubi9jYWL5pNrEuXbqgWbNm3BGiamOGmgczlGqCGWoezFDjYWNbS4IgQKFQcCXLxJRKJfLz8xETEyP2UEhGYmJiUFBQwMscTYw7QlRTzFDzYIZSTTBDzYMZajxMklrIzc3F0aNHuZJlBk899RSaNWvGSU/VIggCXFxc4O3tLfZQLJ5CocDRo0eRm5sr9lBIJpih5sMMpZpghpoPM9Q42NjWwpEjR7iSZSa2trZQKBS8TIOqRa1WczfITLgjRNXFDDUfZijVBDPUfJihxsHf1FoQBAHNmzfnSpaZcDWLqiMnJwdxcXHcDTITb29vuLi4cEeIqowZal7MUKoOZqh5MUONg41tLehWsmxsbMQeilVQKpUoKCjAkSNHxB4KyQB3g8yLO0JUXcxQ82KGUnUwQ82LGWocbGxriCtZ5sfVLKoO3W6Ql5eX2EOxGgqFAnFxccjJyRF7KCRxzFDzY4ZSdTBDzY8ZWntsbGvo8OHDKCwsZCibkY2NDVezqMq4G2R+CoWCO0JUJcxQ82OGUnUwQ82PGVp7bGxrSBAEuLq6ciXLzJRKJVezqFLZ2dk4duwYL6EyM29vbzRv3pw7QlQpZqg4mKFUFcxQcTBDa4+NbQ3pzt7jSpZ5KRQKFBYWcjWLKnTkyBHuBolAtyPEUKbKMEPFwQylqmCGioMZWntsbGuAK1ni8fLygqurKy+logqp1Wq0aNECnTt3FnsoVke3I5SdnS32UEiimKHiYYZSVTBDxcMMrR02tjXAzwaJh6tZVBXcDRIPd4SoMsxQ8TBDqSqYoeJhhtYOG9saUKvVaNmyJTp16iT2UKySQqHAsWPHuJpF5crKysKxY8f4plkknTt3RosWLbgjRI/FDBUXM5QqwgwVFzO0dtjY1gBXssSlVCpRWFiIw4cPiz0UkqDDhw+jqKiIlzmKhDtCVBlmqLiYoVQRZqi4mKG1w8a2mrKysnD8+HGuZImoU6dOaNmyJVezqFy63aCOHTuKPRSrpdsRysrKEnsoJDHMUPExQ6kizFDxMUNrjo1tNUVHR3MlS2RczaKKCIIApVLJ3SARKZVKFBUVcUeIymCGio8ZShVhhoqPGVpzbGyrSRAEuLm5oUOHDmIPxaopFAocP36cq1lUSmZmJk6cOMHdIJF17NgRLVu25BtnKoMZKg3MUCoPM1QamKE1x8a2mtRqNVeyJEC3mhUdHS32UEhCuBskDTY2NlAqlbzUkcpghkoDM5TKwwyVBmZozbGxrQauZElHhw4d4ObmxtUsKkUQBLRq1Qqenp5iD8XqKRQKnDhxApmZmWIPhSSCGSodzFAqDzNUOpihNcPGthoOHTqE4uJihrIE6D4jxNUsKkmtVvNuqxKhUCi4I0SlMEOlgxlK5WGGSgcztGbY2FaDIAhwd3fnSpZEKJVKrmaRXkZGBuLj43kJlUR06NABrVq14o4Q6TFDpYUZSiUxQ6WFGVozbGyrgWfvSYtCoUBxcTFXswiA9rNB3A2SDt55lQwxQ6WFGUolMUOlhRlaM2xsq4grWdLj6ekJd3d3XkpFALSXUHl4eKB9+/ZiD4X+P92OUEZGhthDIZExQ6WHGUolMUOlhxlafWxsq4ifDZIermZRSdwNkh7uCJEOM1R6mKFUEjNUepih1cfGtop0K1lPPvmk2EOhEhQKBeLj47maZeXS09Nx8uRJvmmWmPbt23NHiAAwQ6WKGUoAM1SqmKHVx8a2igRB4Nl7EqRUKlFcXIxDhw6JPRQSkW43iJc5SovuLD7uCBEzVJqYoQQwQ6WKGVp9bGyr4P79+1zJkqgnn3wSHh4eXM2ycmq1Gq1bt0a7du3EHgoZUCgUOHnyJNLT08UeComEGSpdzFACmKFSxgytHja2VXDo0CFoNBquZEkQV7MI4G6QlHFHiJih0sUMJYAZKmXM0OphY1sFgiCgTZs2aNu2rdhDoXJwNcu63b9/HwkJCdwNkqh27dqhdevWfONsxZih0sYMtW7MUGljhlYPG9sqUKvVXMmSMKVSCY1Gg6ioKLGHQiKIioribpCE6XaEeKmj9WKGShsz1LoxQ6WNGVo9bGwrcOXKFfz66684deoUV7IkrG3btmjTpg3++OMP7Nu3T+zhkBnt27cPf/zxB5544gnuBkmYQqFAQkICfvvtN1y5ckXs4ZCZMEPlgRlqvZih8sAMrTo2thXYvn07Xn/9dWg0GnzzzTdITk4We0hUjg8++ADu7u744Ycf8O9//xvFxcViD4nMoLi4GP/+97+xZ88euLu74/333xd7SFSO5ORkfPPNN9BoNBg7diy2b98u9pDITJih8sAMtU7MUHlghlYPG9sK+Pj44P79+6hTpw5OnjyJxo0biz0kKoe9vT1iY2ORmpoKLy8v2Nry19oa2NrawsvLC6mpqYiJiYG9vb3YQ6JyNG7cGAkJCahTpw7u37+Pbt26iT0kMhNmqDwwQ60TM1QemKHVw1evCvTt2xeAdlXrxx9/RIsWLUQeEZXnww8/xEsvvQQAcHd3F3k0ZE5ubm4AgCFDhuDDDz8UeTRUnhYtWmDfvn36XSDd6ypZPmaoPDBDrRczVPqYodXD5ZkKuLq6om/fvhg7diz69Okj9nDoMWxtbbF792706dMHEydOFHs4ZEb+/v64du0avvvuO+4ySFjfvn2xYsUK7Nq1C82bNxd7OGQmzFB5YIZaL2aoPDBDq85Go9FoxB4EERERERERUU1xeYaIiIiIiIhkjY0tERERERERyVqtP2N79epV3LlzxxhjIRNwcXFBmzZtKv061lFeqlJX1lReKqsp6ykvrKdlMawn62d5XFxcAIB1lTnOVctS1T5GT1MLV65c0Tg6OmoA8I9E/zg6OmquXLnCOlrYn8rqyprK709FNWU95feH9bSsPyXryfpZ5p969epp6tevL/o4+Kd2fzhXLetPVfqYkmq1Y3vnzh3k5uZi+/bt8PLyqs1DkQmcO3cO48ePx507dypc7WAd5aUqdWVN5aWymrKe8sJ6WhbDerJ+lkdXYwCsq4xxrlqWqvYxJRnluB8vLy/07NnTGA9FImIdLQ9rallYT8vCesob62eZWFfLw5paD948ioiIiIiIiGRNko2tpoKjdQMDA2v0OI97zIqey9CSJUvg7e2t//umTZvQs2dPpKamAgBWrFiBwMBAzJgxAxqNBl999RUmTZqEoKAg5OTkYMeOHZg8eTKGDx+OW7duAQA+/vhjhIaG4ocffkBRURHGjh2LwMBALF68uNRzL168GKGhoViyZAkA4K233oJKpcLcuXP1X3Po0CH06tWryt+PqUmxjnl5efD398f48eOxevVqAMC0adMwadIkvPfeewCA+fPn46233sLkyZMBAKtXr0ZgYCDGjh2Lhw8fAgDu3bsHT09Pfe1L1jEzMxPBwcEYPXo0tm7dipSUFLz11lsYPXo0vv3221LjMaxjamoqQkNDERoaimvXrmHbtm0YPnw4VCoVbt68WdUfmUlIsZ5A+fNy6tSpmDNnDoDKf6aXL1/G+PHj8cYbb+Dq1av47bffMHnyZAwbNgznz58HAERERCA0NBSbN28GoP2dGT9+PD7++GMAgFqtRkhISKn5CAC7d+/GoEGDEB0dDQCIiorCK6+8gu3bt1f5+zMVqdbT8GdZWT1//vlnBAcHw8/PDykpKQBKz8/ExESMGjUKAQEBOHPmDOLj4xEQEIARI0YgKirqsfPzs88+K/NzMHyNL++5xSTVmg4bNgwqlQrLly8HULam27dvR1BQEF577TVkZWWVyU5BEKBSqTBixAiEhYUhIyMDr732Gvz9/bF58+Yy2ZmTk4MxY8ZApVJh165d5b7u6yxYsAATJkyASqVCYWGhqHNUivUr72dn+N7HsH6ZmZmYNWsWQkJCcObMmTL1u3TpElQqFYYPH45ffvmlTGYCpV9jNRqNPivXrl1b7hh0vv/+e6hUKrzwwguIiIgo877L8HfHHKRYV6Dy97SG+XX06FGMHDkSo0aNQnx8fJm6nj59GiqVCpMmTcLYsWPLvNYCwJQpUxAYGIiZM2cC0L7fCggIwIQJE1BcXKwfy4EDBxAQEIDAwEAUFRVV+tzmJNV6zpkzByqVCp6ensjIyChTz8reCyUlJWHMmDH46KOPAKBMPdPS0qBSqaBSqTBw4EDk5eXp/965c2cAZV/rdQzrafgaAZR9X11TRrkUWeerr77CkSNH4Obmhg8++AAqlQouLi74/fffERcXh8DAQERGRmLBggUIDAxEQkICoqKiUFhYiPDwcAQEBKBTp07w9fWFWq1GZmYmHBwcsHjxYgQHB8PFxQWJiYllnjcuLg47d+5ETk4OgoKCsH//fuTn56N///5YtWoVhgwZgoEDByIiIgL29vaYNGkSioqKsHbtWjz//PMIDg6u0vc3b948/P333/q/T548GdeuXdP//dixY/j222+xfv16xMTE4KeffsL27dvx22+/Ye/evRg3bhzGjRuHvXv3IiYmBu7u7khMTETTpk3h5uaGBw8ewM3NDatWrYK/v7/+cfPy8pCWloa1a9di6tSpyMvLg6OjIwoLC9GyZUsAQH5+Pn7++Wf06NGjhtV7xJLr6ODggG3btgHQNpUA9EGp+/v//d//AQDefvttZGRkYPr06QC0b3QvXLiAbt26YfXq1Rg+fDgA4Pjx46Xq2LhxY2zYsAGFhYWYMWMG3nzzTWzZskX/HKNHj9aPx7COa9asgaOjI/Lz8+Hs7AxbW1vUr18fDg4OaNKkSTUrqWXJ9QTKzsujR48iMjISixYtwpUrV7B+/foKf6br1q3DsmXLUFRUhK1bt2L+/Pl48cUXcerUKfz6669wdnbGgQMH0K5dO7i6ugJ49DujG2NkZCSaN2+uv6umzogRI5CZman/+6BBg1BcXFyrF25Lr6fhz7Kyeg4ZMgRDhgzBrl27kJycjNatW5ean7///jveeecddOnSBbNnz0ZERAQ2b96MjIwMLFiwAKtWrSozPy9evIh69eqVGZvha3x5z82altWgQQMUFhbC3d293JqOHz8e48ePx/Lly3Hr1q1ys1OhUODjjz/Gq6++ihs3bmDAgAGYNWsWJk+ejDFjxpTKzvPnz6Nnz56YO3cuxo8fjzFjxpR53depW7cu6tSpAxcXF9jb29dojlpy/crLTMP3Pob1++GHH2BnZweNRgNXV1d06dKlVP2efPJJRERE4ObNm1i7di1efvnlMplZ8jX23r17cHBwwPr16/Hmm2+isLCwzBh0Ro4ciZEjRyIkJASjRo2Ci4tLqfddnTt3LvW7ExAQYJV1BSp/T2uYXwkJCfomMiYmBtOmTStV165du+oXExo0aICePXuWeq0dNGgQCgoKEBkZqX+Pe+3aNWzevBnTp09HdnY2GjduDAD44YcfsHnzZnz55ZeIjo7G+fPnK3zuqrD0en7yySd4+PAhpkyZAicnpzL1rOz9ZYsWLbBs2TL9op5hPVu1aoWIiAgcOXIEx44dg4ODAyIiIpCamoqGDRsCKPtar2NYz2PHjpV6jQBQKrdrw6g7ttevX0ePHj0wdepUnD59Gt7e3li8eDFatWpV7tdrNBrY2tri5MmTuHXrFoqLizFt2jQ0a9YMx44dQ6NGjZCSkoLExER4eXnh448/1jcAJa1fvx5OTk5o0aIF4uPjAWhfaP38/NCsWTPMmTMHcXFxePvtt7FhwwZ88cUXAAClUlnqF+bUqVP61QeVSoXw8PBqff8TJ05EaGgojh49irS0NMyYMQPTp0/HL7/8grS0NADaBnTfvn144YUXkJycjI4dO+Lzzz9HREQEHB0dcffuXfj5+eHpp5/WP+7du3fRvHlzAEDz5s1x7949/b9JS0vD5cuXsWbNmir/8lfGGup44MABDBgwQP/3+Ph4PPnkk/q/X716FQDg5OQEAEhPT8fp06fRtWtXHDp0CD4+PvqJbFhHAIiOjsZzzz0HpVKpf8wNGzZgzJgxpcZhWMeEhAQEBwdj+PDh2LFjByZMmICdO3fihRdewDfffPO4klXIGupZkq2t9mXNw8MDaWlplf5Mb9y4gVatWqF169a4fv26/mcQGRmJ0aNH49KlS3B2dsbq1av1O3pJSUl49dVX9auUx48fx6pVq/DgwQMkJydXpzzVZun1NPxZVlZPAFi+fDnWrFmDp556qsz8HD9+PL766issXboUOTk5+udZtWoV3nzzTf3fS87PiIiIclfeDV/jDZ+7piy9pl9//TUiIyPx888/Iy8vr0xNAWD27Nn4888/0aJFi3KzU/c83bp1Q+vWrXHo0CG8/PLLGDZsWJns7NGjBzIzMzF79uxS/97wdR/QvrnfsmULWrZsiYMHD1a5ZiVZev0e97MrqWT9kpOTMWTIEMydO7fUDrmufgCwb98+DBs2DC+99BKAsplZ8jW2WbNmaN++PWbNmoWUlBSkp6dXVA48fPgQWVlZ+sWxku+7DH93KmINda2O5557DnPnzsXs2bPh5+dX6nl0dQWAn376Ca+88or+7yVfa52dnTFs2DD99+3j4wM/Pz+kp6frm1rgUY63bt0aaWlpVX7uilhDPffs2YOhQ4eW+/3U9P2lYT137NiB119/Xf/37du344033gBQ9rVex7Cehq8RhrldG0ZtbN9991307NkT/v7+5W6f29jYAAByc3MBAP/973+xdOlSdOnSBbm5ubCzs0ODBg2g0Wjg6+uLBQsW4KuvvgIA1KlTB4B2ddWQRqNBWFgYFi1ahKCgIADQTxDd/5Ycj24cJSeRMfj5+eHzzz9H586d4enpCV9fX6xduxb9+vWDp6cnCgoKMGPGDLz//vtwdHSEm5sbWrVqBTs7O9ja2iI+Ph69e/fG/v37cfr0af3jNm3aFLdv3wYA3L59G02bNtV/D82bN0d2djbOnj2LVatWITY2Fvv27avV92HpdRQEQX+JDACcPXsWX3zxhf5S5LS0NCxcuBBLly4FAGRlZWHWrFlYvnw5bG1tER0djaioKPzyyy/YvHlzmToCwMCBA3Hw4EHs2bMHALBz507Y2trqQ9zwe9DV0c3NDW5ubmjatCmys7P1/93V1RVZWVnV+j51LL2ehoqKigBoV4J1P8+KfqYtWrTA9evXce3aNX0ohYWFYdKkSfp/qws+3ffr7e2NAwcO4MSJEwCA7t27w8bGBs7OzqWaJ1Ow9Hoa/iwrq6fuZ7Jq1Srs3LmzzPxs0aIF1qxZgylTpuhXhsPDw/H000/Dx8cHQOn5efPmTaSkpGD27NmIjY1FUlKSfmyGr/GGz11Tll5T3b9zcnJCfn5+mZoCwMqVKxEYGIg//vijTHYC2l3e3r17A9C+0QoMDMRvv/2GAwcOlMlOW1tbLF68GJ988ol+Thu+7huOja+xj/e4n11JJeune810dnbWz9GS9QOAoUOHIjo6Gps2bQJQNjMNX2PnzJmD8PBwuLq6olmzZhWOd+/evfjXv/4FAGXedxn+7lTE0utaXZ9++in27NmDPXv26BfxDet6/fp1uLi46L+/kq+1d+7cQXFxMfbu3YuCggLk5OTg+PHj2L9/P7p3744LFy7oH8fwNaIqz10Za6jn/v37H7uDXZP3l4b1zMvLQ0ZGhj5LAe3GkO5qUcPXep3ycrzka4RhbteGUS9F3rhxI/766y80b94cPj4+WLduHebNm6ffCm/Tpg1WrFihf6Fyd3fH0qVLSzVxANClSxds3rwZc+bMQWFhIVatWoWtW7ciPDwc//zzT5nnnTp1KlQqlf6ytPKMGTMGYWFhqFOnDiZOnKj/IZfUrVs3/YR53Pd3+PBhqFQqrFmzBnv37sX+/fvx999/45NPPsFPP/2EmJgYNG3aFN27d8e+ffuwf/9+2NraYu3atVi0aBHOnz+PZcuWYcKECXjmmWewY8cOTJs2DS+88AK8vLwQHh6OpKQk/S90cHAwNmzYADc3N8ycORMeHh5wcHDAnDlzkJWVBRsbG/3PC9Be3/+41ZqqsuQ6ZmdnY9y4cRg6dCjmzZuHJUuWYPjw4Rg0aBCmTJmCdevW4Y033kCrVq0wY8YMLFq0CO+88w7u3buH//znP5g5cybmzZsHQPvZrICAALi5uZWq48WLFxEeHo6CggIMHjwYFy9eRFhYGIYMGYKMjAzMmTNHX1fDOoaGhmLq1KkoKirC8uXLERkZibi4OKSnp+svzaouS66n7vsrOS/79u2L0NBQ1K1bF23btq30Z5qdnY2wsDBoNBp89NFH2LZtG/73v/8hIyMDaWlpGDp0KAoLCzFz5kz4+PggPT0d//nPf1BUVKT/fNJLL72E0NBQFBcXIyQkRF9fQRDwxRdfwMnJCU2bNkVRURHCw8ORk5ODJ554As8880zVimjw/VpyPQ1/lpXVc/v27Thy5AgyMjKwcOFCtG/fHsCj+Xnx4kV8/PHHePjwIZYvXw5BELBx40YoFArcvn0bzzzzTJn5uWvXLgDa11Nvb299Pbdu3VrqNd7wuWvK0mvq7+8Pe3t7uLm5oVGjRmVqunr1apw7dw7Z2dn47LPPymQnoN0l0F2K2L9/f8ycORP79+9H+/bty83OwMBA5ObmIjAwsNzXfV1Nly5din/++QfZ2dnYvHkzTp8+Xe05asn1K+9n9/3335d677Nr165S9evRowc+/PBD2NjY4J133ilTv6NHj+KLL77AgwcPMGLEiDKZWd5rbFhYGK5fvw4/Pz/Y2NiUGcP69euhUqnQsmVL/Pjjj/r3RIsXLy71vsvwd8da66r7/ip6T3vu3LlS+TV06FBMmzYNxcXFmDRpUpm6Atrdu3HjxgFAmddaf39/ZGRkYMqUKXj48KF+g2fKlCm4d+8egoOD9fNy+PDhCAoKgkajQUREBPLz8yt97spYej1v3LiBpk2b6ptrwzlS2XuhtLQ0vPfee0hOTkaXLl0wfPjwUvUEtFda6BaNAO3Hc0peYWr4Wv+4erZr167Ua4TuiiddbtdKlU+8LceJEyc0ADQnTpyo8OsCAgJq8zRUQ1WtD+soL1WpV1W+hvWUjsrqxXrKizHqqdGwplJhWC/Wz/Loasq6yltN5irrKV1Vfa0tyag7to8TGRlp1MeLjY3FL7/8AgCoV6+e2e+IZq1YR8vCeloW1tPysKbyxvpZJtbVsrCelsUsja2x+fr6wtfXt0b/VnfXM2NITEzExo0bYWtri6VLl2L37t2IiorC7du38eWXX2Lr1q1ITExEdnY2tm3bBnt7e3zwwQfIysrC6NGjMWjQIKOMQ65qU0dDxqzrO++8g4yMDLi7u2P+/PkAtEcpzZw5EydOnMDq1atL1bW8u6daI6nW86233kLdunXh5OSEZcuWYffu3fj000/x8ccfY+DAgfj+++/x+++/4++//8bIkSOhUqmM8rxyJ9V6Tps2DdnZ2fDw8MDixYvL1DMqKgpLly7FuHHjMH78eKM8p6WQak03bdqE9evXY9++ffDw8MD8+fORkpICOzs7bNq0CZmZmViwYAHy8/OhUqnQpUsXozyv3Bizfo9jzLp+9tlnSExMxMOHD7F9+3bs27cPO3fuRH5+Pj777LMa30Xc0sitruvXr0dsbCxycnKwdetWNGrUyCiPaylMWU9Tvu4eOHAAu3fvho2NDTZs2AA7OzujPI+5yaaxLXmb7g8//BALFy7EnTt3MGjQIIwaNQp9+vTBs88+i/z8fDg5OeHMmTPYtWsX/v3vf8PX1xfJycn49NNP9Y+3fft2JCQkID09HeHh4Zg1axacnJzw/PPPP/YaeEMRERGoX78+6tWrh3r16pW55b3hMTHJycm4e/cu7Ozs9DfMsHZSrOuVK1fw7bffIiwsDNeuXUPz5s1LHaVU3vE/pCXFehoeqWR4hIHh8RD0iBTraXg8lymOVLJkUqyp4bEUhkeubdq0qczREFSaFOs6Y8YMANo6PnjwALGxsfj8888hCAISExPZ2FaBFOt68uRJbN26FStXrsTVq1drdUd4ayHFOhq+7hoeyTN48GBj/xjMwqh3RTalkrfpLi4uRlFREZo0aYLdu3cD0N7ta+nSpfjrr7/w3nvvwdvbG6mpqbCzs8M777yDoUOH4vfff9c/3ubNm9GwYUPUr18f58+fR3Z2Nl5++WW8+OKL+q9JTU0tdWtt3V1zdY4cOYJFixahY8eO+PPPPwGUvuU9UPqYmOTkZPj6+uKTTz7BypUrTf0jkwUp1vWVV17BzJkz8ddff+H69evlHqVUsq70iBTraXikUnkMj4cgLSnWEyh7PBdVnVRraqjkkWuPOz6GHpFiXfPz8+Hv748bN27AwcEBw4YNw9ixY7Fu3ToMHDjQPD8YmZNiXXXndwuCgI4dO5rnByFzUqyjIcMjeeRKNo1tydt0x8fHw8XFBfPnz8eDBw8AQH/Oa8uWLWFvb486derojxUoLi5Gfn6+/jbUgPb21gsWLMDnn3+OPn36IDIyEunp6Xj33XerPKbOnTvD0dGx1LETJW95b3hMjO721rodJJJmXf39/fHpp5/C3d0dbdu2LXOUkmFd6REp1tPwSKXylDwegh6RYj0Nj+ei6pFiTQ0ZHrlW3vExVJoU61q3bl1s27YNvXr1wunTp7Fhwwb8/vvvWLlyZY3PZbc2UqzrDz/8gF9//RUBAQH6TR2qmBTraKi8Y9jkSDaXIpe8TXf79u2xYsUKZGdnw96+4m/Bzs4O7733Hi5fvozNmzfrz3h98cUXMW3aNGg0GsybNw9Lly6FnZ2d/lxDQHt4fEW31p40aRKCgoJQUFCAdevWlTmyIDQ0tNQxMa+99hpmzJiB7777Tn+YsbWTYl1XrFiBCxcuwMvLCy4uLmWOUvL39y9V186dOxvhJ2EZpFhPwyOVDI/g8fb2LnU8BD0ixXoaHs8VFRVl9COVLJkUa2p4LIXhkWtvvvlmmeNjqDQp1nX+/Pm4ffs28vPzMX36dCiVSgQFBSEzM5MLU1Ukxbp27doVKpUKd+7c4RUUVSTFOhq+7hoeySNb5r4Ns7lZ8228jX3cj5SwrrU/7kdKrLmeGo3xjoeRCtbTsuqp0Vh3TWt63I8cWHNdS6rOcT9yYK11tbS5aq111KlJ/Sz+Okpj38abpIF1tSysp2VhPS0Pa2qZWFfLxLpaBtax+iyisV2wYIFR74A5bNgwqFQqLF++HID21uaTJk3CqFGjkJWVhe3btyMoKAivvfYasrKySv3be/fuwdPTE6mpqUhPT8fEiRMxYcIEnDx5EpmZmQgODsbo0aOxdetWo43XUhi7joD2zsWBgYEAtHdRValUmDt3LgBg9+7dGDRoEKKjowEAP//8M4KDg+Hn54eUlBT9Y1y+fBl9+vSBSqXCoUOHUFxcjLfeeguvvfaa/mtmz57NSx7/P2PXccmSJfD29tb/fdOmTZg6dSrmzJkDADhw4AACAgIQGBiIoqKiMnX8/fffMXbsWAQEBJS6IUJRURHGjh2LwMBALF68GADw3XffISQkRD/3DZ/bWhmzpnl5efD398f48eP1l7FNmzYNkyZN0l+euHHjRqhUKvj6+uKXX37BV199pf/oR05ODpKSkjBmzBh89NFHpR67vLkJaO/KqvtatVqNkJAQ/euANTL2HP3www/h7++vvwvupk2b0LNnT/1z7N27F2PHjsWIESOQkpKC48ePY+LEiZg4cSIyMzMrnKOBgYF466238PPPPyMnJwdjxoyBSqXCrl27yv1dskbGrqfhHDGcn4bZmZmZiVmzZiEkJARnzpyBIAh47rnnoFKpkJSUVObxR4wYge3btwMAZs2ahSlTpug/CjJnzhyoVCp4enoiIyPDaN+TnJj6PW1iYiJCQkIwffp05ObmlnlPu3r1agQGBmLs2LF4+PBhlecnUPa9kDXX0xzvhUq+zgLaoyl79eql//vFixfh7u4OANixYwcmT56M4cOH49atW8jIyMBrr70Gf3//Mh/FKllHjUajf/+sO5XAsC+SElk0tiqVCnl5ebh06RKWLl2K6OhohIWFISgoCPn5+fqv0zUw27ZtQ3R0NOLi4vD2228jKCgIx48fr/LzNWjQAIWFhfpfBt2tzfv27YurV69i/Pjx2LhxI/r164dbt26V+rerV6/G8OHDAWg/YB8cHIzIyEhERkaicePG2LBhA3bs2FGt8VgKc9fx4sWLpc6YdXR0BIBSx77ojgwBtHf627BhAyZMmIDk5ORSj9WoUSPk5eXBw8MDtra22LJlC5o1a6b/7ytXrkSnTp2q8dOQL3PXcd68eejfv7/+70ePHsW6devg5OSEK1eu6G9Rr3ujZVjHffv24bPPPsOcOXOwbds2/eM8ePAAbm5uiIyMRHJyMoqKirB161bY29vr745s+NyWypw1dXBwwLZt2/THFQDaI3y++OILXL9+HQAQFBSEiIgIeHp64vnnn8dPP/2ELVu24LXXXsPevXvh7e2NZcuWlXns8ubmoUOH0K5dO/3fIyMjS9XYEpl7ji5cuBDbtm3Dw4cPAWiPkRg6dKj+v+uOeXn99deRmJiILVu2IDIyEoGBgfjhhx8eO0ejoqKgVCqxZcsWfPfddzh//jx69uyJiIgI/Pjjj+X+Llkic9fTcI4Yzk/D7Ny4cWOp45hsbGzQsGFDaDQa/QkROjt27MCAAQMAAP/88w+aNWuG9evX48iRIwCATz75BJ9++imeeeYZODk51eCnJX1iv6eNiIiAg4MDGjVqpD+qsuR72unTpyMyMhL9+vXDhQsXqjw/gbLvhSy5nmK/FzJ8nc3Pzy91NCWgbX5feuklAMC4ceOwadMmvPnmm4iJicGNGzcwYMAAbNmyRT//dErW8d69e3BwcEBERASOHz+OwsLCMn2RlMji5lEvv/wyfv31V5w7dw6jRo3C7du3UVhYiNu3b1cYZuvXr8cTTzyBhg0bIj4+Hk8//TQA4NSpU1i/fr3+6zp27IhZs2bp//7111/DxsYG48ePx6hRo/S3Nre3t8fMmTMBaFczzp49i6lTp+r/3aFDh+Dj44PExEQA2tt7K5VKODg4oKCgAAAQHR2N9957D6Ghocb68ciGuesYERGBZcuW4dixYwC0x77obj5y+fJltG3btsxzLV++HD/++KP+RRoAnnjiCfzvf//D7du3MX/+fKxbt66WPwl5M3cdDenuRO3h4YG0tLRyb1Ffso7u7u74v//7P7i4uODevXv6x3F0dMTdu3fh5+eHl19+Gbdu3UJWVhY+/fRTqFQqjB07Vr8YYunEqOmBAwf0b3CBskf4XLt2Da6urrC3t8eMGTMwffp02NvbV+vsy/z8fPz444+YOnWqfofo+PHjOH/+PBYtWoTk5GR06NChyo8nF+au540bNzBjxgy0atWq3MfVHfNSXFyMPXv2YO/evahbty5at26Nw4cPIyQkpNw5ev36df2bcTs7O/To0QO7d+/G7NmzS+0cGf4uWRpz17O8OVLREVvJyckYM2YMPD09sXr1aixatAiDBw/GmTNnsGbNGsyfPx+A9g3yxYsXMWjQIKSmppapr86ePXtKvWG3NGK/pz1y5AiOHDmC//73v/jzzz/x/PPPl3lPqzvSMDQ0tMrz83EstZ5ivxcypDuactGiRQC0i0gjR47Exo0b9V+Tn5+Pffv24fPPPweg7Vt+++03hISEPPZxmzVrhvbt22PWrFlISUlBenp6uX2RVMiisR0yZAhmz56N/Px8eHp64qOPPsLWrVsRHh6O3Nxc/dfpboWt+/80Gg3CwsJK7dpVhe5xnJyckJ+fr7+1+e7du/Hnn3/ipZdewsqVK/Hdd9/hjz/+0O/QRkdH4+bNm4iNjYWNjY3+jbabmxvq1KkDABg4cCAOHjyof4GxJuas482bN5GSkoLZs2cjNjYWSUlJ+ks4Kjr25d1338Wzzz6LnTt36ierbjxNmjRBXl5etb9vS2Pu+Wio5C3plUplqb97eHgAKFvHdevWITY2FrGxsfrHiY+PR+/evREaGorg4GBMnjxZv7PXsGFD5OXlWU1ja+6aCoKA+Ph4fPDBBwAeHeFjeAD9uHHjAAC+vr7w9fXFN998U63nSkpKwo0bN7Bw4UKcP38egYGB6N69O2xsbODs7IycnJxqjVsuzF3Pli1bYteuXQgJCUF6ejqaNGlS6r/rjnlJSEjAN998Azs7OxQUFOiPlejUqVO5c7Rly5b6BraoqAi2trZYvHgxNBqN/mQBw98lS2TuehrOkfLmZ0mGxzHpxuHq6lrqMsWjR48iJSUFa9asQXZ2NlavXg1BEAA8el0HgP3792PLli3VGrOciP2e9nFHVere0z733HOYNWsWPvnkE9ja2lZ5fj6OpdZT7PdChs6ePYurV6/qj6Y8efIk4uLiEBsbi23btuGNN97AjBkz8P7778PR0RHfffcdAgMD8a9//QvBwcEVLj7oPvo1btw4NGvWrNy+SCpk0djqdjx1bzq7du2KRYsW4eTJk6WuJe/Vqxc++ugjnDp1Cj4+Ppg6dSpUKhWcnZ0xZMgQ/cHF3bp1q/BW1v7+/rC3t4ebmxsaNWpU5tbmhsf6bN68GT4+Ppg3bx4A7XX1AQEBaNCgAWbOnAkbGxuEhobi4sWLCA8PR0FBAQYPHmzCn5g0mbOOLVq0wK5duwBoLwPx9vau9NiX+Ph4HDlyBBkZGVi4cCF+/fVX5ObmwtXVFVu2bEFWVhamT58OQDvJDx8+jOXLl+Pdd9/FkiVLcPjwYcydO7fcSyQtibnn48aNG3H48GGoVCqsWbMGffv2RWhoKOrWrYu2bduWuUX99u3bS9UxJiYGmzdvRkFBAdasWYOkpCT88ccfCAgIQHh4OJKSktC4cWM4ODjAy8sLb7/9Nho0aABnZ+cyz13Zrfnlypw1zc7Oxrhx4zB06FDMmzcPS5YsKXOEj62tLY4fP67/jN++ffuwf/9+2NraYu3atUhLS8N7772H5ORkdOnSBR07dsQff/yB6dOnl5mbX375JS5fvozt27ejZcuWeOmllxAaGori4uIKV6nlzNxzNCQkBEVFRahfvz6aNGlS5hgJw2NeevTogcmTJ0Oj0WD16tWPnaO68QiCgBEjRgDQvp7n5uYiMDCw3N8lS2TuehrOkY4dO1Z4xJbhcUx79uzBgQMHkJ6ejkWLFunrOX36dAwZMgSCICA1NRUdO3bE7du3ERoain79+gHQ7v43bdoUdevWNeFPVFxiv6et7lGV9+/fr/L8NHwvZMn1FPu90N69e0u9zhoeTalrVAMDA+Hv748FCxbg/PnzWLZsGSZMmID+/ftj5syZ2L9/P9q3b4/MzEx89NFHWL58eZk6hoWF4fr16/Dz84ONjY20j3wy922YyXws+bgfa2aJx/1YO0s8HsaasZ6WxdKOEKGyLO24H2vFuWpZeNwPERERERERWR2jXFd37tw5YzwMGVl168I6ykN16sSaykNV68R6ygPraVkeVyfWz3KUrCXrKl+cq5alRnWrzRbxlStXNI6OjvrLN/hHen8cHR01V65cYR0t7E9ldWVN5fenopqynvL7w3pa1p+S9WT9LPNPvXr1NPXr1xd9HPxTuz+cq5b1pyp9TEk2Go1Gg1q4evUq7ty5U5uHIBNycXFBmzZtKv061lFeqlJX1lReKqsp6ykvrKdlMawn62d5dOf2sq7yxrlqWarax+jUurElIiIiIiIiEhNvHkVERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW2NgSERERERGRrLGxJSIiIiIiIlljY0tERERERESyxsaWiIiIiIiIZI2NLREREREREckaG1siIiIiIiKSNTa2REREREREJGtsbImIiIiIiEjW/h9JLWPCBOnHkQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg = DTR(max_depth=3)\n",
    "reg.fit(X_train, y_train)\n",
    "ax = plt.subplots(figsize=(12,12))[1]\n",
    "plot_tree(reg,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a54d3d71-4eee-469d-8950-450d5145c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best pruning for tree\n",
    "ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\n",
    "kfold = skm.KFold(5,\n",
    "                  shuffle=True,\n",
    "                  random_state=10)\n",
    "grid = skm.GridSearchCV(reg,\n",
    "                        {'ccp_alpha': ccp_path.ccp_alphas},\n",
    "                        refit=True, # retrains on best alpha\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error')\n",
    "G = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c5188d5-2c25-4f27-a6fc-190d4ae450c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 48364442.4800445$"
      ],
      "text/plain": [
       "48364442.48004447"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get prediction error\n",
    "best_ = grid.best_estimator_\n",
    "tree_mse = np.mean((y_test - best_.predict(X_test))**2)\n",
    "tree_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d6c2481-aa18-457f-a9a1-27259aea898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output pruned Regression Tree\n",
    "ax = plt.subplots(figsize=(12,12))[1]\n",
    "plot_tree(G.best_estimator_,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);\n",
    "plt.savefig(\"pruned_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3747128-9922-488c-9dfa-12e96b229624",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cc80acc-02f1-49f6-944a-f2e5827b4160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 19778946.4332678$"
      ],
      "text/plain": [
       "19778946.4332678"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_tree = RF(max_features=3, # Random forest model\n",
    "               random_state=0).fit(X_train, y_train)\n",
    "y_hat_RF = RF_tree.predict(X_test)\n",
    "rf_mse = np.mean((y_test - y_hat_RF)**2)\n",
    "rf_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67960627-6d72-410c-8d6a-a1901ec6d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rxrd</th>\n",
       "      <td>0.275551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsales</th>\n",
       "      <td>0.264674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rppent</th>\n",
       "      <td>0.112891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gspillsicIV</th>\n",
       "      <td>0.111112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp</th>\n",
       "      <td>0.103030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gspilltecIV</th>\n",
       "      <td>0.084416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pat_count</th>\n",
       "      <td>0.048326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             importance\n",
       "rxrd           0.275551\n",
       "rsales         0.264674\n",
       "rppent         0.112891\n",
       "gspillsicIV    0.111112\n",
       "emp            0.103030\n",
       "gspilltecIV    0.084416\n",
       "pat_count      0.048326"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':RF_tree.feature_importances_},\n",
    "    index=feature_names) # Must include feature importance in report\n",
    "feature_imp = feature_imp.sort_values(by='importance', ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f163025-fb68-4c6d-b8af-7ba3d15d1593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Importance of Random Forest Regressors for Firm Market Value')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(feature_imp.sort_values(by='importance', ascending=False).to_latex()) # print latex output\n",
    "\n",
    "# plot importance values\n",
    "fig = plt.figure(figsize=(7.5,5))\n",
    "\n",
    "plt.bar(feature_imp.index, feature_imp.importance, color ='darkblue', \n",
    "        width = 0.4)\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Importance of Random Forest Regressors for Firm Market Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7ebdd-af00-4928-a015-4d2ebd4e752b",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "466f8c68-ccdd-49f6-8504-b67e067e42cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=7, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=7, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features=7, random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = RF(max_features=X_train.shape[1], random_state=0)\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f09cbe84-2c6a-4584-9218-44dd31228cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 20762300.9751054$"
      ],
      "text/plain": [
       "20762300.97510542"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplots(figsize=(8,8))[1]\n",
    "y_hat_bag = bag.predict(X_test)\n",
    "ax.scatter(y_hat_bag, y_test)\n",
    "bag_mse = np.mean((y_test - y_hat_bag)**2)\n",
    "bag_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c9684-f038-4369-82c1-d56d2e81d11e",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "365151ed-46e6-49e6-8335-fbdabde150ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n",
       "                          random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n",
       "                          random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n",
       "                          random_state=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = GBR(n_estimators=5000, # Gradient Boosting Regressor\n",
    "                   learning_rate=0.001,\n",
    "                   max_depth=3,\n",
    "                   random_state=0)\n",
    "boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b8112cd-eb77-409f-bd3d-3555d401f00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_error = np.zeros_like(boost.train_score_)\n",
    "for idx, y_ in enumerate(boost.staged_predict(X_test)):\n",
    "   test_error[idx] = np.mean((y_test - y_)**2)\n",
    "\n",
    "plot_idx = np.arange(boost.train_score_.shape[0])\n",
    "ax = plt.subplots(figsize=(8,8))[1]\n",
    "ax.plot(plot_idx,\n",
    "        boost.train_score_,\n",
    "        'b',\n",
    "        label='Training')\n",
    "ax.plot(plot_idx,\n",
    "        test_error,\n",
    "        'r',\n",
    "        label='Test')\n",
    "ax.legend();\n",
    "\n",
    "boost_mse = min(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84442aec-7647-410a-9f3e-676e2057043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 38336824.0\n",
      "Prev. MSE: 40865396.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XG Boost\n",
    "y = df['rmkvaf']\n",
    "# Convert the data into XGBoost's DMatrix format\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the XGBoost model with the optimal number of boosting rounds\n",
    "model = xgb.train(params, dtrain, num_boost_round= 10)\n",
    "\n",
    "# Make predictions \n",
    "y_pred = model.predict(dtrain)\n",
    "\n",
    "# Calculate and print the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Prev. MSE: 40865396.0\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y, y_pred, c='grey', alpha=0.3)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "plt.xlabel('True Y')\n",
    "plt.ylabel('Predicted Y')\n",
    "plt.title('Y vs Predicted Y (Y hat)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb857974-d09b-4241-8bfa-6d6bac1c6adf",
   "metadata": {},
   "source": [
    "# Comparing Test Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27eecd93-a25d-470b-a91e-a121cea264f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree:  48364442.48004447\n",
      "Bag:  20762300.97510542\n",
      "Random Forest:  19778946.4332678\n",
      "Boost:  32788863.102553938\n",
      "\\begin{tabular}{llr}\n",
      "\\toprule\n",
      " & model & MSE \\\\\n",
      "\\midrule\n",
      "0 & Tree & 48364442.480044 \\\\\n",
      "1 & Bag & 20762300.975105 \\\\\n",
      "2 & Random Forest & 19778946.433268 \\\\\n",
      "3 & Boost & 32788863.102554 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tree\n",
    "print('Tree: ', tree_mse)\n",
    "# Bag\n",
    "print('Bag: ', bag_mse)\n",
    "# Random Forest\n",
    "print('Random Forest: ', rf_mse)\n",
    "# Boost\n",
    "print('Boost: ', boost_mse)\n",
    "\n",
    "# Make table\n",
    "error_tbl = {\n",
    "    'model' : ['Tree', 'Bag', 'Random Forest', 'Boost'],\n",
    "    'MSE' : [tree_mse, bag_mse, rf_mse, boost_mse]\n",
    "}\n",
    "error_tbl = pd.DataFrame(data=error_tbl)\n",
    "print(error_tbl.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5400d0-4f99-4c11-b548-8d3c3648a1f4",
   "metadata": {},
   "source": [
    "# Predicting Market value of spillovers across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dff34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gspilltecIV :  2657.6857955701757\n",
      "gspillsicIV :  325.8887086615884\n"
     ]
    }
   ],
   "source": [
    "# Use Optimal LASSO coefficients - remember to rescale from their standardization back to unit values.\n",
    "print(x_vars.columns[1],': ',tuned_lasso.coef_[0]) # index 0 for x_vars is constant term\n",
    "print(x_vars.columns[2],': ',tuned_lasso.coef_[1]) # so 1st variable is at index 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c697a390-1f24-4821-a943-9e16da9d6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling lasso coefficients\n",
    "coef_tec = tuned_lasso.coef_[0]/df['gspilltecIV'].std()\n",
    "coef_sic = tuned_lasso.coef_[1]/df['gspillsicIV'].std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7615d301-c511-430d-a2db-2869e63855d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['firmval_tec'] = df['gspilltecIV']*coef_tec\n",
    "df['firmval_sic'] = df['gspillsicIV']*coef_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11e9621a-9efd-40b1-9f36-1c1b45389142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmkvaf</th>\n",
       "      <th>firmval_tec</th>\n",
       "      <th>firmval_sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2314.207764</td>\n",
       "      <td>3473.012695</td>\n",
       "      <td>377.431091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1288.770630</td>\n",
       "      <td>3390.965088</td>\n",
       "      <td>368.395996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1756.920532</td>\n",
       "      <td>3340.894043</td>\n",
       "      <td>362.991760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8698.285156</td>\n",
       "      <td>3321.720459</td>\n",
       "      <td>358.800262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10707.480469</td>\n",
       "      <td>3362.202881</td>\n",
       "      <td>362.979736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11731</th>\n",
       "      <td>765.484863</td>\n",
       "      <td>5178.813965</td>\n",
       "      <td>1174.952026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11732</th>\n",
       "      <td>910.333008</td>\n",
       "      <td>5190.939453</td>\n",
       "      <td>1177.405029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11733</th>\n",
       "      <td>1200.901245</td>\n",
       "      <td>5270.196289</td>\n",
       "      <td>1194.081299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11734</th>\n",
       "      <td>1264.767578</td>\n",
       "      <td>5391.985352</td>\n",
       "      <td>1221.722046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11735</th>\n",
       "      <td>1033.856079</td>\n",
       "      <td>5534.919434</td>\n",
       "      <td>1248.353882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11684 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rmkvaf  firmval_tec  firmval_sic\n",
       "0       2314.207764  3473.012695   377.431091\n",
       "1       1288.770630  3390.965088   368.395996\n",
       "2       1756.920532  3340.894043   362.991760\n",
       "3       8698.285156  3321.720459   358.800262\n",
       "4      10707.480469  3362.202881   362.979736\n",
       "...             ...          ...          ...\n",
       "11731    765.484863  5178.813965  1174.952026\n",
       "11732    910.333008  5190.939453  1177.405029\n",
       "11733   1200.901245  5270.196289  1194.081299\n",
       "11734   1264.767578  5391.985352  1221.722046\n",
       "11735   1033.856079  5534.919434  1248.353882\n",
       "\n",
       "[11684 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['rmkvaf','firmval_tec','firmval_sic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f01af06-eb37-4951-a1e9-6adc1b239424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36409750.0\n",
      "41708496.0\n"
     ]
    }
   ],
   "source": [
    "print(df['rmkvaf'].sum())\n",
    "print(df['firmval_tec'].sum())\n",
    "# Generate predictions for each firm\n",
    "\n",
    "# Aggregate across years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d937715-bc35-41dd-89b6-3139b370c84c",
   "metadata": {},
   "source": [
    "# Impact of spillovers on firm value over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1b5a3fc-da96-4a20-940c-03d155fd1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include confidence intervals. Use 2 or 5 year intervals if need more statistical power\n",
    "# Try with PLR DML to compare results - separate bar i.e. for each year, one OLS bar and one DML bar\n",
    "# Separate plots for tech/product market spillovers\n",
    "\n",
    "# build year-spillover interaction terms\n",
    "years = df.year.sort_values().unique()\n",
    "years = years[1:] # remove first year as ref category\n",
    "\n",
    "spillovers = ['gspilltecIV','gspillsicIV']\n",
    "\n",
    "for spillover in spillovers:\n",
    "    for year in years:\n",
    "        col_name = f\"{spillover}X{year}\"\n",
    "        x_vars[col_name] = x_vars[spillover]*x_vars[year]\n",
    "\n",
    "# drop reference year dummy\n",
    "x_vars = x_vars.drop(columns=['1981'])\n",
    "fixed_effects.remove('1981')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1451932-16b5-45cb-ae75-c2c0afa2e331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.669</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.645</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   28.14</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:35</td>     <th>  Log-Likelihood:    </th> <td>-1.2184e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 11736</td>      <th>  AIC:               </th>  <td>2.453e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 10949</td>      <th>  BIC:               </th>  <td>2.511e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   786</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-7970.3444</td> <td> 1646.372</td> <td>   -4.841</td> <td> 0.000</td> <td>-1.12e+04</td> <td>-4743.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th>      <td>    0.1976</td> <td>    0.137</td> <td>    1.443</td> <td> 0.149</td> <td>   -0.071</td> <td>    0.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th>      <td>    0.9836</td> <td>    0.220</td> <td>    4.474</td> <td> 0.000</td> <td>    0.553</td> <td>    1.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -25.3942</td> <td>    1.772</td> <td>  -14.333</td> <td> 0.000</td> <td>  -28.867</td> <td>  -21.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.9966</td> <td>    0.041</td> <td>   24.161</td> <td> 0.000</td> <td>    0.916</td> <td>    1.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.5445</td> <td>    0.087</td> <td>    6.287</td> <td> 0.000</td> <td>    0.375</td> <td>    0.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>   -4.8664</td> <td>    7.025</td> <td>   -0.693</td> <td> 0.489</td> <td>  -18.638</td> <td>    8.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>    8.7573</td> <td>    0.669</td> <td>   13.099</td> <td> 0.000</td> <td>    7.447</td> <td>   10.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>             <td> -299.1318</td> <td>  893.410</td> <td>   -0.335</td> <td> 0.738</td> <td>-2050.377</td> <td> 1452.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>             <td> -185.4166</td> <td>  885.977</td> <td>   -0.209</td> <td> 0.834</td> <td>-1922.091</td> <td> 1551.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>             <td>   26.8164</td> <td>  880.619</td> <td>    0.030</td> <td> 0.976</td> <td>-1699.356</td> <td> 1752.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>             <td>  258.8063</td> <td>  877.114</td> <td>    0.295</td> <td> 0.768</td> <td>-1460.496</td> <td> 1978.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>             <td>  638.1451</td> <td>  870.983</td> <td>    0.733</td> <td> 0.464</td> <td>-1069.139</td> <td> 2345.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>             <td>  579.2066</td> <td>  867.289</td> <td>    0.668</td> <td> 0.504</td> <td>-1120.836</td> <td> 2279.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>             <td>  830.2504</td> <td>  865.173</td> <td>    0.960</td> <td> 0.337</td> <td> -865.645</td> <td> 2526.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>             <td>  936.1895</td> <td>  861.950</td> <td>    1.086</td> <td> 0.277</td> <td> -753.389</td> <td> 2625.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>             <td> 1110.8190</td> <td>  856.149</td> <td>    1.297</td> <td> 0.195</td> <td> -567.388</td> <td> 2789.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>             <td> 1112.1719</td> <td>  855.495</td> <td>    1.300</td> <td> 0.194</td> <td> -564.753</td> <td> 2789.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>             <td> 1066.7174</td> <td>  855.480</td> <td>    1.247</td> <td> 0.212</td> <td> -610.178</td> <td> 2743.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>             <td>  734.1947</td> <td>  852.340</td> <td>    0.861</td> <td> 0.389</td> <td> -936.545</td> <td> 2404.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>             <td>  621.6407</td> <td>  853.810</td> <td>    0.728</td> <td> 0.467</td> <td>-1051.980</td> <td> 2295.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>             <td>  759.8321</td> <td>  853.042</td> <td>    0.891</td> <td> 0.373</td> <td> -912.284</td> <td> 2431.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>             <td>  797.1663</td> <td>  854.450</td> <td>    0.933</td> <td> 0.351</td> <td> -877.711</td> <td> 2472.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>             <td>  704.0261</td> <td>  858.768</td> <td>    0.820</td> <td> 0.412</td> <td> -979.314</td> <td> 2387.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>             <td> -206.4946</td> <td>  862.245</td> <td>   -0.239</td> <td> 0.811</td> <td>-1896.650</td> <td> 1483.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>             <td>-2373.0192</td> <td>  871.300</td> <td>   -2.724</td> <td> 0.006</td> <td>-4080.924</td> <td> -665.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>          <td> 4423.6332</td> <td> 1945.861</td> <td>    2.273</td> <td> 0.023</td> <td>  609.395</td> <td> 8237.872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>          <td> 3896.3454</td> <td> 2396.271</td> <td>    1.626</td> <td> 0.104</td> <td> -800.779</td> <td> 8593.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>          <td> 2729.6951</td> <td> 2026.089</td> <td>    1.347</td> <td> 0.178</td> <td>-1241.806</td> <td> 6701.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>          <td> 4461.3322</td> <td> 2065.822</td> <td>    2.160</td> <td> 0.031</td> <td>  411.948</td> <td> 8510.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>          <td> 6303.9751</td> <td> 2075.938</td> <td>    3.037</td> <td> 0.002</td> <td> 2234.761</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>           <td> 5572.3407</td> <td> 1982.123</td> <td>    2.811</td> <td> 0.005</td> <td> 1687.022</td> <td> 9457.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>          <td> 2485.7359</td> <td> 1888.099</td> <td>    1.317</td> <td> 0.188</td> <td>-1215.278</td> <td> 6186.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>          <td>-5123.4447</td> <td> 1988.147</td> <td>   -2.577</td> <td> 0.010</td> <td>-9020.571</td> <td>-1226.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>          <td> 5690.4196</td> <td> 4723.700</td> <td>    1.205</td> <td> 0.228</td> <td>-3568.886</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>          <td>-3481.5308</td> <td> 2021.059</td> <td>   -1.723</td> <td> 0.085</td> <td>-7443.171</td> <td>  480.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>           <td> 4456.0415</td> <td> 4739.256</td> <td>    0.940</td> <td> 0.347</td> <td>-4833.757</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>          <td> 7533.2294</td> <td> 2359.993</td> <td>    3.192</td> <td> 0.001</td> <td> 2907.217</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>          <td>  959.4923</td> <td> 1931.531</td> <td>    0.497</td> <td> 0.619</td> <td>-2826.657</td> <td> 4745.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>          <td> 7693.8970</td> <td> 2384.513</td> <td>    3.227</td> <td> 0.001</td> <td> 3019.821</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>           <td>-1833.6524</td> <td> 2057.997</td> <td>   -0.891</td> <td> 0.373</td> <td>-5867.699</td> <td> 2200.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>          <td>-4351.2823</td> <td> 2474.700</td> <td>   -1.758</td> <td> 0.079</td> <td>-9202.141</td> <td>  499.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>          <td>-1.098e+04</td> <td> 4817.183</td> <td>   -2.280</td> <td> 0.023</td> <td>-2.04e+04</td> <td>-1539.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>          <td> 6436.4782</td> <td> 2265.679</td> <td>    2.841</td> <td> 0.005</td> <td> 1995.338</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>          <td> 2352.5604</td> <td> 2426.672</td> <td>    0.969</td> <td> 0.332</td> <td>-2404.155</td> <td> 7109.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>          <td>-1.002e+04</td> <td> 3163.284</td> <td>   -3.168</td> <td> 0.002</td> <td>-1.62e+04</td> <td>-3819.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>          <td> 6266.0516</td> <td> 2059.503</td> <td>    3.043</td> <td> 0.002</td> <td> 2229.054</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>          <td> 7356.7837</td> <td> 2352.848</td> <td>    3.127</td> <td> 0.002</td> <td> 2744.777</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>          <td>-1923.3394</td> <td> 2954.323</td> <td>   -0.651</td> <td> 0.515</td> <td>-7714.347</td> <td> 3867.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>          <td> 5620.6268</td> <td> 2090.424</td> <td>    2.689</td> <td> 0.007</td> <td> 1523.018</td> <td> 9718.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>          <td> 3685.8163</td> <td> 2065.689</td> <td>    1.784</td> <td> 0.074</td> <td> -363.307</td> <td> 7734.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>          <td>-2.369e+04</td> <td> 3266.198</td> <td>   -7.253</td> <td> 0.000</td> <td>-3.01e+04</td> <td>-1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>          <td> 5356.4273</td> <td> 1980.608</td> <td>    2.704</td> <td> 0.007</td> <td> 1474.077</td> <td> 9238.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>          <td>-1702.3331</td> <td> 5029.195</td> <td>   -0.338</td> <td> 0.735</td> <td>-1.16e+04</td> <td> 8155.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>           <td> 1367.8519</td> <td> 2174.242</td> <td>    0.629</td> <td> 0.529</td> <td>-2894.054</td> <td> 5629.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>          <td> 3093.1762</td> <td> 1892.605</td> <td>    1.634</td> <td> 0.102</td> <td> -616.672</td> <td> 6803.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>          <td>-2434.4612</td> <td> 2648.650</td> <td>   -0.919</td> <td> 0.358</td> <td>-7626.294</td> <td> 2757.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>          <td>-4973.4930</td> <td> 3172.560</td> <td>   -1.568</td> <td> 0.117</td> <td>-1.12e+04</td> <td> 1245.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>          <td>-3883.9357</td> <td> 1976.728</td> <td>   -1.965</td> <td> 0.049</td> <td>-7758.680</td> <td>   -9.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>          <td> 3271.1148</td> <td> 2015.881</td> <td>    1.623</td> <td> 0.105</td> <td> -680.376</td> <td> 7222.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>          <td>  877.6333</td> <td> 1968.655</td> <td>    0.446</td> <td> 0.656</td> <td>-2981.286</td> <td> 4736.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>          <td> 5600.8729</td> <td> 2458.851</td> <td>    2.278</td> <td> 0.023</td> <td>  781.080</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>          <td> 6572.9769</td> <td> 2208.111</td> <td>    2.977</td> <td> 0.003</td> <td> 2244.680</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>           <td>-5502.7756</td> <td> 2220.004</td> <td>   -2.479</td> <td> 0.013</td> <td>-9854.385</td> <td>-1151.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>          <td>-1721.5928</td> <td> 2623.601</td> <td>   -0.656</td> <td> 0.512</td> <td>-6864.325</td> <td> 3421.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>          <td>-2.562e+04</td> <td> 2429.201</td> <td>  -10.547</td> <td> 0.000</td> <td>-3.04e+04</td> <td>-2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>          <td> 5215.2149</td> <td> 2047.859</td> <td>    2.547</td> <td> 0.011</td> <td> 1201.042</td> <td> 9229.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>          <td>-7048.4947</td> <td> 3741.693</td> <td>   -1.884</td> <td> 0.060</td> <td>-1.44e+04</td> <td>  285.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>          <td> 6992.8093</td> <td> 2406.487</td> <td>    2.906</td> <td> 0.004</td> <td> 2275.660</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>          <td>-1.618e+04</td> <td> 2611.534</td> <td>   -6.196</td> <td> 0.000</td> <td>-2.13e+04</td> <td>-1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>          <td>  145.7061</td> <td> 2687.829</td> <td>    0.054</td> <td> 0.957</td> <td>-5122.925</td> <td> 5414.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>          <td>  287.3177</td> <td> 2585.115</td> <td>    0.111</td> <td> 0.912</td> <td>-4779.974</td> <td> 5354.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>          <td> 2657.4776</td> <td> 2005.124</td> <td>    1.325</td> <td> 0.185</td> <td>-1272.928</td> <td> 6587.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>          <td> 4904.7695</td> <td> 2084.013</td> <td>    2.354</td> <td> 0.019</td> <td>  819.727</td> <td> 8989.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>          <td> -197.2840</td> <td> 5777.805</td> <td>   -0.034</td> <td> 0.973</td> <td>-1.15e+04</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>          <td> -824.1046</td> <td> 2249.095</td> <td>   -0.366</td> <td> 0.714</td> <td>-5232.738</td> <td> 3584.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>          <td> 6895.7899</td> <td> 2078.804</td> <td>    3.317</td> <td> 0.001</td> <td> 2820.959</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>          <td> 6945.8749</td> <td> 2158.807</td> <td>    3.217</td> <td> 0.001</td> <td> 2714.223</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>          <td> 4199.8439</td> <td> 1976.004</td> <td>    2.125</td> <td> 0.034</td> <td>  326.520</td> <td> 8073.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>          <td>-1.007e+04</td> <td> 2324.374</td> <td>   -4.333</td> <td> 0.000</td> <td>-1.46e+04</td> <td>-5515.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>          <td> 4783.5174</td> <td> 1957.334</td> <td>    2.444</td> <td> 0.015</td> <td>  946.789</td> <td> 8620.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>          <td> 5706.0069</td> <td> 1994.958</td> <td>    2.860</td> <td> 0.004</td> <td> 1795.529</td> <td> 9616.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>          <td> 4197.2794</td> <td> 1940.081</td> <td>    2.163</td> <td> 0.031</td> <td>  394.370</td> <td> 8000.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>          <td> 4765.8184</td> <td> 2072.714</td> <td>    2.299</td> <td> 0.022</td> <td>  702.925</td> <td> 8828.711</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>          <td> 4665.2623</td> <td> 2093.953</td> <td>    2.228</td> <td> 0.026</td> <td>  560.737</td> <td> 8769.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>          <td> 8079.3314</td> <td> 2326.385</td> <td>    3.473</td> <td> 0.001</td> <td> 3519.196</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>          <td> 2089.8900</td> <td> 2639.756</td> <td>    0.792</td> <td> 0.429</td> <td>-3084.508</td> <td> 7264.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>          <td> 6321.4135</td> <td> 2268.622</td> <td>    2.786</td> <td> 0.005</td> <td> 1874.504</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>          <td> 6840.7994</td> <td> 2345.521</td> <td>    2.917</td> <td> 0.004</td> <td> 2243.154</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>          <td> 4644.0715</td> <td> 2100.630</td> <td>    2.211</td> <td> 0.027</td> <td>  526.457</td> <td> 8761.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>           <td>-6366.8780</td> <td> 3363.784</td> <td>   -1.893</td> <td> 0.058</td> <td> -1.3e+04</td> <td>  226.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>          <td> 5133.5940</td> <td> 2073.634</td> <td>    2.476</td> <td> 0.013</td> <td> 1068.896</td> <td> 9198.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>          <td> 3952.6312</td> <td> 1986.684</td> <td>    1.990</td> <td> 0.047</td> <td>   58.371</td> <td> 7846.892</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>          <td> 5744.5432</td> <td> 1983.144</td> <td>    2.897</td> <td> 0.004</td> <td> 1857.222</td> <td> 9631.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>          <td>-8106.7935</td> <td> 3073.932</td> <td>   -2.637</td> <td> 0.008</td> <td>-1.41e+04</td> <td>-2081.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>          <td> 1669.8447</td> <td> 2286.765</td> <td>    0.730</td> <td> 0.465</td> <td>-2812.629</td> <td> 6152.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>          <td> 4536.0583</td> <td> 1955.701</td> <td>    2.319</td> <td> 0.020</td> <td>  702.530</td> <td> 8369.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>          <td> 5413.8389</td> <td> 3138.722</td> <td>    1.725</td> <td> 0.085</td> <td> -738.624</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>          <td>-2.185e+04</td> <td> 2912.705</td> <td>   -7.501</td> <td> 0.000</td> <td>-2.76e+04</td> <td>-1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>           <td> 4218.0132</td> <td> 1946.131</td> <td>    2.167</td> <td> 0.030</td> <td>  403.245</td> <td> 8032.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>          <td> -617.7506</td> <td> 2955.061</td> <td>   -0.209</td> <td> 0.834</td> <td>-6410.204</td> <td> 5174.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>          <td> 2049.0538</td> <td> 2095.262</td> <td>    0.978</td> <td> 0.328</td> <td>-2058.039</td> <td> 6156.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>          <td>-3868.0472</td> <td> 2274.118</td> <td>   -1.701</td> <td> 0.089</td> <td>-8325.729</td> <td>  589.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>           <td> 4931.2580</td> <td> 2128.145</td> <td>    2.317</td> <td> 0.021</td> <td>  759.709</td> <td> 9102.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>          <td> 6165.2247</td> <td> 2062.464</td> <td>    2.989</td> <td> 0.003</td> <td> 2122.423</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>          <td> 4275.1968</td> <td> 1935.192</td> <td>    2.209</td> <td> 0.027</td> <td>  481.871</td> <td> 8068.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>          <td> 1557.1493</td> <td> 2024.883</td> <td>    0.769</td> <td> 0.442</td> <td>-2411.987</td> <td> 5526.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>          <td> 4720.4704</td> <td> 2154.915</td> <td>    2.191</td> <td> 0.029</td> <td>  496.447</td> <td> 8944.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>           <td> 2488.6937</td> <td> 1886.884</td> <td>    1.319</td> <td> 0.187</td> <td>-1209.941</td> <td> 6187.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>          <td> -200.9822</td> <td> 1889.135</td> <td>   -0.106</td> <td> 0.915</td> <td>-3904.028</td> <td> 3502.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>          <td> 6457.2843</td> <td> 2198.200</td> <td>    2.938</td> <td> 0.003</td> <td> 2148.416</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>          <td> 6006.4959</td> <td> 1987.428</td> <td>    3.022</td> <td> 0.003</td> <td> 2110.778</td> <td> 9902.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>          <td> -789.7831</td> <td> 3964.684</td> <td>   -0.199</td> <td> 0.842</td> <td>-8561.281</td> <td> 6981.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>          <td>-1.267e+04</td> <td> 3411.232</td> <td>   -3.715</td> <td> 0.000</td> <td>-1.94e+04</td> <td>-5987.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>          <td>-1.206e+04</td> <td> 2531.389</td> <td>   -4.764</td> <td> 0.000</td> <td> -1.7e+04</td> <td>-7098.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>          <td> 4171.0192</td> <td> 2257.420</td> <td>    1.848</td> <td> 0.065</td> <td> -253.932</td> <td> 8595.970</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>          <td>-5172.1710</td> <td> 2042.636</td> <td>   -2.532</td> <td> 0.011</td> <td>-9176.107</td> <td>-1168.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>         <td>-2.789e+04</td> <td> 6149.417</td> <td>   -4.535</td> <td> 0.000</td> <td>-3.99e+04</td> <td>-1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>          <td>-1181.1886</td> <td> 2495.127</td> <td>   -0.473</td> <td> 0.636</td> <td>-6072.089</td> <td> 3709.712</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>          <td>  545.6900</td> <td> 1922.062</td> <td>    0.284</td> <td> 0.776</td> <td>-3221.899</td> <td> 4313.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>          <td> 5026.9146</td> <td> 2292.070</td> <td>    2.193</td> <td> 0.028</td> <td>  534.044</td> <td> 9519.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>          <td>-1477.5156</td> <td> 2636.836</td> <td>   -0.560</td> <td> 0.575</td> <td>-6646.191</td> <td> 3691.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>          <td> 5191.7781</td> <td> 1975.590</td> <td>    2.628</td> <td> 0.009</td> <td> 1319.264</td> <td> 9064.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>          <td> 7203.5936</td> <td> 2339.239</td> <td>    3.079</td> <td> 0.002</td> <td> 2618.262</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>          <td> 3733.9797</td> <td> 2049.829</td> <td>    1.822</td> <td> 0.069</td> <td> -284.056</td> <td> 7752.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>          <td>-1439.5696</td> <td> 3281.929</td> <td>   -0.439</td> <td> 0.661</td> <td>-7872.744</td> <td> 4993.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>          <td> 5773.2736</td> <td> 2176.516</td> <td>    2.653</td> <td> 0.008</td> <td> 1506.910</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>          <td> 9111.8670</td> <td> 2099.723</td> <td>    4.340</td> <td> 0.000</td> <td> 4996.032</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>           <td>-8301.6528</td> <td> 2839.135</td> <td>   -2.924</td> <td> 0.003</td> <td>-1.39e+04</td> <td>-2736.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>          <td>-1.189e+04</td> <td> 2108.620</td> <td>   -5.641</td> <td> 0.000</td> <td> -1.6e+04</td> <td>-7760.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>          <td> 6682.8046</td> <td> 2394.474</td> <td>    2.791</td> <td> 0.005</td> <td> 1989.204</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>          <td>-1.279e+04</td> <td> 2592.406</td> <td>   -4.934</td> <td> 0.000</td> <td>-1.79e+04</td> <td>-7710.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>          <td> 3919.2168</td> <td> 2498.677</td> <td>    1.569</td> <td> 0.117</td> <td> -978.641</td> <td> 8817.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>          <td> 5747.8300</td> <td> 2143.449</td> <td>    2.682</td> <td> 0.007</td> <td> 1546.283</td> <td> 9949.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>          <td>-8331.5343</td> <td> 3651.748</td> <td>   -2.282</td> <td> 0.023</td> <td>-1.55e+04</td> <td>-1173.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>          <td>-2.112e+04</td> <td> 4023.967</td> <td>   -5.248</td> <td> 0.000</td> <td> -2.9e+04</td> <td>-1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>          <td> 3226.7944</td> <td> 2284.810</td> <td>    1.412</td> <td> 0.158</td> <td>-1251.846</td> <td> 7705.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>          <td> 2422.7192</td> <td> 6280.572</td> <td>    0.386</td> <td> 0.700</td> <td>-9888.336</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>          <td> 6890.8995</td> <td> 2605.114</td> <td>    2.645</td> <td> 0.008</td> <td> 1784.406</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>          <td> 5029.9389</td> <td> 2786.153</td> <td>    1.805</td> <td> 0.071</td> <td> -431.424</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>           <td> 1708.8772</td> <td> 2085.140</td> <td>    0.820</td> <td> 0.412</td> <td>-2378.375</td> <td> 5796.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>          <td>-2.297e+04</td> <td> 3534.423</td> <td>   -6.499</td> <td> 0.000</td> <td>-2.99e+04</td> <td> -1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>          <td>  5.75e+04</td> <td> 2527.854</td> <td>   22.746</td> <td> 0.000</td> <td> 5.25e+04</td> <td> 6.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>          <td>  808.8317</td> <td> 3469.279</td> <td>    0.233</td> <td> 0.816</td> <td>-5991.583</td> <td> 7609.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>          <td>-1.141e+04</td> <td> 2724.193</td> <td>   -4.188</td> <td> 0.000</td> <td>-1.67e+04</td> <td>-6069.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>          <td>-9001.0664</td> <td> 2642.143</td> <td>   -3.407</td> <td> 0.001</td> <td>-1.42e+04</td> <td>-3821.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>          <td>-5606.7171</td> <td> 2305.951</td> <td>   -2.431</td> <td> 0.015</td> <td>-1.01e+04</td> <td>-1086.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>          <td> 4379.7449</td> <td> 2537.612</td> <td>    1.726</td> <td> 0.084</td> <td> -594.434</td> <td> 9353.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>          <td> 4077.6444</td> <td> 2924.043</td> <td>    1.395</td> <td> 0.163</td> <td>-1654.009</td> <td> 9809.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>           <td>  326.7095</td> <td> 1863.890</td> <td>    0.175</td> <td> 0.861</td> <td>-3326.852</td> <td> 3980.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>          <td> 2253.3827</td> <td> 2590.929</td> <td>    0.870</td> <td> 0.384</td> <td>-2825.307</td> <td> 7332.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>          <td>  -22.7316</td> <td> 4710.894</td> <td>   -0.005</td> <td> 0.996</td> <td>-9256.935</td> <td> 9211.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>           <td> 1423.9623</td> <td> 2054.826</td> <td>    0.693</td> <td> 0.488</td> <td>-2603.868</td> <td> 5451.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>          <td> 4518.8382</td> <td> 2505.697</td> <td>    1.803</td> <td> 0.071</td> <td> -392.781</td> <td> 9430.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>          <td> 4122.4380</td> <td> 2402.266</td> <td>    1.716</td> <td> 0.086</td> <td> -586.437</td> <td> 8831.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>          <td> 1494.0695</td> <td> 2664.284</td> <td>    0.561</td> <td> 0.575</td> <td>-3728.408</td> <td> 6716.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>          <td> 1593.8538</td> <td> 2255.282</td> <td>    0.707</td> <td> 0.480</td> <td>-2826.906</td> <td> 6014.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>          <td>  614.0467</td> <td> 4796.181</td> <td>    0.128</td> <td> 0.898</td> <td>-8787.335</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>          <td> 6568.4522</td> <td> 2642.362</td> <td>    2.486</td> <td> 0.013</td> <td> 1388.946</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>          <td>-2.237e+04</td> <td> 3483.810</td> <td>   -6.420</td> <td> 0.000</td> <td>-2.92e+04</td> <td>-1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>           <td> 6170.5617</td> <td> 2354.740</td> <td>    2.620</td> <td> 0.009</td> <td> 1554.845</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>          <td>-1.963e+04</td> <td> 5046.923</td> <td>   -3.890</td> <td> 0.000</td> <td>-2.95e+04</td> <td>-9740.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>           <td> 5074.0949</td> <td> 2453.744</td> <td>    2.068</td> <td> 0.039</td> <td>  264.314</td> <td> 9883.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>           <td> 4313.6075</td> <td> 1939.878</td> <td>    2.224</td> <td> 0.026</td> <td>  511.096</td> <td> 8116.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>          <td> 6303.9339</td> <td> 2463.337</td> <td>    2.559</td> <td> 0.011</td> <td> 1475.348</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>          <td> -114.8493</td> <td> 2351.336</td> <td>   -0.049</td> <td> 0.961</td> <td>-4723.892</td> <td> 4494.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>           <td>-9291.4165</td> <td> 2954.040</td> <td>   -3.145</td> <td> 0.002</td> <td>-1.51e+04</td> <td>-3500.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>          <td>-3949.8216</td> <td> 4698.787</td> <td>   -0.841</td> <td> 0.401</td> <td>-1.32e+04</td> <td> 5260.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>           <td>-1.257e+04</td> <td> 4428.531</td> <td>   -2.838</td> <td> 0.005</td> <td>-2.12e+04</td> <td>-3888.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>          <td> -868.4950</td> <td> 3352.923</td> <td>   -0.259</td> <td> 0.796</td> <td>-7440.829</td> <td> 5703.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>          <td>-2.809e+04</td> <td> 4496.713</td> <td>   -6.248</td> <td> 0.000</td> <td>-3.69e+04</td> <td>-1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>          <td> 2055.5421</td> <td> 2915.430</td> <td>    0.705</td> <td> 0.481</td> <td>-3659.228</td> <td> 7770.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>          <td> 5280.9784</td> <td> 2332.553</td> <td>    2.264</td> <td> 0.024</td> <td>  708.753</td> <td> 9853.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>          <td>-5374.5504</td> <td> 2343.945</td> <td>   -2.293</td> <td> 0.022</td> <td>-9969.106</td> <td> -779.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>          <td> 6302.2748</td> <td> 2678.796</td> <td>    2.353</td> <td> 0.019</td> <td> 1051.351</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>          <td>-9069.5391</td> <td> 3742.808</td> <td>   -2.423</td> <td> 0.015</td> <td>-1.64e+04</td> <td>-1732.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>          <td> 7043.7827</td> <td> 2741.083</td> <td>    2.570</td> <td> 0.010</td> <td> 1670.765</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>           <td>  -1.8e+04</td> <td> 4300.929</td> <td>   -4.185</td> <td> 0.000</td> <td>-2.64e+04</td> <td>-9568.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>          <td> 1884.4479</td> <td> 2395.362</td> <td>    0.787</td> <td> 0.431</td> <td>-2810.893</td> <td> 6579.789</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>           <td>-8586.9305</td> <td> 2750.602</td> <td>   -3.122</td> <td> 0.002</td> <td> -1.4e+04</td> <td>-3195.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>           <td>-6198.7549</td> <td> 2027.587</td> <td>   -3.057</td> <td> 0.002</td> <td>-1.02e+04</td> <td>-2224.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>          <td> 4327.8070</td> <td> 2618.221</td> <td>    1.653</td> <td> 0.098</td> <td> -804.379</td> <td> 9459.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>          <td> 6758.0994</td> <td> 3486.673</td> <td>    1.938</td> <td> 0.053</td> <td>  -76.410</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>          <td>-1.273e+04</td> <td> 3053.517</td> <td>   -4.169</td> <td> 0.000</td> <td>-1.87e+04</td> <td>-6745.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>           <td> 2184.6376</td> <td> 2356.026</td> <td>    0.927</td> <td> 0.354</td> <td>-2433.599</td> <td> 6802.874</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>          <td> 1481.5536</td> <td> 2474.573</td> <td>    0.599</td> <td> 0.549</td> <td>-3369.057</td> <td> 6332.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>          <td> 3460.5188</td> <td> 2779.609</td> <td>    1.245</td> <td> 0.213</td> <td>-1988.018</td> <td> 8909.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>          <td>-1.022e+04</td> <td> 2902.442</td> <td>   -3.522</td> <td> 0.000</td> <td>-1.59e+04</td> <td>-4533.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>          <td> 1784.4463</td> <td> 3174.752</td> <td>    0.562</td> <td> 0.574</td> <td>-4438.641</td> <td> 8007.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>           <td> 5170.3233</td> <td> 4124.969</td> <td>    1.253</td> <td> 0.210</td> <td>-2915.361</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>          <td> 5761.9653</td> <td> 2680.234</td> <td>    2.150</td> <td> 0.032</td> <td>  508.223</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>          <td> 4023.6148</td> <td> 8181.170</td> <td>    0.492</td> <td> 0.623</td> <td> -1.2e+04</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>           <td> 3253.6294</td> <td> 2827.174</td> <td>    1.151</td> <td> 0.250</td> <td>-2288.143</td> <td> 8795.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>           <td> 6773.6570</td> <td> 2589.057</td> <td>    2.616</td> <td> 0.009</td> <td> 1698.637</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>          <td> 2946.6509</td> <td> 4736.264</td> <td>    0.622</td> <td> 0.534</td> <td>-6337.283</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>          <td> 4472.7867</td> <td> 2590.021</td> <td>    1.727</td> <td> 0.084</td> <td> -604.121</td> <td> 9549.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>           <td> 6599.9720</td> <td> 2091.312</td> <td>    3.156</td> <td> 0.002</td> <td> 2500.622</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>          <td> 3568.1880</td> <td> 2481.632</td> <td>    1.438</td> <td> 0.151</td> <td>-1296.259</td> <td> 8432.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>          <td>-2104.0140</td> <td> 2656.087</td> <td>   -0.792</td> <td> 0.428</td> <td>-7310.425</td> <td> 3102.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>           <td> 3367.1680</td> <td> 1934.110</td> <td>    1.741</td> <td> 0.082</td> <td> -424.037</td> <td> 7158.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>          <td> -277.1102</td> <td> 3475.397</td> <td>   -0.080</td> <td> 0.936</td> <td>-7089.517</td> <td> 6535.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>           <td> 6356.9665</td> <td> 2083.527</td> <td>    3.051</td> <td> 0.002</td> <td> 2272.876</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>          <td>-3.007e+04</td> <td> 4710.742</td> <td>   -6.384</td> <td> 0.000</td> <td>-3.93e+04</td> <td>-2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>          <td> 2482.5945</td> <td> 2540.937</td> <td>    0.977</td> <td> 0.329</td> <td>-2498.101</td> <td> 7463.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>          <td> 5239.0419</td> <td> 2946.614</td> <td>    1.778</td> <td> 0.075</td> <td> -536.854</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>           <td>-9946.6783</td> <td> 4696.052</td> <td>   -2.118</td> <td> 0.034</td> <td>-1.92e+04</td> <td> -741.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>           <td> 2771.8015</td> <td> 2169.241</td> <td>    1.278</td> <td> 0.201</td> <td>-1480.302</td> <td> 7023.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>           <td> 8998.4516</td> <td> 2219.156</td> <td>    4.055</td> <td> 0.000</td> <td> 4648.505</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>           <td> 5299.0984</td> <td> 2089.204</td> <td>    2.536</td> <td> 0.011</td> <td> 1203.881</td> <td> 9394.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>          <td>-1213.8922</td> <td> 2753.992</td> <td>   -0.441</td> <td> 0.659</td> <td>-6612.214</td> <td> 4184.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>           <td>-7522.0773</td> <td> 2191.983</td> <td>   -3.432</td> <td> 0.001</td> <td>-1.18e+04</td> <td>-3225.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>           <td> 2211.8725</td> <td> 1905.594</td> <td>    1.161</td> <td> 0.246</td> <td>-1523.437</td> <td> 5947.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>           <td>-2.008e+04</td> <td> 4095.903</td> <td>   -4.902</td> <td> 0.000</td> <td>-2.81e+04</td> <td>-1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>          <td>-1.125e+04</td> <td> 3803.823</td> <td>   -2.958</td> <td> 0.003</td> <td>-1.87e+04</td> <td>-3795.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>          <td>-6466.0225</td> <td> 3287.215</td> <td>   -1.967</td> <td> 0.049</td> <td>-1.29e+04</td> <td>  -22.487</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>           <td>-2151.8718</td> <td> 1896.807</td> <td>   -1.134</td> <td> 0.257</td> <td>-5869.957</td> <td> 1566.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>           <td> 6065.6852</td> <td> 2012.244</td> <td>    3.014</td> <td> 0.003</td> <td> 2121.323</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>           <td> 7504.9048</td> <td> 2023.292</td> <td>    3.709</td> <td> 0.000</td> <td> 3538.887</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>          <td>-2286.6678</td> <td> 2709.468</td> <td>   -0.844</td> <td> 0.399</td> <td>-7597.715</td> <td> 3024.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>          <td>-3762.6345</td> <td> 2609.260</td> <td>   -1.442</td> <td> 0.149</td> <td>-8877.255</td> <td> 1351.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>           <td> -2.08e+04</td> <td> 2791.669</td> <td>   -7.450</td> <td> 0.000</td> <td>-2.63e+04</td> <td>-1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>           <td> 2346.6634</td> <td> 2095.044</td> <td>    1.120</td> <td> 0.263</td> <td>-1760.002</td> <td> 6453.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>          <td> 3189.7453</td> <td> 2706.630</td> <td>    1.178</td> <td> 0.239</td> <td>-2115.739</td> <td> 8495.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>           <td> 3336.7948</td> <td> 1989.516</td> <td>    1.677</td> <td> 0.094</td> <td> -563.016</td> <td> 7236.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>           <td> 6131.9418</td> <td> 2062.060</td> <td>    2.974</td> <td> 0.003</td> <td> 2089.931</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>           <td> 5846.0477</td> <td> 3270.715</td> <td>    1.787</td> <td> 0.074</td> <td> -565.144</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>           <td> 4828.8974</td> <td> 2054.183</td> <td>    2.351</td> <td> 0.019</td> <td>  802.327</td> <td> 8855.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>           <td> -969.7047</td> <td> 3354.419</td> <td>   -0.289</td> <td> 0.773</td> <td>-7544.972</td> <td> 5605.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>           <td> 5652.1795</td> <td> 2378.876</td> <td>    2.376</td> <td> 0.018</td> <td>  989.152</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>           <td>-1.672e+04</td> <td> 3119.593</td> <td>   -5.359</td> <td> 0.000</td> <td>-2.28e+04</td> <td>-1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>          <td> 1594.9441</td> <td> 2630.445</td> <td>    0.606</td> <td> 0.544</td> <td>-3561.204</td> <td> 6751.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>           <td>    2.9836</td> <td> 2130.290</td> <td>    0.001</td> <td> 0.999</td> <td>-4172.770</td> <td> 4178.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>           <td>-1.004e+04</td> <td> 2580.542</td> <td>   -3.889</td> <td> 0.000</td> <td>-1.51e+04</td> <td>-4978.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>          <td> 6006.9608</td> <td> 3720.280</td> <td>    1.615</td> <td> 0.106</td> <td>-1285.460</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>           <td>-2027.7996</td> <td> 4116.389</td> <td>   -0.493</td> <td> 0.622</td> <td>-1.01e+04</td> <td> 6041.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>           <td> 4823.5273</td> <td> 2285.510</td> <td>    2.110</td> <td> 0.035</td> <td>  343.515</td> <td> 9303.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>           <td> -289.6444</td> <td> 2851.514</td> <td>   -0.102</td> <td> 0.919</td> <td>-5879.126</td> <td> 5299.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>           <td> 4567.3493</td> <td> 2086.784</td> <td>    2.189</td> <td> 0.029</td> <td>  476.876</td> <td> 8657.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>           <td> -953.3743</td> <td> 2167.612</td> <td>   -0.440</td> <td> 0.660</td> <td>-5202.285</td> <td> 3295.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>           <td> 3165.5363</td> <td> 2226.409</td> <td>    1.422</td> <td> 0.155</td> <td>-1198.627</td> <td> 7529.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>           <td> 6081.4701</td> <td> 2017.956</td> <td>    3.014</td> <td> 0.003</td> <td> 2125.912</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>           <td> 4850.5498</td> <td> 2023.911</td> <td>    2.397</td> <td> 0.017</td> <td>  883.319</td> <td> 8817.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>           <td>-7544.9919</td> <td> 4408.703</td> <td>   -1.711</td> <td> 0.087</td> <td>-1.62e+04</td> <td> 1096.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>           <td> 3644.4394</td> <td> 2385.214</td> <td>    1.528</td> <td> 0.127</td> <td>-1031.010</td> <td> 8319.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>           <td> 3380.6606</td> <td> 1992.552</td> <td>    1.697</td> <td> 0.090</td> <td> -525.101</td> <td> 7286.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>           <td> 5182.8595</td> <td> 2548.647</td> <td>    2.034</td> <td> 0.042</td> <td>  187.051</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>           <td> 1445.5060</td> <td> 1882.500</td> <td>    0.768</td> <td> 0.443</td> <td>-2244.534</td> <td> 5135.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>           <td> 3380.4050</td> <td> 1967.070</td> <td>    1.718</td> <td> 0.086</td> <td> -475.408</td> <td> 7236.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>           <td> 7278.8502</td> <td> 2303.784</td> <td>    3.160</td> <td> 0.002</td> <td> 2763.016</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>          <td> 2.854e+04</td> <td> 2746.675</td> <td>   10.392</td> <td> 0.000</td> <td> 2.32e+04</td> <td> 3.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>           <td>-1.883e+04</td> <td> 3413.523</td> <td>   -5.517</td> <td> 0.000</td> <td>-2.55e+04</td> <td>-1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>           <td> 2149.7707</td> <td> 1932.564</td> <td>    1.112</td> <td> 0.266</td> <td>-1638.405</td> <td> 5937.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>           <td> 2163.5491</td> <td> 1893.292</td> <td>    1.143</td> <td> 0.253</td> <td>-1547.645</td> <td> 5874.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>          <td>-3625.3580</td> <td> 3523.717</td> <td>   -1.029</td> <td> 0.304</td> <td>-1.05e+04</td> <td> 3281.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>          <td> 4344.0112</td> <td> 2909.842</td> <td>    1.493</td> <td> 0.136</td> <td>-1359.805</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>           <td> 3467.0513</td> <td> 2301.773</td> <td>    1.506</td> <td> 0.132</td> <td>-1044.839</td> <td> 7978.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>           <td> 8200.4701</td> <td> 3715.945</td> <td>    2.207</td> <td> 0.027</td> <td>  916.547</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>          <td>-3.085e+04</td> <td> 4799.510</td> <td>   -6.429</td> <td> 0.000</td> <td>-4.03e+04</td> <td>-2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>           <td> 4205.3975</td> <td> 1940.208</td> <td>    2.167</td> <td> 0.030</td> <td>  402.239</td> <td> 8008.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>           <td> 3.625e+04</td> <td> 2605.300</td> <td>   13.916</td> <td> 0.000</td> <td> 3.11e+04</td> <td> 4.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>           <td> 6803.4859</td> <td> 2312.364</td> <td>    2.942</td> <td> 0.003</td> <td> 2270.835</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>           <td>-2085.8933</td> <td> 2445.950</td> <td>   -0.853</td> <td> 0.394</td> <td>-6880.397</td> <td> 2708.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>           <td> 4192.9522</td> <td> 2000.417</td> <td>    2.096</td> <td> 0.036</td> <td>  271.774</td> <td> 8114.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>          <td> 7163.5586</td> <td> 2911.451</td> <td>    2.460</td> <td> 0.014</td> <td> 1456.589</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>           <td>  333.1924</td> <td> 5725.502</td> <td>    0.058</td> <td> 0.954</td> <td>-1.09e+04</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>           <td> 3004.8609</td> <td> 2510.486</td> <td>    1.197</td> <td> 0.231</td> <td>-1916.146</td> <td> 7925.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>          <td>-1.276e+04</td> <td> 3489.731</td> <td>   -3.655</td> <td> 0.000</td> <td>-1.96e+04</td> <td>-5915.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>           <td> 3816.8344</td> <td> 1983.445</td> <td>    1.924</td> <td> 0.054</td> <td>  -71.076</td> <td> 7704.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>          <td> 5331.2899</td> <td> 2649.322</td> <td>    2.012</td> <td> 0.044</td> <td>  138.139</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>           <td>-1.085e+04</td> <td> 2375.808</td> <td>   -4.567</td> <td> 0.000</td> <td>-1.55e+04</td> <td>-6192.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>          <td>-2744.9098</td> <td> 2591.812</td> <td>   -1.059</td> <td> 0.290</td> <td>-7825.330</td> <td> 2335.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>           <td>-1.624e+04</td> <td> 2387.293</td> <td>   -6.801</td> <td> 0.000</td> <td>-2.09e+04</td> <td>-1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>           <td> -256.5923</td> <td> 1984.286</td> <td>   -0.129</td> <td> 0.897</td> <td>-4146.152</td> <td> 3632.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>           <td> 6491.9746</td> <td> 3565.988</td> <td>    1.821</td> <td> 0.069</td> <td> -498.005</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>           <td>-2443.0783</td> <td> 2378.840</td> <td>   -1.027</td> <td> 0.304</td> <td>-7106.035</td> <td> 2219.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>          <td> 3256.5419</td> <td> 2781.549</td> <td>    1.171</td> <td> 0.242</td> <td>-2195.796</td> <td> 8708.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>          <td>-1.728e+04</td> <td> 4888.147</td> <td>   -3.535</td> <td> 0.000</td> <td>-2.69e+04</td> <td>-7698.600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>           <td>-1.305e+04</td> <td> 4762.126</td> <td>   -2.741</td> <td> 0.006</td> <td>-2.24e+04</td> <td>-3720.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>           <td> 2875.2759</td> <td> 2799.555</td> <td>    1.027</td> <td> 0.304</td> <td>-2612.359</td> <td> 8362.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>          <td>-1.662e+04</td> <td> 4874.835</td> <td>   -3.410</td> <td> 0.001</td> <td>-2.62e+04</td> <td>-7069.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>           <td> 6583.9088</td> <td> 2113.016</td> <td>    3.116</td> <td> 0.002</td> <td> 2442.015</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>           <td>  -47.8831</td> <td> 3109.113</td> <td>   -0.015</td> <td> 0.988</td> <td>-6142.307</td> <td> 6046.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>           <td> 4787.7759</td> <td> 3460.271</td> <td>    1.384</td> <td> 0.166</td> <td>-1994.980</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>           <td> 8463.4987</td> <td> 2321.080</td> <td>    3.646</td> <td> 0.000</td> <td> 3913.763</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>           <td> 1530.5307</td> <td> 2561.779</td> <td>    0.597</td> <td> 0.550</td> <td>-3491.019</td> <td> 6552.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>           <td> 2254.3657</td> <td> 1877.260</td> <td>    1.201</td> <td> 0.230</td> <td>-1425.402</td> <td> 5934.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>           <td> 5903.1174</td> <td> 3746.308</td> <td>    1.576</td> <td> 0.115</td> <td>-1440.323</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>          <td> 4603.8491</td> <td> 2983.561</td> <td>    1.543</td> <td> 0.123</td> <td>-1244.469</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>          <td>-1.921e+04</td> <td> 4150.154</td> <td>   -4.628</td> <td> 0.000</td> <td>-2.73e+04</td> <td>-1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>           <td> 6797.2824</td> <td> 2144.613</td> <td>    3.169</td> <td> 0.002</td> <td> 2593.454</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>          <td> 5782.6663</td> <td> 3238.460</td> <td>    1.786</td> <td> 0.074</td> <td> -565.300</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>           <td>-7972.3078</td> <td> 2257.440</td> <td>   -3.532</td> <td> 0.000</td> <td>-1.24e+04</td> <td>-3547.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>           <td>-8315.0626</td> <td> 4485.169</td> <td>   -1.854</td> <td> 0.064</td> <td>-1.71e+04</td> <td>  476.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>           <td> 5591.6331</td> <td> 2205.029</td> <td>    2.536</td> <td> 0.011</td> <td> 1269.377</td> <td> 9913.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>          <td> 5238.7752</td> <td> 2942.493</td> <td>    1.780</td> <td> 0.075</td> <td> -529.043</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>           <td> 5784.5599</td> <td> 2014.599</td> <td>    2.871</td> <td> 0.004</td> <td> 1835.581</td> <td> 9733.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>          <td> 4986.0492</td> <td> 8160.081</td> <td>    0.611</td> <td> 0.541</td> <td> -1.1e+04</td> <td>  2.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>          <td> 3037.1248</td> <td> 2916.279</td> <td>    1.041</td> <td> 0.298</td> <td>-2679.309</td> <td> 8753.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>           <td>-1.692e+04</td> <td> 3354.084</td> <td>   -5.045</td> <td> 0.000</td> <td>-2.35e+04</td> <td>-1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>           <td> 5632.8772</td> <td> 3163.316</td> <td>    1.781</td> <td> 0.075</td> <td> -567.794</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>          <td> 5075.2008</td> <td> 4745.757</td> <td>    1.069</td> <td> 0.285</td> <td>-4227.339</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>           <td>-6418.4704</td> <td> 2389.841</td> <td>   -2.686</td> <td> 0.007</td> <td>-1.11e+04</td> <td>-1733.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>           <td> 4466.8948</td> <td> 2058.115</td> <td>    2.170</td> <td> 0.030</td> <td>  432.619</td> <td> 8501.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>           <td>-1014.9357</td> <td> 2596.501</td> <td>   -0.391</td> <td> 0.696</td> <td>-6104.548</td> <td> 4074.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>          <td> 4790.8815</td> <td> 3119.773</td> <td>    1.536</td> <td> 0.125</td> <td>-1324.437</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>           <td> 2524.7432</td> <td> 2073.188</td> <td>    1.218</td> <td> 0.223</td> <td>-1539.079</td> <td> 6588.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>           <td> 3346.2565</td> <td> 2078.323</td> <td>    1.610</td> <td> 0.107</td> <td> -727.631</td> <td> 7420.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>           <td> -446.1903</td> <td> 2660.117</td> <td>   -0.168</td> <td> 0.867</td> <td>-5660.500</td> <td> 4768.119</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>           <td> 9004.9554</td> <td> 1991.962</td> <td>    4.521</td> <td> 0.000</td> <td> 5100.349</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>           <td>  -66.6719</td> <td> 2575.189</td> <td>   -0.026</td> <td> 0.979</td> <td>-5114.508</td> <td> 4981.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>           <td> 4881.9340</td> <td> 1979.048</td> <td>    2.467</td> <td> 0.014</td> <td> 1002.643</td> <td> 8761.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>           <td>-1.552e+04</td> <td> 2763.530</td> <td>   -5.615</td> <td> 0.000</td> <td>-2.09e+04</td> <td>-1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>           <td> 6137.3783</td> <td> 2114.748</td> <td>    2.902</td> <td> 0.004</td> <td> 1992.089</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>           <td>-5000.6402</td> <td> 4011.657</td> <td>   -1.247</td> <td> 0.213</td> <td>-1.29e+04</td> <td> 2862.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>          <td>-2.378e+04</td> <td> 4087.266</td> <td>   -5.818</td> <td> 0.000</td> <td>-3.18e+04</td> <td>-1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>          <td> 3013.5790</td> <td> 1921.809</td> <td>    1.568</td> <td> 0.117</td> <td> -753.514</td> <td> 6780.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>          <td>-1.926e+04</td> <td> 4080.807</td> <td>   -4.719</td> <td> 0.000</td> <td>-2.73e+04</td> <td>-1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>           <td> 4118.3943</td> <td> 2300.840</td> <td>    1.790</td> <td> 0.073</td> <td> -391.668</td> <td> 8628.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>           <td> 5739.4685</td> <td> 2778.858</td> <td>    2.065</td> <td> 0.039</td> <td>  292.405</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>           <td> -675.3118</td> <td> 1962.344</td> <td>   -0.344</td> <td> 0.731</td> <td>-4521.861</td> <td> 3171.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>          <td>-1.405e+04</td> <td> 4169.759</td> <td>   -3.369</td> <td> 0.001</td> <td>-2.22e+04</td> <td>-5875.339</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>           <td>-3.247e+04</td> <td> 6182.126</td> <td>   -5.252</td> <td> 0.000</td> <td>-4.46e+04</td> <td>-2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>           <td> 6577.8719</td> <td> 2613.091</td> <td>    2.517</td> <td> 0.012</td> <td> 1455.742</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>           <td> 4119.1629</td> <td> 1920.034</td> <td>    2.145</td> <td> 0.032</td> <td>  355.549</td> <td> 7882.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>           <td> 2169.0739</td> <td> 3042.070</td> <td>    0.713</td> <td> 0.476</td> <td>-3793.933</td> <td> 8132.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>           <td>-5327.7936</td> <td> 1991.703</td> <td>   -2.675</td> <td> 0.007</td> <td>-9231.892</td> <td>-1423.695</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>           <td> 3763.8889</td> <td> 2022.753</td> <td>    1.861</td> <td> 0.063</td> <td> -201.072</td> <td> 7728.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>           <td>-1.351e+04</td> <td> 2696.623</td> <td>   -5.011</td> <td> 0.000</td> <td>-1.88e+04</td> <td>-8225.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>           <td>-1.257e+04</td> <td> 3244.684</td> <td>   -3.873</td> <td> 0.000</td> <td>-1.89e+04</td> <td>-6207.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>           <td> 6493.4216</td> <td> 2213.043</td> <td>    2.934</td> <td> 0.003</td> <td> 2155.458</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>           <td> 2850.5423</td> <td> 1969.259</td> <td>    1.448</td> <td> 0.148</td> <td>-1009.561</td> <td> 6710.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>           <td> -2.59e+04</td> <td> 4494.210</td> <td>   -5.763</td> <td> 0.000</td> <td>-3.47e+04</td> <td>-1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>           <td> 6092.0871</td> <td> 2240.951</td> <td>    2.719</td> <td> 0.007</td> <td> 1699.418</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>           <td>-6736.3744</td> <td> 2825.111</td> <td>   -2.384</td> <td> 0.017</td> <td>-1.23e+04</td> <td>-1198.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>           <td> 4232.4660</td> <td> 3722.982</td> <td>    1.137</td> <td> 0.256</td> <td>-3065.252</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>           <td> 6528.7474</td> <td> 1957.676</td> <td>    3.335</td> <td> 0.001</td> <td> 2691.349</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>           <td> 4801.7854</td> <td> 2046.379</td> <td>    2.346</td> <td> 0.019</td> <td>  790.512</td> <td> 8813.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>           <td> 5.303e+04</td> <td> 2028.324</td> <td>   26.145</td> <td> 0.000</td> <td> 4.91e+04</td> <td>  5.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>           <td> 3484.8827</td> <td> 2531.197</td> <td>    1.377</td> <td> 0.169</td> <td>-1476.720</td> <td> 8446.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>           <td> 3974.6138</td> <td> 1923.791</td> <td>    2.066</td> <td> 0.039</td> <td>  203.636</td> <td> 7745.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>           <td> 3898.8978</td> <td> 1925.219</td> <td>    2.025</td> <td> 0.043</td> <td>  125.121</td> <td> 7672.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>           <td>-5879.0805</td> <td> 3091.558</td> <td>   -1.902</td> <td> 0.057</td> <td>-1.19e+04</td> <td>  180.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>           <td>  732.9240</td> <td> 2267.459</td> <td>    0.323</td> <td> 0.747</td> <td>-3711.706</td> <td> 5177.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>           <td>  425.4199</td> <td> 2389.753</td> <td>    0.178</td> <td> 0.859</td> <td>-4258.928</td> <td> 5109.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>           <td> 4587.2458</td> <td> 2290.006</td> <td>    2.003</td> <td> 0.045</td> <td>   98.420</td> <td> 9076.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>           <td> 4857.3709</td> <td> 2094.704</td> <td>    2.319</td> <td> 0.020</td> <td>  751.373</td> <td> 8963.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>           <td> 4753.1827</td> <td> 1996.424</td> <td>    2.381</td> <td> 0.017</td> <td>  839.832</td> <td> 8666.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>           <td>-2.486e+04</td> <td> 3038.268</td> <td>   -8.182</td> <td> 0.000</td> <td>-3.08e+04</td> <td>-1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>           <td>-4141.7091</td> <td> 2287.741</td> <td>   -1.810</td> <td> 0.070</td> <td>-8626.095</td> <td>  342.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>           <td> 4295.6205</td> <td> 2486.387</td> <td>    1.728</td> <td> 0.084</td> <td> -578.148</td> <td> 9169.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>           <td> 3272.0359</td> <td> 1998.614</td> <td>    1.637</td> <td> 0.102</td> <td> -645.609</td> <td> 7189.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>           <td>-3870.0977</td> <td> 2688.876</td> <td>   -1.439</td> <td> 0.150</td> <td>-9140.780</td> <td> 1400.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>           <td>-3650.5442</td> <td> 1950.871</td> <td>   -1.871</td> <td> 0.061</td> <td>-7474.605</td> <td>  173.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>           <td> 2538.0308</td> <td> 2851.325</td> <td>    0.890</td> <td> 0.373</td> <td>-3051.081</td> <td> 8127.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>           <td> 2211.3835</td> <td> 2418.365</td> <td>    0.914</td> <td> 0.361</td> <td>-2529.050</td> <td> 6951.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>           <td> 4747.7617</td> <td> 1905.472</td> <td>    2.492</td> <td> 0.013</td> <td> 1012.692</td> <td> 8482.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>           <td> 6682.3186</td> <td> 4140.497</td> <td>    1.614</td> <td> 0.107</td> <td>-1433.804</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>           <td> -510.7928</td> <td> 1880.479</td> <td>   -0.272</td> <td> 0.786</td> <td>-4196.872</td> <td> 3175.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>           <td> 7014.3465</td> <td> 2367.323</td> <td>    2.963</td> <td> 0.003</td> <td> 2373.967</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>           <td> 4011.8914</td> <td> 2027.455</td> <td>    1.979</td> <td> 0.048</td> <td>   37.713</td> <td> 7986.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>           <td> 6401.5443</td> <td> 2167.956</td> <td>    2.953</td> <td> 0.003</td> <td> 2151.959</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>           <td>-1.152e+04</td> <td> 2403.097</td> <td>   -4.793</td> <td> 0.000</td> <td>-1.62e+04</td> <td>-6806.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>           <td>-4813.4761</td> <td> 3141.449</td> <td>   -1.532</td> <td> 0.125</td> <td> -1.1e+04</td> <td> 1344.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>           <td> 1825.6041</td> <td> 3032.142</td> <td>    0.602</td> <td> 0.547</td> <td>-4117.943</td> <td> 7769.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>           <td>-1.609e+04</td> <td> 2568.390</td> <td>   -6.266</td> <td> 0.000</td> <td>-2.11e+04</td> <td>-1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>           <td> 1494.7575</td> <td> 3506.972</td> <td>    0.426</td> <td> 0.670</td> <td>-5379.540</td> <td> 8369.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>           <td>-2039.2412</td> <td> 2351.367</td> <td>   -0.867</td> <td> 0.386</td> <td>-6648.345</td> <td> 2569.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>           <td>-1.899e+04</td> <td> 4196.664</td> <td>   -4.525</td> <td> 0.000</td> <td>-2.72e+04</td> <td>-1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>           <td>-9243.5870</td> <td> 3349.344</td> <td>   -2.760</td> <td> 0.006</td> <td>-1.58e+04</td> <td>-2678.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>           <td> -1.52e+04</td> <td> 2706.616</td> <td>   -5.615</td> <td> 0.000</td> <td>-2.05e+04</td> <td>-9893.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>           <td> 2531.3596</td> <td> 1994.304</td> <td>    1.269</td> <td> 0.204</td> <td>-1377.837</td> <td> 6440.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>           <td> -1.92e+04</td> <td> 4204.230</td> <td>   -4.566</td> <td> 0.000</td> <td>-2.74e+04</td> <td> -1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>           <td> 5668.5792</td> <td> 2139.984</td> <td>    2.649</td> <td> 0.008</td> <td> 1473.823</td> <td> 9863.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>           <td>  153.2474</td> <td> 2023.175</td> <td>    0.076</td> <td> 0.940</td> <td>-3812.541</td> <td> 4119.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>           <td> -721.8037</td> <td> 3477.675</td> <td>   -0.208</td> <td> 0.836</td> <td>-7538.676</td> <td> 6095.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>           <td>-8269.2236</td> <td> 2791.005</td> <td>   -2.963</td> <td> 0.003</td> <td>-1.37e+04</td> <td>-2798.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>           <td> 5395.6303</td> <td> 2000.925</td> <td>    2.697</td> <td> 0.007</td> <td> 1473.455</td> <td> 9317.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>           <td>-2631.4283</td> <td> 3639.263</td> <td>   -0.723</td> <td> 0.470</td> <td>-9765.041</td> <td> 4502.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>           <td>-2662.5419</td> <td> 2775.924</td> <td>   -0.959</td> <td> 0.338</td> <td>-8103.854</td> <td> 2778.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>           <td> 5387.0743</td> <td> 2156.496</td> <td>    2.498</td> <td> 0.013</td> <td> 1159.952</td> <td> 9614.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>           <td> 6790.9022</td> <td> 2200.562</td> <td>    3.086</td> <td> 0.002</td> <td> 2477.404</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>           <td> 4338.0655</td> <td> 2029.118</td> <td>    2.138</td> <td> 0.033</td> <td>  360.627</td> <td> 8315.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>           <td> 1.259e+04</td> <td> 1934.729</td> <td>    6.506</td> <td> 0.000</td> <td> 8795.154</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>           <td>  531.9537</td> <td> 2366.623</td> <td>    0.225</td> <td> 0.822</td> <td>-4107.055</td> <td> 5170.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>           <td> 6471.5216</td> <td> 2057.068</td> <td>    3.146</td> <td> 0.002</td> <td> 2439.297</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>           <td>-4973.1511</td> <td> 2394.112</td> <td>   -2.077</td> <td> 0.038</td> <td>-9666.044</td> <td> -280.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>           <td> 3996.2886</td> <td> 1893.926</td> <td>    2.110</td> <td> 0.035</td> <td>  283.852</td> <td> 7708.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>           <td>-9530.9087</td> <td> 3065.244</td> <td>   -3.109</td> <td> 0.002</td> <td>-1.55e+04</td> <td>-3522.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>           <td> 8945.6153</td> <td> 2332.155</td> <td>    3.836</td> <td> 0.000</td> <td> 4374.169</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>           <td> 1739.0721</td> <td> 3361.765</td> <td>    0.517</td> <td> 0.605</td> <td>-4850.595</td> <td> 8328.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>           <td>-1.712e+04</td> <td> 3453.701</td> <td>   -4.957</td> <td> 0.000</td> <td>-2.39e+04</td> <td>-1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>           <td> 2174.3419</td> <td> 2469.131</td> <td>    0.881</td> <td> 0.379</td> <td>-2665.601</td> <td> 7014.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>           <td>-4639.1997</td> <td> 1989.707</td> <td>   -2.332</td> <td> 0.020</td> <td>-8539.386</td> <td> -739.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>           <td> 6315.4956</td> <td> 2737.639</td> <td>    2.307</td> <td> 0.021</td> <td>  949.229</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>           <td> 3541.1783</td> <td> 2469.489</td> <td>    1.434</td> <td> 0.152</td> <td>-1299.466</td> <td> 8381.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>           <td> 6613.9864</td> <td> 2116.065</td> <td>    3.126</td> <td> 0.002</td> <td> 2466.117</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>           <td> 3058.6732</td> <td> 2342.642</td> <td>    1.306</td> <td> 0.192</td> <td>-1533.328</td> <td> 7650.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>           <td>-1.326e+04</td> <td> 3489.311</td> <td>   -3.801</td> <td> 0.000</td> <td>-2.01e+04</td> <td>-6422.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>           <td> 4772.4015</td> <td> 1987.898</td> <td>    2.401</td> <td> 0.016</td> <td>  875.762</td> <td> 8669.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>           <td>-1.484e+04</td> <td> 2878.008</td> <td>   -5.155</td> <td> 0.000</td> <td>-2.05e+04</td> <td>-9196.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>           <td> 4468.4025</td> <td> 1948.737</td> <td>    2.293</td> <td> 0.022</td> <td>  648.525</td> <td> 8288.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>           <td> 6080.0881</td> <td> 2067.306</td> <td>    2.941</td> <td> 0.003</td> <td> 2027.795</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>           <td> 2144.9884</td> <td> 2178.035</td> <td>    0.985</td> <td> 0.325</td> <td>-2124.354</td> <td> 6414.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>           <td> 2898.8369</td> <td> 2273.900</td> <td>    1.275</td> <td> 0.202</td> <td>-1558.419</td> <td> 7356.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>           <td> 3703.1068</td> <td> 2821.710</td> <td>    1.312</td> <td> 0.189</td> <td>-1827.954</td> <td> 9234.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>           <td> 2379.0898</td> <td> 3338.800</td> <td>    0.713</td> <td> 0.476</td> <td>-4165.562</td> <td> 8923.741</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>           <td> -223.6093</td> <td> 1947.219</td> <td>   -0.115</td> <td> 0.909</td> <td>-4040.510</td> <td> 3593.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>           <td> 2486.1288</td> <td> 2230.203</td> <td>    1.115</td> <td> 0.265</td> <td>-1885.473</td> <td> 6857.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>           <td> 4544.2085</td> <td> 2123.378</td> <td>    2.140</td> <td> 0.032</td> <td>  382.005</td> <td> 8706.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>           <td> 1884.3765</td> <td> 2055.793</td> <td>    0.917</td> <td> 0.359</td> <td>-2145.349</td> <td> 5914.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>           <td>-1.053e+04</td> <td> 2518.734</td> <td>   -4.182</td> <td> 0.000</td> <td>-1.55e+04</td> <td>-5596.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>           <td>-1667.6496</td> <td> 2866.837</td> <td>   -0.582</td> <td> 0.561</td> <td>-7287.169</td> <td> 3951.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>           <td> 4027.0706</td> <td> 2320.036</td> <td>    1.736</td> <td> 0.083</td> <td> -520.619</td> <td> 8574.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>           <td> 2602.9928</td> <td> 8093.738</td> <td>    0.322</td> <td> 0.748</td> <td>-1.33e+04</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>           <td> 4908.5522</td> <td> 2122.557</td> <td>    2.313</td> <td> 0.021</td> <td>  747.957</td> <td> 9069.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>           <td> 6832.7583</td> <td> 2357.491</td> <td>    2.898</td> <td> 0.004</td> <td> 2211.650</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>           <td> 6237.8526</td> <td> 2103.798</td> <td>    2.965</td> <td> 0.003</td> <td> 2114.028</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>           <td>  755.1966</td> <td> 1861.033</td> <td>    0.406</td> <td> 0.685</td> <td>-2892.765</td> <td> 4403.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>           <td>-1.258e+04</td> <td> 2882.596</td> <td>   -4.363</td> <td> 0.000</td> <td>-1.82e+04</td> <td>-6927.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>           <td> 6017.5190</td> <td> 2080.271</td> <td>    2.893</td> <td> 0.004</td> <td> 1939.812</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>           <td>-1.093e+04</td> <td> 2885.744</td> <td>   -3.787</td> <td> 0.000</td> <td>-1.66e+04</td> <td>-5273.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>           <td> -393.8328</td> <td> 2850.116</td> <td>   -0.138</td> <td> 0.890</td> <td>-5980.575</td> <td> 5192.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>           <td> 4209.3349</td> <td> 2064.664</td> <td>    2.039</td> <td> 0.041</td> <td>  162.221</td> <td> 8256.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>           <td> 4446.4872</td> <td> 1984.877</td> <td>    2.240</td> <td> 0.025</td> <td>  555.769</td> <td> 8337.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>           <td> 6541.1562</td> <td> 2149.132</td> <td>    3.044</td> <td> 0.002</td> <td> 2328.470</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>           <td> 4660.6183</td> <td> 2131.455</td> <td>    2.187</td> <td> 0.029</td> <td>  482.581</td> <td> 8838.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>           <td> 6035.9091</td> <td> 2020.560</td> <td>    2.987</td> <td> 0.003</td> <td> 2075.247</td> <td> 9996.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>           <td> 5398.4787</td> <td> 2020.839</td> <td>    2.671</td> <td> 0.008</td> <td> 1437.269</td> <td> 9359.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>           <td>-1.066e+05</td> <td> 4610.547</td> <td>  -23.127</td> <td> 0.000</td> <td>-1.16e+05</td> <td>-9.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>           <td>-1.791e+04</td> <td> 4421.977</td> <td>   -4.050</td> <td> 0.000</td> <td>-2.66e+04</td> <td>-9242.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>           <td> 2395.9800</td> <td> 2081.622</td> <td>    1.151</td> <td> 0.250</td> <td>-1684.376</td> <td> 6476.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>           <td> 2114.6443</td> <td> 2024.100</td> <td>    1.045</td> <td> 0.296</td> <td>-1852.958</td> <td> 6082.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>           <td> 2443.3153</td> <td> 2216.329</td> <td>    1.102</td> <td> 0.270</td> <td>-1901.091</td> <td> 6787.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>           <td> 3719.7663</td> <td> 1971.092</td> <td>    1.887</td> <td> 0.059</td> <td> -143.930</td> <td> 7583.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>           <td>-1.363e+04</td> <td> 3589.438</td> <td>   -3.798</td> <td> 0.000</td> <td>-2.07e+04</td> <td>-6597.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>           <td> 1.173e+04</td> <td> 2160.613</td> <td>    5.431</td> <td> 0.000</td> <td> 7498.977</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>           <td> 7348.2283</td> <td> 2349.934</td> <td>    3.127</td> <td> 0.002</td> <td> 2741.933</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>           <td>-2178.1172</td> <td> 3711.933</td> <td>   -0.587</td> <td> 0.557</td> <td>-9454.177</td> <td> 5097.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>           <td>-1.562e+04</td> <td> 4033.921</td> <td>   -3.873</td> <td> 0.000</td> <td>-2.35e+04</td> <td>-7714.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>           <td>-1942.9461</td> <td> 1936.749</td> <td>   -1.003</td> <td> 0.316</td> <td>-5739.323</td> <td> 1853.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>           <td> 4989.8999</td> <td> 1968.604</td> <td>    2.535</td> <td> 0.011</td> <td> 1131.081</td> <td> 8848.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>           <td> -661.5468</td> <td> 1861.195</td> <td>   -0.355</td> <td> 0.722</td> <td>-4309.826</td> <td> 2986.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>           <td>-1.112e+04</td> <td> 3197.692</td> <td>   -3.476</td> <td> 0.001</td> <td>-1.74e+04</td> <td>-4848.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>           <td> 3.589e+04</td> <td> 4751.439</td> <td>    7.553</td> <td> 0.000</td> <td> 2.66e+04</td> <td> 4.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>           <td> 5337.3935</td> <td> 2385.615</td> <td>    2.237</td> <td> 0.025</td> <td>  661.157</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>           <td> 6205.3078</td> <td> 2402.340</td> <td>    2.583</td> <td> 0.010</td> <td> 1496.287</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>           <td>-1.592e+05</td> <td> 6404.441</td> <td>  -24.851</td> <td> 0.000</td> <td>-1.72e+05</td> <td>-1.47e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>           <td>-1022.6138</td> <td> 1857.490</td> <td>   -0.551</td> <td> 0.582</td> <td>-4663.630</td> <td> 2618.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>           <td> 6506.3275</td> <td> 2168.818</td> <td>    3.000</td> <td> 0.003</td> <td> 2255.052</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>           <td>-1.375e+04</td> <td> 3103.757</td> <td>   -4.429</td> <td> 0.000</td> <td>-1.98e+04</td> <td>-7663.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>           <td>-1841.4695</td> <td> 1957.797</td> <td>   -0.941</td> <td> 0.347</td> <td>-5679.106</td> <td> 1996.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>           <td>-3195.4551</td> <td> 2290.240</td> <td>   -1.395</td> <td> 0.163</td> <td>-7684.739</td> <td> 1293.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>           <td> 1196.8891</td> <td> 2704.292</td> <td>    0.443</td> <td> 0.658</td> <td>-4104.012</td> <td> 6497.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>           <td> -601.4087</td> <td> 2853.378</td> <td>   -0.211</td> <td> 0.833</td> <td>-6194.544</td> <td> 4991.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>           <td> 1.483e+04</td> <td> 1995.122</td> <td>    7.432</td> <td> 0.000</td> <td> 1.09e+04</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>           <td>  246.1982</td> <td> 2276.688</td> <td>    0.108</td> <td> 0.914</td> <td>-4216.522</td> <td> 4708.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>           <td> 5138.2287</td> <td> 2045.922</td> <td>    2.511</td> <td> 0.012</td> <td> 1127.853</td> <td> 9148.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>           <td> 6546.0573</td> <td> 2167.329</td> <td>    3.020</td> <td> 0.003</td> <td> 2297.701</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>           <td> 5977.9824</td> <td> 2517.907</td> <td>    2.374</td> <td> 0.018</td> <td> 1042.431</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>           <td>-5047.5424</td> <td> 2516.110</td> <td>   -2.006</td> <td> 0.045</td> <td>-9979.573</td> <td> -115.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>           <td>-5097.0336</td> <td> 2435.130</td> <td>   -2.093</td> <td> 0.036</td> <td>-9870.328</td> <td> -323.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>           <td> 4462.3487</td> <td> 1960.595</td> <td>    2.276</td> <td> 0.023</td> <td>  619.229</td> <td> 8305.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>           <td> 3669.1467</td> <td> 1959.630</td> <td>    1.872</td> <td> 0.061</td> <td> -172.082</td> <td> 7510.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>           <td> 4127.3979</td> <td> 2002.788</td> <td>    2.061</td> <td> 0.039</td> <td>  201.572</td> <td> 8053.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>           <td> 1736.3687</td> <td> 3072.112</td> <td>    0.565</td> <td> 0.572</td> <td>-4285.527</td> <td> 7758.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>           <td> 5385.4727</td> <td> 1968.103</td> <td>    2.736</td> <td> 0.006</td> <td> 1527.636</td> <td> 9243.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>           <td> 6697.9531</td> <td> 2202.006</td> <td>    3.042</td> <td> 0.002</td> <td> 2381.624</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>           <td> 3987.4013</td> <td> 2024.473</td> <td>    1.970</td> <td> 0.049</td> <td>   19.069</td> <td> 7955.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>           <td> 7257.8902</td> <td> 2341.843</td> <td>    3.099</td> <td> 0.002</td> <td> 2667.454</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>           <td> 3610.5273</td> <td> 2537.108</td> <td>    1.423</td> <td> 0.155</td> <td>-1362.662</td> <td> 8583.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>           <td> 6037.4731</td> <td> 2429.463</td> <td>    2.485</td> <td> 0.013</td> <td> 1275.287</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>           <td>-2.453e+04</td> <td> 3757.022</td> <td>   -6.530</td> <td> 0.000</td> <td>-3.19e+04</td> <td>-1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>           <td> 4161.9913</td> <td> 1937.190</td> <td>    2.148</td> <td> 0.032</td> <td>  364.749</td> <td> 7959.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>           <td> 6187.1679</td> <td> 2129.255</td> <td>    2.906</td> <td> 0.004</td> <td> 2013.443</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>           <td> 4941.5364</td> <td> 2326.090</td> <td>    2.124</td> <td> 0.034</td> <td>  381.979</td> <td> 9501.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>           <td> 1121.7689</td> <td> 1866.155</td> <td>    0.601</td> <td> 0.548</td> <td>-2536.232</td> <td> 4779.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>           <td> 5949.3888</td> <td> 2098.188</td> <td>    2.835</td> <td> 0.005</td> <td> 1836.561</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>           <td> 8101.5847</td> <td> 1978.173</td> <td>    4.095</td> <td> 0.000</td> <td> 4224.008</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>           <td> 6377.2591</td> <td> 2126.470</td> <td>    2.999</td> <td> 0.003</td> <td> 2208.993</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>           <td> 5302.4738</td> <td> 2003.961</td> <td>    2.646</td> <td> 0.008</td> <td> 1374.348</td> <td> 9230.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>           <td> 4808.9439</td> <td> 1948.376</td> <td>    2.468</td> <td> 0.014</td> <td>  989.775</td> <td> 8628.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>           <td>-4771.1716</td> <td> 2836.259</td> <td>   -1.682</td> <td> 0.093</td> <td>-1.03e+04</td> <td>  788.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>           <td> 7113.0802</td> <td> 2651.787</td> <td>    2.682</td> <td> 0.007</td> <td> 1915.099</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>           <td>-2.466e+04</td> <td> 2952.624</td> <td>   -8.351</td> <td> 0.000</td> <td>-3.04e+04</td> <td>-1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>           <td> 7328.6885</td> <td> 2184.972</td> <td>    3.354</td> <td> 0.001</td> <td> 3045.748</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>           <td> -201.0064</td> <td> 2813.432</td> <td>   -0.071</td> <td> 0.943</td> <td>-5715.842</td> <td> 5313.830</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>           <td> 6088.4431</td> <td> 2009.766</td> <td>    3.029</td> <td> 0.002</td> <td> 2148.938</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>           <td> 5048.1264</td> <td> 2044.923</td> <td>    2.469</td> <td> 0.014</td> <td> 1039.708</td> <td> 9056.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>           <td> 4158.0942</td> <td> 2033.694</td> <td>    2.045</td> <td> 0.041</td> <td>  171.687</td> <td> 8144.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>           <td> 2695.4324</td> <td> 2198.979</td> <td>    1.226</td> <td> 0.220</td> <td>-1614.964</td> <td> 7005.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>           <td> 4947.1529</td> <td> 1977.066</td> <td>    2.502</td> <td> 0.012</td> <td> 1071.746</td> <td> 8822.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>           <td> -2.73e+04</td> <td> 2902.364</td> <td>   -9.407</td> <td> 0.000</td> <td> -3.3e+04</td> <td>-2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>           <td> 6021.2749</td> <td> 2074.299</td> <td>    2.903</td> <td> 0.004</td> <td> 1955.274</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>           <td>-3027.9789</td> <td> 2157.060</td> <td>   -1.404</td> <td> 0.160</td> <td>-7256.206</td> <td> 1200.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>           <td> 3433.9024</td> <td> 2123.438</td> <td>    1.617</td> <td> 0.106</td> <td> -728.419</td> <td> 7596.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>           <td> 1057.0909</td> <td> 2289.670</td> <td>    0.462</td> <td> 0.644</td> <td>-3431.075</td> <td> 5545.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>           <td> 2.181e+04</td> <td> 2946.548</td> <td>    7.402</td> <td> 0.000</td> <td>  1.6e+04</td> <td> 2.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>           <td> 1717.6108</td> <td> 3401.702</td> <td>    0.505</td> <td> 0.614</td> <td>-4950.340</td> <td> 8385.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>           <td>-6513.4996</td> <td> 2973.737</td> <td>   -2.190</td> <td> 0.029</td> <td>-1.23e+04</td> <td> -684.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>           <td>-8207.0573</td> <td> 2208.030</td> <td>   -3.717</td> <td> 0.000</td> <td>-1.25e+04</td> <td>-3878.919</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>           <td> 4740.5056</td> <td> 1961.317</td> <td>    2.417</td> <td> 0.016</td> <td>  895.971</td> <td> 8585.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>           <td> 6841.0167</td> <td> 2458.590</td> <td>    2.782</td> <td> 0.005</td> <td> 2021.736</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>           <td>-1201.4683</td> <td> 4305.167</td> <td>   -0.279</td> <td> 0.780</td> <td>-9640.374</td> <td> 7237.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>           <td> 5028.0231</td> <td> 1915.403</td> <td>    2.625</td> <td> 0.009</td> <td> 1273.487</td> <td> 8782.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>           <td>-2.241e+04</td> <td> 3007.553</td> <td>   -7.451</td> <td> 0.000</td> <td>-2.83e+04</td> <td>-1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>          <td>-2.179e+04</td> <td> 5707.523</td> <td>   -3.817</td> <td> 0.000</td> <td> -3.3e+04</td> <td>-1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>           <td> 5310.8121</td> <td> 2237.007</td> <td>    2.374</td> <td> 0.018</td> <td>  925.874</td> <td> 9695.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>           <td> 4521.5138</td> <td> 2016.058</td> <td>    2.243</td> <td> 0.025</td> <td>  569.675</td> <td> 8473.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>           <td>-5446.4132</td> <td> 2123.959</td> <td>   -2.564</td> <td> 0.010</td> <td>-9609.757</td> <td>-1283.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>           <td>-8647.2793</td> <td> 2254.616</td> <td>   -3.835</td> <td> 0.000</td> <td>-1.31e+04</td> <td>-4227.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>           <td>-2273.4055</td> <td> 2419.539</td> <td>   -0.940</td> <td> 0.347</td> <td>-7016.140</td> <td> 2469.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>          <td> -1.48e+04</td> <td> 3905.941</td> <td>   -3.790</td> <td> 0.000</td> <td>-2.25e+04</td> <td>-7146.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>           <td>   23.5432</td> <td> 2042.855</td> <td>    0.012</td> <td> 0.991</td> <td>-3980.822</td> <td> 4027.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>           <td> 4551.4901</td> <td> 1945.254</td> <td>    2.340</td> <td> 0.019</td> <td>  738.441</td> <td> 8364.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>          <td> 3838.6494</td> <td> 4125.994</td> <td>    0.930</td> <td> 0.352</td> <td>-4249.045</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>           <td> 4384.7826</td> <td> 2028.038</td> <td>    2.162</td> <td> 0.031</td> <td>  409.462</td> <td> 8360.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>           <td> 5771.6803</td> <td> 1991.297</td> <td>    2.898</td> <td> 0.004</td> <td> 1868.378</td> <td> 9674.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>           <td> 6070.2664</td> <td> 2354.559</td> <td>    2.578</td> <td> 0.010</td> <td> 1454.905</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>          <td> 5879.7196</td> <td> 4132.114</td> <td>    1.423</td> <td> 0.155</td> <td>-2219.970</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>           <td>-5651.9887</td> <td> 3239.653</td> <td>   -1.745</td> <td> 0.081</td> <td> -1.2e+04</td> <td>  698.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>          <td> 7996.1466</td> <td> 5739.457</td> <td>    1.393</td> <td> 0.164</td> <td>-3254.226</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>           <td>  460.1801</td> <td> 2933.545</td> <td>    0.157</td> <td> 0.875</td> <td>-5290.099</td> <td> 6210.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>           <td> -322.9903</td> <td> 2588.159</td> <td>   -0.125</td> <td> 0.901</td> <td>-5396.250</td> <td> 4750.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>           <td> 3344.6810</td> <td> 2023.859</td> <td>    1.653</td> <td> 0.098</td> <td> -622.448</td> <td> 7311.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>           <td> 5443.1242</td> <td> 2099.984</td> <td>    2.592</td> <td> 0.010</td> <td> 1326.776</td> <td> 9559.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>           <td> -1.67e+04</td> <td> 3045.710</td> <td>   -5.484</td> <td> 0.000</td> <td>-2.27e+04</td> <td>-1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>           <td> 5069.1679</td> <td> 3416.150</td> <td>    1.484</td> <td> 0.138</td> <td>-1627.104</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>           <td> 5427.4416</td> <td> 2181.813</td> <td>    2.488</td> <td> 0.013</td> <td> 1150.694</td> <td> 9704.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>           <td>-3138.9060</td> <td> 2734.876</td> <td>   -1.148</td> <td> 0.251</td> <td>-8499.757</td> <td> 2221.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>           <td> 4156.5332</td> <td> 1929.335</td> <td>    2.154</td> <td> 0.031</td> <td>  374.689</td> <td> 7938.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>           <td> 6145.1418</td> <td> 2241.745</td> <td>    2.741</td> <td> 0.006</td> <td> 1750.917</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>           <td> 1.096e+04</td> <td> 2043.515</td> <td>    5.361</td> <td> 0.000</td> <td> 6949.632</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>           <td> 6035.0514</td> <td> 2136.285</td> <td>    2.825</td> <td> 0.005</td> <td> 1847.548</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>           <td> 7991.0460</td> <td> 5764.851</td> <td>    1.386</td> <td> 0.166</td> <td>-3309.103</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>           <td> 6133.8056</td> <td> 1999.923</td> <td>    3.067</td> <td> 0.002</td> <td> 2213.595</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>           <td>-1207.7520</td> <td> 1989.092</td> <td>   -0.607</td> <td> 0.544</td> <td>-5106.731</td> <td> 2691.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>           <td> 7154.0765</td> <td> 2278.649</td> <td>    3.140</td> <td> 0.002</td> <td> 2687.514</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>           <td>-4498.3883</td> <td> 2252.685</td> <td>   -1.997</td> <td> 0.046</td> <td>-8914.057</td> <td>  -82.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>           <td> 5450.4644</td> <td> 2064.605</td> <td>    2.640</td> <td> 0.008</td> <td> 1403.467</td> <td> 9497.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>           <td> 5115.9446</td> <td> 2334.275</td> <td>    2.192</td> <td> 0.028</td> <td>  540.343</td> <td> 9691.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>           <td> 4921.6992</td> <td> 2293.983</td> <td>    2.145</td> <td> 0.032</td> <td>  425.078</td> <td> 9418.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>           <td> 1056.4009</td> <td> 2661.382</td> <td>    0.397</td> <td> 0.691</td> <td>-4160.388</td> <td> 6273.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>           <td> -1.01e+04</td> <td> 3655.257</td> <td>   -2.762</td> <td> 0.006</td> <td>-1.73e+04</td> <td>-2931.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>           <td>  991.3577</td> <td> 8288.834</td> <td>    0.120</td> <td> 0.905</td> <td>-1.53e+04</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>           <td> 4465.2253</td> <td> 1955.208</td> <td>    2.284</td> <td> 0.022</td> <td>  632.664</td> <td> 8297.787</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>           <td> 6716.1652</td> <td> 2490.058</td> <td>    2.697</td> <td> 0.007</td> <td> 1835.202</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>           <td> 2951.2741</td> <td> 2207.527</td> <td>    1.337</td> <td> 0.181</td> <td>-1375.879</td> <td> 7278.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>           <td>-4679.5758</td> <td> 3071.259</td> <td>   -1.524</td> <td> 0.128</td> <td>-1.07e+04</td> <td> 1340.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>           <td> -509.4835</td> <td> 1916.918</td> <td>   -0.266</td> <td> 0.790</td> <td>-4266.989</td> <td> 3248.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>           <td> 6419.3028</td> <td> 2107.324</td> <td>    3.046</td> <td> 0.002</td> <td> 2288.568</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>           <td> 5552.1721</td> <td> 2096.722</td> <td>    2.648</td> <td> 0.008</td> <td> 1442.218</td> <td> 9662.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>           <td> 5597.9612</td> <td> 2211.098</td> <td>    2.532</td> <td> 0.011</td> <td> 1263.810</td> <td> 9932.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>           <td> 5856.5128</td> <td> 2325.727</td> <td>    2.518</td> <td> 0.012</td> <td> 1297.667</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>           <td>  630.6711</td> <td> 3361.704</td> <td>    0.188</td> <td> 0.851</td> <td>-5958.876</td> <td> 7220.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>           <td> 5346.9369</td> <td> 1987.817</td> <td>    2.690</td> <td> 0.007</td> <td> 1450.457</td> <td> 9243.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>           <td> 4818.2195</td> <td> 1944.367</td> <td>    2.478</td> <td> 0.013</td> <td> 1006.910</td> <td> 8629.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>           <td>  179.2243</td> <td> 4100.843</td> <td>    0.044</td> <td> 0.965</td> <td>-7859.168</td> <td> 8217.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>           <td> 7111.9543</td> <td> 2183.314</td> <td>    3.257</td> <td> 0.001</td> <td> 2832.264</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>           <td>-2536.2199</td> <td> 3238.822</td> <td>   -0.783</td> <td> 0.434</td> <td>-8884.896</td> <td> 3812.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>           <td> 1366.9114</td> <td> 2702.349</td> <td>    0.506</td> <td> 0.613</td> <td>-3930.181</td> <td> 6664.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>           <td> 4887.4493</td> <td> 4329.125</td> <td>    1.129</td> <td> 0.259</td> <td>-3598.418</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>           <td> 6471.5926</td> <td> 2068.738</td> <td>    3.128</td> <td> 0.002</td> <td> 2416.492</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>           <td> 5320.6972</td> <td> 2246.481</td> <td>    2.368</td> <td> 0.018</td> <td>  917.188</td> <td> 9724.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>           <td>-1.713e+04</td> <td> 2756.595</td> <td>   -6.213</td> <td> 0.000</td> <td>-2.25e+04</td> <td>-1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>           <td> 7876.9957</td> <td> 2429.684</td> <td>    3.242</td> <td> 0.001</td> <td> 3114.375</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>           <td>-2.531e+04</td> <td> 3191.060</td> <td>   -7.930</td> <td> 0.000</td> <td>-3.16e+04</td> <td>-1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>           <td> 6710.5132</td> <td> 2628.816</td> <td>    2.553</td> <td> 0.011</td> <td> 1557.559</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>           <td> 5808.6380</td> <td> 2162.285</td> <td>    2.686</td> <td> 0.007</td> <td> 1570.169</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>           <td> 5125.4429</td> <td> 1989.770</td> <td>    2.576</td> <td> 0.010</td> <td> 1225.135</td> <td> 9025.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>           <td> 3448.3552</td> <td> 2117.951</td> <td>    1.628</td> <td> 0.104</td> <td> -703.211</td> <td> 7599.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>           <td> 2002.9582</td> <td> 2777.820</td> <td>    0.721</td> <td> 0.471</td> <td>-3442.070</td> <td> 7447.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>           <td> 3530.4877</td> <td> 2452.979</td> <td>    1.439</td> <td> 0.150</td> <td>-1277.795</td> <td> 8338.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>           <td>  957.8911</td> <td> 2818.602</td> <td>    0.340</td> <td> 0.734</td> <td>-4567.078</td> <td> 6482.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>           <td> 3170.6036</td> <td> 2344.626</td> <td>    1.352</td> <td> 0.176</td> <td>-1425.287</td> <td> 7766.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>           <td> 3031.8917</td> <td> 2217.822</td> <td>    1.367</td> <td> 0.172</td> <td>-1315.439</td> <td> 7379.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>           <td> 2257.5843</td> <td> 2155.260</td> <td>    1.047</td> <td> 0.295</td> <td>-1967.114</td> <td> 6482.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>           <td>-8933.5649</td> <td> 3046.767</td> <td>   -2.932</td> <td> 0.003</td> <td>-1.49e+04</td> <td>-2961.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>           <td> 8885.8741</td> <td> 2108.030</td> <td>    4.215</td> <td> 0.000</td> <td> 4753.755</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>           <td> 7510.1248</td> <td> 1992.266</td> <td>    3.770</td> <td> 0.000</td> <td> 3604.923</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>           <td> 4030.3376</td> <td> 2347.487</td> <td>    1.717</td> <td> 0.086</td> <td> -571.160</td> <td> 8631.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>           <td> 7345.8253</td> <td> 2307.329</td> <td>    3.184</td> <td> 0.001</td> <td> 2823.044</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>           <td> 7449.5209</td> <td> 3386.575</td> <td>    2.200</td> <td> 0.028</td> <td>  811.221</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>           <td> 3930.8037</td> <td> 1967.469</td> <td>    1.998</td> <td> 0.046</td> <td>   74.210</td> <td> 7787.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>           <td> 2601.1446</td> <td> 2515.212</td> <td>    1.034</td> <td> 0.301</td> <td>-2329.126</td> <td> 7531.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>           <td> 5136.5066</td> <td> 1972.034</td> <td>    2.605</td> <td> 0.009</td> <td> 1270.963</td> <td> 9002.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>           <td> 5854.8528</td> <td> 2036.846</td> <td>    2.874</td> <td> 0.004</td> <td> 1862.267</td> <td> 9847.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>           <td> 8500.3657</td> <td> 2071.979</td> <td>    4.103</td> <td> 0.000</td> <td> 4438.912</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>           <td> 1457.2876</td> <td> 2001.475</td> <td>    0.728</td> <td> 0.467</td> <td>-2465.965</td> <td> 5380.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>           <td> 1277.7106</td> <td> 2493.046</td> <td>    0.513</td> <td> 0.608</td> <td>-3609.110</td> <td> 6164.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>           <td> 1.278e+04</td> <td> 2150.609</td> <td>    5.942</td> <td> 0.000</td> <td> 8563.950</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>           <td> 1690.0190</td> <td> 3382.763</td> <td>    0.500</td> <td> 0.617</td> <td>-4940.807</td> <td> 8320.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>           <td> -147.9826</td> <td> 2226.379</td> <td>   -0.066</td> <td> 0.947</td> <td>-4512.088</td> <td> 4216.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>           <td> 1.904e+04</td> <td> 3240.032</td> <td>    5.877</td> <td> 0.000</td> <td> 1.27e+04</td> <td> 2.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>           <td> 4722.0406</td> <td> 1937.818</td> <td>    2.437</td> <td> 0.015</td> <td>  923.568</td> <td> 8520.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>           <td> 1629.6320</td> <td> 2338.455</td> <td>    0.697</td> <td> 0.486</td> <td>-2954.162</td> <td> 6213.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>           <td> -1.25e+04</td> <td> 3828.950</td> <td>   -3.264</td> <td> 0.001</td> <td>   -2e+04</td> <td>-4991.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>           <td> 5886.6848</td> <td> 2988.715</td> <td>    1.970</td> <td> 0.049</td> <td>   28.263</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>           <td> 2896.8188</td> <td> 2299.209</td> <td>    1.260</td> <td> 0.208</td> <td>-1610.046</td> <td> 7403.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>           <td>-1.232e+04</td> <td> 3684.981</td> <td>   -3.344</td> <td> 0.001</td> <td>-1.95e+04</td> <td>-5099.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>           <td>-7097.0654</td> <td> 2330.495</td> <td>   -3.045</td> <td> 0.002</td> <td>-1.17e+04</td> <td>-2528.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>           <td> 5849.8446</td> <td> 1993.130</td> <td>    2.935</td> <td> 0.003</td> <td> 1942.950</td> <td> 9756.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>           <td> 4645.0055</td> <td> 2011.931</td> <td>    2.309</td> <td> 0.021</td> <td>  701.258</td> <td> 8588.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>           <td>  317.5942</td> <td> 1943.010</td> <td>    0.163</td> <td> 0.870</td> <td>-3491.057</td> <td> 4126.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>           <td> 5582.4116</td> <td> 3713.583</td> <td>    1.503</td> <td> 0.133</td> <td>-1696.881</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>           <td> 3307.8459</td> <td> 2163.349</td> <td>    1.529</td> <td> 0.126</td> <td> -932.709</td> <td> 7548.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>           <td>-1.151e+04</td> <td> 3320.513</td> <td>   -3.465</td> <td> 0.001</td> <td> -1.8e+04</td> <td>-4997.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>           <td> 4506.7226</td> <td> 3669.330</td> <td>    1.228</td> <td> 0.219</td> <td>-2685.826</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>           <td> 5504.9567</td> <td> 1979.844</td> <td>    2.781</td> <td> 0.005</td> <td> 1624.105</td> <td> 9385.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>           <td> 4957.2716</td> <td> 2125.435</td> <td>    2.332</td> <td> 0.020</td> <td>  791.035</td> <td> 9123.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>           <td> 3019.5153</td> <td> 2228.615</td> <td>    1.355</td> <td> 0.175</td> <td>-1348.972</td> <td> 7388.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>           <td> 5336.2602</td> <td> 2084.761</td> <td>    2.560</td> <td> 0.010</td> <td> 1249.752</td> <td> 9422.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>           <td> 2153.4872</td> <td> 2639.742</td> <td>    0.816</td> <td> 0.415</td> <td>-3020.884</td> <td> 7327.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>           <td> -1.36e+04</td> <td> 3433.259</td> <td>   -3.963</td> <td> 0.000</td> <td>-2.03e+04</td> <td>-6875.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>           <td> 4154.8022</td> <td> 1984.316</td> <td>    2.094</td> <td> 0.036</td> <td>  265.184</td> <td> 8044.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>           <td> -821.8510</td> <td> 2689.827</td> <td>   -0.306</td> <td> 0.760</td> <td>-6094.398</td> <td> 4450.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>           <td> 4797.4446</td> <td> 2046.431</td> <td>    2.344</td> <td> 0.019</td> <td>  786.071</td> <td> 8808.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>           <td> 5303.9547</td> <td> 2052.240</td> <td>    2.584</td> <td> 0.010</td> <td> 1281.194</td> <td> 9326.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>           <td> 1284.4351</td> <td> 2504.106</td> <td>    0.513</td> <td> 0.608</td> <td>-3624.065</td> <td> 6192.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>           <td> 6440.1126</td> <td> 2214.527</td> <td>    2.908</td> <td> 0.004</td> <td> 2099.239</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>           <td> 4481.2970</td> <td> 2178.464</td> <td>    2.057</td> <td> 0.040</td> <td>  211.115</td> <td> 8751.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>           <td>-2269.1270</td> <td> 1901.551</td> <td>   -1.193</td> <td> 0.233</td> <td>-5996.511</td> <td> 1458.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>           <td> 5358.1340</td> <td> 1966.060</td> <td>    2.725</td> <td> 0.006</td> <td> 1504.301</td> <td> 9211.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>           <td> -1.01e+04</td> <td> 3070.597</td> <td>   -3.290</td> <td> 0.001</td> <td>-1.61e+04</td> <td>-4083.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>           <td> 4882.6359</td> <td> 1956.612</td> <td>    2.495</td> <td> 0.013</td> <td> 1047.323</td> <td> 8717.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>           <td>-1601.3844</td> <td> 1983.215</td> <td>   -0.807</td> <td> 0.419</td> <td>-5488.844</td> <td> 2286.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>           <td> 6229.8904</td> <td> 2022.998</td> <td>    3.080</td> <td> 0.002</td> <td> 2264.448</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>           <td>-1.114e+04</td> <td> 2945.929</td> <td>   -3.782</td> <td> 0.000</td> <td>-1.69e+04</td> <td>-5367.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>           <td> 1914.0467</td> <td> 2841.995</td> <td>    0.673</td> <td> 0.501</td> <td>-3656.777</td> <td> 7484.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>           <td> 1318.1351</td> <td> 2789.858</td> <td>    0.472</td> <td> 0.637</td> <td>-4150.491</td> <td> 6786.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>           <td> 8691.5911</td> <td> 2172.525</td> <td>    4.001</td> <td> 0.000</td> <td> 4433.050</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>           <td> 5611.4532</td> <td> 1981.638</td> <td>    2.832</td> <td> 0.005</td> <td> 1727.085</td> <td> 9495.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>           <td> 5575.1029</td> <td> 2150.900</td> <td>    2.592</td> <td> 0.010</td> <td> 1358.950</td> <td> 9791.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>           <td> 1458.3238</td> <td> 1875.001</td> <td>    0.778</td> <td> 0.437</td> <td>-2217.016</td> <td> 5133.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>           <td> 3645.7205</td> <td> 1992.882</td> <td>    1.829</td> <td> 0.067</td> <td> -260.688</td> <td> 7552.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>           <td> -2.33e+04</td> <td> 3645.781</td> <td>   -6.391</td> <td> 0.000</td> <td>-3.04e+04</td> <td>-1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>           <td> 2538.9901</td> <td> 2055.058</td> <td>    1.235</td> <td> 0.217</td> <td>-1489.295</td> <td> 6567.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>           <td> 6713.3535</td> <td> 2262.090</td> <td>    2.968</td> <td> 0.003</td> <td> 2279.249</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>           <td>-1.147e+04</td> <td> 2918.829</td> <td>   -3.929</td> <td> 0.000</td> <td>-1.72e+04</td> <td>-5746.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>           <td> 4739.0798</td> <td> 2769.646</td> <td>    1.711</td> <td> 0.087</td> <td> -689.926</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>           <td> 2403.6790</td> <td> 1954.092</td> <td>    1.230</td> <td> 0.219</td> <td>-1426.694</td> <td> 6234.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>           <td> -1.32e+04</td> <td> 2297.190</td> <td>   -5.744</td> <td> 0.000</td> <td>-1.77e+04</td> <td>-8693.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>           <td>-1.733e+04</td> <td> 3327.412</td> <td>   -5.209</td> <td> 0.000</td> <td>-2.39e+04</td> <td>-1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>           <td> 5389.9526</td> <td> 1993.917</td> <td>    2.703</td> <td> 0.007</td> <td> 1481.515</td> <td> 9298.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>           <td> 5616.0905</td> <td> 2207.738</td> <td>    2.544</td> <td> 0.011</td> <td> 1288.524</td> <td> 9943.657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>           <td> 5813.8222</td> <td> 1999.646</td> <td>    2.907</td> <td> 0.004</td> <td> 1894.155</td> <td> 9733.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>           <td>  871.3964</td> <td> 3388.433</td> <td>    0.257</td> <td> 0.797</td> <td>-5770.545</td> <td> 7513.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>           <td> 6442.7903</td> <td> 2100.605</td> <td>    3.067</td> <td> 0.002</td> <td> 2325.224</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>           <td> 2714.2043</td> <td> 2831.158</td> <td>    0.959</td> <td> 0.338</td> <td>-2835.376</td> <td> 8263.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>           <td>-5203.2892</td> <td> 3580.491</td> <td>   -1.453</td> <td> 0.146</td> <td>-1.22e+04</td> <td> 1815.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>           <td> 2178.7879</td> <td> 2337.863</td> <td>    0.932</td> <td> 0.351</td> <td>-2403.846</td> <td> 6761.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>           <td> 1944.2843</td> <td> 2251.271</td> <td>    0.864</td> <td> 0.388</td> <td>-2468.613</td> <td> 6357.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>           <td> -437.8140</td> <td> 2847.646</td> <td>   -0.154</td> <td> 0.878</td> <td>-6019.715</td> <td> 5144.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>           <td> 6755.8024</td> <td> 2264.195</td> <td>    2.984</td> <td> 0.003</td> <td> 2317.571</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>           <td> 2349.8825</td> <td> 2395.266</td> <td>    0.981</td> <td> 0.327</td> <td>-2345.272</td> <td> 7045.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>           <td>-1.675e+04</td> <td> 2825.572</td> <td>   -5.928</td> <td> 0.000</td> <td>-2.23e+04</td> <td>-1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>           <td> 2540.1043</td> <td> 2074.917</td> <td>    1.224</td> <td> 0.221</td> <td>-1527.108</td> <td> 6607.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>           <td> 3223.8736</td> <td> 2646.356</td> <td>    1.218</td> <td> 0.223</td> <td>-1963.462</td> <td> 8411.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>           <td> 5941.3109</td> <td> 1944.877</td> <td>    3.055</td> <td> 0.002</td> <td> 2129.001</td> <td> 9753.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>           <td> 4945.2836</td> <td> 2505.577</td> <td>    1.974</td> <td> 0.048</td> <td>   33.901</td> <td> 9856.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>           <td> 5192.6038</td> <td> 1978.114</td> <td>    2.625</td> <td> 0.009</td> <td> 1315.143</td> <td> 9070.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>           <td> 4623.6809</td> <td> 1958.565</td> <td>    2.361</td> <td> 0.018</td> <td>  784.539</td> <td> 8462.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>           <td> 3027.7137</td> <td> 2213.170</td> <td>    1.368</td> <td> 0.171</td> <td>-1310.499</td> <td> 7365.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>           <td>-5645.4833</td> <td> 2437.820</td> <td>   -2.316</td> <td> 0.021</td> <td>-1.04e+04</td> <td> -866.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>           <td> 7363.6112</td> <td> 2679.665</td> <td>    2.748</td> <td> 0.006</td> <td> 2110.983</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>           <td> 4768.4814</td> <td> 1986.783</td> <td>    2.400</td> <td> 0.016</td> <td>  874.028</td> <td> 8662.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>           <td> 8453.9041</td> <td> 2795.778</td> <td>    3.024</td> <td> 0.003</td> <td> 2973.674</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>           <td> 1294.8967</td> <td> 3434.783</td> <td>    0.377</td> <td> 0.706</td> <td>-5437.899</td> <td> 8027.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>           <td>  568.3196</td> <td> 2436.000</td> <td>    0.233</td> <td> 0.816</td> <td>-4206.680</td> <td> 5343.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>           <td> 2.457e+04</td> <td> 2341.182</td> <td>   10.494</td> <td> 0.000</td> <td>    2e+04</td> <td> 2.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>           <td>-9620.0071</td> <td> 2140.883</td> <td>   -4.493</td> <td> 0.000</td> <td>-1.38e+04</td> <td>-5423.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>           <td> 6311.2443</td> <td> 2195.472</td> <td>    2.875</td> <td> 0.004</td> <td> 2007.722</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>           <td>-1711.9393</td> <td> 2534.546</td> <td>   -0.675</td> <td> 0.499</td> <td>-6680.107</td> <td> 3256.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>           <td>-1.212e+04</td> <td> 3864.307</td> <td>   -3.137</td> <td> 0.002</td> <td>-1.97e+04</td> <td>-4546.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>           <td> 6891.7721</td> <td> 2025.029</td> <td>    3.403</td> <td> 0.001</td> <td> 2922.349</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>           <td> 6207.0136</td> <td> 2254.459</td> <td>    2.753</td> <td> 0.006</td> <td> 1787.868</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>           <td> 4346.6432</td> <td> 1940.585</td> <td>    2.240</td> <td> 0.025</td> <td>  542.746</td> <td> 8150.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>           <td>-2923.9860</td> <td> 1925.895</td> <td>   -1.518</td> <td> 0.129</td> <td>-6699.088</td> <td>  851.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>           <td> 5395.2034</td> <td> 3799.119</td> <td>    1.420</td> <td> 0.156</td> <td>-2051.757</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>           <td>-1716.7105</td> <td> 1968.780</td> <td>   -0.872</td> <td> 0.383</td> <td>-5575.874</td> <td> 2142.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>           <td>  285.4047</td> <td> 2280.382</td> <td>    0.125</td> <td> 0.900</td> <td>-4184.557</td> <td> 4755.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>           <td> 1969.4231</td> <td> 2046.587</td> <td>    0.962</td> <td> 0.336</td> <td>-2042.257</td> <td> 5981.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>           <td> 5431.4437</td> <td> 1978.129</td> <td>    2.746</td> <td> 0.006</td> <td> 1553.953</td> <td> 9308.934</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>           <td> 6442.4178</td> <td> 2088.535</td> <td>    3.085</td> <td> 0.002</td> <td> 2348.511</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>           <td>-2170.4472</td> <td> 2521.954</td> <td>   -0.861</td> <td> 0.389</td> <td>-7113.933</td> <td> 2773.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>           <td> 8728.1797</td> <td> 2013.914</td> <td>    4.334</td> <td> 0.000</td> <td> 4780.543</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>           <td> 7326.5310</td> <td> 2376.484</td> <td>    3.083</td> <td> 0.002</td> <td> 2668.194</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>           <td> 5312.7484</td> <td> 1972.564</td> <td>    2.693</td> <td> 0.007</td> <td> 1446.167</td> <td> 9179.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>           <td> 5513.0555</td> <td> 1980.033</td> <td>    2.784</td> <td> 0.005</td> <td> 1631.833</td> <td> 9394.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>           <td> 5951.9728</td> <td> 2270.753</td> <td>    2.621</td> <td> 0.009</td> <td> 1500.886</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>           <td>-4059.6169</td> <td> 2046.407</td> <td>   -1.984</td> <td> 0.047</td> <td>-8070.945</td> <td>  -48.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>           <td> 2986.3093</td> <td> 1910.549</td> <td>    1.563</td> <td> 0.118</td> <td> -758.713</td> <td> 6731.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>           <td>  409.6492</td> <td> 1922.388</td> <td>    0.213</td> <td> 0.831</td> <td>-3358.579</td> <td> 4177.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>           <td>-2.382e+04</td> <td> 3474.873</td> <td>   -6.855</td> <td> 0.000</td> <td>-3.06e+04</td> <td> -1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>           <td>-1.299e+04</td> <td> 2785.199</td> <td>   -4.663</td> <td> 0.000</td> <td>-1.84e+04</td> <td>-7529.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>           <td> 6370.8630</td> <td> 2445.448</td> <td>    2.605</td> <td> 0.009</td> <td> 1577.344</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>           <td> 1646.0339</td> <td> 1887.092</td> <td>    0.872</td> <td> 0.383</td> <td>-2053.008</td> <td> 5345.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>           <td>  724.1859</td> <td> 2189.711</td> <td>    0.331</td> <td> 0.741</td> <td>-3568.043</td> <td> 5016.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>           <td>-9141.6529</td> <td> 2593.530</td> <td>   -3.525</td> <td> 0.000</td> <td>-1.42e+04</td> <td>-4057.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>           <td> 1539.7304</td> <td> 3405.147</td> <td>    0.452</td> <td> 0.651</td> <td>-5134.973</td> <td> 8214.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>           <td>-1294.5066</td> <td> 1865.801</td> <td>   -0.694</td> <td> 0.488</td> <td>-4951.814</td> <td> 2362.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>           <td> -786.7494</td> <td> 2545.408</td> <td>   -0.309</td> <td> 0.757</td> <td>-5776.208</td> <td> 4202.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>           <td> 5005.3325</td> <td> 3665.571</td> <td>    1.365</td> <td> 0.172</td> <td>-2179.850</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>           <td> 4003.6703</td> <td> 2351.469</td> <td>    1.703</td> <td> 0.089</td> <td> -605.633</td> <td> 8612.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>           <td> 6421.3254</td> <td> 2132.547</td> <td>    3.011</td> <td> 0.003</td> <td> 2241.147</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>           <td> 3854.8123</td> <td> 1958.733</td> <td>    1.968</td> <td> 0.049</td> <td>   15.342</td> <td> 7694.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>           <td> -285.1558</td> <td> 3643.258</td> <td>   -0.078</td> <td> 0.938</td> <td>-7426.601</td> <td> 6856.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>           <td>-4621.4846</td> <td> 2959.645</td> <td>   -1.561</td> <td> 0.118</td> <td>-1.04e+04</td> <td> 1179.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>           <td>-7235.6113</td> <td> 2883.697</td> <td>   -2.509</td> <td> 0.012</td> <td>-1.29e+04</td> <td>-1583.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>           <td> 6083.6812</td> <td> 1990.954</td> <td>    3.056</td> <td> 0.002</td> <td> 2181.052</td> <td> 9986.310</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>           <td> 5852.3744</td> <td> 2765.716</td> <td>    2.116</td> <td> 0.034</td> <td>  431.072</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>           <td> 7360.1537</td> <td> 2370.308</td> <td>    3.105</td> <td> 0.002</td> <td> 2713.922</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>           <td> 6391.6828</td> <td> 2053.579</td> <td>    3.112</td> <td> 0.002</td> <td> 2366.296</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>           <td> 1049.0531</td> <td> 2159.566</td> <td>    0.486</td> <td> 0.627</td> <td>-3184.087</td> <td> 5282.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>           <td>-4597.2882</td> <td> 2864.090</td> <td>   -1.605</td> <td> 0.108</td> <td>-1.02e+04</td> <td> 1016.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>           <td>-2022.6328</td> <td> 3099.035</td> <td>   -0.653</td> <td> 0.514</td> <td>-8097.301</td> <td> 4052.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>           <td>-2690.2515</td> <td> 1868.891</td> <td>   -1.439</td> <td> 0.150</td> <td>-6353.615</td> <td>  973.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>           <td> 4126.4851</td> <td> 1990.485</td> <td>    2.073</td> <td> 0.038</td> <td>  224.775</td> <td> 8028.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>           <td> 3741.1541</td> <td> 2202.187</td> <td>    1.699</td> <td> 0.089</td> <td> -575.529</td> <td> 8057.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>           <td>-3.719e+04</td> <td> 5651.950</td> <td>   -6.580</td> <td> 0.000</td> <td>-4.83e+04</td> <td>-2.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>           <td> 4675.5631</td> <td> 2613.572</td> <td>    1.789</td> <td> 0.074</td> <td> -447.509</td> <td> 9798.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>           <td> 2465.0315</td> <td> 2077.114</td> <td>    1.187</td> <td> 0.235</td> <td>-1606.486</td> <td> 6536.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>           <td>-9777.9787</td> <td> 3402.592</td> <td>   -2.874</td> <td> 0.004</td> <td>-1.64e+04</td> <td>-3108.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>           <td>  1.03e+04</td> <td> 2315.523</td> <td>    4.446</td> <td> 0.000</td> <td> 5756.281</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>           <td>-4233.5049</td> <td> 1980.103</td> <td>   -2.138</td> <td> 0.033</td> <td>-8114.864</td> <td> -352.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>           <td>-1.445e+04</td> <td> 2884.651</td> <td>   -5.011</td> <td> 0.000</td> <td>-2.01e+04</td> <td>-8800.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>           <td> -2.22e+04</td> <td> 3469.930</td> <td>   -6.397</td> <td> 0.000</td> <td> -2.9e+04</td> <td>-1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>           <td> -532.7499</td> <td> 2355.448</td> <td>   -0.226</td> <td> 0.821</td> <td>-5149.853</td> <td> 4084.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>           <td>-5916.2834</td> <td> 2878.893</td> <td>   -2.055</td> <td> 0.040</td> <td>-1.16e+04</td> <td> -273.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>           <td>-6542.5034</td> <td> 2214.962</td> <td>   -2.954</td> <td> 0.003</td> <td>-1.09e+04</td> <td>-2200.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>           <td>-5901.8827</td> <td> 3362.432</td> <td>   -1.755</td> <td> 0.079</td> <td>-1.25e+04</td> <td>  689.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>           <td> 5408.6323</td> <td> 2415.143</td> <td>    2.239</td> <td> 0.025</td> <td>  674.516</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>           <td> 2269.3154</td> <td> 2875.822</td> <td>    0.789</td> <td> 0.430</td> <td>-3367.815</td> <td> 7906.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>           <td> 1432.3734</td> <td> 3349.810</td> <td>    0.428</td> <td> 0.669</td> <td>-5133.858</td> <td> 7998.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>           <td>-1.649e+04</td> <td> 4718.370</td> <td>   -3.495</td> <td> 0.000</td> <td>-2.57e+04</td> <td>-7242.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>           <td> 1356.2435</td> <td> 2878.268</td> <td>    0.471</td> <td> 0.638</td> <td>-4285.681</td> <td> 6998.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>           <td> 2654.6416</td> <td> 2090.304</td> <td>    1.270</td> <td> 0.204</td> <td>-1442.732</td> <td> 6752.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>           <td> 4621.7781</td> <td> 1913.605</td> <td>    2.415</td> <td> 0.016</td> <td>  870.767</td> <td> 8372.789</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>           <td>-8896.1406</td> <td> 2273.544</td> <td>   -3.913</td> <td> 0.000</td> <td>-1.34e+04</td> <td>-4439.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>           <td>-1.214e+04</td> <td> 2764.066</td> <td>   -4.391</td> <td> 0.000</td> <td>-1.76e+04</td> <td>-6718.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>           <td> 5375.5350</td> <td> 1993.720</td> <td>    2.696</td> <td> 0.007</td> <td> 1467.484</td> <td> 9283.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>           <td>-6020.1534</td> <td> 2290.075</td> <td>   -2.629</td> <td> 0.009</td> <td>-1.05e+04</td> <td>-1531.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>           <td> 5715.4592</td> <td> 2157.821</td> <td>    2.649</td> <td> 0.008</td> <td> 1485.739</td> <td> 9945.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>           <td> 2213.9534</td> <td> 2103.369</td> <td>    1.053</td> <td> 0.293</td> <td>-1909.030</td> <td> 6336.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>           <td>-9015.9095</td> <td> 2984.836</td> <td>   -3.021</td> <td> 0.003</td> <td>-1.49e+04</td> <td>-3165.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>           <td> 5038.7964</td> <td> 2085.544</td> <td>    2.416</td> <td> 0.016</td> <td>  950.754</td> <td> 9126.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>           <td>-3.225e+04</td> <td> 2303.032</td> <td>  -14.003</td> <td> 0.000</td> <td>-3.68e+04</td> <td>-2.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>           <td> 7009.7043</td> <td> 2302.429</td> <td>    3.044</td> <td> 0.002</td> <td> 2496.527</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>           <td>-7420.8977</td> <td> 2141.400</td> <td>   -3.465</td> <td> 0.001</td> <td>-1.16e+04</td> <td>-3223.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>           <td> 1021.0202</td> <td> 4247.919</td> <td>    0.240</td> <td> 0.810</td> <td>-7305.669</td> <td> 9347.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>           <td> 3373.2039</td> <td> 2130.490</td> <td>    1.583</td> <td> 0.113</td> <td> -802.942</td> <td> 7549.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>           <td> 6745.7386</td> <td> 2341.755</td> <td>    2.881</td> <td> 0.004</td> <td> 2155.477</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9999.0</th>           <td>-1.108e+04</td> <td> 2729.767</td> <td>   -4.061</td> <td> 0.000</td> <td>-1.64e+04</td> <td>-5733.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1982</th> <td>    0.0279</td> <td>    0.057</td> <td>    0.492</td> <td> 0.623</td> <td>   -0.083</td> <td>    0.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1983</th> <td>    0.0190</td> <td>    0.056</td> <td>    0.339</td> <td> 0.735</td> <td>   -0.091</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1984</th> <td>   -0.0298</td> <td>    0.056</td> <td>   -0.534</td> <td> 0.593</td> <td>   -0.139</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1985</th> <td>   -0.0536</td> <td>    0.056</td> <td>   -0.952</td> <td> 0.341</td> <td>   -0.164</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1986</th> <td>   -0.0862</td> <td>    0.057</td> <td>   -1.503</td> <td> 0.133</td> <td>   -0.199</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1987</th> <td>   -0.0938</td> <td>    0.059</td> <td>   -1.600</td> <td> 0.110</td> <td>   -0.209</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1988</th> <td>   -0.1227</td> <td>    0.060</td> <td>   -2.049</td> <td> 0.040</td> <td>   -0.240</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1989</th> <td>   -0.1236</td> <td>    0.061</td> <td>   -2.026</td> <td> 0.043</td> <td>   -0.243</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1990</th> <td>   -0.1578</td> <td>    0.062</td> <td>   -2.530</td> <td> 0.011</td> <td>   -0.280</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1991</th> <td>   -0.1388</td> <td>    0.064</td> <td>   -2.181</td> <td> 0.029</td> <td>   -0.264</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1992</th> <td>   -0.1285</td> <td>    0.065</td> <td>   -1.979</td> <td> 0.048</td> <td>   -0.256</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1993</th> <td>   -0.1041</td> <td>    0.067</td> <td>   -1.565</td> <td> 0.118</td> <td>   -0.235</td> <td>    0.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1994</th> <td>   -0.1123</td> <td>    0.068</td> <td>   -1.647</td> <td> 0.100</td> <td>   -0.246</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1995</th> <td>   -0.1026</td> <td>    0.070</td> <td>   -1.456</td> <td> 0.145</td> <td>   -0.241</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1996</th> <td>   -0.0870</td> <td>    0.073</td> <td>   -1.189</td> <td> 0.235</td> <td>   -0.231</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1997</th> <td>   -0.0746</td> <td>    0.076</td> <td>   -0.978</td> <td> 0.328</td> <td>   -0.224</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1998</th> <td>   -0.0502</td> <td>    0.079</td> <td>   -0.634</td> <td> 0.526</td> <td>   -0.206</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1999</th> <td>    0.0445</td> <td>    0.082</td> <td>    0.545</td> <td> 0.586</td> <td>   -0.115</td> <td>    0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1982</th> <td>   -0.0244</td> <td>    0.106</td> <td>   -0.230</td> <td> 0.818</td> <td>   -0.232</td> <td>    0.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1983</th> <td>   -0.0548</td> <td>    0.104</td> <td>   -0.526</td> <td> 0.599</td> <td>   -0.259</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1984</th> <td>   -0.0790</td> <td>    0.103</td> <td>   -0.768</td> <td> 0.443</td> <td>   -0.281</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1985</th> <td>   -0.1083</td> <td>    0.103</td> <td>   -1.050</td> <td> 0.294</td> <td>   -0.310</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1986</th> <td>   -0.1409</td> <td>    0.104</td> <td>   -1.354</td> <td> 0.176</td> <td>   -0.345</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1987</th> <td>   -0.1645</td> <td>    0.105</td> <td>   -1.565</td> <td> 0.118</td> <td>   -0.371</td> <td>    0.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1988</th> <td>   -0.1827</td> <td>    0.106</td> <td>   -1.721</td> <td> 0.085</td> <td>   -0.391</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1989</th> <td>   -0.1796</td> <td>    0.107</td> <td>   -1.674</td> <td> 0.094</td> <td>   -0.390</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1990</th> <td>   -0.1799</td> <td>    0.109</td> <td>   -1.655</td> <td> 0.098</td> <td>   -0.393</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1991</th> <td>   -0.1849</td> <td>    0.110</td> <td>   -1.677</td> <td> 0.094</td> <td>   -0.401</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1992</th> <td>   -0.2424</td> <td>    0.112</td> <td>   -2.168</td> <td> 0.030</td> <td>   -0.461</td> <td>   -0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1993</th> <td>   -0.2737</td> <td>    0.114</td> <td>   -2.408</td> <td> 0.016</td> <td>   -0.496</td> <td>   -0.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1994</th> <td>   -0.2781</td> <td>    0.115</td> <td>   -2.410</td> <td> 0.016</td> <td>   -0.504</td> <td>   -0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1995</th> <td>   -0.2600</td> <td>    0.118</td> <td>   -2.200</td> <td> 0.028</td> <td>   -0.492</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1996</th> <td>   -0.3052</td> <td>    0.122</td> <td>   -2.505</td> <td> 0.012</td> <td>   -0.544</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1997</th> <td>   -0.2848</td> <td>    0.126</td> <td>   -2.263</td> <td> 0.024</td> <td>   -0.532</td> <td>   -0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1998</th> <td>   -0.2418</td> <td>    0.130</td> <td>   -1.861</td> <td> 0.063</td> <td>   -0.496</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1999</th> <td>   -0.2678</td> <td>    0.134</td> <td>   -2.004</td> <td> 0.045</td> <td>   -0.530</td> <td>   -0.006</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22454.933</td> <th>  Durbin-Watson:     </th>   <td>   0.747</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>133216647.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>14.365</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>524.154</td>  <th>  Cond. No.          </th>   <td>6.09e+19</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.5e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &       0.669    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &       0.645    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &       28.14    \\\\\n",
       "\\textbf{Date:}             & Mon, 14 Oct 2024 & \\textbf{  Prob (F-statistic):} &       0.00     \\\\\n",
       "\\textbf{Time:}             &     16:57:35     & \\textbf{  Log-Likelihood:    } &  -1.2184e+05   \\\\\n",
       "\\textbf{No. Observations:} &       11736      & \\textbf{  AIC:               } &   2.453e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       10949      & \\textbf{  BIC:               } &   2.511e+05    \\\\\n",
       "\\textbf{Df Model:}         &         786      & \\textbf{                     } &                \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &                \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &   -7970.3444  &     1646.372     &    -4.841  &         0.000        &    -1.12e+04    &    -4743.159     \\\\\n",
       "\\textbf{gspilltecIV}      &       0.1976  &        0.137     &     1.443  &         0.149        &       -0.071    &        0.466     \\\\\n",
       "\\textbf{gspillsicIV}      &       0.9836  &        0.220     &     4.474  &         0.000        &        0.553    &        1.415     \\\\\n",
       "\\textbf{pat\\_count}       &     -25.3942  &        1.772     &   -14.333  &         0.000        &      -28.867    &      -21.921     \\\\\n",
       "\\textbf{rsales}           &       0.9966  &        0.041     &    24.161  &         0.000        &        0.916    &        1.078     \\\\\n",
       "\\textbf{rppent}           &       0.5445  &        0.087     &     6.287  &         0.000        &        0.375    &        0.714     \\\\\n",
       "\\textbf{emp}              &      -4.8664  &        7.025     &    -0.693  &         0.489        &      -18.638    &        8.905     \\\\\n",
       "\\textbf{rxrd}             &       8.7573  &        0.669     &    13.099  &         0.000        &        7.447    &       10.068     \\\\\n",
       "\\textbf{1982}             &    -299.1318  &      893.410     &    -0.335  &         0.738        &    -2050.377    &     1452.114     \\\\\n",
       "\\textbf{1983}             &    -185.4166  &      885.977     &    -0.209  &         0.834        &    -1922.091    &     1551.258     \\\\\n",
       "\\textbf{1984}             &      26.8164  &      880.619     &     0.030  &         0.976        &    -1699.356    &     1752.989     \\\\\n",
       "\\textbf{1985}             &     258.8063  &      877.114     &     0.295  &         0.768        &    -1460.496    &     1978.108     \\\\\n",
       "\\textbf{1986}             &     638.1451  &      870.983     &     0.733  &         0.464        &    -1069.139    &     2345.429     \\\\\n",
       "\\textbf{1987}             &     579.2066  &      867.289     &     0.668  &         0.504        &    -1120.836    &     2279.249     \\\\\n",
       "\\textbf{1988}             &     830.2504  &      865.173     &     0.960  &         0.337        &     -865.645    &     2526.146     \\\\\n",
       "\\textbf{1989}             &     936.1895  &      861.950     &     1.086  &         0.277        &     -753.389    &     2625.768     \\\\\n",
       "\\textbf{1990}             &    1110.8190  &      856.149     &     1.297  &         0.195        &     -567.388    &     2789.026     \\\\\n",
       "\\textbf{1991}             &    1112.1719  &      855.495     &     1.300  &         0.194        &     -564.753    &     2789.096     \\\\\n",
       "\\textbf{1992}             &    1066.7174  &      855.480     &     1.247  &         0.212        &     -610.178    &     2743.612     \\\\\n",
       "\\textbf{1993}             &     734.1947  &      852.340     &     0.861  &         0.389        &     -936.545    &     2404.935     \\\\\n",
       "\\textbf{1994}             &     621.6407  &      853.810     &     0.728  &         0.467        &    -1051.980    &     2295.262     \\\\\n",
       "\\textbf{1995}             &     759.8321  &      853.042     &     0.891  &         0.373        &     -912.284    &     2431.949     \\\\\n",
       "\\textbf{1996}             &     797.1663  &      854.450     &     0.933  &         0.351        &     -877.711    &     2472.043     \\\\\n",
       "\\textbf{1997}             &     704.0261  &      858.768     &     0.820  &         0.412        &     -979.314    &     2387.366     \\\\\n",
       "\\textbf{1998}             &    -206.4946  &      862.245     &    -0.239  &         0.811        &    -1896.650    &     1483.661     \\\\\n",
       "\\textbf{1999}             &   -2373.0192  &      871.300     &    -2.724  &         0.006        &    -4080.924    &     -665.115     \\\\\n",
       "\\textbf{10005.0}          &    4423.6332  &     1945.861     &     2.273  &         0.023        &      609.395    &     8237.872     \\\\\n",
       "\\textbf{10006.0}          &    3896.3454  &     2396.271     &     1.626  &         0.104        &     -800.779    &     8593.470     \\\\\n",
       "\\textbf{10008.0}          &    2729.6951  &     2026.089     &     1.347  &         0.178        &    -1241.806    &     6701.196     \\\\\n",
       "\\textbf{10016.0}          &    4461.3322  &     2065.822     &     2.160  &         0.031        &      411.948    &     8510.716     \\\\\n",
       "\\textbf{10030.0}          &    6303.9751  &     2075.938     &     3.037  &         0.002        &     2234.761    &     1.04e+04     \\\\\n",
       "\\textbf{1004.0}           &    5572.3407  &     1982.123     &     2.811  &         0.005        &     1687.022    &     9457.659     \\\\\n",
       "\\textbf{10056.0}          &    2485.7359  &     1888.099     &     1.317  &         0.188        &    -1215.278    &     6186.750     \\\\\n",
       "\\textbf{10085.0}          &   -5123.4447  &     1988.147     &    -2.577  &         0.010        &    -9020.571    &    -1226.318     \\\\\n",
       "\\textbf{10092.0}          &    5690.4196  &     4723.700     &     1.205  &         0.228        &    -3568.886    &     1.49e+04     \\\\\n",
       "\\textbf{10097.0}          &   -3481.5308  &     2021.059     &    -1.723  &         0.085        &    -7443.171    &      480.110     \\\\\n",
       "\\textbf{1010.0}           &    4456.0415  &     4739.256     &     0.940  &         0.347        &    -4833.757    &     1.37e+04     \\\\\n",
       "\\textbf{10109.0}          &    7533.2294  &     2359.993     &     3.192  &         0.001        &     2907.217    &     1.22e+04     \\\\\n",
       "\\textbf{10115.0}          &     959.4923  &     1931.531     &     0.497  &         0.619        &    -2826.657    &     4745.642     \\\\\n",
       "\\textbf{10124.0}          &    7693.8970  &     2384.513     &     3.227  &         0.001        &     3019.821    &     1.24e+04     \\\\\n",
       "\\textbf{1013.0}           &   -1833.6524  &     2057.997     &    -0.891  &         0.373        &    -5867.699    &     2200.394     \\\\\n",
       "\\textbf{10150.0}          &   -4351.2823  &     2474.700     &    -1.758  &         0.079        &    -9202.141    &      499.577     \\\\\n",
       "\\textbf{10159.0}          &   -1.098e+04  &     4817.183     &    -2.280  &         0.023        &    -2.04e+04    &    -1539.339     \\\\\n",
       "\\textbf{10174.0}          &    6436.4782  &     2265.679     &     2.841  &         0.005        &     1995.338    &     1.09e+04     \\\\\n",
       "\\textbf{10185.0}          &    2352.5604  &     2426.672     &     0.969  &         0.332        &    -2404.155    &     7109.276     \\\\\n",
       "\\textbf{10195.0}          &   -1.002e+04  &     3163.284     &    -3.168  &         0.002        &    -1.62e+04    &    -3819.945     \\\\\n",
       "\\textbf{10198.0}          &    6266.0516  &     2059.503     &     3.043  &         0.002        &     2229.054    &     1.03e+04     \\\\\n",
       "\\textbf{10215.0}          &    7356.7837  &     2352.848     &     3.127  &         0.002        &     2744.777    &      1.2e+04     \\\\\n",
       "\\textbf{10232.0}          &   -1923.3394  &     2954.323     &    -0.651  &         0.515        &    -7714.347    &     3867.668     \\\\\n",
       "\\textbf{10236.0}          &    5620.6268  &     2090.424     &     2.689  &         0.007        &     1523.018    &     9718.236     \\\\\n",
       "\\textbf{10286.0}          &    3685.8163  &     2065.689     &     1.784  &         0.074        &     -363.307    &     7734.939     \\\\\n",
       "\\textbf{10301.0}          &   -2.369e+04  &     3266.198     &    -7.253  &         0.000        &    -3.01e+04    &    -1.73e+04     \\\\\n",
       "\\textbf{10312.0}          &    5356.4273  &     1980.608     &     2.704  &         0.007        &     1474.077    &     9238.777     \\\\\n",
       "\\textbf{10332.0}          &   -1702.3331  &     5029.195     &    -0.338  &         0.735        &    -1.16e+04    &     8155.798     \\\\\n",
       "\\textbf{1036.0}           &    1367.8519  &     2174.242     &     0.629  &         0.529        &    -2894.054    &     5629.758     \\\\\n",
       "\\textbf{10374.0}          &    3093.1762  &     1892.605     &     1.634  &         0.102        &     -616.672    &     6803.025     \\\\\n",
       "\\textbf{10386.0}          &   -2434.4612  &     2648.650     &    -0.919  &         0.358        &    -7626.294    &     2757.371     \\\\\n",
       "\\textbf{10391.0}          &   -4973.4930  &     3172.560     &    -1.568  &         0.117        &    -1.12e+04    &     1245.299     \\\\\n",
       "\\textbf{10407.0}          &   -3883.9357  &     1976.728     &    -1.965  &         0.049        &    -7758.680    &       -9.191     \\\\\n",
       "\\textbf{10420.0}          &    3271.1148  &     2015.881     &     1.623  &         0.105        &     -680.376    &     7222.605     \\\\\n",
       "\\textbf{10422.0}          &     877.6333  &     1968.655     &     0.446  &         0.656        &    -2981.286    &     4736.553     \\\\\n",
       "\\textbf{10426.0}          &    5600.8729  &     2458.851     &     2.278  &         0.023        &      781.080    &     1.04e+04     \\\\\n",
       "\\textbf{10441.0}          &    6572.9769  &     2208.111     &     2.977  &         0.003        &     2244.680    &     1.09e+04     \\\\\n",
       "\\textbf{1045.0}           &   -5502.7756  &     2220.004     &    -2.479  &         0.013        &    -9854.385    &    -1151.166     \\\\\n",
       "\\textbf{10453.0}          &   -1721.5928  &     2623.601     &    -0.656  &         0.512        &    -6864.325    &     3421.139     \\\\\n",
       "\\textbf{10482.0}          &   -2.562e+04  &     2429.201     &   -10.547  &         0.000        &    -3.04e+04    &    -2.09e+04     \\\\\n",
       "\\textbf{10498.0}          &    5215.2149  &     2047.859     &     2.547  &         0.011        &     1201.042    &     9229.388     \\\\\n",
       "\\textbf{10499.0}          &   -7048.4947  &     3741.693     &    -1.884  &         0.060        &    -1.44e+04    &      285.899     \\\\\n",
       "\\textbf{10511.0}          &    6992.8093  &     2406.487     &     2.906  &         0.004        &     2275.660    &     1.17e+04     \\\\\n",
       "\\textbf{10519.0}          &   -1.618e+04  &     2611.534     &    -6.196  &         0.000        &    -2.13e+04    &    -1.11e+04     \\\\\n",
       "\\textbf{10530.0}          &     145.7061  &     2687.829     &     0.054  &         0.957        &    -5122.925    &     5414.337     \\\\\n",
       "\\textbf{10537.0}          &     287.3177  &     2585.115     &     0.111  &         0.912        &    -4779.974    &     5354.610     \\\\\n",
       "\\textbf{10540.0}          &    2657.4776  &     2005.124     &     1.325  &         0.185        &    -1272.928    &     6587.884     \\\\\n",
       "\\textbf{10541.0}          &    4904.7695  &     2084.013     &     2.354  &         0.019        &      819.727    &     8989.812     \\\\\n",
       "\\textbf{10550.0}          &    -197.2840  &     5777.805     &    -0.034  &         0.973        &    -1.15e+04    &     1.11e+04     \\\\\n",
       "\\textbf{10553.0}          &    -824.1046  &     2249.095     &    -0.366  &         0.714        &    -5232.738    &     3584.529     \\\\\n",
       "\\textbf{10565.0}          &    6895.7899  &     2078.804     &     3.317  &         0.001        &     2820.959    &      1.1e+04     \\\\\n",
       "\\textbf{10580.0}          &    6945.8749  &     2158.807     &     3.217  &         0.001        &     2714.223    &     1.12e+04     \\\\\n",
       "\\textbf{10581.0}          &    4199.8439  &     1976.004     &     2.125  &         0.034        &      326.520    &     8073.168     \\\\\n",
       "\\textbf{10588.0}          &   -1.007e+04  &     2324.374     &    -4.333  &         0.000        &    -1.46e+04    &    -5515.036     \\\\\n",
       "\\textbf{10597.0}          &    4783.5174  &     1957.334     &     2.444  &         0.015        &      946.789    &     8620.246     \\\\\n",
       "\\textbf{10599.0}          &    5706.0069  &     1994.958     &     2.860  &         0.004        &     1795.529    &     9616.485     \\\\\n",
       "\\textbf{10618.0}          &    4197.2794  &     1940.081     &     2.163  &         0.031        &      394.370    &     8000.189     \\\\\n",
       "\\textbf{10656.0}          &    4765.8184  &     2072.714     &     2.299  &         0.022        &      702.925    &     8828.711     \\\\\n",
       "\\textbf{10658.0}          &    4665.2623  &     2093.953     &     2.228  &         0.026        &      560.737    &     8769.788     \\\\\n",
       "\\textbf{10726.0}          &    8079.3314  &     2326.385     &     3.473  &         0.001        &     3519.196    &     1.26e+04     \\\\\n",
       "\\textbf{10734.0}          &    2089.8900  &     2639.756     &     0.792  &         0.429        &    -3084.508    &     7264.288     \\\\\n",
       "\\textbf{10735.0}          &    6321.4135  &     2268.622     &     2.786  &         0.005        &     1874.504    &     1.08e+04     \\\\\n",
       "\\textbf{10764.0}          &    6840.7994  &     2345.521     &     2.917  &         0.004        &     2243.154    &     1.14e+04     \\\\\n",
       "\\textbf{10777.0}          &    4644.0715  &     2100.630     &     2.211  &         0.027        &      526.457    &     8761.686     \\\\\n",
       "\\textbf{1078.0}           &   -6366.8780  &     3363.784     &    -1.893  &         0.058        &     -1.3e+04    &      226.746     \\\\\n",
       "\\textbf{10793.0}          &    5133.5940  &     2073.634     &     2.476  &         0.013        &     1068.896    &     9198.292     \\\\\n",
       "\\textbf{10816.0}          &    3952.6312  &     1986.684     &     1.990  &         0.047        &       58.371    &     7846.892     \\\\\n",
       "\\textbf{10839.0}          &    5744.5432  &     1983.144     &     2.897  &         0.004        &     1857.222    &     9631.864     \\\\\n",
       "\\textbf{10857.0}          &   -8106.7935  &     3073.932     &    -2.637  &         0.008        &    -1.41e+04    &    -2081.331     \\\\\n",
       "\\textbf{10867.0}          &    1669.8447  &     2286.765     &     0.730  &         0.465        &    -2812.629    &     6152.318     \\\\\n",
       "\\textbf{10906.0}          &    4536.0583  &     1955.701     &     2.319  &         0.020        &      702.530    &     8369.587     \\\\\n",
       "\\textbf{10950.0}          &    5413.8389  &     3138.722     &     1.725  &         0.085        &     -738.624    &     1.16e+04     \\\\\n",
       "\\textbf{10983.0}          &   -2.185e+04  &     2912.705     &    -7.501  &         0.000        &    -2.76e+04    &    -1.61e+04     \\\\\n",
       "\\textbf{1099.0}           &    4218.0132  &     1946.131     &     2.167  &         0.030        &      403.245    &     8032.781     \\\\\n",
       "\\textbf{10991.0}          &    -617.7506  &     2955.061     &    -0.209  &         0.834        &    -6410.204    &     5174.703     \\\\\n",
       "\\textbf{11012.0}          &    2049.0538  &     2095.262     &     0.978  &         0.328        &    -2058.039    &     6156.146     \\\\\n",
       "\\textbf{11038.0}          &   -3868.0472  &     2274.118     &    -1.701  &         0.089        &    -8325.729    &      589.634     \\\\\n",
       "\\textbf{1104.0}           &    4931.2580  &     2128.145     &     2.317  &         0.021        &      759.709    &     9102.807     \\\\\n",
       "\\textbf{11060.0}          &    6165.2247  &     2062.464     &     2.989  &         0.003        &     2122.423    &     1.02e+04     \\\\\n",
       "\\textbf{11094.0}          &    4275.1968  &     1935.192     &     2.209  &         0.027        &      481.871    &     8068.522     \\\\\n",
       "\\textbf{11096.0}          &    1557.1493  &     2024.883     &     0.769  &         0.442        &    -2411.987    &     5526.286     \\\\\n",
       "\\textbf{11113.0}          &    4720.4704  &     2154.915     &     2.191  &         0.029        &      496.447    &     8944.494     \\\\\n",
       "\\textbf{1115.0}           &    2488.6937  &     1886.884     &     1.319  &         0.187        &    -1209.941    &     6187.328     \\\\\n",
       "\\textbf{11161.0}          &    -200.9822  &     1889.135     &    -0.106  &         0.915        &    -3904.028    &     3502.064     \\\\\n",
       "\\textbf{11225.0}          &    6457.2843  &     2198.200     &     2.938  &         0.003        &     2148.416    &     1.08e+04     \\\\\n",
       "\\textbf{11228.0}          &    6006.4959  &     1987.428     &     3.022  &         0.003        &     2110.778    &     9902.214     \\\\\n",
       "\\textbf{11236.0}          &    -789.7831  &     3964.684     &    -0.199  &         0.842        &    -8561.281    &     6981.715     \\\\\n",
       "\\textbf{11288.0}          &   -1.267e+04  &     3411.232     &    -3.715  &         0.000        &    -1.94e+04    &    -5987.105     \\\\\n",
       "\\textbf{11312.0}          &   -1.206e+04  &     2531.389     &    -4.764  &         0.000        &     -1.7e+04    &    -7098.089     \\\\\n",
       "\\textbf{11361.0}          &    4171.0192  &     2257.420     &     1.848  &         0.065        &     -253.932    &     8595.970     \\\\\n",
       "\\textbf{11399.0}          &   -5172.1710  &     2042.636     &    -2.532  &         0.011        &    -9176.107    &    -1168.235     \\\\\n",
       "\\textbf{114303.0}         &   -2.789e+04  &     6149.417     &    -4.535  &         0.000        &    -3.99e+04    &    -1.58e+04     \\\\\n",
       "\\textbf{11456.0}          &   -1181.1886  &     2495.127     &    -0.473  &         0.636        &    -6072.089    &     3709.712     \\\\\n",
       "\\textbf{11465.0}          &     545.6900  &     1922.062     &     0.284  &         0.776        &    -3221.899    &     4313.279     \\\\\n",
       "\\textbf{11502.0}          &    5026.9146  &     2292.070     &     2.193  &         0.028        &      534.044    &     9519.785     \\\\\n",
       "\\textbf{11506.0}          &   -1477.5156  &     2636.836     &    -0.560  &         0.575        &    -6646.191    &     3691.160     \\\\\n",
       "\\textbf{11537.0}          &    5191.7781  &     1975.590     &     2.628  &         0.009        &     1319.264    &     9064.292     \\\\\n",
       "\\textbf{11566.0}          &    7203.5936  &     2339.239     &     3.079  &         0.002        &     2618.262    &     1.18e+04     \\\\\n",
       "\\textbf{11573.0}          &    3733.9797  &     2049.829     &     1.822  &         0.069        &     -284.056    &     7752.016     \\\\\n",
       "\\textbf{11580.0}          &   -1439.5696  &     3281.929     &    -0.439  &         0.661        &    -7872.744    &     4993.605     \\\\\n",
       "\\textbf{11600.0}          &    5773.2736  &     2176.516     &     2.653  &         0.008        &     1506.910    &        1e+04     \\\\\n",
       "\\textbf{11609.0}          &    9111.8670  &     2099.723     &     4.340  &         0.000        &     4996.032    &     1.32e+04     \\\\\n",
       "\\textbf{1161.0}           &   -8301.6528  &     2839.135     &    -2.924  &         0.003        &    -1.39e+04    &    -2736.435     \\\\\n",
       "\\textbf{11636.0}          &   -1.189e+04  &     2108.620     &    -5.641  &         0.000        &     -1.6e+04    &    -7760.674     \\\\\n",
       "\\textbf{11670.0}          &    6682.8046  &     2394.474     &     2.791  &         0.005        &     1989.204    &     1.14e+04     \\\\\n",
       "\\textbf{11678.0}          &   -1.279e+04  &     2592.406     &    -4.934  &         0.000        &    -1.79e+04    &    -7710.066     \\\\\n",
       "\\textbf{11682.0}          &    3919.2168  &     2498.677     &     1.569  &         0.117        &     -978.641    &     8817.074     \\\\\n",
       "\\textbf{11694.0}          &    5747.8300  &     2143.449     &     2.682  &         0.007        &     1546.283    &     9949.377     \\\\\n",
       "\\textbf{11720.0}          &   -8331.5343  &     3651.748     &    -2.282  &         0.023        &    -1.55e+04    &    -1173.449     \\\\\n",
       "\\textbf{11721.0}          &   -2.112e+04  &     4023.967     &    -5.248  &         0.000        &     -2.9e+04    &    -1.32e+04     \\\\\n",
       "\\textbf{11722.0}          &    3226.7944  &     2284.810     &     1.412  &         0.158        &    -1251.846    &     7705.435     \\\\\n",
       "\\textbf{11793.0}          &    2422.7192  &     6280.572     &     0.386  &         0.700        &    -9888.336    &     1.47e+04     \\\\\n",
       "\\textbf{11797.0}          &    6890.8995  &     2605.114     &     2.645  &         0.008        &     1784.406    &      1.2e+04     \\\\\n",
       "\\textbf{11914.0}          &    5029.9389  &     2786.153     &     1.805  &         0.071        &     -431.424    &     1.05e+04     \\\\\n",
       "\\textbf{1209.0}           &    1708.8772  &     2085.140     &     0.820  &         0.412        &    -2378.375    &     5796.129     \\\\\n",
       "\\textbf{12136.0}          &   -2.297e+04  &     3534.423     &    -6.499  &         0.000        &    -2.99e+04    &     -1.6e+04     \\\\\n",
       "\\textbf{12141.0}          &     5.75e+04  &     2527.854     &    22.746  &         0.000        &     5.25e+04    &     6.25e+04     \\\\\n",
       "\\textbf{12181.0}          &     808.8317  &     3469.279     &     0.233  &         0.816        &    -5991.583    &     7609.246     \\\\\n",
       "\\textbf{12215.0}          &   -1.141e+04  &     2724.193     &    -4.188  &         0.000        &    -1.67e+04    &    -6069.870     \\\\\n",
       "\\textbf{12216.0}          &   -9001.0664  &     2642.143     &    -3.407  &         0.001        &    -1.42e+04    &    -3821.988     \\\\\n",
       "\\textbf{12256.0}          &   -5606.7171  &     2305.951     &    -2.431  &         0.015        &    -1.01e+04    &    -1086.636     \\\\\n",
       "\\textbf{12262.0}          &    4379.7449  &     2537.612     &     1.726  &         0.084        &     -594.434    &     9353.924     \\\\\n",
       "\\textbf{12389.0}          &    4077.6444  &     2924.043     &     1.395  &         0.163        &    -1654.009    &     9809.297     \\\\\n",
       "\\textbf{1239.0}           &     326.7095  &     1863.890     &     0.175  &         0.861        &    -3326.852    &     3980.271     \\\\\n",
       "\\textbf{12390.0}          &    2253.3827  &     2590.929     &     0.870  &         0.384        &    -2825.307    &     7332.072     \\\\\n",
       "\\textbf{12397.0}          &     -22.7316  &     4710.894     &    -0.005  &         0.996        &    -9256.935    &     9211.472     \\\\\n",
       "\\textbf{1243.0}           &    1423.9623  &     2054.826     &     0.693  &         0.488        &    -2603.868    &     5451.793     \\\\\n",
       "\\textbf{12548.0}          &    4518.8382  &     2505.697     &     1.803  &         0.071        &     -392.781    &     9430.457     \\\\\n",
       "\\textbf{12570.0}          &    4122.4380  &     2402.266     &     1.716  &         0.086        &     -586.437    &     8831.313     \\\\\n",
       "\\textbf{12581.0}          &    1494.0695  &     2664.284     &     0.561  &         0.575        &    -3728.408    &     6716.547     \\\\\n",
       "\\textbf{12592.0}          &    1593.8538  &     2255.282     &     0.707  &         0.480        &    -2826.906    &     6014.613     \\\\\n",
       "\\textbf{12604.0}          &     614.0467  &     4796.181     &     0.128  &         0.898        &    -8787.335    &        1e+04     \\\\\n",
       "\\textbf{12656.0}          &    6568.4522  &     2642.362     &     2.486  &         0.013        &     1388.946    &     1.17e+04     \\\\\n",
       "\\textbf{12679.0}          &   -2.237e+04  &     3483.810     &    -6.420  &         0.000        &    -2.92e+04    &    -1.55e+04     \\\\\n",
       "\\textbf{1278.0}           &    6170.5617  &     2354.740     &     2.620  &         0.009        &     1554.845    &     1.08e+04     \\\\\n",
       "\\textbf{12788.0}          &   -1.963e+04  &     5046.923     &    -3.890  &         0.000        &    -2.95e+04    &    -9740.117     \\\\\n",
       "\\textbf{1283.0}           &    5074.0949  &     2453.744     &     2.068  &         0.039        &      264.314    &     9883.876     \\\\\n",
       "\\textbf{1297.0}           &    4313.6075  &     1939.878     &     2.224  &         0.026        &      511.096    &     8116.119     \\\\\n",
       "\\textbf{12992.0}          &    6303.9339  &     2463.337     &     2.559  &         0.011        &     1475.348    &     1.11e+04     \\\\\n",
       "\\textbf{13135.0}          &    -114.8493  &     2351.336     &    -0.049  &         0.961        &    -4723.892    &     4494.194     \\\\\n",
       "\\textbf{1327.0}           &   -9291.4165  &     2954.040     &    -3.145  &         0.002        &    -1.51e+04    &    -3500.965     \\\\\n",
       "\\textbf{13282.0}          &   -3949.8216  &     4698.787     &    -0.841  &         0.401        &    -1.32e+04    &     5260.649     \\\\\n",
       "\\textbf{1334.0}           &   -1.257e+04  &     4428.531     &    -2.838  &         0.005        &    -2.12e+04    &    -3888.374     \\\\\n",
       "\\textbf{13351.0}          &    -868.4950  &     3352.923     &    -0.259  &         0.796        &    -7440.829    &     5703.839     \\\\\n",
       "\\textbf{13365.0}          &   -2.809e+04  &     4496.713     &    -6.248  &         0.000        &    -3.69e+04    &    -1.93e+04     \\\\\n",
       "\\textbf{13369.0}          &    2055.5421  &     2915.430     &     0.705  &         0.481        &    -3659.228    &     7770.312     \\\\\n",
       "\\textbf{13406.0}          &    5280.9784  &     2332.553     &     2.264  &         0.024        &      708.753    &     9853.203     \\\\\n",
       "\\textbf{13407.0}          &   -5374.5504  &     2343.945     &    -2.293  &         0.022        &    -9969.106    &     -779.995     \\\\\n",
       "\\textbf{13417.0}          &    6302.2748  &     2678.796     &     2.353  &         0.019        &     1051.351    &     1.16e+04     \\\\\n",
       "\\textbf{13525.0}          &   -9069.5391  &     3742.808     &    -2.423  &         0.015        &    -1.64e+04    &    -1732.960     \\\\\n",
       "\\textbf{13554.0}          &    7043.7827  &     2741.083     &     2.570  &         0.010        &     1670.765    &     1.24e+04     \\\\\n",
       "\\textbf{1359.0}           &     -1.8e+04  &     4300.929     &    -4.185  &         0.000        &    -2.64e+04    &    -9568.791     \\\\\n",
       "\\textbf{13623.0}          &    1884.4479  &     2395.362     &     0.787  &         0.431        &    -2810.893    &     6579.789     \\\\\n",
       "\\textbf{1372.0}           &   -8586.9305  &     2750.602     &    -3.122  &         0.002        &     -1.4e+04    &    -3195.254     \\\\\n",
       "\\textbf{1380.0}           &   -6198.7549  &     2027.587     &    -3.057  &         0.002        &    -1.02e+04    &    -2224.317     \\\\\n",
       "\\textbf{13923.0}          &    4327.8070  &     2618.221     &     1.653  &         0.098        &     -804.379    &     9459.993     \\\\\n",
       "\\textbf{13932.0}          &    6758.0994  &     3486.673     &     1.938  &         0.053        &      -76.410    &     1.36e+04     \\\\\n",
       "\\textbf{13941.0}          &   -1.273e+04  &     3053.517     &    -4.169  &         0.000        &    -1.87e+04    &    -6745.062     \\\\\n",
       "\\textbf{1397.0}           &    2184.6376  &     2356.026     &     0.927  &         0.354        &    -2433.599    &     6802.874     \\\\\n",
       "\\textbf{14064.0}          &    1481.5536  &     2474.573     &     0.599  &         0.549        &    -3369.057    &     6332.165     \\\\\n",
       "\\textbf{14084.0}          &    3460.5188  &     2779.609     &     1.245  &         0.213        &    -1988.018    &     8909.055     \\\\\n",
       "\\textbf{14324.0}          &   -1.022e+04  &     2902.442     &    -3.522  &         0.000        &    -1.59e+04    &    -4533.698     \\\\\n",
       "\\textbf{14462.0}          &    1784.4463  &     3174.752     &     0.562  &         0.574        &    -4438.641    &     8007.533     \\\\\n",
       "\\textbf{1447.0}           &    5170.3233  &     4124.969     &     1.253  &         0.210        &    -2915.361    &     1.33e+04     \\\\\n",
       "\\textbf{14593.0}          &    5761.9653  &     2680.234     &     2.150  &         0.032        &      508.223    &      1.1e+04     \\\\\n",
       "\\textbf{14622.0}          &    4023.6148  &     8181.170     &     0.492  &         0.623        &     -1.2e+04    &     2.01e+04     \\\\\n",
       "\\textbf{1465.0}           &    3253.6294  &     2827.174     &     1.151  &         0.250        &    -2288.143    &     8795.402     \\\\\n",
       "\\textbf{1468.0}           &    6773.6570  &     2589.057     &     2.616  &         0.009        &     1698.637    &     1.18e+04     \\\\\n",
       "\\textbf{14897.0}          &    2946.6509  &     4736.264     &     0.622  &         0.534        &    -6337.283    &     1.22e+04     \\\\\n",
       "\\textbf{14954.0}          &    4472.7867  &     2590.021     &     1.727  &         0.084        &     -604.121    &     9549.695     \\\\\n",
       "\\textbf{1496.0}           &    6599.9720  &     2091.312     &     3.156  &         0.002        &     2500.622    &     1.07e+04     \\\\\n",
       "\\textbf{15267.0}          &    3568.1880  &     2481.632     &     1.438  &         0.151        &    -1296.259    &     8432.635     \\\\\n",
       "\\textbf{15354.0}          &   -2104.0140  &     2656.087     &    -0.792  &         0.428        &    -7310.425    &     3102.397     \\\\\n",
       "\\textbf{1542.0}           &    3367.1680  &     1934.110     &     1.741  &         0.082        &     -424.037    &     7158.373     \\\\\n",
       "\\textbf{15459.0}          &    -277.1102  &     3475.397     &    -0.080  &         0.936        &    -7089.517    &     6535.297     \\\\\n",
       "\\textbf{1554.0}           &    6356.9665  &     2083.527     &     3.051  &         0.002        &     2272.876    &     1.04e+04     \\\\\n",
       "\\textbf{15708.0}          &   -3.007e+04  &     4710.742     &    -6.384  &         0.000        &    -3.93e+04    &    -2.08e+04     \\\\\n",
       "\\textbf{15711.0}          &    2482.5945  &     2540.937     &     0.977  &         0.329        &    -2498.101    &     7463.290     \\\\\n",
       "\\textbf{15761.0}          &    5239.0419  &     2946.614     &     1.778  &         0.075        &     -536.854    &      1.1e+04     \\\\\n",
       "\\textbf{1581.0}           &   -9946.6783  &     4696.052     &    -2.118  &         0.034        &    -1.92e+04    &     -741.569     \\\\\n",
       "\\textbf{1593.0}           &    2771.8015  &     2169.241     &     1.278  &         0.201        &    -1480.302    &     7023.905     \\\\\n",
       "\\textbf{1602.0}           &    8998.4516  &     2219.156     &     4.055  &         0.000        &     4648.505    &     1.33e+04     \\\\\n",
       "\\textbf{1613.0}           &    5299.0984  &     2089.204     &     2.536  &         0.011        &     1203.881    &     9394.315     \\\\\n",
       "\\textbf{16188.0}          &   -1213.8922  &     2753.992     &    -0.441  &         0.659        &    -6612.214    &     4184.430     \\\\\n",
       "\\textbf{1632.0}           &   -7522.0773  &     2191.983     &    -3.432  &         0.001        &    -1.18e+04    &    -3225.395     \\\\\n",
       "\\textbf{1633.0}           &    2211.8725  &     1905.594     &     1.161  &         0.246        &    -1523.437    &     5947.182     \\\\\n",
       "\\textbf{1635.0}           &   -2.008e+04  &     4095.903     &    -4.902  &         0.000        &    -2.81e+04    &    -1.21e+04     \\\\\n",
       "\\textbf{16401.0}          &   -1.125e+04  &     3803.823     &    -2.958  &         0.003        &    -1.87e+04    &    -3795.056     \\\\\n",
       "\\textbf{16437.0}          &   -6466.0225  &     3287.215     &    -1.967  &         0.049        &    -1.29e+04    &      -22.487     \\\\\n",
       "\\textbf{1651.0}           &   -2151.8718  &     1896.807     &    -1.134  &         0.257        &    -5869.957    &     1566.213     \\\\\n",
       "\\textbf{1655.0}           &    6065.6852  &     2012.244     &     3.014  &         0.003        &     2121.323    &        1e+04     \\\\\n",
       "\\textbf{1663.0}           &    7504.9048  &     2023.292     &     3.709  &         0.000        &     3538.887    &     1.15e+04     \\\\\n",
       "\\textbf{16710.0}          &   -2286.6678  &     2709.468     &    -0.844  &         0.399        &    -7597.715    &     3024.379     \\\\\n",
       "\\textbf{16729.0}          &   -3762.6345  &     2609.260     &    -1.442  &         0.149        &    -8877.255    &     1351.986     \\\\\n",
       "\\textbf{1690.0}           &    -2.08e+04  &     2791.669     &    -7.450  &         0.000        &    -2.63e+04    &    -1.53e+04     \\\\\n",
       "\\textbf{1703.0}           &    2346.6634  &     2095.044     &     1.120  &         0.263        &    -1760.002    &     6453.329     \\\\\n",
       "\\textbf{17202.0}          &    3189.7453  &     2706.630     &     1.178  &         0.239        &    -2115.739    &     8495.230     \\\\\n",
       "\\textbf{1722.0}           &    3336.7948  &     1989.516     &     1.677  &         0.094        &     -563.016    &     7236.606     \\\\\n",
       "\\textbf{1728.0}           &    6131.9418  &     2062.060     &     2.974  &         0.003        &     2089.931    &     1.02e+04     \\\\\n",
       "\\textbf{1743.0}           &    5846.0477  &     3270.715     &     1.787  &         0.074        &     -565.144    &     1.23e+04     \\\\\n",
       "\\textbf{1754.0}           &    4828.8974  &     2054.183     &     2.351  &         0.019        &      802.327    &     8855.467     \\\\\n",
       "\\textbf{1762.0}           &    -969.7047  &     3354.419     &    -0.289  &         0.773        &    -7544.972    &     5605.563     \\\\\n",
       "\\textbf{1773.0}           &    5652.1795  &     2378.876     &     2.376  &         0.018        &      989.152    &     1.03e+04     \\\\\n",
       "\\textbf{1786.0}           &   -1.672e+04  &     3119.593     &    -5.359  &         0.000        &    -2.28e+04    &    -1.06e+04     \\\\\n",
       "\\textbf{18100.0}          &    1594.9441  &     2630.445     &     0.606  &         0.544        &    -3561.204    &     6751.092     \\\\\n",
       "\\textbf{1820.0}           &       2.9836  &     2130.290     &     0.001  &         0.999        &    -4172.770    &     4178.737     \\\\\n",
       "\\textbf{1848.0}           &   -1.004e+04  &     2580.542     &    -3.889  &         0.000        &    -1.51e+04    &    -4978.477     \\\\\n",
       "\\textbf{18654.0}          &    6006.9608  &     3720.280     &     1.615  &         0.106        &    -1285.460    &     1.33e+04     \\\\\n",
       "\\textbf{1875.0}           &   -2027.7996  &     4116.389     &    -0.493  &         0.622        &    -1.01e+04    &     6041.068     \\\\\n",
       "\\textbf{1884.0}           &    4823.5273  &     2285.510     &     2.110  &         0.035        &      343.515    &     9303.539     \\\\\n",
       "\\textbf{1913.0}           &    -289.6444  &     2851.514     &    -0.102  &         0.919        &    -5879.126    &     5299.837     \\\\\n",
       "\\textbf{1919.0}           &    4567.3493  &     2086.784     &     2.189  &         0.029        &      476.876    &     8657.823     \\\\\n",
       "\\textbf{1920.0}           &    -953.3743  &     2167.612     &    -0.440  &         0.660        &    -5202.285    &     3295.537     \\\\\n",
       "\\textbf{1968.0}           &    3165.5363  &     2226.409     &     1.422  &         0.155        &    -1198.627    &     7529.700     \\\\\n",
       "\\textbf{1976.0}           &    6081.4701  &     2017.956     &     3.014  &         0.003        &     2125.912    &        1e+04     \\\\\n",
       "\\textbf{1981.0}           &    4850.5498  &     2023.911     &     2.397  &         0.017        &      883.319    &     8817.781     \\\\\n",
       "\\textbf{1988.0}           &   -7544.9919  &     4408.703     &    -1.711  &         0.087        &    -1.62e+04    &     1096.862     \\\\\n",
       "\\textbf{1992.0}           &    3644.4394  &     2385.214     &     1.528  &         0.127        &    -1031.010    &     8319.889     \\\\\n",
       "\\textbf{2008.0}           &    3380.6606  &     1992.552     &     1.697  &         0.090        &     -525.101    &     7286.422     \\\\\n",
       "\\textbf{2033.0}           &    5182.8595  &     2548.647     &     2.034  &         0.042        &      187.051    &     1.02e+04     \\\\\n",
       "\\textbf{2044.0}           &    1445.5060  &     1882.500     &     0.768  &         0.443        &    -2244.534    &     5135.546     \\\\\n",
       "\\textbf{2049.0}           &    3380.4050  &     1967.070     &     1.718  &         0.086        &     -475.408    &     7236.218     \\\\\n",
       "\\textbf{2061.0}           &    7278.8502  &     2303.784     &     3.160  &         0.002        &     2763.016    &     1.18e+04     \\\\\n",
       "\\textbf{20779.0}          &    2.854e+04  &     2746.675     &    10.392  &         0.000        &     2.32e+04    &     3.39e+04     \\\\\n",
       "\\textbf{2085.0}           &   -1.883e+04  &     3413.523     &    -5.517  &         0.000        &    -2.55e+04    &    -1.21e+04     \\\\\n",
       "\\textbf{2086.0}           &    2149.7707  &     1932.564     &     1.112  &         0.266        &    -1638.405    &     5937.946     \\\\\n",
       "\\textbf{2111.0}           &    2163.5491  &     1893.292     &     1.143  &         0.253        &    -1547.645    &     5874.743     \\\\\n",
       "\\textbf{21204.0}          &   -3625.3580  &     3523.717     &    -1.029  &         0.304        &    -1.05e+04    &     3281.763     \\\\\n",
       "\\textbf{21238.0}          &    4344.0112  &     2909.842     &     1.493  &         0.136        &    -1359.805    &        1e+04     \\\\\n",
       "\\textbf{2124.0}           &    3467.0513  &     2301.773     &     1.506  &         0.132        &    -1044.839    &     7978.942     \\\\\n",
       "\\textbf{2146.0}           &    8200.4701  &     3715.945     &     2.207  &         0.027        &      916.547    &     1.55e+04     \\\\\n",
       "\\textbf{21496.0}          &   -3.085e+04  &     4799.510     &    -6.429  &         0.000        &    -4.03e+04    &    -2.14e+04     \\\\\n",
       "\\textbf{2154.0}           &    4205.3975  &     1940.208     &     2.167  &         0.030        &      402.239    &     8008.556     \\\\\n",
       "\\textbf{2176.0}           &    3.625e+04  &     2605.300     &    13.916  &         0.000        &     3.11e+04    &     4.14e+04     \\\\\n",
       "\\textbf{2188.0}           &    6803.4859  &     2312.364     &     2.942  &         0.003        &     2270.835    &     1.13e+04     \\\\\n",
       "\\textbf{2189.0}           &   -2085.8933  &     2445.950     &    -0.853  &         0.394        &    -6880.397    &     2708.611     \\\\\n",
       "\\textbf{2220.0}           &    4192.9522  &     2000.417     &     2.096  &         0.036        &      271.774    &     8114.130     \\\\\n",
       "\\textbf{22205.0}          &    7163.5586  &     2911.451     &     2.460  &         0.014        &     1456.589    &     1.29e+04     \\\\\n",
       "\\textbf{2226.0}           &     333.1924  &     5725.502     &     0.058  &         0.954        &    -1.09e+04    &     1.16e+04     \\\\\n",
       "\\textbf{2230.0}           &    3004.8609  &     2510.486     &     1.197  &         0.231        &    -1916.146    &     7925.868     \\\\\n",
       "\\textbf{22325.0}          &   -1.276e+04  &     3489.731     &    -3.655  &         0.000        &    -1.96e+04    &    -5915.945     \\\\\n",
       "\\textbf{2255.0}           &    3816.8344  &     1983.445     &     1.924  &         0.054        &      -71.076    &     7704.745     \\\\\n",
       "\\textbf{22619.0}          &    5331.2899  &     2649.322     &     2.012  &         0.044        &      138.139    &     1.05e+04     \\\\\n",
       "\\textbf{2267.0}           &   -1.085e+04  &     2375.808     &    -4.567  &         0.000        &    -1.55e+04    &    -6192.365     \\\\\n",
       "\\textbf{22815.0}          &   -2744.9098  &     2591.812     &    -1.059  &         0.290        &    -7825.330    &     2335.510     \\\\\n",
       "\\textbf{2285.0}           &   -1.624e+04  &     2387.293     &    -6.801  &         0.000        &    -2.09e+04    &    -1.16e+04     \\\\\n",
       "\\textbf{2290.0}           &    -256.5923  &     1984.286     &    -0.129  &         0.897        &    -4146.152    &     3632.967     \\\\\n",
       "\\textbf{2295.0}           &    6491.9746  &     3565.988     &     1.821  &         0.069        &     -498.005    &     1.35e+04     \\\\\n",
       "\\textbf{2316.0}           &   -2443.0783  &     2378.840     &    -1.027  &         0.304        &    -7106.035    &     2219.878     \\\\\n",
       "\\textbf{23220.0}          &    3256.5419  &     2781.549     &     1.171  &         0.242        &    -2195.796    &     8708.880     \\\\\n",
       "\\textbf{23224.0}          &   -1.728e+04  &     4888.147     &    -3.535  &         0.000        &    -2.69e+04    &    -7698.600     \\\\\n",
       "\\textbf{2343.0}           &   -1.305e+04  &     4762.126     &    -2.741  &         0.006        &    -2.24e+04    &    -3720.089     \\\\\n",
       "\\textbf{2352.0}           &    2875.2759  &     2799.555     &     1.027  &         0.304        &    -2612.359    &     8362.910     \\\\\n",
       "\\textbf{23700.0}          &   -1.662e+04  &     4874.835     &    -3.410  &         0.001        &    -2.62e+04    &    -7069.000     \\\\\n",
       "\\textbf{2390.0}           &    6583.9088  &     2113.016     &     3.116  &         0.002        &     2442.015    &     1.07e+04     \\\\\n",
       "\\textbf{2393.0}           &     -47.8831  &     3109.113     &    -0.015  &         0.988        &    -6142.307    &     6046.541     \\\\\n",
       "\\textbf{2403.0}           &    4787.7759  &     3460.271     &     1.384  &         0.166        &    -1994.980    &     1.16e+04     \\\\\n",
       "\\textbf{2435.0}           &    8463.4987  &     2321.080     &     3.646  &         0.000        &     3913.763    &      1.3e+04     \\\\\n",
       "\\textbf{2444.0}           &    1530.5307  &     2561.779     &     0.597  &         0.550        &    -3491.019    &     6552.081     \\\\\n",
       "\\textbf{2448.0}           &    2254.3657  &     1877.260     &     1.201  &         0.230        &    -1425.402    &     5934.134     \\\\\n",
       "\\textbf{2469.0}           &    5903.1174  &     3746.308     &     1.576  &         0.115        &    -1440.323    &     1.32e+04     \\\\\n",
       "\\textbf{24720.0}          &    4603.8491  &     2983.561     &     1.543  &         0.123        &    -1244.469    &     1.05e+04     \\\\\n",
       "\\textbf{24800.0}          &   -1.921e+04  &     4150.154     &    -4.628  &         0.000        &    -2.73e+04    &    -1.11e+04     \\\\\n",
       "\\textbf{2482.0}           &    6797.2824  &     2144.613     &     3.169  &         0.002        &     2593.454    &      1.1e+04     \\\\\n",
       "\\textbf{24969.0}          &    5782.6663  &     3238.460     &     1.786  &         0.074        &     -565.300    &     1.21e+04     \\\\\n",
       "\\textbf{2498.0}           &   -7972.3078  &     2257.440     &    -3.532  &         0.000        &    -1.24e+04    &    -3547.317     \\\\\n",
       "\\textbf{2504.0}           &   -8315.0626  &     4485.169     &    -1.854  &         0.064        &    -1.71e+04    &      476.678     \\\\\n",
       "\\textbf{2508.0}           &    5591.6331  &     2205.029     &     2.536  &         0.011        &     1269.377    &     9913.889     \\\\\n",
       "\\textbf{25124.0}          &    5238.7752  &     2942.493     &     1.780  &         0.075        &     -529.043    &      1.1e+04     \\\\\n",
       "\\textbf{2518.0}           &    5784.5599  &     2014.599     &     2.871  &         0.004        &     1835.581    &     9733.539     \\\\\n",
       "\\textbf{25224.0}          &    4986.0492  &     8160.081     &     0.611  &         0.541        &     -1.1e+04    &      2.1e+04     \\\\\n",
       "\\textbf{25279.0}          &    3037.1248  &     2916.279     &     1.041  &         0.298        &    -2679.309    &     8753.559     \\\\\n",
       "\\textbf{2537.0}           &   -1.692e+04  &     3354.084     &    -5.045  &         0.000        &    -2.35e+04    &    -1.03e+04     \\\\\n",
       "\\textbf{2538.0}           &    5632.8772  &     3163.316     &     1.781  &         0.075        &     -567.794    &     1.18e+04     \\\\\n",
       "\\textbf{25389.0}          &    5075.2008  &     4745.757     &     1.069  &         0.285        &    -4227.339    &     1.44e+04     \\\\\n",
       "\\textbf{2547.0}           &   -6418.4704  &     2389.841     &    -2.686  &         0.007        &    -1.11e+04    &    -1733.949     \\\\\n",
       "\\textbf{2553.0}           &    4466.8948  &     2058.115     &     2.170  &         0.030        &      432.619    &     8501.171     \\\\\n",
       "\\textbf{2574.0}           &   -1014.9357  &     2596.501     &    -0.391  &         0.696        &    -6104.548    &     4074.676     \\\\\n",
       "\\textbf{25747.0}          &    4790.8815  &     3119.773     &     1.536  &         0.125        &    -1324.437    &     1.09e+04     \\\\\n",
       "\\textbf{2577.0}           &    2524.7432  &     2073.188     &     1.218  &         0.223        &    -1539.079    &     6588.565     \\\\\n",
       "\\textbf{2593.0}           &    3346.2565  &     2078.323     &     1.610  &         0.107        &     -727.631    &     7420.144     \\\\\n",
       "\\textbf{2596.0}           &    -446.1903  &     2660.117     &    -0.168  &         0.867        &    -5660.500    &     4768.119     \\\\\n",
       "\\textbf{2663.0}           &    9004.9554  &     1991.962     &     4.521  &         0.000        &     5100.349    &     1.29e+04     \\\\\n",
       "\\textbf{2771.0}           &     -66.6719  &     2575.189     &    -0.026  &         0.979        &    -5114.508    &     4981.164     \\\\\n",
       "\\textbf{2787.0}           &    4881.9340  &     1979.048     &     2.467  &         0.014        &     1002.643    &     8761.225     \\\\\n",
       "\\textbf{2797.0}           &   -1.552e+04  &     2763.530     &    -5.615  &         0.000        &    -2.09e+04    &    -1.01e+04     \\\\\n",
       "\\textbf{2802.0}           &    6137.3783  &     2114.748     &     2.902  &         0.004        &     1992.089    &     1.03e+04     \\\\\n",
       "\\textbf{2817.0}           &   -5000.6402  &     4011.657     &    -1.247  &         0.213        &    -1.29e+04    &     2862.932     \\\\\n",
       "\\textbf{28678.0}          &   -2.378e+04  &     4087.266     &    -5.818  &         0.000        &    -3.18e+04    &    -1.58e+04     \\\\\n",
       "\\textbf{28701.0}          &    3013.5790  &     1921.809     &     1.568  &         0.117        &     -753.514    &     6780.672     \\\\\n",
       "\\textbf{28742.0}          &   -1.926e+04  &     4080.807     &    -4.719  &         0.000        &    -2.73e+04    &    -1.13e+04     \\\\\n",
       "\\textbf{2888.0}           &    4118.3943  &     2300.840     &     1.790  &         0.073        &     -391.668    &     8628.457     \\\\\n",
       "\\textbf{2897.0}           &    5739.4685  &     2778.858     &     2.065  &         0.039        &      292.405    &     1.12e+04     \\\\\n",
       "\\textbf{2917.0}           &    -675.3118  &     1962.344     &    -0.344  &         0.731        &    -4521.861    &     3171.238     \\\\\n",
       "\\textbf{29392.0}          &   -1.405e+04  &     4169.759     &    -3.369  &         0.001        &    -2.22e+04    &    -5875.339     \\\\\n",
       "\\textbf{2950.0}           &   -3.247e+04  &     6182.126     &    -5.252  &         0.000        &    -4.46e+04    &    -2.04e+04     \\\\\n",
       "\\textbf{2951.0}           &    6577.8719  &     2613.091     &     2.517  &         0.012        &     1455.742    &     1.17e+04     \\\\\n",
       "\\textbf{2953.0}           &    4119.1629  &     1920.034     &     2.145  &         0.032        &      355.549    &     7882.777     \\\\\n",
       "\\textbf{2960.0}           &    2169.0739  &     3042.070     &     0.713  &         0.476        &    -3793.933    &     8132.081     \\\\\n",
       "\\textbf{2975.0}           &   -5327.7936  &     1991.703     &    -2.675  &         0.007        &    -9231.892    &    -1423.695     \\\\\n",
       "\\textbf{2982.0}           &    3763.8889  &     2022.753     &     1.861  &         0.063        &     -201.072    &     7728.850     \\\\\n",
       "\\textbf{2991.0}           &   -1.351e+04  &     2696.623     &    -5.011  &         0.000        &    -1.88e+04    &    -8225.948     \\\\\n",
       "\\textbf{3011.0}           &   -1.257e+04  &     3244.684     &    -3.873  &         0.000        &    -1.89e+04    &    -6207.562     \\\\\n",
       "\\textbf{3015.0}           &    6493.4216  &     2213.043     &     2.934  &         0.003        &     2155.458    &     1.08e+04     \\\\\n",
       "\\textbf{3026.0}           &    2850.5423  &     1969.259     &     1.448  &         0.148        &    -1009.561    &     6710.646     \\\\\n",
       "\\textbf{3031.0}           &    -2.59e+04  &     4494.210     &    -5.763  &         0.000        &    -3.47e+04    &    -1.71e+04     \\\\\n",
       "\\textbf{3062.0}           &    6092.0871  &     2240.951     &     2.719  &         0.007        &     1699.418    &     1.05e+04     \\\\\n",
       "\\textbf{3093.0}           &   -6736.3744  &     2825.111     &    -2.384  &         0.017        &    -1.23e+04    &    -1198.646     \\\\\n",
       "\\textbf{3107.0}           &    4232.4660  &     3722.982     &     1.137  &         0.256        &    -3065.252    &     1.15e+04     \\\\\n",
       "\\textbf{3121.0}           &    6528.7474  &     1957.676     &     3.335  &         0.001        &     2691.349    &     1.04e+04     \\\\\n",
       "\\textbf{3126.0}           &    4801.7854  &     2046.379     &     2.346  &         0.019        &      790.512    &     8813.059     \\\\\n",
       "\\textbf{3144.0}           &    5.303e+04  &     2028.324     &    26.145  &         0.000        &     4.91e+04    &      5.7e+04     \\\\\n",
       "\\textbf{3156.0}           &    3484.8827  &     2531.197     &     1.377  &         0.169        &    -1476.720    &     8446.486     \\\\\n",
       "\\textbf{3157.0}           &    3974.6138  &     1923.791     &     2.066  &         0.039        &      203.636    &     7745.592     \\\\\n",
       "\\textbf{3170.0}           &    3898.8978  &     1925.219     &     2.025  &         0.043        &      125.121    &     7672.675     \\\\\n",
       "\\textbf{3178.0}           &   -5879.0805  &     3091.558     &    -1.902  &         0.057        &    -1.19e+04    &      180.932     \\\\\n",
       "\\textbf{3206.0}           &     732.9240  &     2267.459     &     0.323  &         0.747        &    -3711.706    &     5177.554     \\\\\n",
       "\\textbf{3229.0}           &     425.4199  &     2389.753     &     0.178  &         0.859        &    -4258.928    &     5109.768     \\\\\n",
       "\\textbf{3235.0}           &    4587.2458  &     2290.006     &     2.003  &         0.045        &       98.420    &     9076.072     \\\\\n",
       "\\textbf{3246.0}           &    4857.3709  &     2094.704     &     2.319  &         0.020        &      751.373    &     8963.368     \\\\\n",
       "\\textbf{3248.0}           &    4753.1827  &     1996.424     &     2.381  &         0.017        &      839.832    &     8666.534     \\\\\n",
       "\\textbf{3282.0}           &   -2.486e+04  &     3038.268     &    -8.182  &         0.000        &    -3.08e+04    &    -1.89e+04     \\\\\n",
       "\\textbf{3362.0}           &   -4141.7091  &     2287.741     &    -1.810  &         0.070        &    -8626.095    &      342.676     \\\\\n",
       "\\textbf{3372.0}           &    4295.6205  &     2486.387     &     1.728  &         0.084        &     -578.148    &     9169.389     \\\\\n",
       "\\textbf{3422.0}           &    3272.0359  &     1998.614     &     1.637  &         0.102        &     -645.609    &     7189.680     \\\\\n",
       "\\textbf{3497.0}           &   -3870.0977  &     2688.876     &    -1.439  &         0.150        &    -9140.780    &     1400.585     \\\\\n",
       "\\textbf{3502.0}           &   -3650.5442  &     1950.871     &    -1.871  &         0.061        &    -7474.605    &      173.516     \\\\\n",
       "\\textbf{3504.0}           &    2538.0308  &     2851.325     &     0.890  &         0.373        &    -3051.081    &     8127.143     \\\\\n",
       "\\textbf{3505.0}           &    2211.3835  &     2418.365     &     0.914  &         0.361        &    -2529.050    &     6951.817     \\\\\n",
       "\\textbf{3532.0}           &    4747.7617  &     1905.472     &     2.492  &         0.013        &     1012.692    &     8482.831     \\\\\n",
       "\\textbf{3574.0}           &    6682.3186  &     4140.497     &     1.614  &         0.107        &    -1433.804    &     1.48e+04     \\\\\n",
       "\\textbf{3580.0}           &    -510.7928  &     1880.479     &    -0.272  &         0.786        &    -4196.872    &     3175.286     \\\\\n",
       "\\textbf{3612.0}           &    7014.3465  &     2367.323     &     2.963  &         0.003        &     2373.967    &     1.17e+04     \\\\\n",
       "\\textbf{3619.0}           &    4011.8914  &     2027.455     &     1.979  &         0.048        &       37.713    &     7986.070     \\\\\n",
       "\\textbf{3622.0}           &    6401.5443  &     2167.956     &     2.953  &         0.003        &     2151.959    &     1.07e+04     \\\\\n",
       "\\textbf{3639.0}           &   -1.152e+04  &     2403.097     &    -4.793  &         0.000        &    -1.62e+04    &    -6806.727     \\\\\n",
       "\\textbf{3650.0}           &   -4813.4761  &     3141.449     &    -1.532  &         0.125        &     -1.1e+04    &     1344.332     \\\\\n",
       "\\textbf{3662.0}           &    1825.6041  &     3032.142     &     0.602  &         0.547        &    -4117.943    &     7769.151     \\\\\n",
       "\\textbf{3734.0}           &   -1.609e+04  &     2568.390     &    -6.266  &         0.000        &    -2.11e+04    &    -1.11e+04     \\\\\n",
       "\\textbf{3735.0}           &    1494.7575  &     3506.972     &     0.426  &         0.670        &    -5379.540    &     8369.055     \\\\\n",
       "\\textbf{3761.0}           &   -2039.2412  &     2351.367     &    -0.867  &         0.386        &    -6648.345    &     2569.863     \\\\\n",
       "\\textbf{3779.0}           &   -1.899e+04  &     4196.664     &    -4.525  &         0.000        &    -2.72e+04    &    -1.08e+04     \\\\\n",
       "\\textbf{3781.0}           &   -9243.5870  &     3349.344     &    -2.760  &         0.006        &    -1.58e+04    &    -2678.267     \\\\\n",
       "\\textbf{3782.0}           &    -1.52e+04  &     2706.616     &    -5.615  &         0.000        &    -2.05e+04    &    -9893.211     \\\\\n",
       "\\textbf{3786.0}           &    2531.3596  &     1994.304     &     1.269  &         0.204        &    -1377.837    &     6440.556     \\\\\n",
       "\\textbf{3796.0}           &    -1.92e+04  &     4204.230     &    -4.566  &         0.000        &    -2.74e+04    &     -1.1e+04     \\\\\n",
       "\\textbf{3821.0}           &    5668.5792  &     2139.984     &     2.649  &         0.008        &     1473.823    &     9863.335     \\\\\n",
       "\\textbf{3835.0}           &     153.2474  &     2023.175     &     0.076  &         0.940        &    -3812.541    &     4119.036     \\\\\n",
       "\\textbf{3839.0}           &    -721.8037  &     3477.675     &    -0.208  &         0.836        &    -7538.676    &     6095.068     \\\\\n",
       "\\textbf{3840.0}           &   -8269.2236  &     2791.005     &    -2.963  &         0.003        &    -1.37e+04    &    -2798.350     \\\\\n",
       "\\textbf{3895.0}           &    5395.6303  &     2000.925     &     2.697  &         0.007        &     1473.455    &     9317.805     \\\\\n",
       "\\textbf{3908.0}           &   -2631.4283  &     3639.263     &    -0.723  &         0.470        &    -9765.041    &     4502.184     \\\\\n",
       "\\textbf{3911.0}           &   -2662.5419  &     2775.924     &    -0.959  &         0.338        &    -8103.854    &     2778.770     \\\\\n",
       "\\textbf{3917.0}           &    5387.0743  &     2156.496     &     2.498  &         0.013        &     1159.952    &     9614.197     \\\\\n",
       "\\textbf{3946.0}           &    6790.9022  &     2200.562     &     3.086  &         0.002        &     2477.404    &     1.11e+04     \\\\\n",
       "\\textbf{3971.0}           &    4338.0655  &     2029.118     &     2.138  &         0.033        &      360.627    &     8315.504     \\\\\n",
       "\\textbf{3980.0}           &    1.259e+04  &     1934.729     &     6.506  &         0.000        &     8795.154    &     1.64e+04     \\\\\n",
       "\\textbf{4034.0}           &     531.9537  &     2366.623     &     0.225  &         0.822        &    -4107.055    &     5170.962     \\\\\n",
       "\\textbf{4036.0}           &    6471.5216  &     2057.068     &     3.146  &         0.002        &     2439.297    &     1.05e+04     \\\\\n",
       "\\textbf{4040.0}           &   -4973.1511  &     2394.112     &    -2.077  &         0.038        &    -9666.044    &     -280.258     \\\\\n",
       "\\textbf{4058.0}           &    3996.2886  &     1893.926     &     2.110  &         0.035        &      283.852    &     7708.725     \\\\\n",
       "\\textbf{4060.0}           &   -9530.9087  &     3065.244     &    -3.109  &         0.002        &    -1.55e+04    &    -3522.477     \\\\\n",
       "\\textbf{4062.0}           &    8945.6153  &     2332.155     &     3.836  &         0.000        &     4374.169    &     1.35e+04     \\\\\n",
       "\\textbf{4077.0}           &    1739.0721  &     3361.765     &     0.517  &         0.605        &    -4850.595    &     8328.739     \\\\\n",
       "\\textbf{4087.0}           &   -1.712e+04  &     3453.701     &    -4.957  &         0.000        &    -2.39e+04    &    -1.04e+04     \\\\\n",
       "\\textbf{4091.0}           &    2174.3419  &     2469.131     &     0.881  &         0.379        &    -2665.601    &     7014.285     \\\\\n",
       "\\textbf{4127.0}           &   -4639.1997  &     1989.707     &    -2.332  &         0.020        &    -8539.386    &     -739.014     \\\\\n",
       "\\textbf{4138.0}           &    6315.4956  &     2737.639     &     2.307  &         0.021        &      949.229    &     1.17e+04     \\\\\n",
       "\\textbf{4162.0}           &    3541.1783  &     2469.489     &     1.434  &         0.152        &    -1299.466    &     8381.823     \\\\\n",
       "\\textbf{4186.0}           &    6613.9864  &     2116.065     &     3.126  &         0.002        &     2466.117    &     1.08e+04     \\\\\n",
       "\\textbf{4194.0}           &    3058.6732  &     2342.642     &     1.306  &         0.192        &    -1533.328    &     7650.674     \\\\\n",
       "\\textbf{4199.0}           &   -1.326e+04  &     3489.311     &    -3.801  &         0.000        &    -2.01e+04    &    -6422.966     \\\\\n",
       "\\textbf{4213.0}           &    4772.4015  &     1987.898     &     2.401  &         0.016        &      875.762    &     8669.041     \\\\\n",
       "\\textbf{4222.0}           &   -1.484e+04  &     2878.008     &    -5.155  &         0.000        &    -2.05e+04    &    -9196.052     \\\\\n",
       "\\textbf{4223.0}           &    4468.4025  &     1948.737     &     2.293  &         0.022        &      648.525    &     8288.280     \\\\\n",
       "\\textbf{4251.0}           &    6080.0881  &     2067.306     &     2.941  &         0.003        &     2027.795    &     1.01e+04     \\\\\n",
       "\\textbf{4265.0}           &    2144.9884  &     2178.035     &     0.985  &         0.325        &    -2124.354    &     6414.331     \\\\\n",
       "\\textbf{4274.0}           &    2898.8369  &     2273.900     &     1.275  &         0.202        &    -1558.419    &     7356.092     \\\\\n",
       "\\textbf{4321.0}           &    3703.1068  &     2821.710     &     1.312  &         0.189        &    -1827.954    &     9234.168     \\\\\n",
       "\\textbf{4335.0}           &    2379.0898  &     3338.800     &     0.713  &         0.476        &    -4165.562    &     8923.741     \\\\\n",
       "\\textbf{4340.0}           &    -223.6093  &     1947.219     &    -0.115  &         0.909        &    -4040.510    &     3593.292     \\\\\n",
       "\\textbf{4371.0}           &    2486.1288  &     2230.203     &     1.115  &         0.265        &    -1885.473    &     6857.730     \\\\\n",
       "\\textbf{4415.0}           &    4544.2085  &     2123.378     &     2.140  &         0.032        &      382.005    &     8706.413     \\\\\n",
       "\\textbf{4450.0}           &    1884.3765  &     2055.793     &     0.917  &         0.359        &    -2145.349    &     5914.102     \\\\\n",
       "\\textbf{4476.0}           &   -1.053e+04  &     2518.734     &    -4.182  &         0.000        &    -1.55e+04    &    -5596.305     \\\\\n",
       "\\textbf{4510.0}           &   -1667.6496  &     2866.837     &    -0.582  &         0.561        &    -7287.169    &     3951.870     \\\\\n",
       "\\textbf{4520.0}           &    4027.0706  &     2320.036     &     1.736  &         0.083        &     -520.619    &     8574.760     \\\\\n",
       "\\textbf{4551.0}           &    2602.9928  &     8093.738     &     0.322  &         0.748        &    -1.33e+04    &     1.85e+04     \\\\\n",
       "\\textbf{4568.0}           &    4908.5522  &     2122.557     &     2.313  &         0.021        &      747.957    &     9069.147     \\\\\n",
       "\\textbf{4579.0}           &    6832.7583  &     2357.491     &     2.898  &         0.004        &     2211.650    &     1.15e+04     \\\\\n",
       "\\textbf{4585.0}           &    6237.8526  &     2103.798     &     2.965  &         0.003        &     2114.028    &     1.04e+04     \\\\\n",
       "\\textbf{4595.0}           &     755.1966  &     1861.033     &     0.406  &         0.685        &    -2892.765    &     4403.158     \\\\\n",
       "\\textbf{4600.0}           &   -1.258e+04  &     2882.596     &    -4.363  &         0.000        &    -1.82e+04    &    -6927.249     \\\\\n",
       "\\textbf{4607.0}           &    6017.5190  &     2080.271     &     2.893  &         0.004        &     1939.812    &     1.01e+04     \\\\\n",
       "\\textbf{4608.0}           &   -1.093e+04  &     2885.744     &    -3.787  &         0.000        &    -1.66e+04    &    -5273.139     \\\\\n",
       "\\textbf{4622.0}           &    -393.8328  &     2850.116     &    -0.138  &         0.890        &    -5980.575    &     5192.909     \\\\\n",
       "\\textbf{4623.0}           &    4209.3349  &     2064.664     &     2.039  &         0.041        &      162.221    &     8256.449     \\\\\n",
       "\\textbf{4768.0}           &    4446.4872  &     1984.877     &     2.240  &         0.025        &      555.769    &     8337.205     \\\\\n",
       "\\textbf{4771.0}           &    6541.1562  &     2149.132     &     3.044  &         0.002        &     2328.470    &     1.08e+04     \\\\\n",
       "\\textbf{4800.0}           &    4660.6183  &     2131.455     &     2.187  &         0.029        &      482.581    &     8838.656     \\\\\n",
       "\\textbf{4802.0}           &    6035.9091  &     2020.560     &     2.987  &         0.003        &     2075.247    &     9996.571     \\\\\n",
       "\\textbf{4807.0}           &    5398.4787  &     2020.839     &     2.671  &         0.008        &     1437.269    &     9359.689     \\\\\n",
       "\\textbf{4839.0}           &   -1.066e+05  &     4610.547     &   -23.127  &         0.000        &    -1.16e+05    &    -9.76e+04     \\\\\n",
       "\\textbf{4843.0}           &   -1.791e+04  &     4421.977     &    -4.050  &         0.000        &    -2.66e+04    &    -9242.766     \\\\\n",
       "\\textbf{4881.0}           &    2395.9800  &     2081.622     &     1.151  &         0.250        &    -1684.376    &     6476.336     \\\\\n",
       "\\textbf{4900.0}           &    2114.6443  &     2024.100     &     1.045  &         0.296        &    -1852.958    &     6082.246     \\\\\n",
       "\\textbf{4926.0}           &    2443.3153  &     2216.329     &     1.102  &         0.270        &    -1901.091    &     6787.721     \\\\\n",
       "\\textbf{4941.0}           &    3719.7663  &     1971.092     &     1.887  &         0.059        &     -143.930    &     7583.462     \\\\\n",
       "\\textbf{4961.0}           &   -1.363e+04  &     3589.438     &    -3.798  &         0.000        &    -2.07e+04    &    -6597.510     \\\\\n",
       "\\textbf{4988.0}           &    1.173e+04  &     2160.613     &     5.431  &         0.000        &     7498.977    &      1.6e+04     \\\\\n",
       "\\textbf{4993.0}           &    7348.2283  &     2349.934     &     3.127  &         0.002        &     2741.933    &      1.2e+04     \\\\\n",
       "\\textbf{5018.0}           &   -2178.1172  &     3711.933     &    -0.587  &         0.557        &    -9454.177    &     5097.942     \\\\\n",
       "\\textbf{5020.0}           &   -1.562e+04  &     4033.921     &    -3.873  &         0.000        &    -2.35e+04    &    -7714.359     \\\\\n",
       "\\textbf{5027.0}           &   -1942.9461  &     1936.749     &    -1.003  &         0.316        &    -5739.323    &     1853.431     \\\\\n",
       "\\textbf{5032.0}           &    4989.8999  &     1968.604     &     2.535  &         0.011        &     1131.081    &     8848.719     \\\\\n",
       "\\textbf{5043.0}           &    -661.5468  &     1861.195     &    -0.355  &         0.722        &    -4309.826    &     2986.732     \\\\\n",
       "\\textbf{5046.0}           &   -1.112e+04  &     3197.692     &    -3.476  &         0.001        &    -1.74e+04    &    -4848.659     \\\\\n",
       "\\textbf{5047.0}           &    3.589e+04  &     4751.439     &     7.553  &         0.000        &     2.66e+04    &     4.52e+04     \\\\\n",
       "\\textbf{5065.0}           &    5337.3935  &     2385.615     &     2.237  &         0.025        &      661.157    &        1e+04     \\\\\n",
       "\\textbf{5071.0}           &    6205.3078  &     2402.340     &     2.583  &         0.010        &     1496.287    &     1.09e+04     \\\\\n",
       "\\textbf{5073.0}           &   -1.592e+05  &     6404.441     &   -24.851  &         0.000        &    -1.72e+05    &    -1.47e+05     \\\\\n",
       "\\textbf{5087.0}           &   -1022.6138  &     1857.490     &    -0.551  &         0.582        &    -4663.630    &     2618.403     \\\\\n",
       "\\textbf{5109.0}           &    6506.3275  &     2168.818     &     3.000  &         0.003        &     2255.052    &     1.08e+04     \\\\\n",
       "\\textbf{5116.0}           &   -1.375e+04  &     3103.757     &    -4.429  &         0.000        &    -1.98e+04    &    -7663.903     \\\\\n",
       "\\textbf{5122.0}           &   -1841.4695  &     1957.797     &    -0.941  &         0.347        &    -5679.106    &     1996.167     \\\\\n",
       "\\textbf{5134.0}           &   -3195.4551  &     2290.240     &    -1.395  &         0.163        &    -7684.739    &     1293.829     \\\\\n",
       "\\textbf{5142.0}           &    1196.8891  &     2704.292     &     0.443  &         0.658        &    -4104.012    &     6497.790     \\\\\n",
       "\\textbf{5165.0}           &    -601.4087  &     2853.378     &    -0.211  &         0.833        &    -6194.544    &     4991.727     \\\\\n",
       "\\textbf{5169.0}           &    1.483e+04  &     1995.122     &     7.432  &         0.000        &     1.09e+04    &     1.87e+04     \\\\\n",
       "\\textbf{5174.0}           &     246.1982  &     2276.688     &     0.108  &         0.914        &    -4216.522    &     4708.918     \\\\\n",
       "\\textbf{5179.0}           &    5138.2287  &     2045.922     &     2.511  &         0.012        &     1127.853    &     9148.605     \\\\\n",
       "\\textbf{5181.0}           &    6546.0573  &     2167.329     &     3.020  &         0.003        &     2297.701    &     1.08e+04     \\\\\n",
       "\\textbf{5187.0}           &    5977.9824  &     2517.907     &     2.374  &         0.018        &     1042.431    &     1.09e+04     \\\\\n",
       "\\textbf{5229.0}           &   -5047.5424  &     2516.110     &    -2.006  &         0.045        &    -9979.573    &     -115.512     \\\\\n",
       "\\textbf{5234.0}           &   -5097.0336  &     2435.130     &    -2.093  &         0.036        &    -9870.328    &     -323.739     \\\\\n",
       "\\textbf{5237.0}           &    4462.3487  &     1960.595     &     2.276  &         0.023        &      619.229    &     8305.469     \\\\\n",
       "\\textbf{5252.0}           &    3669.1467  &     1959.630     &     1.872  &         0.061        &     -172.082    &     7510.375     \\\\\n",
       "\\textbf{5254.0}           &    4127.3979  &     2002.788     &     2.061  &         0.039        &      201.572    &     8053.224     \\\\\n",
       "\\textbf{5306.0}           &    1736.3687  &     3072.112     &     0.565  &         0.572        &    -4285.527    &     7758.264     \\\\\n",
       "\\textbf{5338.0}           &    5385.4727  &     1968.103     &     2.736  &         0.006        &     1527.636    &     9243.309     \\\\\n",
       "\\textbf{5377.0}           &    6697.9531  &     2202.006     &     3.042  &         0.002        &     2381.624    &      1.1e+04     \\\\\n",
       "\\textbf{5439.0}           &    3987.4013  &     2024.473     &     1.970  &         0.049        &       19.069    &     7955.734     \\\\\n",
       "\\textbf{5456.0}           &    7257.8902  &     2341.843     &     3.099  &         0.002        &     2667.454    &     1.18e+04     \\\\\n",
       "\\textbf{5464.0}           &    3610.5273  &     2537.108     &     1.423  &         0.155        &    -1362.662    &     8583.717     \\\\\n",
       "\\textbf{5476.0}           &    6037.4731  &     2429.463     &     2.485  &         0.013        &     1275.287    &     1.08e+04     \\\\\n",
       "\\textbf{5492.0}           &   -2.453e+04  &     3757.022     &    -6.530  &         0.000        &    -3.19e+04    &    -1.72e+04     \\\\\n",
       "\\textbf{5496.0}           &    4161.9913  &     1937.190     &     2.148  &         0.032        &      364.749    &     7959.234     \\\\\n",
       "\\textbf{5505.0}           &    6187.1679  &     2129.255     &     2.906  &         0.004        &     2013.443    &     1.04e+04     \\\\\n",
       "\\textbf{5518.0}           &    4941.5364  &     2326.090     &     2.124  &         0.034        &      381.979    &     9501.094     \\\\\n",
       "\\textbf{5520.0}           &    1121.7689  &     1866.155     &     0.601  &         0.548        &    -2536.232    &     4779.770     \\\\\n",
       "\\textbf{5545.0}           &    5949.3888  &     2098.188     &     2.835  &         0.005        &     1836.561    &     1.01e+04     \\\\\n",
       "\\textbf{5568.0}           &    8101.5847  &     1978.173     &     4.095  &         0.000        &     4224.008    &      1.2e+04     \\\\\n",
       "\\textbf{5569.0}           &    6377.2591  &     2126.470     &     2.999  &         0.003        &     2208.993    &     1.05e+04     \\\\\n",
       "\\textbf{5578.0}           &    5302.4738  &     2003.961     &     2.646  &         0.008        &     1374.348    &     9230.599     \\\\\n",
       "\\textbf{5581.0}           &    4808.9439  &     1948.376     &     2.468  &         0.014        &      989.775    &     8628.113     \\\\\n",
       "\\textbf{5589.0}           &   -4771.1716  &     2836.259     &    -1.682  &         0.093        &    -1.03e+04    &      788.409     \\\\\n",
       "\\textbf{5597.0}           &    7113.0802  &     2651.787     &     2.682  &         0.007        &     1915.099    &     1.23e+04     \\\\\n",
       "\\textbf{5606.0}           &   -2.466e+04  &     2952.624     &    -8.351  &         0.000        &    -3.04e+04    &    -1.89e+04     \\\\\n",
       "\\textbf{5639.0}           &    7328.6885  &     2184.972     &     3.354  &         0.001        &     3045.748    &     1.16e+04     \\\\\n",
       "\\textbf{5667.0}           &    -201.0064  &     2813.432     &    -0.071  &         0.943        &    -5715.842    &     5313.830     \\\\\n",
       "\\textbf{5690.0}           &    6088.4431  &     2009.766     &     3.029  &         0.002        &     2148.938    &        1e+04     \\\\\n",
       "\\textbf{5709.0}           &    5048.1264  &     2044.923     &     2.469  &         0.014        &     1039.708    &     9056.545     \\\\\n",
       "\\textbf{5726.0}           &    4158.0942  &     2033.694     &     2.045  &         0.041        &      171.687    &     8144.501     \\\\\n",
       "\\textbf{5764.0}           &    2695.4324  &     2198.979     &     1.226  &         0.220        &    -1614.964    &     7005.828     \\\\\n",
       "\\textbf{5772.0}           &    4947.1529  &     1977.066     &     2.502  &         0.012        &     1071.746    &     8822.559     \\\\\n",
       "\\textbf{5860.0}           &    -2.73e+04  &     2902.364     &    -9.407  &         0.000        &     -3.3e+04    &    -2.16e+04     \\\\\n",
       "\\textbf{5878.0}           &    6021.2749  &     2074.299     &     2.903  &         0.004        &     1955.274    &     1.01e+04     \\\\\n",
       "\\textbf{5903.0}           &   -3027.9789  &     2157.060     &    -1.404  &         0.160        &    -7256.206    &     1200.248     \\\\\n",
       "\\textbf{5905.0}           &    3433.9024  &     2123.438     &     1.617  &         0.106        &     -728.419    &     7596.224     \\\\\n",
       "\\textbf{5959.0}           &    1057.0909  &     2289.670     &     0.462  &         0.644        &    -3431.075    &     5545.257     \\\\\n",
       "\\textbf{6008.0}           &    2.181e+04  &     2946.548     &     7.402  &         0.000        &      1.6e+04    &     2.76e+04     \\\\\n",
       "\\textbf{6034.0}           &    1717.6108  &     3401.702     &     0.505  &         0.614        &    -4950.340    &     8385.562     \\\\\n",
       "\\textbf{6035.0}           &   -6513.4996  &     2973.737     &    -2.190  &         0.029        &    -1.23e+04    &     -684.438     \\\\\n",
       "\\textbf{6036.0}           &   -8207.0573  &     2208.030     &    -3.717  &         0.000        &    -1.25e+04    &    -3878.919     \\\\\n",
       "\\textbf{6039.0}           &    4740.5056  &     1961.317     &     2.417  &         0.016        &      895.971    &     8585.040     \\\\\n",
       "\\textbf{6044.0}           &    6841.0167  &     2458.590     &     2.782  &         0.005        &     2021.736    &     1.17e+04     \\\\\n",
       "\\textbf{6066.0}           &   -1201.4683  &     4305.167     &    -0.279  &         0.780        &    -9640.374    &     7237.438     \\\\\n",
       "\\textbf{6078.0}           &    5028.0231  &     1915.403     &     2.625  &         0.009        &     1273.487    &     8782.560     \\\\\n",
       "\\textbf{6081.0}           &   -2.241e+04  &     3007.553     &    -7.451  &         0.000        &    -2.83e+04    &    -1.65e+04     \\\\\n",
       "\\textbf{60893.0}          &   -2.179e+04  &     5707.523     &    -3.817  &         0.000        &     -3.3e+04    &    -1.06e+04     \\\\\n",
       "\\textbf{6097.0}           &    5310.8121  &     2237.007     &     2.374  &         0.018        &      925.874    &     9695.750     \\\\\n",
       "\\textbf{6102.0}           &    4521.5138  &     2016.058     &     2.243  &         0.025        &      569.675    &     8473.353     \\\\\n",
       "\\textbf{6104.0}           &   -5446.4132  &     2123.959     &    -2.564  &         0.010        &    -9609.757    &    -1283.069     \\\\\n",
       "\\textbf{6109.0}           &   -8647.2793  &     2254.616     &    -3.835  &         0.000        &    -1.31e+04    &    -4227.826     \\\\\n",
       "\\textbf{6127.0}           &   -2273.4055  &     2419.539     &    -0.940  &         0.347        &    -7016.140    &     2469.329     \\\\\n",
       "\\textbf{61552.0}          &    -1.48e+04  &     3905.941     &    -3.790  &         0.000        &    -2.25e+04    &    -7146.206     \\\\\n",
       "\\textbf{6158.0}           &      23.5432  &     2042.855     &     0.012  &         0.991        &    -3980.822    &     4027.909     \\\\\n",
       "\\textbf{6171.0}           &    4551.4901  &     1945.254     &     2.340  &         0.019        &      738.441    &     8364.540     \\\\\n",
       "\\textbf{61780.0}          &    3838.6494  &     4125.994     &     0.930  &         0.352        &    -4249.045    &     1.19e+04     \\\\\n",
       "\\textbf{6207.0}           &    4384.7826  &     2028.038     &     2.162  &         0.031        &      409.462    &     8360.104     \\\\\n",
       "\\textbf{6214.0}           &    5771.6803  &     1991.297     &     2.898  &         0.004        &     1868.378    &     9674.983     \\\\\n",
       "\\textbf{6216.0}           &    6070.2664  &     2354.559     &     2.578  &         0.010        &     1454.905    &     1.07e+04     \\\\\n",
       "\\textbf{62221.0}          &    5879.7196  &     4132.114     &     1.423  &         0.155        &    -2219.970    &      1.4e+04     \\\\\n",
       "\\textbf{6259.0}           &   -5651.9887  &     3239.653     &    -1.745  &         0.081        &     -1.2e+04    &      698.317     \\\\\n",
       "\\textbf{62599.0}          &    7996.1466  &     5739.457     &     1.393  &         0.164        &    -3254.226    &     1.92e+04     \\\\\n",
       "\\textbf{6266.0}           &     460.1801  &     2933.545     &     0.157  &         0.875        &    -5290.099    &     6210.459     \\\\\n",
       "\\textbf{6268.0}           &    -322.9903  &     2588.159     &    -0.125  &         0.901        &    -5396.250    &     4750.269     \\\\\n",
       "\\textbf{6288.0}           &    3344.6810  &     2023.859     &     1.653  &         0.098        &     -622.448    &     7311.810     \\\\\n",
       "\\textbf{6297.0}           &    5443.1242  &     2099.984     &     2.592  &         0.010        &     1326.776    &     9559.473     \\\\\n",
       "\\textbf{6307.0}           &    -1.67e+04  &     3045.710     &    -5.484  &         0.000        &    -2.27e+04    &    -1.07e+04     \\\\\n",
       "\\textbf{6313.0}           &    5069.1679  &     3416.150     &     1.484  &         0.138        &    -1627.104    &     1.18e+04     \\\\\n",
       "\\textbf{6314.0}           &    5427.4416  &     2181.813     &     2.488  &         0.013        &     1150.694    &     9704.189     \\\\\n",
       "\\textbf{6326.0}           &   -3138.9060  &     2734.876     &    -1.148  &         0.251        &    -8499.757    &     2221.945     \\\\\n",
       "\\textbf{6349.0}           &    4156.5332  &     1929.335     &     2.154  &         0.031        &      374.689    &     7938.377     \\\\\n",
       "\\textbf{6357.0}           &    6145.1418  &     2241.745     &     2.741  &         0.006        &     1750.917    &     1.05e+04     \\\\\n",
       "\\textbf{6375.0}           &    1.096e+04  &     2043.515     &     5.361  &         0.000        &     6949.632    &      1.5e+04     \\\\\n",
       "\\textbf{6376.0}           &    6035.0514  &     2136.285     &     2.825  &         0.005        &     1847.548    &     1.02e+04     \\\\\n",
       "\\textbf{6379.0}           &    7991.0460  &     5764.851     &     1.386  &         0.166        &    -3309.103    &     1.93e+04     \\\\\n",
       "\\textbf{6386.0}           &    6133.8056  &     1999.923     &     3.067  &         0.002        &     2213.595    &     1.01e+04     \\\\\n",
       "\\textbf{6403.0}           &   -1207.7520  &     1989.092     &    -0.607  &         0.544        &    -5106.731    &     2691.227     \\\\\n",
       "\\textbf{6410.0}           &    7154.0765  &     2278.649     &     3.140  &         0.002        &     2687.514    &     1.16e+04     \\\\\n",
       "\\textbf{6416.0}           &   -4498.3883  &     2252.685     &    -1.997  &         0.046        &    -8914.057    &      -82.720     \\\\\n",
       "\\textbf{6424.0}           &    5450.4644  &     2064.605     &     2.640  &         0.008        &     1403.467    &     9497.462     \\\\\n",
       "\\textbf{6433.0}           &    5115.9446  &     2334.275     &     2.192  &         0.028        &      540.343    &     9691.546     \\\\\n",
       "\\textbf{6435.0}           &    4921.6992  &     2293.983     &     2.145  &         0.032        &      425.078    &     9418.321     \\\\\n",
       "\\textbf{6492.0}           &    1056.4009  &     2661.382     &     0.397  &         0.691        &    -4160.388    &     6273.190     \\\\\n",
       "\\textbf{6497.0}           &    -1.01e+04  &     3655.257     &    -2.762  &         0.006        &    -1.73e+04    &    -2931.514     \\\\\n",
       "\\textbf{6500.0}           &     991.3577  &     8288.834     &     0.120  &         0.905        &    -1.53e+04    &     1.72e+04     \\\\\n",
       "\\textbf{6509.0}           &    4465.2253  &     1955.208     &     2.284  &         0.022        &      632.664    &     8297.787     \\\\\n",
       "\\textbf{6527.0}           &    6716.1652  &     2490.058     &     2.697  &         0.007        &     1835.202    &     1.16e+04     \\\\\n",
       "\\textbf{6528.0}           &    2951.2741  &     2207.527     &     1.337  &         0.181        &    -1375.879    &     7278.427     \\\\\n",
       "\\textbf{6531.0}           &   -4679.5758  &     3071.259     &    -1.524  &         0.128        &    -1.07e+04    &     1340.646     \\\\\n",
       "\\textbf{6532.0}           &    -509.4835  &     1916.918     &    -0.266  &         0.790        &    -4266.989    &     3248.022     \\\\\n",
       "\\textbf{6543.0}           &    6419.3028  &     2107.324     &     3.046  &         0.002        &     2288.568    &     1.06e+04     \\\\\n",
       "\\textbf{6548.0}           &    5552.1721  &     2096.722     &     2.648  &         0.008        &     1442.218    &     9662.126     \\\\\n",
       "\\textbf{6550.0}           &    5597.9612  &     2211.098     &     2.532  &         0.011        &     1263.810    &     9932.112     \\\\\n",
       "\\textbf{6552.0}           &    5856.5128  &     2325.727     &     2.518  &         0.012        &     1297.667    &     1.04e+04     \\\\\n",
       "\\textbf{6565.0}           &     630.6711  &     3361.704     &     0.188  &         0.851        &    -5958.876    &     7220.219     \\\\\n",
       "\\textbf{6571.0}           &    5346.9369  &     1987.817     &     2.690  &         0.007        &     1450.457    &     9243.417     \\\\\n",
       "\\textbf{6573.0}           &    4818.2195  &     1944.367     &     2.478  &         0.013        &     1006.910    &     8629.530     \\\\\n",
       "\\textbf{6641.0}           &     179.2243  &     4100.843     &     0.044  &         0.965        &    -7859.168    &     8217.617     \\\\\n",
       "\\textbf{6649.0}           &    7111.9543  &     2183.314     &     3.257  &         0.001        &     2832.264    &     1.14e+04     \\\\\n",
       "\\textbf{6730.0}           &   -2536.2199  &     3238.822     &    -0.783  &         0.434        &    -8884.896    &     3812.456     \\\\\n",
       "\\textbf{6731.0}           &    1366.9114  &     2702.349     &     0.506  &         0.613        &    -3930.181    &     6664.004     \\\\\n",
       "\\textbf{6742.0}           &    4887.4493  &     4329.125     &     1.129  &         0.259        &    -3598.418    &     1.34e+04     \\\\\n",
       "\\textbf{6745.0}           &    6471.5926  &     2068.738     &     3.128  &         0.002        &     2416.492    &     1.05e+04     \\\\\n",
       "\\textbf{6756.0}           &    5320.6972  &     2246.481     &     2.368  &         0.018        &      917.188    &     9724.206     \\\\\n",
       "\\textbf{6765.0}           &   -1.713e+04  &     2756.595     &    -6.213  &         0.000        &    -2.25e+04    &    -1.17e+04     \\\\\n",
       "\\textbf{6768.0}           &    7876.9957  &     2429.684     &     3.242  &         0.001        &     3114.375    &     1.26e+04     \\\\\n",
       "\\textbf{6774.0}           &   -2.531e+04  &     3191.060     &    -7.930  &         0.000        &    -3.16e+04    &    -1.91e+04     \\\\\n",
       "\\textbf{6797.0}           &    6710.5132  &     2628.816     &     2.553  &         0.011        &     1557.559    &     1.19e+04     \\\\\n",
       "\\textbf{6803.0}           &    5808.6380  &     2162.285     &     2.686  &         0.007        &     1570.169    &        1e+04     \\\\\n",
       "\\textbf{6821.0}           &    5125.4429  &     1989.770     &     2.576  &         0.010        &     1225.135    &     9025.751     \\\\\n",
       "\\textbf{6830.0}           &    3448.3552  &     2117.951     &     1.628  &         0.104        &     -703.211    &     7599.922     \\\\\n",
       "\\textbf{6845.0}           &    2002.9582  &     2777.820     &     0.721  &         0.471        &    -3442.070    &     7447.987     \\\\\n",
       "\\textbf{6848.0}           &    3530.4877  &     2452.979     &     1.439  &         0.150        &    -1277.795    &     8338.770     \\\\\n",
       "\\textbf{6873.0}           &     957.8911  &     2818.602     &     0.340  &         0.734        &    -4567.078    &     6482.861     \\\\\n",
       "\\textbf{6900.0}           &    3170.6036  &     2344.626     &     1.352  &         0.176        &    -1425.287    &     7766.495     \\\\\n",
       "\\textbf{6908.0}           &    3031.8917  &     2217.822     &     1.367  &         0.172        &    -1315.439    &     7379.223     \\\\\n",
       "\\textbf{6994.0}           &    2257.5843  &     2155.260     &     1.047  &         0.295        &    -1967.114    &     6482.283     \\\\\n",
       "\\textbf{7045.0}           &   -8933.5649  &     3046.767     &    -2.932  &         0.003        &    -1.49e+04    &    -2961.351     \\\\\n",
       "\\textbf{7065.0}           &    8885.8741  &     2108.030     &     4.215  &         0.000        &     4753.755    &      1.3e+04     \\\\\n",
       "\\textbf{7085.0}           &    7510.1248  &     1992.266     &     3.770  &         0.000        &     3604.923    &     1.14e+04     \\\\\n",
       "\\textbf{7107.0}           &    4030.3376  &     2347.487     &     1.717  &         0.086        &     -571.160    &     8631.835     \\\\\n",
       "\\textbf{7116.0}           &    7345.8253  &     2307.329     &     3.184  &         0.001        &     2823.044    &     1.19e+04     \\\\\n",
       "\\textbf{7117.0}           &    7449.5209  &     3386.575     &     2.200  &         0.028        &      811.221    &     1.41e+04     \\\\\n",
       "\\textbf{7121.0}           &    3930.8037  &     1967.469     &     1.998  &         0.046        &       74.210    &     7787.398     \\\\\n",
       "\\textbf{7127.0}           &    2601.1446  &     2515.212     &     1.034  &         0.301        &    -2329.126    &     7531.415     \\\\\n",
       "\\textbf{7139.0}           &    5136.5066  &     1972.034     &     2.605  &         0.009        &     1270.963    &     9002.050     \\\\\n",
       "\\textbf{7146.0}           &    5854.8528  &     2036.846     &     2.874  &         0.004        &     1862.267    &     9847.439     \\\\\n",
       "\\textbf{7163.0}           &    8500.3657  &     2071.979     &     4.103  &         0.000        &     4438.912    &     1.26e+04     \\\\\n",
       "\\textbf{7180.0}           &    1457.2876  &     2001.475     &     0.728  &         0.467        &    -2465.965    &     5380.540     \\\\\n",
       "\\textbf{7183.0}           &    1277.7106  &     2493.046     &     0.513  &         0.608        &    -3609.110    &     6164.532     \\\\\n",
       "\\textbf{7228.0}           &    1.278e+04  &     2150.609     &     5.942  &         0.000        &     8563.950    &      1.7e+04     \\\\\n",
       "\\textbf{7232.0}           &    1690.0190  &     3382.763     &     0.500  &         0.617        &    -4940.807    &     8320.845     \\\\\n",
       "\\textbf{7250.0}           &    -147.9826  &     2226.379     &    -0.066  &         0.947        &    -4512.088    &     4216.123     \\\\\n",
       "\\textbf{7257.0}           &    1.904e+04  &     3240.032     &     5.877  &         0.000        &     1.27e+04    &     2.54e+04     \\\\\n",
       "\\textbf{7260.0}           &    4722.0406  &     1937.818     &     2.437  &         0.015        &      923.568    &     8520.513     \\\\\n",
       "\\textbf{7267.0}           &    1629.6320  &     2338.455     &     0.697  &         0.486        &    -2954.162    &     6213.426     \\\\\n",
       "\\textbf{7268.0}           &    -1.25e+04  &     3828.950     &    -3.264  &         0.001        &       -2e+04    &    -4991.117     \\\\\n",
       "\\textbf{7281.0}           &    5886.6848  &     2988.715     &     1.970  &         0.049        &       28.263    &     1.17e+04     \\\\\n",
       "\\textbf{7291.0}           &    2896.8188  &     2299.209     &     1.260  &         0.208        &    -1610.046    &     7403.684     \\\\\n",
       "\\textbf{7343.0}           &   -1.232e+04  &     3684.981     &    -3.344  &         0.001        &    -1.95e+04    &    -5099.012     \\\\\n",
       "\\textbf{7346.0}           &   -7097.0654  &     2330.495     &    -3.045  &         0.002        &    -1.17e+04    &    -2528.875     \\\\\n",
       "\\textbf{7401.0}           &    5849.8446  &     1993.130     &     2.935  &         0.003        &     1942.950    &     9756.739     \\\\\n",
       "\\textbf{7409.0}           &    4645.0055  &     2011.931     &     2.309  &         0.021        &      701.258    &     8588.753     \\\\\n",
       "\\textbf{7420.0}           &     317.5942  &     1943.010     &     0.163  &         0.870        &    -3491.057    &     4126.245     \\\\\n",
       "\\textbf{7435.0}           &    5582.4116  &     3713.583     &     1.503  &         0.133        &    -1696.881    &     1.29e+04     \\\\\n",
       "\\textbf{7466.0}           &    3307.8459  &     2163.349     &     1.529  &         0.126        &     -932.709    &     7548.400     \\\\\n",
       "\\textbf{7486.0}           &   -1.151e+04  &     3320.513     &    -3.465  &         0.001        &     -1.8e+04    &    -4997.388     \\\\\n",
       "\\textbf{7503.0}           &    4506.7226  &     3669.330     &     1.228  &         0.219        &    -2685.826    &     1.17e+04     \\\\\n",
       "\\textbf{7506.0}           &    5504.9567  &     1979.844     &     2.781  &         0.005        &     1624.105    &     9385.808     \\\\\n",
       "\\textbf{7537.0}           &    4957.2716  &     2125.435     &     2.332  &         0.020        &      791.035    &     9123.508     \\\\\n",
       "\\textbf{7549.0}           &    3019.5153  &     2228.615     &     1.355  &         0.175        &    -1348.972    &     7388.003     \\\\\n",
       "\\textbf{7554.0}           &    5336.2602  &     2084.761     &     2.560  &         0.010        &     1249.752    &     9422.768     \\\\\n",
       "\\textbf{7557.0}           &    2153.4872  &     2639.742     &     0.816  &         0.415        &    -3020.884    &     7327.859     \\\\\n",
       "\\textbf{7585.0}           &    -1.36e+04  &     3433.259     &    -3.963  &         0.000        &    -2.03e+04    &    -6875.065     \\\\\n",
       "\\textbf{7602.0}           &    4154.8022  &     1984.316     &     2.094  &         0.036        &      265.184    &     8044.420     \\\\\n",
       "\\textbf{7620.0}           &    -821.8510  &     2689.827     &    -0.306  &         0.760        &    -6094.398    &     4450.696     \\\\\n",
       "\\textbf{7636.0}           &    4797.4446  &     2046.431     &     2.344  &         0.019        &      786.071    &     8808.819     \\\\\n",
       "\\textbf{7646.0}           &    5303.9547  &     2052.240     &     2.584  &         0.010        &     1281.194    &     9326.716     \\\\\n",
       "\\textbf{7658.0}           &    1284.4351  &     2504.106     &     0.513  &         0.608        &    -3624.065    &     6192.936     \\\\\n",
       "\\textbf{7683.0}           &    6440.1126  &     2214.527     &     2.908  &         0.004        &     2099.239    &     1.08e+04     \\\\\n",
       "\\textbf{7685.0}           &    4481.2970  &     2178.464     &     2.057  &         0.040        &      211.115    &     8751.479     \\\\\n",
       "\\textbf{7692.0}           &   -2269.1270  &     1901.551     &    -1.193  &         0.233        &    -5996.511    &     1458.257     \\\\\n",
       "\\textbf{7762.0}           &    5358.1340  &     1966.060     &     2.725  &         0.006        &     1504.301    &     9211.967     \\\\\n",
       "\\textbf{7772.0}           &    -1.01e+04  &     3070.597     &    -3.290  &         0.001        &    -1.61e+04    &    -4083.439     \\\\\n",
       "\\textbf{7773.0}           &    4882.6359  &     1956.612     &     2.495  &         0.013        &     1047.323    &     8717.949     \\\\\n",
       "\\textbf{7777.0}           &   -1601.3844  &     1983.215     &    -0.807  &         0.419        &    -5488.844    &     2286.075     \\\\\n",
       "\\textbf{7835.0}           &    6229.8904  &     2022.998     &     3.080  &         0.002        &     2264.448    &     1.02e+04     \\\\\n",
       "\\textbf{7873.0}           &   -1.114e+04  &     2945.929     &    -3.782  &         0.000        &    -1.69e+04    &    -5367.782     \\\\\n",
       "\\textbf{7883.0}           &    1914.0467  &     2841.995     &     0.673  &         0.501        &    -3656.777    &     7484.870     \\\\\n",
       "\\textbf{7904.0}           &    1318.1351  &     2789.858     &     0.472  &         0.637        &    -4150.491    &     6786.761     \\\\\n",
       "\\textbf{7906.0}           &    8691.5911  &     2172.525     &     4.001  &         0.000        &     4433.050    &      1.3e+04     \\\\\n",
       "\\textbf{7921.0}           &    5611.4532  &     1981.638     &     2.832  &         0.005        &     1727.085    &     9495.821     \\\\\n",
       "\\textbf{7923.0}           &    5575.1029  &     2150.900     &     2.592  &         0.010        &     1358.950    &     9791.256     \\\\\n",
       "\\textbf{7935.0}           &    1458.3238  &     1875.001     &     0.778  &         0.437        &    -2217.016    &     5133.664     \\\\\n",
       "\\textbf{7938.0}           &    3645.7205  &     1992.882     &     1.829  &         0.067        &     -260.688    &     7552.129     \\\\\n",
       "\\textbf{7985.0}           &    -2.33e+04  &     3645.781     &    -6.391  &         0.000        &    -3.04e+04    &    -1.62e+04     \\\\\n",
       "\\textbf{8014.0}           &    2538.9901  &     2055.058     &     1.235  &         0.217        &    -1489.295    &     6567.276     \\\\\n",
       "\\textbf{8030.0}           &    6713.3535  &     2262.090     &     2.968  &         0.003        &     2279.249    &     1.11e+04     \\\\\n",
       "\\textbf{8046.0}           &   -1.147e+04  &     2918.829     &    -3.929  &         0.000        &    -1.72e+04    &    -5746.634     \\\\\n",
       "\\textbf{8047.0}           &    4739.0798  &     2769.646     &     1.711  &         0.087        &     -689.926    &     1.02e+04     \\\\\n",
       "\\textbf{8062.0}           &    2403.6790  &     1954.092     &     1.230  &         0.219        &    -1426.694    &     6234.052     \\\\\n",
       "\\textbf{8068.0}           &    -1.32e+04  &     2297.190     &    -5.744  &         0.000        &    -1.77e+04    &    -8693.260     \\\\\n",
       "\\textbf{8087.0}           &   -1.733e+04  &     3327.412     &    -5.209  &         0.000        &    -2.39e+04    &    -1.08e+04     \\\\\n",
       "\\textbf{8095.0}           &    5389.9526  &     1993.917     &     2.703  &         0.007        &     1481.515    &     9298.391     \\\\\n",
       "\\textbf{8096.0}           &    5616.0905  &     2207.738     &     2.544  &         0.011        &     1288.524    &     9943.657     \\\\\n",
       "\\textbf{8109.0}           &    5813.8222  &     1999.646     &     2.907  &         0.004        &     1894.155    &     9733.489     \\\\\n",
       "\\textbf{8123.0}           &     871.3964  &     3388.433     &     0.257  &         0.797        &    -5770.545    &     7513.338     \\\\\n",
       "\\textbf{8150.0}           &    6442.7903  &     2100.605     &     3.067  &         0.002        &     2325.224    &     1.06e+04     \\\\\n",
       "\\textbf{8163.0}           &    2714.2043  &     2831.158     &     0.959  &         0.338        &    -2835.376    &     8263.785     \\\\\n",
       "\\textbf{8176.0}           &   -5203.2892  &     3580.491     &    -1.453  &         0.146        &    -1.22e+04    &     1815.120     \\\\\n",
       "\\textbf{8202.0}           &    2178.7879  &     2337.863     &     0.932  &         0.351        &    -2403.846    &     6761.422     \\\\\n",
       "\\textbf{8214.0}           &    1944.2843  &     2251.271     &     0.864  &         0.388        &    -2468.613    &     6357.182     \\\\\n",
       "\\textbf{8215.0}           &    -437.8140  &     2847.646     &    -0.154  &         0.878        &    -6019.715    &     5144.087     \\\\\n",
       "\\textbf{8219.0}           &    6755.8024  &     2264.195     &     2.984  &         0.003        &     2317.571    &     1.12e+04     \\\\\n",
       "\\textbf{8247.0}           &    2349.8825  &     2395.266     &     0.981  &         0.327        &    -2345.272    &     7045.037     \\\\\n",
       "\\textbf{8253.0}           &   -1.675e+04  &     2825.572     &    -5.928  &         0.000        &    -2.23e+04    &    -1.12e+04     \\\\\n",
       "\\textbf{8290.0}           &    2540.1043  &     2074.917     &     1.224  &         0.221        &    -1527.108    &     6607.317     \\\\\n",
       "\\textbf{8293.0}           &    3223.8736  &     2646.356     &     1.218  &         0.223        &    -1963.462    &     8411.209     \\\\\n",
       "\\textbf{8304.0}           &    5941.3109  &     1944.877     &     3.055  &         0.002        &     2129.001    &     9753.621     \\\\\n",
       "\\textbf{8334.0}           &    4945.2836  &     2505.577     &     1.974  &         0.048        &       33.901    &     9856.667     \\\\\n",
       "\\textbf{8348.0}           &    5192.6038  &     1978.114     &     2.625  &         0.009        &     1315.143    &     9070.065     \\\\\n",
       "\\textbf{8357.0}           &    4623.6809  &     1958.565     &     2.361  &         0.018        &      784.539    &     8462.823     \\\\\n",
       "\\textbf{8358.0}           &    3027.7137  &     2213.170     &     1.368  &         0.171        &    -1310.499    &     7365.927     \\\\\n",
       "\\textbf{8446.0}           &   -5645.4833  &     2437.820     &    -2.316  &         0.021        &    -1.04e+04    &     -866.915     \\\\\n",
       "\\textbf{8460.0}           &    7363.6112  &     2679.665     &     2.748  &         0.006        &     2110.983    &     1.26e+04     \\\\\n",
       "\\textbf{8463.0}           &    4768.4814  &     1986.783     &     2.400  &         0.016        &      874.028    &     8662.935     \\\\\n",
       "\\textbf{8479.0}           &    8453.9041  &     2795.778     &     3.024  &         0.003        &     2973.674    &     1.39e+04     \\\\\n",
       "\\textbf{8530.0}           &    1294.8967  &     3434.783     &     0.377  &         0.706        &    -5437.899    &     8027.692     \\\\\n",
       "\\textbf{8536.0}           &     568.3196  &     2436.000     &     0.233  &         0.816        &    -4206.680    &     5343.320     \\\\\n",
       "\\textbf{8543.0}           &    2.457e+04  &     2341.182     &    10.494  &         0.000        &        2e+04    &     2.92e+04     \\\\\n",
       "\\textbf{8549.0}           &   -9620.0071  &     2140.883     &    -4.493  &         0.000        &    -1.38e+04    &    -5423.490     \\\\\n",
       "\\textbf{8551.0}           &    6311.2443  &     2195.472     &     2.875  &         0.004        &     2007.722    &     1.06e+04     \\\\\n",
       "\\textbf{8559.0}           &   -1711.9393  &     2534.546     &    -0.675  &         0.499        &    -6680.107    &     3256.229     \\\\\n",
       "\\textbf{8573.0}           &   -1.212e+04  &     3864.307     &    -3.137  &         0.002        &    -1.97e+04    &    -4546.777     \\\\\n",
       "\\textbf{8606.0}           &    6891.7721  &     2025.029     &     3.403  &         0.001        &     2922.349    &     1.09e+04     \\\\\n",
       "\\textbf{8607.0}           &    6207.0136  &     2254.459     &     2.753  &         0.006        &     1787.868    &     1.06e+04     \\\\\n",
       "\\textbf{8648.0}           &    4346.6432  &     1940.585     &     2.240  &         0.025        &      542.746    &     8150.541     \\\\\n",
       "\\textbf{8657.0}           &   -2923.9860  &     1925.895     &    -1.518  &         0.129        &    -6699.088    &      851.116     \\\\\n",
       "\\textbf{8675.0}           &    5395.2034  &     3799.119     &     1.420  &         0.156        &    -2051.757    &     1.28e+04     \\\\\n",
       "\\textbf{8681.0}           &   -1716.7105  &     1968.780     &    -0.872  &         0.383        &    -5575.874    &     2142.453     \\\\\n",
       "\\textbf{8687.0}           &     285.4047  &     2280.382     &     0.125  &         0.900        &    -4184.557    &     4755.366     \\\\\n",
       "\\textbf{8692.0}           &    1969.4231  &     2046.587     &     0.962  &         0.336        &    -2042.257    &     5981.103     \\\\\n",
       "\\textbf{8699.0}           &    5431.4437  &     1978.129     &     2.746  &         0.006        &     1553.953    &     9308.934     \\\\\n",
       "\\textbf{8717.0}           &    6442.4178  &     2088.535     &     3.085  &         0.002        &     2348.511    &     1.05e+04     \\\\\n",
       "\\textbf{8759.0}           &   -2170.4472  &     2521.954     &    -0.861  &         0.389        &    -7113.933    &     2773.038     \\\\\n",
       "\\textbf{8762.0}           &    8728.1797  &     2013.914     &     4.334  &         0.000        &     4780.543    &     1.27e+04     \\\\\n",
       "\\textbf{8819.0}           &    7326.5310  &     2376.484     &     3.083  &         0.002        &     2668.194    &      1.2e+04     \\\\\n",
       "\\textbf{8850.0}           &    5312.7484  &     1972.564     &     2.693  &         0.007        &     1446.167    &     9179.330     \\\\\n",
       "\\textbf{8852.0}           &    5513.0555  &     1980.033     &     2.784  &         0.005        &     1631.833    &     9394.278     \\\\\n",
       "\\textbf{8859.0}           &    5951.9728  &     2270.753     &     2.621  &         0.009        &     1500.886    &     1.04e+04     \\\\\n",
       "\\textbf{8867.0}           &   -4059.6169  &     2046.407     &    -1.984  &         0.047        &    -8070.945    &      -48.289     \\\\\n",
       "\\textbf{8881.0}           &    2986.3093  &     1910.549     &     1.563  &         0.118        &     -758.713    &     6731.331     \\\\\n",
       "\\textbf{8958.0}           &     409.6492  &     1922.388     &     0.213  &         0.831        &    -3358.579    &     4177.878     \\\\\n",
       "\\textbf{8972.0}           &   -2.382e+04  &     3474.873     &    -6.855  &         0.000        &    -3.06e+04    &     -1.7e+04     \\\\\n",
       "\\textbf{8990.0}           &   -1.299e+04  &     2785.199     &    -4.663  &         0.000        &    -1.84e+04    &    -7529.179     \\\\\n",
       "\\textbf{9004.0}           &    6370.8630  &     2445.448     &     2.605  &         0.009        &     1577.344    &     1.12e+04     \\\\\n",
       "\\textbf{9016.0}           &    1646.0339  &     1887.092     &     0.872  &         0.383        &    -2053.008    &     5345.076     \\\\\n",
       "\\textbf{9048.0}           &     724.1859  &     2189.711     &     0.331  &         0.741        &    -3568.043    &     5016.415     \\\\\n",
       "\\textbf{9051.0}           &   -9141.6529  &     2593.530     &    -3.525  &         0.000        &    -1.42e+04    &    -4057.866     \\\\\n",
       "\\textbf{9071.0}           &    1539.7304  &     3405.147     &     0.452  &         0.651        &    -5134.973    &     8214.434     \\\\\n",
       "\\textbf{9112.0}           &   -1294.5066  &     1865.801     &    -0.694  &         0.488        &    -4951.814    &     2362.801     \\\\\n",
       "\\textbf{9114.0}           &    -786.7494  &     2545.408     &    -0.309  &         0.757        &    -5776.208    &     4202.709     \\\\\n",
       "\\textbf{9132.0}           &    5005.3325  &     3665.571     &     1.365  &         0.172        &    -2179.850    &     1.22e+04     \\\\\n",
       "\\textbf{9173.0}           &    4003.6703  &     2351.469     &     1.703  &         0.089        &     -605.633    &     8612.973     \\\\\n",
       "\\textbf{9180.0}           &    6421.3254  &     2132.547     &     3.011  &         0.003        &     2241.147    &     1.06e+04     \\\\\n",
       "\\textbf{9186.0}           &    3854.8123  &     1958.733     &     1.968  &         0.049        &       15.342    &     7694.283     \\\\\n",
       "\\textbf{9191.0}           &    -285.1558  &     3643.258     &    -0.078  &         0.938        &    -7426.601    &     6856.289     \\\\\n",
       "\\textbf{9216.0}           &   -4621.4846  &     2959.645     &    -1.561  &         0.118        &    -1.04e+04    &     1179.955     \\\\\n",
       "\\textbf{9217.0}           &   -7235.6113  &     2883.697     &    -2.509  &         0.012        &    -1.29e+04    &    -1583.044     \\\\\n",
       "\\textbf{9225.0}           &    6083.6812  &     1990.954     &     3.056  &         0.002        &     2181.052    &     9986.310     \\\\\n",
       "\\textbf{9230.0}           &    5852.3744  &     2765.716     &     2.116  &         0.034        &      431.072    &     1.13e+04     \\\\\n",
       "\\textbf{9259.0}           &    7360.1537  &     2370.308     &     3.105  &         0.002        &     2713.922    &      1.2e+04     \\\\\n",
       "\\textbf{9293.0}           &    6391.6828  &     2053.579     &     3.112  &         0.002        &     2366.296    &     1.04e+04     \\\\\n",
       "\\textbf{9299.0}           &    1049.0531  &     2159.566     &     0.486  &         0.627        &    -3184.087    &     5282.193     \\\\\n",
       "\\textbf{9308.0}           &   -4597.2882  &     2864.090     &    -1.605  &         0.108        &    -1.02e+04    &     1016.845     \\\\\n",
       "\\textbf{9311.0}           &   -2022.6328  &     3099.035     &    -0.653  &         0.514        &    -8097.301    &     4052.035     \\\\\n",
       "\\textbf{9313.0}           &   -2690.2515  &     1868.891     &    -1.439  &         0.150        &    -6353.615    &      973.112     \\\\\n",
       "\\textbf{9325.0}           &    4126.4851  &     1990.485     &     2.073  &         0.038        &      224.775    &     8028.195     \\\\\n",
       "\\textbf{9332.0}           &    3741.1541  &     2202.187     &     1.699  &         0.089        &     -575.529    &     8057.838     \\\\\n",
       "\\textbf{9340.0}           &   -3.719e+04  &     5651.950     &    -6.580  &         0.000        &    -4.83e+04    &    -2.61e+04     \\\\\n",
       "\\textbf{9372.0}           &    4675.5631  &     2613.572     &     1.789  &         0.074        &     -447.509    &     9798.635     \\\\\n",
       "\\textbf{9411.0}           &    2465.0315  &     2077.114     &     1.187  &         0.235        &    -1606.486    &     6536.549     \\\\\n",
       "\\textbf{9459.0}           &   -9777.9787  &     3402.592     &    -2.874  &         0.004        &    -1.64e+04    &    -3108.283     \\\\\n",
       "\\textbf{9465.0}           &     1.03e+04  &     2315.523     &     4.446  &         0.000        &     5756.281    &     1.48e+04     \\\\\n",
       "\\textbf{9472.0}           &   -4233.5049  &     1980.103     &    -2.138  &         0.033        &    -8114.864    &     -352.146     \\\\\n",
       "\\textbf{9483.0}           &   -1.445e+04  &     2884.651     &    -5.011  &         0.000        &    -2.01e+04    &    -8800.151     \\\\\n",
       "\\textbf{9563.0}           &    -2.22e+04  &     3469.930     &    -6.397  &         0.000        &     -2.9e+04    &    -1.54e+04     \\\\\n",
       "\\textbf{9590.0}           &    -532.7499  &     2355.448     &    -0.226  &         0.821        &    -5149.853    &     4084.353     \\\\\n",
       "\\textbf{9598.0}           &   -5916.2834  &     2878.893     &    -2.055  &         0.040        &    -1.16e+04    &     -273.133     \\\\\n",
       "\\textbf{9599.0}           &   -6542.5034  &     2214.962     &    -2.954  &         0.003        &    -1.09e+04    &    -2200.777     \\\\\n",
       "\\textbf{9602.0}           &   -5901.8827  &     3362.432     &    -1.755  &         0.079        &    -1.25e+04    &      689.091     \\\\\n",
       "\\textbf{9619.0}           &    5408.6323  &     2415.143     &     2.239  &         0.025        &      674.516    &     1.01e+04     \\\\\n",
       "\\textbf{9643.0}           &    2269.3154  &     2875.822     &     0.789  &         0.430        &    -3367.815    &     7906.446     \\\\\n",
       "\\textbf{9650.0}           &    1432.3734  &     3349.810     &     0.428  &         0.669        &    -5133.858    &     7998.605     \\\\\n",
       "\\textbf{9653.0}           &   -1.649e+04  &     4718.370     &    -3.495  &         0.000        &    -2.57e+04    &    -7242.812     \\\\\n",
       "\\textbf{9667.0}           &    1356.2435  &     2878.268     &     0.471  &         0.638        &    -4285.681    &     6998.168     \\\\\n",
       "\\textbf{9698.0}           &    2654.6416  &     2090.304     &     1.270  &         0.204        &    -1442.732    &     6752.016     \\\\\n",
       "\\textbf{9699.0}           &    4621.7781  &     1913.605     &     2.415  &         0.016        &      870.767    &     8372.789     \\\\\n",
       "\\textbf{9719.0}           &   -8896.1406  &     2273.544     &    -3.913  &         0.000        &    -1.34e+04    &    -4439.585     \\\\\n",
       "\\textbf{9742.0}           &   -1.214e+04  &     2764.066     &    -4.391  &         0.000        &    -1.76e+04    &    -6718.013     \\\\\n",
       "\\textbf{9761.0}           &    5375.5350  &     1993.720     &     2.696  &         0.007        &     1467.484    &     9283.586     \\\\\n",
       "\\textbf{9771.0}           &   -6020.1534  &     2290.075     &    -2.629  &         0.009        &    -1.05e+04    &    -1531.192     \\\\\n",
       "\\textbf{9772.0}           &    5715.4592  &     2157.821     &     2.649  &         0.008        &     1485.739    &     9945.179     \\\\\n",
       "\\textbf{9778.0}           &    2213.9534  &     2103.369     &     1.053  &         0.293        &    -1909.030    &     6336.937     \\\\\n",
       "\\textbf{9799.0}           &   -9015.9095  &     2984.836     &    -3.021  &         0.003        &    -1.49e+04    &    -3165.092     \\\\\n",
       "\\textbf{9815.0}           &    5038.7964  &     2085.544     &     2.416  &         0.016        &      950.754    &     9126.839     \\\\\n",
       "\\textbf{9818.0}           &   -3.225e+04  &     2303.032     &   -14.003  &         0.000        &    -3.68e+04    &    -2.77e+04     \\\\\n",
       "\\textbf{9837.0}           &    7009.7043  &     2302.429     &     3.044  &         0.002        &     2496.527    &     1.15e+04     \\\\\n",
       "\\textbf{9922.0}           &   -7420.8977  &     2141.400     &    -3.465  &         0.001        &    -1.16e+04    &    -3223.368     \\\\\n",
       "\\textbf{9954.0}           &    1021.0202  &     4247.919     &     0.240  &         0.810        &    -7305.669    &     9347.709     \\\\\n",
       "\\textbf{9963.0}           &    3373.2039  &     2130.490     &     1.583  &         0.113        &     -802.942    &     7549.350     \\\\\n",
       "\\textbf{9988.0}           &    6745.7386  &     2341.755     &     2.881  &         0.004        &     2155.477    &     1.13e+04     \\\\\n",
       "\\textbf{9999.0}           &   -1.108e+04  &     2729.767     &    -4.061  &         0.000        &    -1.64e+04    &    -5733.534     \\\\\n",
       "\\textbf{gspilltecIVX1982} &       0.0279  &        0.057     &     0.492  &         0.623        &       -0.083    &        0.139     \\\\\n",
       "\\textbf{gspilltecIVX1983} &       0.0190  &        0.056     &     0.339  &         0.735        &       -0.091    &        0.129     \\\\\n",
       "\\textbf{gspilltecIVX1984} &      -0.0298  &        0.056     &    -0.534  &         0.593        &       -0.139    &        0.079     \\\\\n",
       "\\textbf{gspilltecIVX1985} &      -0.0536  &        0.056     &    -0.952  &         0.341        &       -0.164    &        0.057     \\\\\n",
       "\\textbf{gspilltecIVX1986} &      -0.0862  &        0.057     &    -1.503  &         0.133        &       -0.199    &        0.026     \\\\\n",
       "\\textbf{gspilltecIVX1987} &      -0.0938  &        0.059     &    -1.600  &         0.110        &       -0.209    &        0.021     \\\\\n",
       "\\textbf{gspilltecIVX1988} &      -0.1227  &        0.060     &    -2.049  &         0.040        &       -0.240    &       -0.005     \\\\\n",
       "\\textbf{gspilltecIVX1989} &      -0.1236  &        0.061     &    -2.026  &         0.043        &       -0.243    &       -0.004     \\\\\n",
       "\\textbf{gspilltecIVX1990} &      -0.1578  &        0.062     &    -2.530  &         0.011        &       -0.280    &       -0.036     \\\\\n",
       "\\textbf{gspilltecIVX1991} &      -0.1388  &        0.064     &    -2.181  &         0.029        &       -0.264    &       -0.014     \\\\\n",
       "\\textbf{gspilltecIVX1992} &      -0.1285  &        0.065     &    -1.979  &         0.048        &       -0.256    &       -0.001     \\\\\n",
       "\\textbf{gspilltecIVX1993} &      -0.1041  &        0.067     &    -1.565  &         0.118        &       -0.235    &        0.026     \\\\\n",
       "\\textbf{gspilltecIVX1994} &      -0.1123  &        0.068     &    -1.647  &         0.100        &       -0.246    &        0.021     \\\\\n",
       "\\textbf{gspilltecIVX1995} &      -0.1026  &        0.070     &    -1.456  &         0.145        &       -0.241    &        0.035     \\\\\n",
       "\\textbf{gspilltecIVX1996} &      -0.0870  &        0.073     &    -1.189  &         0.235        &       -0.231    &        0.057     \\\\\n",
       "\\textbf{gspilltecIVX1997} &      -0.0746  &        0.076     &    -0.978  &         0.328        &       -0.224    &        0.075     \\\\\n",
       "\\textbf{gspilltecIVX1998} &      -0.0502  &        0.079     &    -0.634  &         0.526        &       -0.206    &        0.105     \\\\\n",
       "\\textbf{gspilltecIVX1999} &       0.0445  &        0.082     &     0.545  &         0.586        &       -0.115    &        0.204     \\\\\n",
       "\\textbf{gspillsicIVX1982} &      -0.0244  &        0.106     &    -0.230  &         0.818        &       -0.232    &        0.183     \\\\\n",
       "\\textbf{gspillsicIVX1983} &      -0.0548  &        0.104     &    -0.526  &         0.599        &       -0.259    &        0.149     \\\\\n",
       "\\textbf{gspillsicIVX1984} &      -0.0790  &        0.103     &    -0.768  &         0.443        &       -0.281    &        0.123     \\\\\n",
       "\\textbf{gspillsicIVX1985} &      -0.1083  &        0.103     &    -1.050  &         0.294        &       -0.310    &        0.094     \\\\\n",
       "\\textbf{gspillsicIVX1986} &      -0.1409  &        0.104     &    -1.354  &         0.176        &       -0.345    &        0.063     \\\\\n",
       "\\textbf{gspillsicIVX1987} &      -0.1645  &        0.105     &    -1.565  &         0.118        &       -0.371    &        0.042     \\\\\n",
       "\\textbf{gspillsicIVX1988} &      -0.1827  &        0.106     &    -1.721  &         0.085        &       -0.391    &        0.025     \\\\\n",
       "\\textbf{gspillsicIVX1989} &      -0.1796  &        0.107     &    -1.674  &         0.094        &       -0.390    &        0.031     \\\\\n",
       "\\textbf{gspillsicIVX1990} &      -0.1799  &        0.109     &    -1.655  &         0.098        &       -0.393    &        0.033     \\\\\n",
       "\\textbf{gspillsicIVX1991} &      -0.1849  &        0.110     &    -1.677  &         0.094        &       -0.401    &        0.031     \\\\\n",
       "\\textbf{gspillsicIVX1992} &      -0.2424  &        0.112     &    -2.168  &         0.030        &       -0.461    &       -0.023     \\\\\n",
       "\\textbf{gspillsicIVX1993} &      -0.2737  &        0.114     &    -2.408  &         0.016        &       -0.496    &       -0.051     \\\\\n",
       "\\textbf{gspillsicIVX1994} &      -0.2781  &        0.115     &    -2.410  &         0.016        &       -0.504    &       -0.052     \\\\\n",
       "\\textbf{gspillsicIVX1995} &      -0.2600  &        0.118     &    -2.200  &         0.028        &       -0.492    &       -0.028     \\\\\n",
       "\\textbf{gspillsicIVX1996} &      -0.3052  &        0.122     &    -2.505  &         0.012        &       -0.544    &       -0.066     \\\\\n",
       "\\textbf{gspillsicIVX1997} &      -0.2848  &        0.126     &    -2.263  &         0.024        &       -0.532    &       -0.038     \\\\\n",
       "\\textbf{gspillsicIVX1998} &      -0.2418  &        0.130     &    -1.861  &         0.063        &       -0.496    &        0.013     \\\\\n",
       "\\textbf{gspillsicIVX1999} &      -0.2678  &        0.134     &    -2.004  &         0.045        &       -0.530    &       -0.006     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 22454.933 & \\textbf{  Durbin-Watson:     } &       0.747    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 133216647.765  \\\\\n",
       "\\textbf{Skew:}          &   14.365  & \\textbf{  Prob(JB):          } &        0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  524.154  & \\textbf{  Cond. No.          } &    6.09e+19    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 2.5e-27. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.669\n",
       "Model:                            OLS   Adj. R-squared:                  0.645\n",
       "Method:                 Least Squares   F-statistic:                     28.14\n",
       "Date:                Mon, 14 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        16:57:35   Log-Likelihood:            -1.2184e+05\n",
       "No. Observations:               11736   AIC:                         2.453e+05\n",
       "Df Residuals:                   10949   BIC:                         2.511e+05\n",
       "Df Model:                         786                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -7970.3444   1646.372     -4.841      0.000   -1.12e+04   -4743.159\n",
       "gspilltecIV          0.1976      0.137      1.443      0.149      -0.071       0.466\n",
       "gspillsicIV          0.9836      0.220      4.474      0.000       0.553       1.415\n",
       "pat_count          -25.3942      1.772    -14.333      0.000     -28.867     -21.921\n",
       "rsales               0.9966      0.041     24.161      0.000       0.916       1.078\n",
       "rppent               0.5445      0.087      6.287      0.000       0.375       0.714\n",
       "emp                 -4.8664      7.025     -0.693      0.489     -18.638       8.905\n",
       "rxrd                 8.7573      0.669     13.099      0.000       7.447      10.068\n",
       "1982              -299.1318    893.410     -0.335      0.738   -2050.377    1452.114\n",
       "1983              -185.4166    885.977     -0.209      0.834   -1922.091    1551.258\n",
       "1984                26.8164    880.619      0.030      0.976   -1699.356    1752.989\n",
       "1985               258.8063    877.114      0.295      0.768   -1460.496    1978.108\n",
       "1986               638.1451    870.983      0.733      0.464   -1069.139    2345.429\n",
       "1987               579.2066    867.289      0.668      0.504   -1120.836    2279.249\n",
       "1988               830.2504    865.173      0.960      0.337    -865.645    2526.146\n",
       "1989               936.1895    861.950      1.086      0.277    -753.389    2625.768\n",
       "1990              1110.8190    856.149      1.297      0.195    -567.388    2789.026\n",
       "1991              1112.1719    855.495      1.300      0.194    -564.753    2789.096\n",
       "1992              1066.7174    855.480      1.247      0.212    -610.178    2743.612\n",
       "1993               734.1947    852.340      0.861      0.389    -936.545    2404.935\n",
       "1994               621.6407    853.810      0.728      0.467   -1051.980    2295.262\n",
       "1995               759.8321    853.042      0.891      0.373    -912.284    2431.949\n",
       "1996               797.1663    854.450      0.933      0.351    -877.711    2472.043\n",
       "1997               704.0261    858.768      0.820      0.412    -979.314    2387.366\n",
       "1998              -206.4946    862.245     -0.239      0.811   -1896.650    1483.661\n",
       "1999             -2373.0192    871.300     -2.724      0.006   -4080.924    -665.115\n",
       "10005.0           4423.6332   1945.861      2.273      0.023     609.395    8237.872\n",
       "10006.0           3896.3454   2396.271      1.626      0.104    -800.779    8593.470\n",
       "10008.0           2729.6951   2026.089      1.347      0.178   -1241.806    6701.196\n",
       "10016.0           4461.3322   2065.822      2.160      0.031     411.948    8510.716\n",
       "10030.0           6303.9751   2075.938      3.037      0.002    2234.761    1.04e+04\n",
       "1004.0            5572.3407   1982.123      2.811      0.005    1687.022    9457.659\n",
       "10056.0           2485.7359   1888.099      1.317      0.188   -1215.278    6186.750\n",
       "10085.0          -5123.4447   1988.147     -2.577      0.010   -9020.571   -1226.318\n",
       "10092.0           5690.4196   4723.700      1.205      0.228   -3568.886    1.49e+04\n",
       "10097.0          -3481.5308   2021.059     -1.723      0.085   -7443.171     480.110\n",
       "1010.0            4456.0415   4739.256      0.940      0.347   -4833.757    1.37e+04\n",
       "10109.0           7533.2294   2359.993      3.192      0.001    2907.217    1.22e+04\n",
       "10115.0            959.4923   1931.531      0.497      0.619   -2826.657    4745.642\n",
       "10124.0           7693.8970   2384.513      3.227      0.001    3019.821    1.24e+04\n",
       "1013.0           -1833.6524   2057.997     -0.891      0.373   -5867.699    2200.394\n",
       "10150.0          -4351.2823   2474.700     -1.758      0.079   -9202.141     499.577\n",
       "10159.0          -1.098e+04   4817.183     -2.280      0.023   -2.04e+04   -1539.339\n",
       "10174.0           6436.4782   2265.679      2.841      0.005    1995.338    1.09e+04\n",
       "10185.0           2352.5604   2426.672      0.969      0.332   -2404.155    7109.276\n",
       "10195.0          -1.002e+04   3163.284     -3.168      0.002   -1.62e+04   -3819.945\n",
       "10198.0           6266.0516   2059.503      3.043      0.002    2229.054    1.03e+04\n",
       "10215.0           7356.7837   2352.848      3.127      0.002    2744.777     1.2e+04\n",
       "10232.0          -1923.3394   2954.323     -0.651      0.515   -7714.347    3867.668\n",
       "10236.0           5620.6268   2090.424      2.689      0.007    1523.018    9718.236\n",
       "10286.0           3685.8163   2065.689      1.784      0.074    -363.307    7734.939\n",
       "10301.0          -2.369e+04   3266.198     -7.253      0.000   -3.01e+04   -1.73e+04\n",
       "10312.0           5356.4273   1980.608      2.704      0.007    1474.077    9238.777\n",
       "10332.0          -1702.3331   5029.195     -0.338      0.735   -1.16e+04    8155.798\n",
       "1036.0            1367.8519   2174.242      0.629      0.529   -2894.054    5629.758\n",
       "10374.0           3093.1762   1892.605      1.634      0.102    -616.672    6803.025\n",
       "10386.0          -2434.4612   2648.650     -0.919      0.358   -7626.294    2757.371\n",
       "10391.0          -4973.4930   3172.560     -1.568      0.117   -1.12e+04    1245.299\n",
       "10407.0          -3883.9357   1976.728     -1.965      0.049   -7758.680      -9.191\n",
       "10420.0           3271.1148   2015.881      1.623      0.105    -680.376    7222.605\n",
       "10422.0            877.6333   1968.655      0.446      0.656   -2981.286    4736.553\n",
       "10426.0           5600.8729   2458.851      2.278      0.023     781.080    1.04e+04\n",
       "10441.0           6572.9769   2208.111      2.977      0.003    2244.680    1.09e+04\n",
       "1045.0           -5502.7756   2220.004     -2.479      0.013   -9854.385   -1151.166\n",
       "10453.0          -1721.5928   2623.601     -0.656      0.512   -6864.325    3421.139\n",
       "10482.0          -2.562e+04   2429.201    -10.547      0.000   -3.04e+04   -2.09e+04\n",
       "10498.0           5215.2149   2047.859      2.547      0.011    1201.042    9229.388\n",
       "10499.0          -7048.4947   3741.693     -1.884      0.060   -1.44e+04     285.899\n",
       "10511.0           6992.8093   2406.487      2.906      0.004    2275.660    1.17e+04\n",
       "10519.0          -1.618e+04   2611.534     -6.196      0.000   -2.13e+04   -1.11e+04\n",
       "10530.0            145.7061   2687.829      0.054      0.957   -5122.925    5414.337\n",
       "10537.0            287.3177   2585.115      0.111      0.912   -4779.974    5354.610\n",
       "10540.0           2657.4776   2005.124      1.325      0.185   -1272.928    6587.884\n",
       "10541.0           4904.7695   2084.013      2.354      0.019     819.727    8989.812\n",
       "10550.0           -197.2840   5777.805     -0.034      0.973   -1.15e+04    1.11e+04\n",
       "10553.0           -824.1046   2249.095     -0.366      0.714   -5232.738    3584.529\n",
       "10565.0           6895.7899   2078.804      3.317      0.001    2820.959     1.1e+04\n",
       "10580.0           6945.8749   2158.807      3.217      0.001    2714.223    1.12e+04\n",
       "10581.0           4199.8439   1976.004      2.125      0.034     326.520    8073.168\n",
       "10588.0          -1.007e+04   2324.374     -4.333      0.000   -1.46e+04   -5515.036\n",
       "10597.0           4783.5174   1957.334      2.444      0.015     946.789    8620.246\n",
       "10599.0           5706.0069   1994.958      2.860      0.004    1795.529    9616.485\n",
       "10618.0           4197.2794   1940.081      2.163      0.031     394.370    8000.189\n",
       "10656.0           4765.8184   2072.714      2.299      0.022     702.925    8828.711\n",
       "10658.0           4665.2623   2093.953      2.228      0.026     560.737    8769.788\n",
       "10726.0           8079.3314   2326.385      3.473      0.001    3519.196    1.26e+04\n",
       "10734.0           2089.8900   2639.756      0.792      0.429   -3084.508    7264.288\n",
       "10735.0           6321.4135   2268.622      2.786      0.005    1874.504    1.08e+04\n",
       "10764.0           6840.7994   2345.521      2.917      0.004    2243.154    1.14e+04\n",
       "10777.0           4644.0715   2100.630      2.211      0.027     526.457    8761.686\n",
       "1078.0           -6366.8780   3363.784     -1.893      0.058    -1.3e+04     226.746\n",
       "10793.0           5133.5940   2073.634      2.476      0.013    1068.896    9198.292\n",
       "10816.0           3952.6312   1986.684      1.990      0.047      58.371    7846.892\n",
       "10839.0           5744.5432   1983.144      2.897      0.004    1857.222    9631.864\n",
       "10857.0          -8106.7935   3073.932     -2.637      0.008   -1.41e+04   -2081.331\n",
       "10867.0           1669.8447   2286.765      0.730      0.465   -2812.629    6152.318\n",
       "10906.0           4536.0583   1955.701      2.319      0.020     702.530    8369.587\n",
       "10950.0           5413.8389   3138.722      1.725      0.085    -738.624    1.16e+04\n",
       "10983.0          -2.185e+04   2912.705     -7.501      0.000   -2.76e+04   -1.61e+04\n",
       "1099.0            4218.0132   1946.131      2.167      0.030     403.245    8032.781\n",
       "10991.0           -617.7506   2955.061     -0.209      0.834   -6410.204    5174.703\n",
       "11012.0           2049.0538   2095.262      0.978      0.328   -2058.039    6156.146\n",
       "11038.0          -3868.0472   2274.118     -1.701      0.089   -8325.729     589.634\n",
       "1104.0            4931.2580   2128.145      2.317      0.021     759.709    9102.807\n",
       "11060.0           6165.2247   2062.464      2.989      0.003    2122.423    1.02e+04\n",
       "11094.0           4275.1968   1935.192      2.209      0.027     481.871    8068.522\n",
       "11096.0           1557.1493   2024.883      0.769      0.442   -2411.987    5526.286\n",
       "11113.0           4720.4704   2154.915      2.191      0.029     496.447    8944.494\n",
       "1115.0            2488.6937   1886.884      1.319      0.187   -1209.941    6187.328\n",
       "11161.0           -200.9822   1889.135     -0.106      0.915   -3904.028    3502.064\n",
       "11225.0           6457.2843   2198.200      2.938      0.003    2148.416    1.08e+04\n",
       "11228.0           6006.4959   1987.428      3.022      0.003    2110.778    9902.214\n",
       "11236.0           -789.7831   3964.684     -0.199      0.842   -8561.281    6981.715\n",
       "11288.0          -1.267e+04   3411.232     -3.715      0.000   -1.94e+04   -5987.105\n",
       "11312.0          -1.206e+04   2531.389     -4.764      0.000    -1.7e+04   -7098.089\n",
       "11361.0           4171.0192   2257.420      1.848      0.065    -253.932    8595.970\n",
       "11399.0          -5172.1710   2042.636     -2.532      0.011   -9176.107   -1168.235\n",
       "114303.0         -2.789e+04   6149.417     -4.535      0.000   -3.99e+04   -1.58e+04\n",
       "11456.0          -1181.1886   2495.127     -0.473      0.636   -6072.089    3709.712\n",
       "11465.0            545.6900   1922.062      0.284      0.776   -3221.899    4313.279\n",
       "11502.0           5026.9146   2292.070      2.193      0.028     534.044    9519.785\n",
       "11506.0          -1477.5156   2636.836     -0.560      0.575   -6646.191    3691.160\n",
       "11537.0           5191.7781   1975.590      2.628      0.009    1319.264    9064.292\n",
       "11566.0           7203.5936   2339.239      3.079      0.002    2618.262    1.18e+04\n",
       "11573.0           3733.9797   2049.829      1.822      0.069    -284.056    7752.016\n",
       "11580.0          -1439.5696   3281.929     -0.439      0.661   -7872.744    4993.605\n",
       "11600.0           5773.2736   2176.516      2.653      0.008    1506.910       1e+04\n",
       "11609.0           9111.8670   2099.723      4.340      0.000    4996.032    1.32e+04\n",
       "1161.0           -8301.6528   2839.135     -2.924      0.003   -1.39e+04   -2736.435\n",
       "11636.0          -1.189e+04   2108.620     -5.641      0.000    -1.6e+04   -7760.674\n",
       "11670.0           6682.8046   2394.474      2.791      0.005    1989.204    1.14e+04\n",
       "11678.0          -1.279e+04   2592.406     -4.934      0.000   -1.79e+04   -7710.066\n",
       "11682.0           3919.2168   2498.677      1.569      0.117    -978.641    8817.074\n",
       "11694.0           5747.8300   2143.449      2.682      0.007    1546.283    9949.377\n",
       "11720.0          -8331.5343   3651.748     -2.282      0.023   -1.55e+04   -1173.449\n",
       "11721.0          -2.112e+04   4023.967     -5.248      0.000    -2.9e+04   -1.32e+04\n",
       "11722.0           3226.7944   2284.810      1.412      0.158   -1251.846    7705.435\n",
       "11793.0           2422.7192   6280.572      0.386      0.700   -9888.336    1.47e+04\n",
       "11797.0           6890.8995   2605.114      2.645      0.008    1784.406     1.2e+04\n",
       "11914.0           5029.9389   2786.153      1.805      0.071    -431.424    1.05e+04\n",
       "1209.0            1708.8772   2085.140      0.820      0.412   -2378.375    5796.129\n",
       "12136.0          -2.297e+04   3534.423     -6.499      0.000   -2.99e+04    -1.6e+04\n",
       "12141.0            5.75e+04   2527.854     22.746      0.000    5.25e+04    6.25e+04\n",
       "12181.0            808.8317   3469.279      0.233      0.816   -5991.583    7609.246\n",
       "12215.0          -1.141e+04   2724.193     -4.188      0.000   -1.67e+04   -6069.870\n",
       "12216.0          -9001.0664   2642.143     -3.407      0.001   -1.42e+04   -3821.988\n",
       "12256.0          -5606.7171   2305.951     -2.431      0.015   -1.01e+04   -1086.636\n",
       "12262.0           4379.7449   2537.612      1.726      0.084    -594.434    9353.924\n",
       "12389.0           4077.6444   2924.043      1.395      0.163   -1654.009    9809.297\n",
       "1239.0             326.7095   1863.890      0.175      0.861   -3326.852    3980.271\n",
       "12390.0           2253.3827   2590.929      0.870      0.384   -2825.307    7332.072\n",
       "12397.0            -22.7316   4710.894     -0.005      0.996   -9256.935    9211.472\n",
       "1243.0            1423.9623   2054.826      0.693      0.488   -2603.868    5451.793\n",
       "12548.0           4518.8382   2505.697      1.803      0.071    -392.781    9430.457\n",
       "12570.0           4122.4380   2402.266      1.716      0.086    -586.437    8831.313\n",
       "12581.0           1494.0695   2664.284      0.561      0.575   -3728.408    6716.547\n",
       "12592.0           1593.8538   2255.282      0.707      0.480   -2826.906    6014.613\n",
       "12604.0            614.0467   4796.181      0.128      0.898   -8787.335       1e+04\n",
       "12656.0           6568.4522   2642.362      2.486      0.013    1388.946    1.17e+04\n",
       "12679.0          -2.237e+04   3483.810     -6.420      0.000   -2.92e+04   -1.55e+04\n",
       "1278.0            6170.5617   2354.740      2.620      0.009    1554.845    1.08e+04\n",
       "12788.0          -1.963e+04   5046.923     -3.890      0.000   -2.95e+04   -9740.117\n",
       "1283.0            5074.0949   2453.744      2.068      0.039     264.314    9883.876\n",
       "1297.0            4313.6075   1939.878      2.224      0.026     511.096    8116.119\n",
       "12992.0           6303.9339   2463.337      2.559      0.011    1475.348    1.11e+04\n",
       "13135.0           -114.8493   2351.336     -0.049      0.961   -4723.892    4494.194\n",
       "1327.0           -9291.4165   2954.040     -3.145      0.002   -1.51e+04   -3500.965\n",
       "13282.0          -3949.8216   4698.787     -0.841      0.401   -1.32e+04    5260.649\n",
       "1334.0           -1.257e+04   4428.531     -2.838      0.005   -2.12e+04   -3888.374\n",
       "13351.0           -868.4950   3352.923     -0.259      0.796   -7440.829    5703.839\n",
       "13365.0          -2.809e+04   4496.713     -6.248      0.000   -3.69e+04   -1.93e+04\n",
       "13369.0           2055.5421   2915.430      0.705      0.481   -3659.228    7770.312\n",
       "13406.0           5280.9784   2332.553      2.264      0.024     708.753    9853.203\n",
       "13407.0          -5374.5504   2343.945     -2.293      0.022   -9969.106    -779.995\n",
       "13417.0           6302.2748   2678.796      2.353      0.019    1051.351    1.16e+04\n",
       "13525.0          -9069.5391   3742.808     -2.423      0.015   -1.64e+04   -1732.960\n",
       "13554.0           7043.7827   2741.083      2.570      0.010    1670.765    1.24e+04\n",
       "1359.0             -1.8e+04   4300.929     -4.185      0.000   -2.64e+04   -9568.791\n",
       "13623.0           1884.4479   2395.362      0.787      0.431   -2810.893    6579.789\n",
       "1372.0           -8586.9305   2750.602     -3.122      0.002    -1.4e+04   -3195.254\n",
       "1380.0           -6198.7549   2027.587     -3.057      0.002   -1.02e+04   -2224.317\n",
       "13923.0           4327.8070   2618.221      1.653      0.098    -804.379    9459.993\n",
       "13932.0           6758.0994   3486.673      1.938      0.053     -76.410    1.36e+04\n",
       "13941.0          -1.273e+04   3053.517     -4.169      0.000   -1.87e+04   -6745.062\n",
       "1397.0            2184.6376   2356.026      0.927      0.354   -2433.599    6802.874\n",
       "14064.0           1481.5536   2474.573      0.599      0.549   -3369.057    6332.165\n",
       "14084.0           3460.5188   2779.609      1.245      0.213   -1988.018    8909.055\n",
       "14324.0          -1.022e+04   2902.442     -3.522      0.000   -1.59e+04   -4533.698\n",
       "14462.0           1784.4463   3174.752      0.562      0.574   -4438.641    8007.533\n",
       "1447.0            5170.3233   4124.969      1.253      0.210   -2915.361    1.33e+04\n",
       "14593.0           5761.9653   2680.234      2.150      0.032     508.223     1.1e+04\n",
       "14622.0           4023.6148   8181.170      0.492      0.623    -1.2e+04    2.01e+04\n",
       "1465.0            3253.6294   2827.174      1.151      0.250   -2288.143    8795.402\n",
       "1468.0            6773.6570   2589.057      2.616      0.009    1698.637    1.18e+04\n",
       "14897.0           2946.6509   4736.264      0.622      0.534   -6337.283    1.22e+04\n",
       "14954.0           4472.7867   2590.021      1.727      0.084    -604.121    9549.695\n",
       "1496.0            6599.9720   2091.312      3.156      0.002    2500.622    1.07e+04\n",
       "15267.0           3568.1880   2481.632      1.438      0.151   -1296.259    8432.635\n",
       "15354.0          -2104.0140   2656.087     -0.792      0.428   -7310.425    3102.397\n",
       "1542.0            3367.1680   1934.110      1.741      0.082    -424.037    7158.373\n",
       "15459.0           -277.1102   3475.397     -0.080      0.936   -7089.517    6535.297\n",
       "1554.0            6356.9665   2083.527      3.051      0.002    2272.876    1.04e+04\n",
       "15708.0          -3.007e+04   4710.742     -6.384      0.000   -3.93e+04   -2.08e+04\n",
       "15711.0           2482.5945   2540.937      0.977      0.329   -2498.101    7463.290\n",
       "15761.0           5239.0419   2946.614      1.778      0.075    -536.854     1.1e+04\n",
       "1581.0           -9946.6783   4696.052     -2.118      0.034   -1.92e+04    -741.569\n",
       "1593.0            2771.8015   2169.241      1.278      0.201   -1480.302    7023.905\n",
       "1602.0            8998.4516   2219.156      4.055      0.000    4648.505    1.33e+04\n",
       "1613.0            5299.0984   2089.204      2.536      0.011    1203.881    9394.315\n",
       "16188.0          -1213.8922   2753.992     -0.441      0.659   -6612.214    4184.430\n",
       "1632.0           -7522.0773   2191.983     -3.432      0.001   -1.18e+04   -3225.395\n",
       "1633.0            2211.8725   1905.594      1.161      0.246   -1523.437    5947.182\n",
       "1635.0           -2.008e+04   4095.903     -4.902      0.000   -2.81e+04   -1.21e+04\n",
       "16401.0          -1.125e+04   3803.823     -2.958      0.003   -1.87e+04   -3795.056\n",
       "16437.0          -6466.0225   3287.215     -1.967      0.049   -1.29e+04     -22.487\n",
       "1651.0           -2151.8718   1896.807     -1.134      0.257   -5869.957    1566.213\n",
       "1655.0            6065.6852   2012.244      3.014      0.003    2121.323       1e+04\n",
       "1663.0            7504.9048   2023.292      3.709      0.000    3538.887    1.15e+04\n",
       "16710.0          -2286.6678   2709.468     -0.844      0.399   -7597.715    3024.379\n",
       "16729.0          -3762.6345   2609.260     -1.442      0.149   -8877.255    1351.986\n",
       "1690.0            -2.08e+04   2791.669     -7.450      0.000   -2.63e+04   -1.53e+04\n",
       "1703.0            2346.6634   2095.044      1.120      0.263   -1760.002    6453.329\n",
       "17202.0           3189.7453   2706.630      1.178      0.239   -2115.739    8495.230\n",
       "1722.0            3336.7948   1989.516      1.677      0.094    -563.016    7236.606\n",
       "1728.0            6131.9418   2062.060      2.974      0.003    2089.931    1.02e+04\n",
       "1743.0            5846.0477   3270.715      1.787      0.074    -565.144    1.23e+04\n",
       "1754.0            4828.8974   2054.183      2.351      0.019     802.327    8855.467\n",
       "1762.0            -969.7047   3354.419     -0.289      0.773   -7544.972    5605.563\n",
       "1773.0            5652.1795   2378.876      2.376      0.018     989.152    1.03e+04\n",
       "1786.0           -1.672e+04   3119.593     -5.359      0.000   -2.28e+04   -1.06e+04\n",
       "18100.0           1594.9441   2630.445      0.606      0.544   -3561.204    6751.092\n",
       "1820.0               2.9836   2130.290      0.001      0.999   -4172.770    4178.737\n",
       "1848.0           -1.004e+04   2580.542     -3.889      0.000   -1.51e+04   -4978.477\n",
       "18654.0           6006.9608   3720.280      1.615      0.106   -1285.460    1.33e+04\n",
       "1875.0           -2027.7996   4116.389     -0.493      0.622   -1.01e+04    6041.068\n",
       "1884.0            4823.5273   2285.510      2.110      0.035     343.515    9303.539\n",
       "1913.0            -289.6444   2851.514     -0.102      0.919   -5879.126    5299.837\n",
       "1919.0            4567.3493   2086.784      2.189      0.029     476.876    8657.823\n",
       "1920.0            -953.3743   2167.612     -0.440      0.660   -5202.285    3295.537\n",
       "1968.0            3165.5363   2226.409      1.422      0.155   -1198.627    7529.700\n",
       "1976.0            6081.4701   2017.956      3.014      0.003    2125.912       1e+04\n",
       "1981.0            4850.5498   2023.911      2.397      0.017     883.319    8817.781\n",
       "1988.0           -7544.9919   4408.703     -1.711      0.087   -1.62e+04    1096.862\n",
       "1992.0            3644.4394   2385.214      1.528      0.127   -1031.010    8319.889\n",
       "2008.0            3380.6606   1992.552      1.697      0.090    -525.101    7286.422\n",
       "2033.0            5182.8595   2548.647      2.034      0.042     187.051    1.02e+04\n",
       "2044.0            1445.5060   1882.500      0.768      0.443   -2244.534    5135.546\n",
       "2049.0            3380.4050   1967.070      1.718      0.086    -475.408    7236.218\n",
       "2061.0            7278.8502   2303.784      3.160      0.002    2763.016    1.18e+04\n",
       "20779.0           2.854e+04   2746.675     10.392      0.000    2.32e+04    3.39e+04\n",
       "2085.0           -1.883e+04   3413.523     -5.517      0.000   -2.55e+04   -1.21e+04\n",
       "2086.0            2149.7707   1932.564      1.112      0.266   -1638.405    5937.946\n",
       "2111.0            2163.5491   1893.292      1.143      0.253   -1547.645    5874.743\n",
       "21204.0          -3625.3580   3523.717     -1.029      0.304   -1.05e+04    3281.763\n",
       "21238.0           4344.0112   2909.842      1.493      0.136   -1359.805       1e+04\n",
       "2124.0            3467.0513   2301.773      1.506      0.132   -1044.839    7978.942\n",
       "2146.0            8200.4701   3715.945      2.207      0.027     916.547    1.55e+04\n",
       "21496.0          -3.085e+04   4799.510     -6.429      0.000   -4.03e+04   -2.14e+04\n",
       "2154.0            4205.3975   1940.208      2.167      0.030     402.239    8008.556\n",
       "2176.0            3.625e+04   2605.300     13.916      0.000    3.11e+04    4.14e+04\n",
       "2188.0            6803.4859   2312.364      2.942      0.003    2270.835    1.13e+04\n",
       "2189.0           -2085.8933   2445.950     -0.853      0.394   -6880.397    2708.611\n",
       "2220.0            4192.9522   2000.417      2.096      0.036     271.774    8114.130\n",
       "22205.0           7163.5586   2911.451      2.460      0.014    1456.589    1.29e+04\n",
       "2226.0             333.1924   5725.502      0.058      0.954   -1.09e+04    1.16e+04\n",
       "2230.0            3004.8609   2510.486      1.197      0.231   -1916.146    7925.868\n",
       "22325.0          -1.276e+04   3489.731     -3.655      0.000   -1.96e+04   -5915.945\n",
       "2255.0            3816.8344   1983.445      1.924      0.054     -71.076    7704.745\n",
       "22619.0           5331.2899   2649.322      2.012      0.044     138.139    1.05e+04\n",
       "2267.0           -1.085e+04   2375.808     -4.567      0.000   -1.55e+04   -6192.365\n",
       "22815.0          -2744.9098   2591.812     -1.059      0.290   -7825.330    2335.510\n",
       "2285.0           -1.624e+04   2387.293     -6.801      0.000   -2.09e+04   -1.16e+04\n",
       "2290.0            -256.5923   1984.286     -0.129      0.897   -4146.152    3632.967\n",
       "2295.0            6491.9746   3565.988      1.821      0.069    -498.005    1.35e+04\n",
       "2316.0           -2443.0783   2378.840     -1.027      0.304   -7106.035    2219.878\n",
       "23220.0           3256.5419   2781.549      1.171      0.242   -2195.796    8708.880\n",
       "23224.0          -1.728e+04   4888.147     -3.535      0.000   -2.69e+04   -7698.600\n",
       "2343.0           -1.305e+04   4762.126     -2.741      0.006   -2.24e+04   -3720.089\n",
       "2352.0            2875.2759   2799.555      1.027      0.304   -2612.359    8362.910\n",
       "23700.0          -1.662e+04   4874.835     -3.410      0.001   -2.62e+04   -7069.000\n",
       "2390.0            6583.9088   2113.016      3.116      0.002    2442.015    1.07e+04\n",
       "2393.0             -47.8831   3109.113     -0.015      0.988   -6142.307    6046.541\n",
       "2403.0            4787.7759   3460.271      1.384      0.166   -1994.980    1.16e+04\n",
       "2435.0            8463.4987   2321.080      3.646      0.000    3913.763     1.3e+04\n",
       "2444.0            1530.5307   2561.779      0.597      0.550   -3491.019    6552.081\n",
       "2448.0            2254.3657   1877.260      1.201      0.230   -1425.402    5934.134\n",
       "2469.0            5903.1174   3746.308      1.576      0.115   -1440.323    1.32e+04\n",
       "24720.0           4603.8491   2983.561      1.543      0.123   -1244.469    1.05e+04\n",
       "24800.0          -1.921e+04   4150.154     -4.628      0.000   -2.73e+04   -1.11e+04\n",
       "2482.0            6797.2824   2144.613      3.169      0.002    2593.454     1.1e+04\n",
       "24969.0           5782.6663   3238.460      1.786      0.074    -565.300    1.21e+04\n",
       "2498.0           -7972.3078   2257.440     -3.532      0.000   -1.24e+04   -3547.317\n",
       "2504.0           -8315.0626   4485.169     -1.854      0.064   -1.71e+04     476.678\n",
       "2508.0            5591.6331   2205.029      2.536      0.011    1269.377    9913.889\n",
       "25124.0           5238.7752   2942.493      1.780      0.075    -529.043     1.1e+04\n",
       "2518.0            5784.5599   2014.599      2.871      0.004    1835.581    9733.539\n",
       "25224.0           4986.0492   8160.081      0.611      0.541    -1.1e+04     2.1e+04\n",
       "25279.0           3037.1248   2916.279      1.041      0.298   -2679.309    8753.559\n",
       "2537.0           -1.692e+04   3354.084     -5.045      0.000   -2.35e+04   -1.03e+04\n",
       "2538.0            5632.8772   3163.316      1.781      0.075    -567.794    1.18e+04\n",
       "25389.0           5075.2008   4745.757      1.069      0.285   -4227.339    1.44e+04\n",
       "2547.0           -6418.4704   2389.841     -2.686      0.007   -1.11e+04   -1733.949\n",
       "2553.0            4466.8948   2058.115      2.170      0.030     432.619    8501.171\n",
       "2574.0           -1014.9357   2596.501     -0.391      0.696   -6104.548    4074.676\n",
       "25747.0           4790.8815   3119.773      1.536      0.125   -1324.437    1.09e+04\n",
       "2577.0            2524.7432   2073.188      1.218      0.223   -1539.079    6588.565\n",
       "2593.0            3346.2565   2078.323      1.610      0.107    -727.631    7420.144\n",
       "2596.0            -446.1903   2660.117     -0.168      0.867   -5660.500    4768.119\n",
       "2663.0            9004.9554   1991.962      4.521      0.000    5100.349    1.29e+04\n",
       "2771.0             -66.6719   2575.189     -0.026      0.979   -5114.508    4981.164\n",
       "2787.0            4881.9340   1979.048      2.467      0.014    1002.643    8761.225\n",
       "2797.0           -1.552e+04   2763.530     -5.615      0.000   -2.09e+04   -1.01e+04\n",
       "2802.0            6137.3783   2114.748      2.902      0.004    1992.089    1.03e+04\n",
       "2817.0           -5000.6402   4011.657     -1.247      0.213   -1.29e+04    2862.932\n",
       "28678.0          -2.378e+04   4087.266     -5.818      0.000   -3.18e+04   -1.58e+04\n",
       "28701.0           3013.5790   1921.809      1.568      0.117    -753.514    6780.672\n",
       "28742.0          -1.926e+04   4080.807     -4.719      0.000   -2.73e+04   -1.13e+04\n",
       "2888.0            4118.3943   2300.840      1.790      0.073    -391.668    8628.457\n",
       "2897.0            5739.4685   2778.858      2.065      0.039     292.405    1.12e+04\n",
       "2917.0            -675.3118   1962.344     -0.344      0.731   -4521.861    3171.238\n",
       "29392.0          -1.405e+04   4169.759     -3.369      0.001   -2.22e+04   -5875.339\n",
       "2950.0           -3.247e+04   6182.126     -5.252      0.000   -4.46e+04   -2.04e+04\n",
       "2951.0            6577.8719   2613.091      2.517      0.012    1455.742    1.17e+04\n",
       "2953.0            4119.1629   1920.034      2.145      0.032     355.549    7882.777\n",
       "2960.0            2169.0739   3042.070      0.713      0.476   -3793.933    8132.081\n",
       "2975.0           -5327.7936   1991.703     -2.675      0.007   -9231.892   -1423.695\n",
       "2982.0            3763.8889   2022.753      1.861      0.063    -201.072    7728.850\n",
       "2991.0           -1.351e+04   2696.623     -5.011      0.000   -1.88e+04   -8225.948\n",
       "3011.0           -1.257e+04   3244.684     -3.873      0.000   -1.89e+04   -6207.562\n",
       "3015.0            6493.4216   2213.043      2.934      0.003    2155.458    1.08e+04\n",
       "3026.0            2850.5423   1969.259      1.448      0.148   -1009.561    6710.646\n",
       "3031.0            -2.59e+04   4494.210     -5.763      0.000   -3.47e+04   -1.71e+04\n",
       "3062.0            6092.0871   2240.951      2.719      0.007    1699.418    1.05e+04\n",
       "3093.0           -6736.3744   2825.111     -2.384      0.017   -1.23e+04   -1198.646\n",
       "3107.0            4232.4660   3722.982      1.137      0.256   -3065.252    1.15e+04\n",
       "3121.0            6528.7474   1957.676      3.335      0.001    2691.349    1.04e+04\n",
       "3126.0            4801.7854   2046.379      2.346      0.019     790.512    8813.059\n",
       "3144.0            5.303e+04   2028.324     26.145      0.000    4.91e+04     5.7e+04\n",
       "3156.0            3484.8827   2531.197      1.377      0.169   -1476.720    8446.486\n",
       "3157.0            3974.6138   1923.791      2.066      0.039     203.636    7745.592\n",
       "3170.0            3898.8978   1925.219      2.025      0.043     125.121    7672.675\n",
       "3178.0           -5879.0805   3091.558     -1.902      0.057   -1.19e+04     180.932\n",
       "3206.0             732.9240   2267.459      0.323      0.747   -3711.706    5177.554\n",
       "3229.0             425.4199   2389.753      0.178      0.859   -4258.928    5109.768\n",
       "3235.0            4587.2458   2290.006      2.003      0.045      98.420    9076.072\n",
       "3246.0            4857.3709   2094.704      2.319      0.020     751.373    8963.368\n",
       "3248.0            4753.1827   1996.424      2.381      0.017     839.832    8666.534\n",
       "3282.0           -2.486e+04   3038.268     -8.182      0.000   -3.08e+04   -1.89e+04\n",
       "3362.0           -4141.7091   2287.741     -1.810      0.070   -8626.095     342.676\n",
       "3372.0            4295.6205   2486.387      1.728      0.084    -578.148    9169.389\n",
       "3422.0            3272.0359   1998.614      1.637      0.102    -645.609    7189.680\n",
       "3497.0           -3870.0977   2688.876     -1.439      0.150   -9140.780    1400.585\n",
       "3502.0           -3650.5442   1950.871     -1.871      0.061   -7474.605     173.516\n",
       "3504.0            2538.0308   2851.325      0.890      0.373   -3051.081    8127.143\n",
       "3505.0            2211.3835   2418.365      0.914      0.361   -2529.050    6951.817\n",
       "3532.0            4747.7617   1905.472      2.492      0.013    1012.692    8482.831\n",
       "3574.0            6682.3186   4140.497      1.614      0.107   -1433.804    1.48e+04\n",
       "3580.0            -510.7928   1880.479     -0.272      0.786   -4196.872    3175.286\n",
       "3612.0            7014.3465   2367.323      2.963      0.003    2373.967    1.17e+04\n",
       "3619.0            4011.8914   2027.455      1.979      0.048      37.713    7986.070\n",
       "3622.0            6401.5443   2167.956      2.953      0.003    2151.959    1.07e+04\n",
       "3639.0           -1.152e+04   2403.097     -4.793      0.000   -1.62e+04   -6806.727\n",
       "3650.0           -4813.4761   3141.449     -1.532      0.125    -1.1e+04    1344.332\n",
       "3662.0            1825.6041   3032.142      0.602      0.547   -4117.943    7769.151\n",
       "3734.0           -1.609e+04   2568.390     -6.266      0.000   -2.11e+04   -1.11e+04\n",
       "3735.0            1494.7575   3506.972      0.426      0.670   -5379.540    8369.055\n",
       "3761.0           -2039.2412   2351.367     -0.867      0.386   -6648.345    2569.863\n",
       "3779.0           -1.899e+04   4196.664     -4.525      0.000   -2.72e+04   -1.08e+04\n",
       "3781.0           -9243.5870   3349.344     -2.760      0.006   -1.58e+04   -2678.267\n",
       "3782.0            -1.52e+04   2706.616     -5.615      0.000   -2.05e+04   -9893.211\n",
       "3786.0            2531.3596   1994.304      1.269      0.204   -1377.837    6440.556\n",
       "3796.0            -1.92e+04   4204.230     -4.566      0.000   -2.74e+04    -1.1e+04\n",
       "3821.0            5668.5792   2139.984      2.649      0.008    1473.823    9863.335\n",
       "3835.0             153.2474   2023.175      0.076      0.940   -3812.541    4119.036\n",
       "3839.0            -721.8037   3477.675     -0.208      0.836   -7538.676    6095.068\n",
       "3840.0           -8269.2236   2791.005     -2.963      0.003   -1.37e+04   -2798.350\n",
       "3895.0            5395.6303   2000.925      2.697      0.007    1473.455    9317.805\n",
       "3908.0           -2631.4283   3639.263     -0.723      0.470   -9765.041    4502.184\n",
       "3911.0           -2662.5419   2775.924     -0.959      0.338   -8103.854    2778.770\n",
       "3917.0            5387.0743   2156.496      2.498      0.013    1159.952    9614.197\n",
       "3946.0            6790.9022   2200.562      3.086      0.002    2477.404    1.11e+04\n",
       "3971.0            4338.0655   2029.118      2.138      0.033     360.627    8315.504\n",
       "3980.0            1.259e+04   1934.729      6.506      0.000    8795.154    1.64e+04\n",
       "4034.0             531.9537   2366.623      0.225      0.822   -4107.055    5170.962\n",
       "4036.0            6471.5216   2057.068      3.146      0.002    2439.297    1.05e+04\n",
       "4040.0           -4973.1511   2394.112     -2.077      0.038   -9666.044    -280.258\n",
       "4058.0            3996.2886   1893.926      2.110      0.035     283.852    7708.725\n",
       "4060.0           -9530.9087   3065.244     -3.109      0.002   -1.55e+04   -3522.477\n",
       "4062.0            8945.6153   2332.155      3.836      0.000    4374.169    1.35e+04\n",
       "4077.0            1739.0721   3361.765      0.517      0.605   -4850.595    8328.739\n",
       "4087.0           -1.712e+04   3453.701     -4.957      0.000   -2.39e+04   -1.04e+04\n",
       "4091.0            2174.3419   2469.131      0.881      0.379   -2665.601    7014.285\n",
       "4127.0           -4639.1997   1989.707     -2.332      0.020   -8539.386    -739.014\n",
       "4138.0            6315.4956   2737.639      2.307      0.021     949.229    1.17e+04\n",
       "4162.0            3541.1783   2469.489      1.434      0.152   -1299.466    8381.823\n",
       "4186.0            6613.9864   2116.065      3.126      0.002    2466.117    1.08e+04\n",
       "4194.0            3058.6732   2342.642      1.306      0.192   -1533.328    7650.674\n",
       "4199.0           -1.326e+04   3489.311     -3.801      0.000   -2.01e+04   -6422.966\n",
       "4213.0            4772.4015   1987.898      2.401      0.016     875.762    8669.041\n",
       "4222.0           -1.484e+04   2878.008     -5.155      0.000   -2.05e+04   -9196.052\n",
       "4223.0            4468.4025   1948.737      2.293      0.022     648.525    8288.280\n",
       "4251.0            6080.0881   2067.306      2.941      0.003    2027.795    1.01e+04\n",
       "4265.0            2144.9884   2178.035      0.985      0.325   -2124.354    6414.331\n",
       "4274.0            2898.8369   2273.900      1.275      0.202   -1558.419    7356.092\n",
       "4321.0            3703.1068   2821.710      1.312      0.189   -1827.954    9234.168\n",
       "4335.0            2379.0898   3338.800      0.713      0.476   -4165.562    8923.741\n",
       "4340.0            -223.6093   1947.219     -0.115      0.909   -4040.510    3593.292\n",
       "4371.0            2486.1288   2230.203      1.115      0.265   -1885.473    6857.730\n",
       "4415.0            4544.2085   2123.378      2.140      0.032     382.005    8706.413\n",
       "4450.0            1884.3765   2055.793      0.917      0.359   -2145.349    5914.102\n",
       "4476.0           -1.053e+04   2518.734     -4.182      0.000   -1.55e+04   -5596.305\n",
       "4510.0           -1667.6496   2866.837     -0.582      0.561   -7287.169    3951.870\n",
       "4520.0            4027.0706   2320.036      1.736      0.083    -520.619    8574.760\n",
       "4551.0            2602.9928   8093.738      0.322      0.748   -1.33e+04    1.85e+04\n",
       "4568.0            4908.5522   2122.557      2.313      0.021     747.957    9069.147\n",
       "4579.0            6832.7583   2357.491      2.898      0.004    2211.650    1.15e+04\n",
       "4585.0            6237.8526   2103.798      2.965      0.003    2114.028    1.04e+04\n",
       "4595.0             755.1966   1861.033      0.406      0.685   -2892.765    4403.158\n",
       "4600.0           -1.258e+04   2882.596     -4.363      0.000   -1.82e+04   -6927.249\n",
       "4607.0            6017.5190   2080.271      2.893      0.004    1939.812    1.01e+04\n",
       "4608.0           -1.093e+04   2885.744     -3.787      0.000   -1.66e+04   -5273.139\n",
       "4622.0            -393.8328   2850.116     -0.138      0.890   -5980.575    5192.909\n",
       "4623.0            4209.3349   2064.664      2.039      0.041     162.221    8256.449\n",
       "4768.0            4446.4872   1984.877      2.240      0.025     555.769    8337.205\n",
       "4771.0            6541.1562   2149.132      3.044      0.002    2328.470    1.08e+04\n",
       "4800.0            4660.6183   2131.455      2.187      0.029     482.581    8838.656\n",
       "4802.0            6035.9091   2020.560      2.987      0.003    2075.247    9996.571\n",
       "4807.0            5398.4787   2020.839      2.671      0.008    1437.269    9359.689\n",
       "4839.0           -1.066e+05   4610.547    -23.127      0.000   -1.16e+05   -9.76e+04\n",
       "4843.0           -1.791e+04   4421.977     -4.050      0.000   -2.66e+04   -9242.766\n",
       "4881.0            2395.9800   2081.622      1.151      0.250   -1684.376    6476.336\n",
       "4900.0            2114.6443   2024.100      1.045      0.296   -1852.958    6082.246\n",
       "4926.0            2443.3153   2216.329      1.102      0.270   -1901.091    6787.721\n",
       "4941.0            3719.7663   1971.092      1.887      0.059    -143.930    7583.462\n",
       "4961.0           -1.363e+04   3589.438     -3.798      0.000   -2.07e+04   -6597.510\n",
       "4988.0            1.173e+04   2160.613      5.431      0.000    7498.977     1.6e+04\n",
       "4993.0            7348.2283   2349.934      3.127      0.002    2741.933     1.2e+04\n",
       "5018.0           -2178.1172   3711.933     -0.587      0.557   -9454.177    5097.942\n",
       "5020.0           -1.562e+04   4033.921     -3.873      0.000   -2.35e+04   -7714.359\n",
       "5027.0           -1942.9461   1936.749     -1.003      0.316   -5739.323    1853.431\n",
       "5032.0            4989.8999   1968.604      2.535      0.011    1131.081    8848.719\n",
       "5043.0            -661.5468   1861.195     -0.355      0.722   -4309.826    2986.732\n",
       "5046.0           -1.112e+04   3197.692     -3.476      0.001   -1.74e+04   -4848.659\n",
       "5047.0            3.589e+04   4751.439      7.553      0.000    2.66e+04    4.52e+04\n",
       "5065.0            5337.3935   2385.615      2.237      0.025     661.157       1e+04\n",
       "5071.0            6205.3078   2402.340      2.583      0.010    1496.287    1.09e+04\n",
       "5073.0           -1.592e+05   6404.441    -24.851      0.000   -1.72e+05   -1.47e+05\n",
       "5087.0           -1022.6138   1857.490     -0.551      0.582   -4663.630    2618.403\n",
       "5109.0            6506.3275   2168.818      3.000      0.003    2255.052    1.08e+04\n",
       "5116.0           -1.375e+04   3103.757     -4.429      0.000   -1.98e+04   -7663.903\n",
       "5122.0           -1841.4695   1957.797     -0.941      0.347   -5679.106    1996.167\n",
       "5134.0           -3195.4551   2290.240     -1.395      0.163   -7684.739    1293.829\n",
       "5142.0            1196.8891   2704.292      0.443      0.658   -4104.012    6497.790\n",
       "5165.0            -601.4087   2853.378     -0.211      0.833   -6194.544    4991.727\n",
       "5169.0            1.483e+04   1995.122      7.432      0.000    1.09e+04    1.87e+04\n",
       "5174.0             246.1982   2276.688      0.108      0.914   -4216.522    4708.918\n",
       "5179.0            5138.2287   2045.922      2.511      0.012    1127.853    9148.605\n",
       "5181.0            6546.0573   2167.329      3.020      0.003    2297.701    1.08e+04\n",
       "5187.0            5977.9824   2517.907      2.374      0.018    1042.431    1.09e+04\n",
       "5229.0           -5047.5424   2516.110     -2.006      0.045   -9979.573    -115.512\n",
       "5234.0           -5097.0336   2435.130     -2.093      0.036   -9870.328    -323.739\n",
       "5237.0            4462.3487   1960.595      2.276      0.023     619.229    8305.469\n",
       "5252.0            3669.1467   1959.630      1.872      0.061    -172.082    7510.375\n",
       "5254.0            4127.3979   2002.788      2.061      0.039     201.572    8053.224\n",
       "5306.0            1736.3687   3072.112      0.565      0.572   -4285.527    7758.264\n",
       "5338.0            5385.4727   1968.103      2.736      0.006    1527.636    9243.309\n",
       "5377.0            6697.9531   2202.006      3.042      0.002    2381.624     1.1e+04\n",
       "5439.0            3987.4013   2024.473      1.970      0.049      19.069    7955.734\n",
       "5456.0            7257.8902   2341.843      3.099      0.002    2667.454    1.18e+04\n",
       "5464.0            3610.5273   2537.108      1.423      0.155   -1362.662    8583.717\n",
       "5476.0            6037.4731   2429.463      2.485      0.013    1275.287    1.08e+04\n",
       "5492.0           -2.453e+04   3757.022     -6.530      0.000   -3.19e+04   -1.72e+04\n",
       "5496.0            4161.9913   1937.190      2.148      0.032     364.749    7959.234\n",
       "5505.0            6187.1679   2129.255      2.906      0.004    2013.443    1.04e+04\n",
       "5518.0            4941.5364   2326.090      2.124      0.034     381.979    9501.094\n",
       "5520.0            1121.7689   1866.155      0.601      0.548   -2536.232    4779.770\n",
       "5545.0            5949.3888   2098.188      2.835      0.005    1836.561    1.01e+04\n",
       "5568.0            8101.5847   1978.173      4.095      0.000    4224.008     1.2e+04\n",
       "5569.0            6377.2591   2126.470      2.999      0.003    2208.993    1.05e+04\n",
       "5578.0            5302.4738   2003.961      2.646      0.008    1374.348    9230.599\n",
       "5581.0            4808.9439   1948.376      2.468      0.014     989.775    8628.113\n",
       "5589.0           -4771.1716   2836.259     -1.682      0.093   -1.03e+04     788.409\n",
       "5597.0            7113.0802   2651.787      2.682      0.007    1915.099    1.23e+04\n",
       "5606.0           -2.466e+04   2952.624     -8.351      0.000   -3.04e+04   -1.89e+04\n",
       "5639.0            7328.6885   2184.972      3.354      0.001    3045.748    1.16e+04\n",
       "5667.0            -201.0064   2813.432     -0.071      0.943   -5715.842    5313.830\n",
       "5690.0            6088.4431   2009.766      3.029      0.002    2148.938       1e+04\n",
       "5709.0            5048.1264   2044.923      2.469      0.014    1039.708    9056.545\n",
       "5726.0            4158.0942   2033.694      2.045      0.041     171.687    8144.501\n",
       "5764.0            2695.4324   2198.979      1.226      0.220   -1614.964    7005.828\n",
       "5772.0            4947.1529   1977.066      2.502      0.012    1071.746    8822.559\n",
       "5860.0            -2.73e+04   2902.364     -9.407      0.000    -3.3e+04   -2.16e+04\n",
       "5878.0            6021.2749   2074.299      2.903      0.004    1955.274    1.01e+04\n",
       "5903.0           -3027.9789   2157.060     -1.404      0.160   -7256.206    1200.248\n",
       "5905.0            3433.9024   2123.438      1.617      0.106    -728.419    7596.224\n",
       "5959.0            1057.0909   2289.670      0.462      0.644   -3431.075    5545.257\n",
       "6008.0            2.181e+04   2946.548      7.402      0.000     1.6e+04    2.76e+04\n",
       "6034.0            1717.6108   3401.702      0.505      0.614   -4950.340    8385.562\n",
       "6035.0           -6513.4996   2973.737     -2.190      0.029   -1.23e+04    -684.438\n",
       "6036.0           -8207.0573   2208.030     -3.717      0.000   -1.25e+04   -3878.919\n",
       "6039.0            4740.5056   1961.317      2.417      0.016     895.971    8585.040\n",
       "6044.0            6841.0167   2458.590      2.782      0.005    2021.736    1.17e+04\n",
       "6066.0           -1201.4683   4305.167     -0.279      0.780   -9640.374    7237.438\n",
       "6078.0            5028.0231   1915.403      2.625      0.009    1273.487    8782.560\n",
       "6081.0           -2.241e+04   3007.553     -7.451      0.000   -2.83e+04   -1.65e+04\n",
       "60893.0          -2.179e+04   5707.523     -3.817      0.000    -3.3e+04   -1.06e+04\n",
       "6097.0            5310.8121   2237.007      2.374      0.018     925.874    9695.750\n",
       "6102.0            4521.5138   2016.058      2.243      0.025     569.675    8473.353\n",
       "6104.0           -5446.4132   2123.959     -2.564      0.010   -9609.757   -1283.069\n",
       "6109.0           -8647.2793   2254.616     -3.835      0.000   -1.31e+04   -4227.826\n",
       "6127.0           -2273.4055   2419.539     -0.940      0.347   -7016.140    2469.329\n",
       "61552.0           -1.48e+04   3905.941     -3.790      0.000   -2.25e+04   -7146.206\n",
       "6158.0              23.5432   2042.855      0.012      0.991   -3980.822    4027.909\n",
       "6171.0            4551.4901   1945.254      2.340      0.019     738.441    8364.540\n",
       "61780.0           3838.6494   4125.994      0.930      0.352   -4249.045    1.19e+04\n",
       "6207.0            4384.7826   2028.038      2.162      0.031     409.462    8360.104\n",
       "6214.0            5771.6803   1991.297      2.898      0.004    1868.378    9674.983\n",
       "6216.0            6070.2664   2354.559      2.578      0.010    1454.905    1.07e+04\n",
       "62221.0           5879.7196   4132.114      1.423      0.155   -2219.970     1.4e+04\n",
       "6259.0           -5651.9887   3239.653     -1.745      0.081    -1.2e+04     698.317\n",
       "62599.0           7996.1466   5739.457      1.393      0.164   -3254.226    1.92e+04\n",
       "6266.0             460.1801   2933.545      0.157      0.875   -5290.099    6210.459\n",
       "6268.0            -322.9903   2588.159     -0.125      0.901   -5396.250    4750.269\n",
       "6288.0            3344.6810   2023.859      1.653      0.098    -622.448    7311.810\n",
       "6297.0            5443.1242   2099.984      2.592      0.010    1326.776    9559.473\n",
       "6307.0            -1.67e+04   3045.710     -5.484      0.000   -2.27e+04   -1.07e+04\n",
       "6313.0            5069.1679   3416.150      1.484      0.138   -1627.104    1.18e+04\n",
       "6314.0            5427.4416   2181.813      2.488      0.013    1150.694    9704.189\n",
       "6326.0           -3138.9060   2734.876     -1.148      0.251   -8499.757    2221.945\n",
       "6349.0            4156.5332   1929.335      2.154      0.031     374.689    7938.377\n",
       "6357.0            6145.1418   2241.745      2.741      0.006    1750.917    1.05e+04\n",
       "6375.0            1.096e+04   2043.515      5.361      0.000    6949.632     1.5e+04\n",
       "6376.0            6035.0514   2136.285      2.825      0.005    1847.548    1.02e+04\n",
       "6379.0            7991.0460   5764.851      1.386      0.166   -3309.103    1.93e+04\n",
       "6386.0            6133.8056   1999.923      3.067      0.002    2213.595    1.01e+04\n",
       "6403.0           -1207.7520   1989.092     -0.607      0.544   -5106.731    2691.227\n",
       "6410.0            7154.0765   2278.649      3.140      0.002    2687.514    1.16e+04\n",
       "6416.0           -4498.3883   2252.685     -1.997      0.046   -8914.057     -82.720\n",
       "6424.0            5450.4644   2064.605      2.640      0.008    1403.467    9497.462\n",
       "6433.0            5115.9446   2334.275      2.192      0.028     540.343    9691.546\n",
       "6435.0            4921.6992   2293.983      2.145      0.032     425.078    9418.321\n",
       "6492.0            1056.4009   2661.382      0.397      0.691   -4160.388    6273.190\n",
       "6497.0            -1.01e+04   3655.257     -2.762      0.006   -1.73e+04   -2931.514\n",
       "6500.0             991.3577   8288.834      0.120      0.905   -1.53e+04    1.72e+04\n",
       "6509.0            4465.2253   1955.208      2.284      0.022     632.664    8297.787\n",
       "6527.0            6716.1652   2490.058      2.697      0.007    1835.202    1.16e+04\n",
       "6528.0            2951.2741   2207.527      1.337      0.181   -1375.879    7278.427\n",
       "6531.0           -4679.5758   3071.259     -1.524      0.128   -1.07e+04    1340.646\n",
       "6532.0            -509.4835   1916.918     -0.266      0.790   -4266.989    3248.022\n",
       "6543.0            6419.3028   2107.324      3.046      0.002    2288.568    1.06e+04\n",
       "6548.0            5552.1721   2096.722      2.648      0.008    1442.218    9662.126\n",
       "6550.0            5597.9612   2211.098      2.532      0.011    1263.810    9932.112\n",
       "6552.0            5856.5128   2325.727      2.518      0.012    1297.667    1.04e+04\n",
       "6565.0             630.6711   3361.704      0.188      0.851   -5958.876    7220.219\n",
       "6571.0            5346.9369   1987.817      2.690      0.007    1450.457    9243.417\n",
       "6573.0            4818.2195   1944.367      2.478      0.013    1006.910    8629.530\n",
       "6641.0             179.2243   4100.843      0.044      0.965   -7859.168    8217.617\n",
       "6649.0            7111.9543   2183.314      3.257      0.001    2832.264    1.14e+04\n",
       "6730.0           -2536.2199   3238.822     -0.783      0.434   -8884.896    3812.456\n",
       "6731.0            1366.9114   2702.349      0.506      0.613   -3930.181    6664.004\n",
       "6742.0            4887.4493   4329.125      1.129      0.259   -3598.418    1.34e+04\n",
       "6745.0            6471.5926   2068.738      3.128      0.002    2416.492    1.05e+04\n",
       "6756.0            5320.6972   2246.481      2.368      0.018     917.188    9724.206\n",
       "6765.0           -1.713e+04   2756.595     -6.213      0.000   -2.25e+04   -1.17e+04\n",
       "6768.0            7876.9957   2429.684      3.242      0.001    3114.375    1.26e+04\n",
       "6774.0           -2.531e+04   3191.060     -7.930      0.000   -3.16e+04   -1.91e+04\n",
       "6797.0            6710.5132   2628.816      2.553      0.011    1557.559    1.19e+04\n",
       "6803.0            5808.6380   2162.285      2.686      0.007    1570.169       1e+04\n",
       "6821.0            5125.4429   1989.770      2.576      0.010    1225.135    9025.751\n",
       "6830.0            3448.3552   2117.951      1.628      0.104    -703.211    7599.922\n",
       "6845.0            2002.9582   2777.820      0.721      0.471   -3442.070    7447.987\n",
       "6848.0            3530.4877   2452.979      1.439      0.150   -1277.795    8338.770\n",
       "6873.0             957.8911   2818.602      0.340      0.734   -4567.078    6482.861\n",
       "6900.0            3170.6036   2344.626      1.352      0.176   -1425.287    7766.495\n",
       "6908.0            3031.8917   2217.822      1.367      0.172   -1315.439    7379.223\n",
       "6994.0            2257.5843   2155.260      1.047      0.295   -1967.114    6482.283\n",
       "7045.0           -8933.5649   3046.767     -2.932      0.003   -1.49e+04   -2961.351\n",
       "7065.0            8885.8741   2108.030      4.215      0.000    4753.755     1.3e+04\n",
       "7085.0            7510.1248   1992.266      3.770      0.000    3604.923    1.14e+04\n",
       "7107.0            4030.3376   2347.487      1.717      0.086    -571.160    8631.835\n",
       "7116.0            7345.8253   2307.329      3.184      0.001    2823.044    1.19e+04\n",
       "7117.0            7449.5209   3386.575      2.200      0.028     811.221    1.41e+04\n",
       "7121.0            3930.8037   1967.469      1.998      0.046      74.210    7787.398\n",
       "7127.0            2601.1446   2515.212      1.034      0.301   -2329.126    7531.415\n",
       "7139.0            5136.5066   1972.034      2.605      0.009    1270.963    9002.050\n",
       "7146.0            5854.8528   2036.846      2.874      0.004    1862.267    9847.439\n",
       "7163.0            8500.3657   2071.979      4.103      0.000    4438.912    1.26e+04\n",
       "7180.0            1457.2876   2001.475      0.728      0.467   -2465.965    5380.540\n",
       "7183.0            1277.7106   2493.046      0.513      0.608   -3609.110    6164.532\n",
       "7228.0            1.278e+04   2150.609      5.942      0.000    8563.950     1.7e+04\n",
       "7232.0            1690.0190   3382.763      0.500      0.617   -4940.807    8320.845\n",
       "7250.0            -147.9826   2226.379     -0.066      0.947   -4512.088    4216.123\n",
       "7257.0            1.904e+04   3240.032      5.877      0.000    1.27e+04    2.54e+04\n",
       "7260.0            4722.0406   1937.818      2.437      0.015     923.568    8520.513\n",
       "7267.0            1629.6320   2338.455      0.697      0.486   -2954.162    6213.426\n",
       "7268.0            -1.25e+04   3828.950     -3.264      0.001      -2e+04   -4991.117\n",
       "7281.0            5886.6848   2988.715      1.970      0.049      28.263    1.17e+04\n",
       "7291.0            2896.8188   2299.209      1.260      0.208   -1610.046    7403.684\n",
       "7343.0           -1.232e+04   3684.981     -3.344      0.001   -1.95e+04   -5099.012\n",
       "7346.0           -7097.0654   2330.495     -3.045      0.002   -1.17e+04   -2528.875\n",
       "7401.0            5849.8446   1993.130      2.935      0.003    1942.950    9756.739\n",
       "7409.0            4645.0055   2011.931      2.309      0.021     701.258    8588.753\n",
       "7420.0             317.5942   1943.010      0.163      0.870   -3491.057    4126.245\n",
       "7435.0            5582.4116   3713.583      1.503      0.133   -1696.881    1.29e+04\n",
       "7466.0            3307.8459   2163.349      1.529      0.126    -932.709    7548.400\n",
       "7486.0           -1.151e+04   3320.513     -3.465      0.001    -1.8e+04   -4997.388\n",
       "7503.0            4506.7226   3669.330      1.228      0.219   -2685.826    1.17e+04\n",
       "7506.0            5504.9567   1979.844      2.781      0.005    1624.105    9385.808\n",
       "7537.0            4957.2716   2125.435      2.332      0.020     791.035    9123.508\n",
       "7549.0            3019.5153   2228.615      1.355      0.175   -1348.972    7388.003\n",
       "7554.0            5336.2602   2084.761      2.560      0.010    1249.752    9422.768\n",
       "7557.0            2153.4872   2639.742      0.816      0.415   -3020.884    7327.859\n",
       "7585.0            -1.36e+04   3433.259     -3.963      0.000   -2.03e+04   -6875.065\n",
       "7602.0            4154.8022   1984.316      2.094      0.036     265.184    8044.420\n",
       "7620.0            -821.8510   2689.827     -0.306      0.760   -6094.398    4450.696\n",
       "7636.0            4797.4446   2046.431      2.344      0.019     786.071    8808.819\n",
       "7646.0            5303.9547   2052.240      2.584      0.010    1281.194    9326.716\n",
       "7658.0            1284.4351   2504.106      0.513      0.608   -3624.065    6192.936\n",
       "7683.0            6440.1126   2214.527      2.908      0.004    2099.239    1.08e+04\n",
       "7685.0            4481.2970   2178.464      2.057      0.040     211.115    8751.479\n",
       "7692.0           -2269.1270   1901.551     -1.193      0.233   -5996.511    1458.257\n",
       "7762.0            5358.1340   1966.060      2.725      0.006    1504.301    9211.967\n",
       "7772.0            -1.01e+04   3070.597     -3.290      0.001   -1.61e+04   -4083.439\n",
       "7773.0            4882.6359   1956.612      2.495      0.013    1047.323    8717.949\n",
       "7777.0           -1601.3844   1983.215     -0.807      0.419   -5488.844    2286.075\n",
       "7835.0            6229.8904   2022.998      3.080      0.002    2264.448    1.02e+04\n",
       "7873.0           -1.114e+04   2945.929     -3.782      0.000   -1.69e+04   -5367.782\n",
       "7883.0            1914.0467   2841.995      0.673      0.501   -3656.777    7484.870\n",
       "7904.0            1318.1351   2789.858      0.472      0.637   -4150.491    6786.761\n",
       "7906.0            8691.5911   2172.525      4.001      0.000    4433.050     1.3e+04\n",
       "7921.0            5611.4532   1981.638      2.832      0.005    1727.085    9495.821\n",
       "7923.0            5575.1029   2150.900      2.592      0.010    1358.950    9791.256\n",
       "7935.0            1458.3238   1875.001      0.778      0.437   -2217.016    5133.664\n",
       "7938.0            3645.7205   1992.882      1.829      0.067    -260.688    7552.129\n",
       "7985.0            -2.33e+04   3645.781     -6.391      0.000   -3.04e+04   -1.62e+04\n",
       "8014.0            2538.9901   2055.058      1.235      0.217   -1489.295    6567.276\n",
       "8030.0            6713.3535   2262.090      2.968      0.003    2279.249    1.11e+04\n",
       "8046.0           -1.147e+04   2918.829     -3.929      0.000   -1.72e+04   -5746.634\n",
       "8047.0            4739.0798   2769.646      1.711      0.087    -689.926    1.02e+04\n",
       "8062.0            2403.6790   1954.092      1.230      0.219   -1426.694    6234.052\n",
       "8068.0            -1.32e+04   2297.190     -5.744      0.000   -1.77e+04   -8693.260\n",
       "8087.0           -1.733e+04   3327.412     -5.209      0.000   -2.39e+04   -1.08e+04\n",
       "8095.0            5389.9526   1993.917      2.703      0.007    1481.515    9298.391\n",
       "8096.0            5616.0905   2207.738      2.544      0.011    1288.524    9943.657\n",
       "8109.0            5813.8222   1999.646      2.907      0.004    1894.155    9733.489\n",
       "8123.0             871.3964   3388.433      0.257      0.797   -5770.545    7513.338\n",
       "8150.0            6442.7903   2100.605      3.067      0.002    2325.224    1.06e+04\n",
       "8163.0            2714.2043   2831.158      0.959      0.338   -2835.376    8263.785\n",
       "8176.0           -5203.2892   3580.491     -1.453      0.146   -1.22e+04    1815.120\n",
       "8202.0            2178.7879   2337.863      0.932      0.351   -2403.846    6761.422\n",
       "8214.0            1944.2843   2251.271      0.864      0.388   -2468.613    6357.182\n",
       "8215.0            -437.8140   2847.646     -0.154      0.878   -6019.715    5144.087\n",
       "8219.0            6755.8024   2264.195      2.984      0.003    2317.571    1.12e+04\n",
       "8247.0            2349.8825   2395.266      0.981      0.327   -2345.272    7045.037\n",
       "8253.0           -1.675e+04   2825.572     -5.928      0.000   -2.23e+04   -1.12e+04\n",
       "8290.0            2540.1043   2074.917      1.224      0.221   -1527.108    6607.317\n",
       "8293.0            3223.8736   2646.356      1.218      0.223   -1963.462    8411.209\n",
       "8304.0            5941.3109   1944.877      3.055      0.002    2129.001    9753.621\n",
       "8334.0            4945.2836   2505.577      1.974      0.048      33.901    9856.667\n",
       "8348.0            5192.6038   1978.114      2.625      0.009    1315.143    9070.065\n",
       "8357.0            4623.6809   1958.565      2.361      0.018     784.539    8462.823\n",
       "8358.0            3027.7137   2213.170      1.368      0.171   -1310.499    7365.927\n",
       "8446.0           -5645.4833   2437.820     -2.316      0.021   -1.04e+04    -866.915\n",
       "8460.0            7363.6112   2679.665      2.748      0.006    2110.983    1.26e+04\n",
       "8463.0            4768.4814   1986.783      2.400      0.016     874.028    8662.935\n",
       "8479.0            8453.9041   2795.778      3.024      0.003    2973.674    1.39e+04\n",
       "8530.0            1294.8967   3434.783      0.377      0.706   -5437.899    8027.692\n",
       "8536.0             568.3196   2436.000      0.233      0.816   -4206.680    5343.320\n",
       "8543.0            2.457e+04   2341.182     10.494      0.000       2e+04    2.92e+04\n",
       "8549.0           -9620.0071   2140.883     -4.493      0.000   -1.38e+04   -5423.490\n",
       "8551.0            6311.2443   2195.472      2.875      0.004    2007.722    1.06e+04\n",
       "8559.0           -1711.9393   2534.546     -0.675      0.499   -6680.107    3256.229\n",
       "8573.0           -1.212e+04   3864.307     -3.137      0.002   -1.97e+04   -4546.777\n",
       "8606.0            6891.7721   2025.029      3.403      0.001    2922.349    1.09e+04\n",
       "8607.0            6207.0136   2254.459      2.753      0.006    1787.868    1.06e+04\n",
       "8648.0            4346.6432   1940.585      2.240      0.025     542.746    8150.541\n",
       "8657.0           -2923.9860   1925.895     -1.518      0.129   -6699.088     851.116\n",
       "8675.0            5395.2034   3799.119      1.420      0.156   -2051.757    1.28e+04\n",
       "8681.0           -1716.7105   1968.780     -0.872      0.383   -5575.874    2142.453\n",
       "8687.0             285.4047   2280.382      0.125      0.900   -4184.557    4755.366\n",
       "8692.0            1969.4231   2046.587      0.962      0.336   -2042.257    5981.103\n",
       "8699.0            5431.4437   1978.129      2.746      0.006    1553.953    9308.934\n",
       "8717.0            6442.4178   2088.535      3.085      0.002    2348.511    1.05e+04\n",
       "8759.0           -2170.4472   2521.954     -0.861      0.389   -7113.933    2773.038\n",
       "8762.0            8728.1797   2013.914      4.334      0.000    4780.543    1.27e+04\n",
       "8819.0            7326.5310   2376.484      3.083      0.002    2668.194     1.2e+04\n",
       "8850.0            5312.7484   1972.564      2.693      0.007    1446.167    9179.330\n",
       "8852.0            5513.0555   1980.033      2.784      0.005    1631.833    9394.278\n",
       "8859.0            5951.9728   2270.753      2.621      0.009    1500.886    1.04e+04\n",
       "8867.0           -4059.6169   2046.407     -1.984      0.047   -8070.945     -48.289\n",
       "8881.0            2986.3093   1910.549      1.563      0.118    -758.713    6731.331\n",
       "8958.0             409.6492   1922.388      0.213      0.831   -3358.579    4177.878\n",
       "8972.0           -2.382e+04   3474.873     -6.855      0.000   -3.06e+04    -1.7e+04\n",
       "8990.0           -1.299e+04   2785.199     -4.663      0.000   -1.84e+04   -7529.179\n",
       "9004.0            6370.8630   2445.448      2.605      0.009    1577.344    1.12e+04\n",
       "9016.0            1646.0339   1887.092      0.872      0.383   -2053.008    5345.076\n",
       "9048.0             724.1859   2189.711      0.331      0.741   -3568.043    5016.415\n",
       "9051.0           -9141.6529   2593.530     -3.525      0.000   -1.42e+04   -4057.866\n",
       "9071.0            1539.7304   3405.147      0.452      0.651   -5134.973    8214.434\n",
       "9112.0           -1294.5066   1865.801     -0.694      0.488   -4951.814    2362.801\n",
       "9114.0            -786.7494   2545.408     -0.309      0.757   -5776.208    4202.709\n",
       "9132.0            5005.3325   3665.571      1.365      0.172   -2179.850    1.22e+04\n",
       "9173.0            4003.6703   2351.469      1.703      0.089    -605.633    8612.973\n",
       "9180.0            6421.3254   2132.547      3.011      0.003    2241.147    1.06e+04\n",
       "9186.0            3854.8123   1958.733      1.968      0.049      15.342    7694.283\n",
       "9191.0            -285.1558   3643.258     -0.078      0.938   -7426.601    6856.289\n",
       "9216.0           -4621.4846   2959.645     -1.561      0.118   -1.04e+04    1179.955\n",
       "9217.0           -7235.6113   2883.697     -2.509      0.012   -1.29e+04   -1583.044\n",
       "9225.0            6083.6812   1990.954      3.056      0.002    2181.052    9986.310\n",
       "9230.0            5852.3744   2765.716      2.116      0.034     431.072    1.13e+04\n",
       "9259.0            7360.1537   2370.308      3.105      0.002    2713.922     1.2e+04\n",
       "9293.0            6391.6828   2053.579      3.112      0.002    2366.296    1.04e+04\n",
       "9299.0            1049.0531   2159.566      0.486      0.627   -3184.087    5282.193\n",
       "9308.0           -4597.2882   2864.090     -1.605      0.108   -1.02e+04    1016.845\n",
       "9311.0           -2022.6328   3099.035     -0.653      0.514   -8097.301    4052.035\n",
       "9313.0           -2690.2515   1868.891     -1.439      0.150   -6353.615     973.112\n",
       "9325.0            4126.4851   1990.485      2.073      0.038     224.775    8028.195\n",
       "9332.0            3741.1541   2202.187      1.699      0.089    -575.529    8057.838\n",
       "9340.0           -3.719e+04   5651.950     -6.580      0.000   -4.83e+04   -2.61e+04\n",
       "9372.0            4675.5631   2613.572      1.789      0.074    -447.509    9798.635\n",
       "9411.0            2465.0315   2077.114      1.187      0.235   -1606.486    6536.549\n",
       "9459.0           -9777.9787   3402.592     -2.874      0.004   -1.64e+04   -3108.283\n",
       "9465.0             1.03e+04   2315.523      4.446      0.000    5756.281    1.48e+04\n",
       "9472.0           -4233.5049   1980.103     -2.138      0.033   -8114.864    -352.146\n",
       "9483.0           -1.445e+04   2884.651     -5.011      0.000   -2.01e+04   -8800.151\n",
       "9563.0            -2.22e+04   3469.930     -6.397      0.000    -2.9e+04   -1.54e+04\n",
       "9590.0            -532.7499   2355.448     -0.226      0.821   -5149.853    4084.353\n",
       "9598.0           -5916.2834   2878.893     -2.055      0.040   -1.16e+04    -273.133\n",
       "9599.0           -6542.5034   2214.962     -2.954      0.003   -1.09e+04   -2200.777\n",
       "9602.0           -5901.8827   3362.432     -1.755      0.079   -1.25e+04     689.091\n",
       "9619.0            5408.6323   2415.143      2.239      0.025     674.516    1.01e+04\n",
       "9643.0            2269.3154   2875.822      0.789      0.430   -3367.815    7906.446\n",
       "9650.0            1432.3734   3349.810      0.428      0.669   -5133.858    7998.605\n",
       "9653.0           -1.649e+04   4718.370     -3.495      0.000   -2.57e+04   -7242.812\n",
       "9667.0            1356.2435   2878.268      0.471      0.638   -4285.681    6998.168\n",
       "9698.0            2654.6416   2090.304      1.270      0.204   -1442.732    6752.016\n",
       "9699.0            4621.7781   1913.605      2.415      0.016     870.767    8372.789\n",
       "9719.0           -8896.1406   2273.544     -3.913      0.000   -1.34e+04   -4439.585\n",
       "9742.0           -1.214e+04   2764.066     -4.391      0.000   -1.76e+04   -6718.013\n",
       "9761.0            5375.5350   1993.720      2.696      0.007    1467.484    9283.586\n",
       "9771.0           -6020.1534   2290.075     -2.629      0.009   -1.05e+04   -1531.192\n",
       "9772.0            5715.4592   2157.821      2.649      0.008    1485.739    9945.179\n",
       "9778.0            2213.9534   2103.369      1.053      0.293   -1909.030    6336.937\n",
       "9799.0           -9015.9095   2984.836     -3.021      0.003   -1.49e+04   -3165.092\n",
       "9815.0            5038.7964   2085.544      2.416      0.016     950.754    9126.839\n",
       "9818.0           -3.225e+04   2303.032    -14.003      0.000   -3.68e+04   -2.77e+04\n",
       "9837.0            7009.7043   2302.429      3.044      0.002    2496.527    1.15e+04\n",
       "9922.0           -7420.8977   2141.400     -3.465      0.001   -1.16e+04   -3223.368\n",
       "9954.0            1021.0202   4247.919      0.240      0.810   -7305.669    9347.709\n",
       "9963.0            3373.2039   2130.490      1.583      0.113    -802.942    7549.350\n",
       "9988.0            6745.7386   2341.755      2.881      0.004    2155.477    1.13e+04\n",
       "9999.0           -1.108e+04   2729.767     -4.061      0.000   -1.64e+04   -5733.534\n",
       "gspilltecIVX1982     0.0279      0.057      0.492      0.623      -0.083       0.139\n",
       "gspilltecIVX1983     0.0190      0.056      0.339      0.735      -0.091       0.129\n",
       "gspilltecIVX1984    -0.0298      0.056     -0.534      0.593      -0.139       0.079\n",
       "gspilltecIVX1985    -0.0536      0.056     -0.952      0.341      -0.164       0.057\n",
       "gspilltecIVX1986    -0.0862      0.057     -1.503      0.133      -0.199       0.026\n",
       "gspilltecIVX1987    -0.0938      0.059     -1.600      0.110      -0.209       0.021\n",
       "gspilltecIVX1988    -0.1227      0.060     -2.049      0.040      -0.240      -0.005\n",
       "gspilltecIVX1989    -0.1236      0.061     -2.026      0.043      -0.243      -0.004\n",
       "gspilltecIVX1990    -0.1578      0.062     -2.530      0.011      -0.280      -0.036\n",
       "gspilltecIVX1991    -0.1388      0.064     -2.181      0.029      -0.264      -0.014\n",
       "gspilltecIVX1992    -0.1285      0.065     -1.979      0.048      -0.256      -0.001\n",
       "gspilltecIVX1993    -0.1041      0.067     -1.565      0.118      -0.235       0.026\n",
       "gspilltecIVX1994    -0.1123      0.068     -1.647      0.100      -0.246       0.021\n",
       "gspilltecIVX1995    -0.1026      0.070     -1.456      0.145      -0.241       0.035\n",
       "gspilltecIVX1996    -0.0870      0.073     -1.189      0.235      -0.231       0.057\n",
       "gspilltecIVX1997    -0.0746      0.076     -0.978      0.328      -0.224       0.075\n",
       "gspilltecIVX1998    -0.0502      0.079     -0.634      0.526      -0.206       0.105\n",
       "gspilltecIVX1999     0.0445      0.082      0.545      0.586      -0.115       0.204\n",
       "gspillsicIVX1982    -0.0244      0.106     -0.230      0.818      -0.232       0.183\n",
       "gspillsicIVX1983    -0.0548      0.104     -0.526      0.599      -0.259       0.149\n",
       "gspillsicIVX1984    -0.0790      0.103     -0.768      0.443      -0.281       0.123\n",
       "gspillsicIVX1985    -0.1083      0.103     -1.050      0.294      -0.310       0.094\n",
       "gspillsicIVX1986    -0.1409      0.104     -1.354      0.176      -0.345       0.063\n",
       "gspillsicIVX1987    -0.1645      0.105     -1.565      0.118      -0.371       0.042\n",
       "gspillsicIVX1988    -0.1827      0.106     -1.721      0.085      -0.391       0.025\n",
       "gspillsicIVX1989    -0.1796      0.107     -1.674      0.094      -0.390       0.031\n",
       "gspillsicIVX1990    -0.1799      0.109     -1.655      0.098      -0.393       0.033\n",
       "gspillsicIVX1991    -0.1849      0.110     -1.677      0.094      -0.401       0.031\n",
       "gspillsicIVX1992    -0.2424      0.112     -2.168      0.030      -0.461      -0.023\n",
       "gspillsicIVX1993    -0.2737      0.114     -2.408      0.016      -0.496      -0.051\n",
       "gspillsicIVX1994    -0.2781      0.115     -2.410      0.016      -0.504      -0.052\n",
       "gspillsicIVX1995    -0.2600      0.118     -2.200      0.028      -0.492      -0.028\n",
       "gspillsicIVX1996    -0.3052      0.122     -2.505      0.012      -0.544      -0.066\n",
       "gspillsicIVX1997    -0.2848      0.126     -2.263      0.024      -0.532      -0.038\n",
       "gspillsicIVX1998    -0.2418      0.130     -1.861      0.063      -0.496       0.013\n",
       "gspillsicIVX1999    -0.2678      0.134     -2.004      0.045      -0.530      -0.006\n",
       "==============================================================================\n",
       "Omnibus:                    22454.933   Durbin-Watson:                   0.747\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        133216647.765\n",
       "Skew:                          14.365   Prob(JB):                         0.00\n",
       "Kurtosis:                     524.154   Cond. No.                     6.09e+19\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.5e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate effect of spillovers on firm value, OLS w/o firm FE's. Estimate spillovers separately, then together.\n",
    "\n",
    "# Full model\n",
    "year_model1 = sm.OLS(y_var,x_vars).fit()\n",
    "year_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "650d2db3-e371-4728-b906-7120bba05230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.370</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.368</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   286.0</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:42</td>     <th>  Log-Likelihood:    </th> <td>-1.2562e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 11736</td>      <th>  AIC:               </th>  <td>2.513e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 11711</td>      <th>  BIC:               </th>  <td>2.515e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>  779.2803</td> <td>  178.911</td> <td>    4.356</td> <td> 0.000</td> <td>  428.585</td> <td> 1129.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th>      <td>   -0.0633</td> <td>    0.030</td> <td>   -2.102</td> <td> 0.036</td> <td>   -0.122</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>    1.9021</td> <td>    1.555</td> <td>    1.224</td> <td> 0.221</td> <td>   -1.145</td> <td>    4.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.5105</td> <td>    0.034</td> <td>   14.905</td> <td> 0.000</td> <td>    0.443</td> <td>    0.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.5170</td> <td>    0.044</td> <td>   11.649</td> <td> 0.000</td> <td>    0.430</td> <td>    0.604</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>  -29.6582</td> <td>    3.307</td> <td>   -8.969</td> <td> 0.000</td> <td>  -36.140</td> <td>  -23.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>    6.5601</td> <td>    0.447</td> <td>   14.667</td> <td> 0.000</td> <td>    5.683</td> <td>    7.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1982</th> <td>    0.0208</td> <td>    0.039</td> <td>    0.531</td> <td> 0.595</td> <td>   -0.056</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1983</th> <td>    0.0367</td> <td>    0.038</td> <td>    0.957</td> <td> 0.339</td> <td>   -0.039</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1984</th> <td>    0.0220</td> <td>    0.037</td> <td>    0.589</td> <td> 0.556</td> <td>   -0.051</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1985</th> <td>    0.0292</td> <td>    0.036</td> <td>    0.803</td> <td> 0.422</td> <td>   -0.042</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1986</th> <td>    0.0322</td> <td>    0.036</td> <td>    0.903</td> <td> 0.366</td> <td>   -0.038</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1987</th> <td>    0.0317</td> <td>    0.035</td> <td>    0.907</td> <td> 0.365</td> <td>   -0.037</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1988</th> <td>    0.0244</td> <td>    0.035</td> <td>    0.706</td> <td> 0.480</td> <td>   -0.043</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1989</th> <td>    0.0385</td> <td>    0.034</td> <td>    1.128</td> <td> 0.259</td> <td>   -0.028</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1990</th> <td>    0.0233</td> <td>    0.034</td> <td>    0.691</td> <td> 0.490</td> <td>   -0.043</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1991</th> <td>    0.0463</td> <td>    0.033</td> <td>    1.390</td> <td> 0.165</td> <td>   -0.019</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1992</th> <td>    0.0462</td> <td>    0.033</td> <td>    1.397</td> <td> 0.163</td> <td>   -0.019</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1993</th> <td>    0.0594</td> <td>    0.033</td> <td>    1.815</td> <td> 0.069</td> <td>   -0.005</td> <td>    0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1994</th> <td>    0.0544</td> <td>    0.033</td> <td>    1.671</td> <td> 0.095</td> <td>   -0.009</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1995</th> <td>    0.0844</td> <td>    0.032</td> <td>    2.614</td> <td> 0.009</td> <td>    0.021</td> <td>    0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1996</th> <td>    0.1124</td> <td>    0.032</td> <td>    3.514</td> <td> 0.000</td> <td>    0.050</td> <td>    0.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1997</th> <td>    0.1573</td> <td>    0.032</td> <td>    4.951</td> <td> 0.000</td> <td>    0.095</td> <td>    0.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1998</th> <td>    0.2118</td> <td>    0.032</td> <td>    6.703</td> <td> 0.000</td> <td>    0.150</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1999</th> <td>    0.2764</td> <td>    0.032</td> <td>    8.767</td> <td> 0.000</td> <td>    0.215</td> <td>    0.338</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22177.761</td> <th>  Durbin-Watson:     </th>   <td>   0.567</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>87386134.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>14.146</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>424.786</td>  <th>  Cond. No.          </th>   <td>4.87e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.87e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.370    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.368    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      286.0    \\\\\n",
       "\\textbf{Date:}             & Mon, 14 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     16:57:42     & \\textbf{  Log-Likelihood:    } & -1.2562e+05   \\\\\n",
       "\\textbf{No. Observations:} &       11736      & \\textbf{  AIC:               } &  2.513e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       11711      & \\textbf{  BIC:               } &  2.515e+05    \\\\\n",
       "\\textbf{Df Model:}         &          24      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &     779.2803  &      178.911     &     4.356  &         0.000        &      428.585    &     1129.976     \\\\\n",
       "\\textbf{gspilltecIV}      &      -0.0633  &        0.030     &    -2.102  &         0.036        &       -0.122    &       -0.004     \\\\\n",
       "\\textbf{pat\\_count}       &       1.9021  &        1.555     &     1.224  &         0.221        &       -1.145    &        4.949     \\\\\n",
       "\\textbf{rsales}           &       0.5105  &        0.034     &    14.905  &         0.000        &        0.443    &        0.578     \\\\\n",
       "\\textbf{rppent}           &       0.5170  &        0.044     &    11.649  &         0.000        &        0.430    &        0.604     \\\\\n",
       "\\textbf{emp}              &     -29.6582  &        3.307     &    -8.969  &         0.000        &      -36.140    &      -23.177     \\\\\n",
       "\\textbf{rxrd}             &       6.5601  &        0.447     &    14.667  &         0.000        &        5.683    &        7.437     \\\\\n",
       "\\textbf{gspilltecIVX1982} &       0.0208  &        0.039     &     0.531  &         0.595        &       -0.056    &        0.098     \\\\\n",
       "\\textbf{gspilltecIVX1983} &       0.0367  &        0.038     &     0.957  &         0.339        &       -0.039    &        0.112     \\\\\n",
       "\\textbf{gspilltecIVX1984} &       0.0220  &        0.037     &     0.589  &         0.556        &       -0.051    &        0.095     \\\\\n",
       "\\textbf{gspilltecIVX1985} &       0.0292  &        0.036     &     0.803  &         0.422        &       -0.042    &        0.101     \\\\\n",
       "\\textbf{gspilltecIVX1986} &       0.0322  &        0.036     &     0.903  &         0.366        &       -0.038    &        0.102     \\\\\n",
       "\\textbf{gspilltecIVX1987} &       0.0317  &        0.035     &     0.907  &         0.365        &       -0.037    &        0.100     \\\\\n",
       "\\textbf{gspilltecIVX1988} &       0.0244  &        0.035     &     0.706  &         0.480        &       -0.043    &        0.092     \\\\\n",
       "\\textbf{gspilltecIVX1989} &       0.0385  &        0.034     &     1.128  &         0.259        &       -0.028    &        0.105     \\\\\n",
       "\\textbf{gspilltecIVX1990} &       0.0233  &        0.034     &     0.691  &         0.490        &       -0.043    &        0.089     \\\\\n",
       "\\textbf{gspilltecIVX1991} &       0.0463  &        0.033     &     1.390  &         0.165        &       -0.019    &        0.112     \\\\\n",
       "\\textbf{gspilltecIVX1992} &       0.0462  &        0.033     &     1.397  &         0.163        &       -0.019    &        0.111     \\\\\n",
       "\\textbf{gspilltecIVX1993} &       0.0594  &        0.033     &     1.815  &         0.069        &       -0.005    &        0.124     \\\\\n",
       "\\textbf{gspilltecIVX1994} &       0.0544  &        0.033     &     1.671  &         0.095        &       -0.009    &        0.118     \\\\\n",
       "\\textbf{gspilltecIVX1995} &       0.0844  &        0.032     &     2.614  &         0.009        &        0.021    &        0.148     \\\\\n",
       "\\textbf{gspilltecIVX1996} &       0.1124  &        0.032     &     3.514  &         0.000        &        0.050    &        0.175     \\\\\n",
       "\\textbf{gspilltecIVX1997} &       0.1573  &        0.032     &     4.951  &         0.000        &        0.095    &        0.220     \\\\\n",
       "\\textbf{gspilltecIVX1998} &       0.2118  &        0.032     &     6.703  &         0.000        &        0.150    &        0.274     \\\\\n",
       "\\textbf{gspilltecIVX1999} &       0.2764  &        0.032     &     8.767  &         0.000        &        0.215    &        0.338     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 22177.761 & \\textbf{  Durbin-Watson:     } &      0.567    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 87386134.626  \\\\\n",
       "\\textbf{Skew:}          &   14.146  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  424.786  & \\textbf{  Cond. No.          } &   4.87e+04    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 4.87e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.370\n",
       "Model:                            OLS   Adj. R-squared:                  0.368\n",
       "Method:                 Least Squares   F-statistic:                     286.0\n",
       "Date:                Mon, 14 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        16:57:42   Log-Likelihood:            -1.2562e+05\n",
       "No. Observations:               11736   AIC:                         2.513e+05\n",
       "Df Residuals:                   11711   BIC:                         2.515e+05\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const              779.2803    178.911      4.356      0.000     428.585    1129.976\n",
       "gspilltecIV         -0.0633      0.030     -2.102      0.036      -0.122      -0.004\n",
       "pat_count            1.9021      1.555      1.224      0.221      -1.145       4.949\n",
       "rsales               0.5105      0.034     14.905      0.000       0.443       0.578\n",
       "rppent               0.5170      0.044     11.649      0.000       0.430       0.604\n",
       "emp                -29.6582      3.307     -8.969      0.000     -36.140     -23.177\n",
       "rxrd                 6.5601      0.447     14.667      0.000       5.683       7.437\n",
       "gspilltecIVX1982     0.0208      0.039      0.531      0.595      -0.056       0.098\n",
       "gspilltecIVX1983     0.0367      0.038      0.957      0.339      -0.039       0.112\n",
       "gspilltecIVX1984     0.0220      0.037      0.589      0.556      -0.051       0.095\n",
       "gspilltecIVX1985     0.0292      0.036      0.803      0.422      -0.042       0.101\n",
       "gspilltecIVX1986     0.0322      0.036      0.903      0.366      -0.038       0.102\n",
       "gspilltecIVX1987     0.0317      0.035      0.907      0.365      -0.037       0.100\n",
       "gspilltecIVX1988     0.0244      0.035      0.706      0.480      -0.043       0.092\n",
       "gspilltecIVX1989     0.0385      0.034      1.128      0.259      -0.028       0.105\n",
       "gspilltecIVX1990     0.0233      0.034      0.691      0.490      -0.043       0.089\n",
       "gspilltecIVX1991     0.0463      0.033      1.390      0.165      -0.019       0.112\n",
       "gspilltecIVX1992     0.0462      0.033      1.397      0.163      -0.019       0.111\n",
       "gspilltecIVX1993     0.0594      0.033      1.815      0.069      -0.005       0.124\n",
       "gspilltecIVX1994     0.0544      0.033      1.671      0.095      -0.009       0.118\n",
       "gspilltecIVX1995     0.0844      0.032      2.614      0.009       0.021       0.148\n",
       "gspilltecIVX1996     0.1124      0.032      3.514      0.000       0.050       0.175\n",
       "gspilltecIVX1997     0.1573      0.032      4.951      0.000       0.095       0.220\n",
       "gspilltecIVX1998     0.2118      0.032      6.703      0.000       0.150       0.274\n",
       "gspilltecIVX1999     0.2764      0.032      8.767      0.000       0.215       0.338\n",
       "==============================================================================\n",
       "Omnibus:                    22177.761   Durbin-Watson:                   0.567\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         87386134.626\n",
       "Skew:                          14.146   Prob(JB):                         0.00\n",
       "Kurtosis:                     424.786   Cond. No.                     4.87e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.87e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tech spillovers model, no firm FE's\n",
    "drop_columns = [col for col in x_vars.columns if 'gspillsicIV' in col]\n",
    "x_vars_nofe = x_vars.drop(columns=fixed_effects)\n",
    "x_vars_nofe = x_vars_nofe.drop(columns=drop_columns)\n",
    "\n",
    "year_model2 = sm.OLS(y_var,x_vars_nofe).fit()\n",
    "year_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de709391-2187-4eb9-b53d-163e32e529f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.666</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.643</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   28.50</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:50</td>     <th>  Log-Likelihood:    </th> <td>-1.2190e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 11736</td>      <th>  AIC:               </th>  <td>2.453e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 10968</td>      <th>  BIC:               </th>  <td>2.510e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   767</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-9211.7705</td> <td> 1638.776</td> <td>   -5.621</td> <td> 0.000</td> <td>-1.24e+04</td> <td>-5999.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th>      <td>    0.5889</td> <td>    0.115</td> <td>    5.124</td> <td> 0.000</td> <td>    0.364</td> <td>    0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -25.4471</td> <td>    1.772</td> <td>  -14.360</td> <td> 0.000</td> <td>  -28.921</td> <td>  -21.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.9838</td> <td>    0.041</td> <td>   23.821</td> <td> 0.000</td> <td>    0.903</td> <td>    1.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.4660</td> <td>    0.086</td> <td>    5.390</td> <td> 0.000</td> <td>    0.297</td> <td>    0.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>   -6.8225</td> <td>    7.044</td> <td>   -0.969</td> <td> 0.333</td> <td>  -20.629</td> <td>    6.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>   10.1307</td> <td>    0.655</td> <td>   15.470</td> <td> 0.000</td> <td>    8.847</td> <td>   11.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>             <td> -302.7136</td> <td>  894.280</td> <td>   -0.338</td> <td> 0.735</td> <td>-2055.664</td> <td> 1450.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>             <td> -188.3216</td> <td>  886.018</td> <td>   -0.213</td> <td> 0.832</td> <td>-1925.076</td> <td> 1548.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>             <td>   61.4814</td> <td>  880.418</td> <td>    0.070</td> <td> 0.944</td> <td>-1664.297</td> <td> 1787.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>             <td>  310.8801</td> <td>  876.108</td> <td>    0.355</td> <td> 0.723</td> <td>-1406.450</td> <td> 2028.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>             <td>  702.0587</td> <td>  870.202</td> <td>    0.807</td> <td> 0.420</td> <td>-1003.693</td> <td> 2407.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>             <td>  656.7606</td> <td>  866.805</td> <td>    0.758</td> <td> 0.449</td> <td>-1042.333</td> <td> 2355.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>             <td>  933.6216</td> <td>  864.888</td> <td>    1.079</td> <td> 0.280</td> <td> -761.715</td> <td> 2628.958</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>             <td> 1082.0033</td> <td>  862.114</td> <td>    1.255</td> <td> 0.209</td> <td> -607.896</td> <td> 2771.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>             <td> 1284.3781</td> <td>  856.621</td> <td>    1.499</td> <td> 0.134</td> <td> -394.754</td> <td> 2963.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>             <td> 1326.5463</td> <td>  855.595</td> <td>    1.550</td> <td> 0.121</td> <td> -350.574</td> <td> 3003.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>             <td> 1228.9372</td> <td>  855.768</td> <td>    1.436</td> <td> 0.151</td> <td> -448.523</td> <td> 2906.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>             <td>  891.3080</td> <td>  852.779</td> <td>    1.045</td> <td> 0.296</td> <td> -780.293</td> <td> 2562.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>             <td>  808.1006</td> <td>  853.938</td> <td>    0.946</td> <td> 0.344</td> <td> -865.773</td> <td> 2481.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>             <td> 1054.1039</td> <td>  852.975</td> <td>    1.236</td> <td> 0.217</td> <td> -617.882</td> <td> 2726.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>             <td> 1059.0999</td> <td>  854.487</td> <td>    1.239</td> <td> 0.215</td> <td> -615.849</td> <td> 2734.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>             <td> 1111.7247</td> <td>  858.487</td> <td>    1.295</td> <td> 0.195</td> <td> -571.065</td> <td> 2794.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>             <td>  382.0384</td> <td>  861.634</td> <td>    0.443</td> <td> 0.657</td> <td>-1306.919</td> <td> 2070.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>             <td>-1789.5647</td> <td>  870.771</td> <td>   -2.055</td> <td> 0.040</td> <td>-3496.433</td> <td>  -82.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>          <td> 1471.8278</td> <td> 1876.597</td> <td>    0.784</td> <td> 0.433</td> <td>-2206.641</td> <td> 5150.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>          <td> -747.7524</td> <td> 2257.891</td> <td>   -0.331</td> <td> 0.741</td> <td>-5173.627</td> <td> 3678.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>          <td>-1482.7238</td> <td> 1888.179</td> <td>   -0.785</td> <td> 0.432</td> <td>-5183.894</td> <td> 2218.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>          <td> -487.2088</td> <td> 1868.785</td> <td>   -0.261</td> <td> 0.794</td> <td>-4150.365</td> <td> 3175.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>          <td> 5259.6555</td> <td> 2071.946</td> <td>    2.539</td> <td> 0.011</td> <td> 1198.268</td> <td> 9321.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>           <td> 2805.5935</td> <td> 1921.966</td> <td>    1.460</td> <td> 0.144</td> <td> -961.807</td> <td> 6572.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>          <td>  902.2280</td> <td> 1873.077</td> <td>    0.482</td> <td> 0.630</td> <td>-2769.340</td> <td> 4573.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>          <td>-2309.5248</td> <td> 1919.226</td> <td>   -1.203</td> <td> 0.229</td> <td>-6071.554</td> <td> 1452.504</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>          <td> 5069.6464</td> <td> 4737.149</td> <td>    1.070</td> <td> 0.285</td> <td>-4216.020</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>          <td>-5499.5467</td> <td> 2000.689</td> <td>   -2.749</td> <td> 0.006</td> <td>-9421.259</td> <td>-1577.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>           <td>  476.9711</td> <td> 4683.548</td> <td>    0.102</td> <td> 0.919</td> <td>-8703.627</td> <td> 9657.569</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>          <td> 8482.8358</td> <td> 2362.111</td> <td>    3.591</td> <td> 0.000</td> <td> 3852.673</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>          <td> 3202.1094</td> <td> 1895.146</td> <td>    1.690</td> <td> 0.091</td> <td> -512.719</td> <td> 6916.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>          <td> 8671.0722</td> <td> 2386.396</td> <td>    3.634</td> <td> 0.000</td> <td> 3993.305</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>           <td>-3756.1988</td> <td> 2034.674</td> <td>   -1.846</td> <td> 0.065</td> <td>-7744.526</td> <td>  232.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>          <td>-3698.4862</td> <td> 2480.895</td> <td>   -1.491</td> <td> 0.136</td> <td>-8561.488</td> <td> 1164.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>          <td> 5904.5710</td> <td> 3439.553</td> <td>    1.717</td> <td> 0.086</td> <td> -837.574</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>          <td> 4789.7900</td> <td> 2252.427</td> <td>    2.127</td> <td> 0.033</td> <td>  374.627</td> <td> 9204.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>          <td> 5912.9151</td> <td> 2354.896</td> <td>    2.511</td> <td> 0.012</td> <td> 1296.895</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>          <td> 3689.4278</td> <td> 1957.111</td> <td>    1.885</td> <td> 0.059</td> <td> -146.863</td> <td> 7525.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>          <td> 4939.9188</td> <td> 2049.954</td> <td>    2.410</td> <td> 0.016</td> <td>  921.640</td> <td> 8958.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>          <td> 8174.7392</td> <td> 2356.187</td> <td>    3.469</td> <td> 0.001</td> <td> 3556.188</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>          <td> 7734.8066</td> <td> 2430.117</td> <td>    3.183</td> <td> 0.001</td> <td> 2971.339</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>          <td> 5551.7698</td> <td> 2096.550</td> <td>    2.648</td> <td> 0.008</td> <td> 1442.153</td> <td> 9661.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>          <td>-1179.8527</td> <td> 1881.783</td> <td>   -0.627</td> <td> 0.531</td> <td>-4868.486</td> <td> 2508.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>          <td>-2.113e+04</td> <td> 3215.118</td> <td>   -6.571</td> <td> 0.000</td> <td>-2.74e+04</td> <td>-1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>          <td> 2071.0779</td> <td> 1894.980</td> <td>    1.093</td> <td> 0.274</td> <td>-1643.425</td> <td> 5785.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>          <td>-1.848e+04</td> <td> 4178.152</td> <td>   -4.422</td> <td> 0.000</td> <td>-2.67e+04</td> <td>-1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>           <td> 3944.3640</td> <td> 2135.332</td> <td>    1.847</td> <td> 0.065</td> <td> -241.271</td> <td> 8129.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>          <td> 1614.5948</td> <td> 1879.868</td> <td>    0.859</td> <td> 0.390</td> <td>-2070.286</td> <td> 5299.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>          <td>-8535.2040</td> <td> 2431.070</td> <td>   -3.511</td> <td> 0.000</td> <td>-1.33e+04</td> <td>-3769.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>          <td>-1.279e+04</td> <td> 2888.661</td> <td>   -4.427</td> <td> 0.000</td> <td>-1.85e+04</td> <td>-7127.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>          <td> -283.7100</td> <td> 1869.500</td> <td>   -0.152</td> <td> 0.879</td> <td>-3948.267</td> <td> 3380.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>          <td> 5890.4125</td> <td> 1973.724</td> <td>    2.984</td> <td> 0.003</td> <td> 2021.558</td> <td> 9759.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>          <td>  499.2547</td> <td> 1974.630</td> <td>    0.253</td> <td> 0.800</td> <td>-3371.376</td> <td> 4369.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>          <td> 7179.3354</td> <td> 2454.691</td> <td>    2.925</td> <td> 0.003</td> <td> 2367.698</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>          <td> 6884.8418</td> <td> 2214.192</td> <td>    3.109</td> <td> 0.002</td> <td> 2544.626</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>           <td>-9701.5817</td> <td> 2043.591</td> <td>   -4.747</td> <td> 0.000</td> <td>-1.37e+04</td> <td>-5695.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>          <td>-8024.8045</td> <td> 2395.586</td> <td>   -3.350</td> <td> 0.001</td> <td>-1.27e+04</td> <td>-3329.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>          <td>-2.291e+04</td> <td> 2417.172</td> <td>   -9.476</td> <td> 0.000</td> <td>-2.76e+04</td> <td>-1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>          <td> 4666.9864</td> <td> 2050.562</td> <td>    2.276</td> <td> 0.023</td> <td>  647.514</td> <td> 8686.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>          <td>-1.268e+04</td> <td> 3642.486</td> <td>   -3.482</td> <td> 0.001</td> <td>-1.98e+04</td> <td>-5542.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>          <td> 8076.3474</td> <td> 2407.548</td> <td>    3.355</td> <td> 0.001</td> <td> 3357.120</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>          <td>-1.221e+04</td> <td> 2496.373</td> <td>   -4.891</td> <td> 0.000</td> <td>-1.71e+04</td> <td>-7316.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>          <td>-7471.0255</td> <td> 2339.416</td> <td>   -3.194</td> <td> 0.001</td> <td>-1.21e+04</td> <td>-2885.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>          <td>-4822.3296</td> <td> 2439.002</td> <td>   -1.977</td> <td> 0.048</td> <td>-9603.213</td> <td>  -41.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>          <td>-1142.9396</td> <td> 1894.488</td> <td>   -0.603</td> <td> 0.546</td> <td>-4856.479</td> <td> 2570.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>          <td> 1274.9540</td> <td> 1977.196</td> <td>    0.645</td> <td> 0.519</td> <td>-2600.706</td> <td> 5150.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>          <td>-2711.8146</td> <td> 5781.834</td> <td>   -0.469</td> <td> 0.639</td> <td> -1.4e+04</td> <td> 8621.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>          <td>-4014.2561</td> <td> 2194.630</td> <td>   -1.829</td> <td> 0.067</td> <td>-8316.126</td> <td>  287.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>          <td> 5677.5052</td> <td> 2070.467</td> <td>    2.742</td> <td> 0.006</td> <td> 1619.016</td> <td> 9735.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>          <td> 7527.5576</td> <td> 2163.965</td> <td>    3.479</td> <td> 0.001</td> <td> 3285.796</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>          <td> 2587.6587</td> <td> 1957.315</td> <td>    1.322</td> <td> 0.186</td> <td>-1249.031</td> <td> 6424.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>          <td>-6487.9890</td> <td> 2226.401</td> <td>   -2.914</td> <td> 0.004</td> <td>-1.09e+04</td> <td>-2123.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>          <td> 2716.4309</td> <td> 1925.974</td> <td>    1.410</td> <td> 0.158</td> <td>-1058.826</td> <td> 6491.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>          <td> 3047.7307</td> <td> 1940.657</td> <td>    1.570</td> <td> 0.116</td> <td> -756.306</td> <td> 6851.767</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>          <td> 2574.6693</td> <td> 1923.722</td> <td>    1.338</td> <td> 0.181</td> <td>-1196.172</td> <td> 6345.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>          <td> -319.6551</td> <td> 1868.298</td> <td>   -0.171</td> <td> 0.864</td> <td>-3981.856</td> <td> 3342.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>          <td> -658.2818</td> <td> 1871.850</td> <td>   -0.352</td> <td> 0.725</td> <td>-4327.445</td> <td> 3010.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>          <td> 8846.5101</td> <td> 2330.883</td> <td>    3.795</td> <td> 0.000</td> <td> 4277.559</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>          <td> 3514.8776</td> <td> 2637.144</td> <td>    1.333</td> <td> 0.183</td> <td>-1654.399</td> <td> 8684.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>          <td> 7127.5181</td> <td> 2272.095</td> <td>    3.137</td> <td> 0.002</td> <td> 2673.803</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>          <td> 7560.5283</td> <td> 2349.973</td> <td>    3.217</td> <td> 0.001</td> <td> 2954.157</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>          <td> -715.5421</td> <td> 1872.912</td> <td>   -0.382</td> <td> 0.702</td> <td>-4386.788</td> <td> 2955.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>           <td> 6633.3340</td> <td> 2287.959</td> <td>    2.899</td> <td> 0.004</td> <td> 2148.521</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>          <td> 4093.5238</td> <td> 2067.320</td> <td>    1.980</td> <td> 0.048</td> <td>   41.204</td> <td> 8145.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>          <td>  354.3631</td> <td> 1878.627</td> <td>    0.189</td> <td> 0.850</td> <td>-3328.085</td> <td> 4036.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>          <td> 2645.9482</td> <td> 1906.353</td> <td>    1.388</td> <td> 0.165</td> <td>-1090.847</td> <td> 6382.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>          <td>-1.515e+04</td> <td> 2814.102</td> <td>   -5.383</td> <td> 0.000</td> <td>-2.07e+04</td> <td>-9631.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>          <td> 1529.7769</td> <td> 2279.365</td> <td>    0.671</td> <td> 0.502</td> <td>-2938.190</td> <td> 5997.744</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>          <td> 3342.7403</td> <td> 1949.156</td> <td>    1.715</td> <td> 0.086</td> <td> -477.957</td> <td> 7163.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>          <td> 2926.4519</td> <td> 3106.433</td> <td>    0.942</td> <td> 0.346</td> <td>-3162.717</td> <td> 9015.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>          <td>-2.684e+04</td> <td> 2824.291</td> <td>   -9.505</td> <td> 0.000</td> <td>-3.24e+04</td> <td>-2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>           <td> 1191.0221</td> <td> 1874.388</td> <td>    0.635</td> <td> 0.525</td> <td>-2483.116</td> <td> 4865.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>          <td> 6376.8999</td> <td> 2710.799</td> <td>    2.352</td> <td> 0.019</td> <td> 1063.245</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>          <td> 4334.3376</td> <td> 2063.583</td> <td>    2.100</td> <td> 0.036</td> <td>  289.343</td> <td> 8379.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>          <td>  791.9569</td> <td> 2159.054</td> <td>    0.367</td> <td> 0.714</td> <td>-3440.178</td> <td> 5024.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>           <td> 5941.4394</td> <td> 2128.434</td> <td>    2.791</td> <td> 0.005</td> <td> 1769.325</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>          <td> 4935.8143</td> <td> 2053.018</td> <td>    2.404</td> <td> 0.016</td> <td>  911.529</td> <td> 8960.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>          <td> 2368.7159</td> <td> 1909.567</td> <td>    1.240</td> <td> 0.215</td> <td>-1374.381</td> <td> 6111.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>          <td>-2293.5691</td> <td> 1914.395</td> <td>   -1.198</td> <td> 0.231</td> <td>-6046.128</td> <td> 1458.990</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>          <td> 4232.9568</td> <td> 2159.918</td> <td>    1.960</td> <td> 0.050</td> <td>   -0.871</td> <td> 8466.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>           <td>  762.7175</td> <td> 1867.675</td> <td>    0.408</td> <td> 0.683</td> <td>-2898.262</td> <td> 4423.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>          <td> 1303.1616</td> <td> 1875.641</td> <td>    0.695</td> <td> 0.487</td> <td>-2373.432</td> <td> 4979.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>          <td> 6798.8464</td> <td> 2204.200</td> <td>    3.084</td> <td> 0.002</td> <td> 2478.217</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>          <td> 4349.6694</td> <td> 1967.917</td> <td>    2.210</td> <td> 0.027</td> <td>  492.198</td> <td> 8207.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>          <td>-1.066e+04</td> <td> 3625.783</td> <td>   -2.939</td> <td> 0.003</td> <td>-1.78e+04</td> <td>-3549.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>          <td> 1158.9606</td> <td> 2222.013</td> <td>    0.522</td> <td> 0.602</td> <td>-3196.586</td> <td> 5514.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>          <td>-6500.3203</td> <td> 2290.356</td> <td>   -2.838</td> <td> 0.005</td> <td> -1.1e+04</td> <td>-2010.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>          <td>-2480.1543</td> <td> 1927.470</td> <td>   -1.287</td> <td> 0.198</td> <td>-6258.343</td> <td> 1298.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>          <td>-6519.7565</td> <td> 2038.346</td> <td>   -3.199</td> <td> 0.001</td> <td>-1.05e+04</td> <td>-2524.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>         <td>-2.474e+04</td> <td> 6159.589</td> <td>   -4.017</td> <td> 0.000</td> <td>-3.68e+04</td> <td>-1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>          <td>-7977.8559</td> <td> 2147.430</td> <td>   -3.715</td> <td> 0.000</td> <td>-1.22e+04</td> <td>-3768.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>          <td> -815.8950</td> <td> 1911.551</td> <td>   -0.427</td> <td> 0.670</td> <td>-4562.880</td> <td> 2931.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>          <td> 6925.3259</td> <td> 2279.743</td> <td>    3.038</td> <td> 0.002</td> <td> 2456.618</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>          <td>-8431.3839</td> <td> 2306.817</td> <td>   -3.655</td> <td> 0.000</td> <td> -1.3e+04</td> <td>-3909.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>          <td> 1942.3990</td> <td> 1891.341</td> <td>    1.027</td> <td> 0.304</td> <td>-1764.970</td> <td> 5649.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>          <td> 8088.9736</td> <td> 2341.947</td> <td>    3.454</td> <td> 0.001</td> <td> 3498.335</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>          <td> -940.3841</td> <td> 1877.924</td> <td>   -0.501</td> <td> 0.617</td> <td>-4621.454</td> <td> 2740.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>          <td>-1.033e+04</td> <td> 2902.474</td> <td>   -3.558</td> <td> 0.000</td> <td> -1.6e+04</td> <td>-4638.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>          <td> 6702.4017</td> <td> 2178.355</td> <td>    3.077</td> <td> 0.002</td> <td> 2432.434</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>          <td> 8594.4081</td> <td> 2102.710</td> <td>    4.087</td> <td> 0.000</td> <td> 4472.717</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>           <td>-1.068e+04</td> <td> 2823.182</td> <td>   -3.783</td> <td> 0.000</td> <td>-1.62e+04</td> <td>-5147.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>          <td>-9834.1861</td> <td> 2059.968</td> <td>   -4.774</td> <td> 0.000</td> <td>-1.39e+04</td> <td>-5796.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>          <td> 8372.7072</td> <td> 2385.777</td> <td>    3.509</td> <td> 0.000</td> <td> 3696.154</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>          <td>-4858.3619</td> <td> 2105.617</td> <td>   -2.307</td> <td> 0.021</td> <td>-8985.750</td> <td> -730.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>          <td>-3380.1413</td> <td> 2121.584</td> <td>   -1.593</td> <td> 0.111</td> <td>-7538.829</td> <td>  778.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>          <td> 3266.4616</td> <td> 2103.246</td> <td>    1.553</td> <td> 0.120</td> <td> -856.280</td> <td> 7389.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>          <td> 1007.7269</td> <td> 3326.172</td> <td>    0.303</td> <td> 0.762</td> <td>-5512.170</td> <td> 7527.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>          <td>-2154.7270</td> <td> 2200.443</td> <td>   -0.979</td> <td> 0.327</td> <td>-6467.991</td> <td> 2158.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>          <td> -815.4558</td> <td> 2180.736</td> <td>   -0.374</td> <td> 0.708</td> <td>-5090.092</td> <td> 3459.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>          <td>-7904.9485</td> <td> 6007.426</td> <td>   -1.316</td> <td> 0.188</td> <td>-1.97e+04</td> <td> 3870.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>          <td> 7199.2400</td> <td> 2612.880</td> <td>    2.755</td> <td> 0.006</td> <td> 2077.523</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>          <td> 3458.9714</td> <td> 2782.924</td> <td>    1.243</td> <td> 0.214</td> <td>-1996.061</td> <td> 8914.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>           <td>-1930.9294</td> <td> 1982.619</td> <td>   -0.974</td> <td> 0.330</td> <td>-5817.220</td> <td> 1955.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>          <td> -1.37e+04</td> <td> 3093.214</td> <td>   -4.428</td> <td> 0.000</td> <td>-1.98e+04</td> <td>-7632.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>          <td> 6.067e+04</td> <td> 2436.892</td> <td>   24.897</td> <td> 0.000</td> <td> 5.59e+04</td> <td> 6.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>          <td>-3135.3163</td> <td> 3409.956</td> <td>   -0.919</td> <td> 0.358</td> <td>-9819.446</td> <td> 3548.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>          <td>-8042.6444</td> <td> 2659.261</td> <td>   -3.024</td> <td> 0.002</td> <td>-1.33e+04</td> <td>-2830.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>          <td>-2506.4794</td> <td> 2388.160</td> <td>   -1.050</td> <td> 0.294</td> <td>-7187.704</td> <td> 2174.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>          <td>-3633.0708</td> <td> 2284.675</td> <td>   -1.590</td> <td> 0.112</td> <td>-8111.446</td> <td>  845.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>          <td> 7013.6473</td> <td> 2505.314</td> <td>    2.800</td> <td> 0.005</td> <td> 2102.780</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>          <td>-5095.6956</td> <td> 2456.326</td> <td>   -2.075</td> <td> 0.038</td> <td>-9910.538</td> <td> -280.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>           <td> -443.2119</td> <td> 1865.901</td> <td>   -0.238</td> <td> 0.812</td> <td>-4100.713</td> <td> 3214.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>          <td> -114.9577</td> <td> 2568.587</td> <td>   -0.045</td> <td> 0.964</td> <td>-5149.852</td> <td> 4919.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>          <td>-2089.4157</td> <td> 4713.861</td> <td>   -0.443</td> <td> 0.658</td> <td>-1.13e+04</td> <td> 7150.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>           <td>  401.2082</td> <td> 2043.999</td> <td>    0.196</td> <td> 0.844</td> <td>-3605.398</td> <td> 4407.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>          <td> 2141.4505</td> <td> 2480.742</td> <td>    0.863</td> <td> 0.388</td> <td>-2721.251</td> <td> 7004.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>          <td> 4855.7868</td> <td> 2406.932</td> <td>    2.017</td> <td> 0.044</td> <td>  137.766</td> <td> 9573.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>          <td>-3887.0089</td> <td> 2508.536</td> <td>   -1.550</td> <td> 0.121</td> <td>-8804.193</td> <td> 1030.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>          <td> 3392.2388</td> <td> 2241.324</td> <td>    1.513</td> <td> 0.130</td> <td>-1001.160</td> <td> 7785.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>          <td> 3168.3128</td> <td> 4791.460</td> <td>    0.661</td> <td> 0.508</td> <td>-6223.813</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>          <td> 8282.4899</td> <td> 2634.929</td> <td>    3.143</td> <td> 0.002</td> <td> 3117.553</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>          <td>-9569.3139</td> <td> 2636.971</td> <td>   -3.629</td> <td> 0.000</td> <td>-1.47e+04</td> <td>-4400.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>           <td> 7338.8150</td> <td> 2354.335</td> <td>    3.117</td> <td> 0.002</td> <td> 2723.893</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>          <td> 5392.7961</td> <td> 2512.729</td> <td>    2.146</td> <td> 0.032</td> <td>  467.395</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>           <td> 8550.3272</td> <td> 2388.220</td> <td>    3.580</td> <td> 0.000</td> <td> 3868.986</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>           <td> 2782.6322</td> <td> 1925.496</td> <td>    1.445</td> <td> 0.148</td> <td> -991.688</td> <td> 6556.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>          <td> 5535.6962</td> <td> 2467.240</td> <td>    2.244</td> <td> 0.025</td> <td>  699.461</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>          <td>  571.3451</td> <td> 2356.870</td> <td>    0.242</td> <td> 0.808</td> <td>-4048.546</td> <td> 5191.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>           <td> 2979.3791</td> <td> 1975.994</td> <td>    1.508</td> <td> 0.132</td> <td> -893.926</td> <td> 6852.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>          <td>-5447.0197</td> <td> 4709.511</td> <td>   -1.157</td> <td> 0.247</td> <td>-1.47e+04</td> <td> 3784.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>           <td> 8299.7785</td> <td> 2411.836</td> <td>    3.441</td> <td> 0.001</td> <td> 3572.145</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>          <td>-7870.7942</td> <td> 3149.175</td> <td>   -2.499</td> <td> 0.012</td> <td> -1.4e+04</td> <td>-1697.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>          <td>-7295.5691</td> <td> 2618.681</td> <td>   -2.786</td> <td> 0.005</td> <td>-1.24e+04</td> <td>-2162.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>          <td>-6398.1879</td> <td> 2531.195</td> <td>   -2.528</td> <td> 0.011</td> <td>-1.14e+04</td> <td>-1436.590</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>          <td> 3302.4052</td> <td> 2314.075</td> <td>    1.427</td> <td> 0.154</td> <td>-1233.600</td> <td> 7838.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>          <td>-2036.4176</td> <td> 2277.634</td> <td>   -0.894</td> <td> 0.371</td> <td>-6500.990</td> <td> 2428.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>          <td> 7199.1504</td> <td> 2683.288</td> <td>    2.683</td> <td> 0.007</td> <td> 1939.422</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>          <td> 6702.3576</td> <td> 2559.975</td> <td>    2.618</td> <td> 0.009</td> <td> 1684.345</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>          <td> 7946.3676</td> <td> 2745.679</td> <td>    2.894</td> <td> 0.004</td> <td> 2564.341</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>           <td> 3362.1599</td> <td> 1873.562</td> <td>    1.795</td> <td> 0.073</td> <td> -310.360</td> <td> 7034.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>          <td> 2844.3078</td> <td> 2398.032</td> <td>    1.186</td> <td> 0.236</td> <td>-1856.268</td> <td> 7544.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>           <td> 2336.6983</td> <td> 1906.538</td> <td>    1.226</td> <td> 0.220</td> <td>-1400.460</td> <td> 6073.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>           <td>-2063.6147</td> <td> 1916.954</td> <td>   -1.077</td> <td> 0.282</td> <td>-5821.190</td> <td> 1693.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>          <td> 1691.7022</td> <td> 2586.023</td> <td>    0.654</td> <td> 0.513</td> <td>-3377.369</td> <td> 6760.774</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>          <td> 6047.0934</td> <td> 3496.398</td> <td>    1.730</td> <td> 0.084</td> <td> -806.477</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>          <td>-1.013e+04</td> <td> 3024.694</td> <td>   -3.348</td> <td> 0.001</td> <td>-1.61e+04</td> <td>-4196.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>           <td>-1937.1929</td> <td> 2238.879</td> <td>   -0.865</td> <td> 0.387</td> <td>-6325.799</td> <td> 2451.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>          <td> 4789.3911</td> <td> 2416.382</td> <td>    1.982</td> <td> 0.047</td> <td>   52.846</td> <td> 9525.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>          <td>-4685.8260</td> <td> 2401.465</td> <td>   -1.951</td> <td> 0.051</td> <td>-9393.130</td> <td>   21.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>          <td> -786.4427</td> <td> 2397.801</td> <td>   -0.328</td> <td> 0.743</td> <td>-5486.564</td> <td> 3913.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>          <td>-7785.3253</td> <td> 2723.311</td> <td>   -2.859</td> <td> 0.004</td> <td>-1.31e+04</td> <td>-2447.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>           <td> 2800.1178</td> <td> 4103.530</td> <td>    0.682</td> <td> 0.495</td> <td>-5243.540</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>          <td> 5546.5657</td> <td> 2688.525</td> <td>    2.063</td> <td> 0.039</td> <td>  276.571</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>          <td>-1770.3156</td> <td> 8138.760</td> <td>   -0.218</td> <td> 0.828</td> <td>-1.77e+04</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>           <td> 5863.8852</td> <td> 2799.345</td> <td>    2.095</td> <td> 0.036</td> <td>  376.665</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>           <td> 5080.1273</td> <td> 2579.898</td> <td>    1.969</td> <td> 0.049</td> <td>   23.063</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>          <td> 4556.2190</td> <td> 4744.669</td> <td>    0.960</td> <td> 0.337</td> <td>-4744.188</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>          <td> 4844.4134</td> <td> 2598.074</td> <td>    1.865</td> <td> 0.062</td> <td> -248.281</td> <td> 9937.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>           <td> 5418.8534</td> <td> 2084.683</td> <td>    2.599</td> <td> 0.009</td> <td> 1332.499</td> <td> 9505.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>          <td> 2383.3239</td> <td> 2481.831</td> <td>    0.960</td> <td> 0.337</td> <td>-2481.512</td> <td> 7248.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>          <td>-3644.3899</td> <td> 2654.559</td> <td>   -1.373</td> <td> 0.170</td> <td>-8847.804</td> <td> 1559.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>           <td>  389.3899</td> <td> 1865.507</td> <td>    0.209</td> <td> 0.835</td> <td>-3267.341</td> <td> 4046.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>          <td>-9981.6539</td> <td> 3067.375</td> <td>   -3.254</td> <td> 0.001</td> <td> -1.6e+04</td> <td>-3969.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>           <td> 5423.0598</td> <td> 2081.390</td> <td>    2.605</td> <td> 0.009</td> <td> 1343.159</td> <td> 9502.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>          <td>-1.002e+04</td> <td> 3171.947</td> <td>   -3.158</td> <td> 0.002</td> <td>-1.62e+04</td> <td>-3798.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>          <td>-1616.3195</td> <td> 2448.562</td> <td>   -0.660</td> <td> 0.509</td> <td>-6415.941</td> <td> 3183.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>          <td> 3099.4708</td> <td> 2931.098</td> <td>    1.057</td> <td> 0.290</td> <td>-2646.009</td> <td> 8844.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>           <td>-2.374e+04</td> <td> 4109.964</td> <td>   -5.777</td> <td> 0.000</td> <td>-3.18e+04</td> <td>-1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>           <td>-2831.8659</td> <td> 1933.262</td> <td>   -1.465</td> <td> 0.143</td> <td>-6621.408</td> <td>  957.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>           <td> 4666.7591</td> <td> 2105.899</td> <td>    2.216</td> <td> 0.027</td> <td>  538.818</td> <td> 8794.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>           <td> 5439.1842</td> <td> 2095.574</td> <td>    2.596</td> <td> 0.009</td> <td> 1331.481</td> <td> 9546.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>          <td> 1729.2905</td> <td> 2717.624</td> <td>    0.636</td> <td> 0.525</td> <td>-3597.742</td> <td> 7056.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>           <td>-4807.0435</td> <td> 2132.455</td> <td>   -2.254</td> <td> 0.024</td> <td>-8987.040</td> <td> -627.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>           <td> 2565.6965</td> <td> 1911.232</td> <td>    1.342</td> <td> 0.179</td> <td>-1180.662</td> <td> 6312.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>           <td> -126.0121</td> <td> 1866.961</td> <td>   -0.067</td> <td> 0.946</td> <td>-3785.592</td> <td> 3533.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>          <td>-1.629e+04</td> <td> 3721.600</td> <td>   -4.378</td> <td> 0.000</td> <td>-2.36e+04</td> <td>-8997.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>          <td>-7581.4867</td> <td> 3293.674</td> <td>   -2.302</td> <td> 0.021</td> <td> -1.4e+04</td> <td>-1125.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>           <td> -464.6488</td> <td> 1875.442</td> <td>   -0.248</td> <td> 0.804</td> <td>-4140.853</td> <td> 3211.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>           <td> 3893.3420</td> <td> 1976.949</td> <td>    1.969</td> <td> 0.049</td> <td>   18.165</td> <td> 7768.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>           <td> 4451.3397</td> <td> 1925.366</td> <td>    2.312</td> <td> 0.021</td> <td>  677.276</td> <td> 8225.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>          <td>-2151.0226</td> <td> 2719.089</td> <td>   -0.791</td> <td> 0.429</td> <td>-7480.928</td> <td> 3178.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>          <td> -843.8305</td> <td> 2568.706</td> <td>   -0.329</td> <td> 0.743</td> <td>-5878.957</td> <td> 4191.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>           <td>-1.168e+04</td> <td> 2193.646</td> <td>   -5.324</td> <td> 0.000</td> <td> -1.6e+04</td> <td>-7379.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>           <td>-1955.1330</td> <td> 1955.560</td> <td>   -1.000</td> <td> 0.317</td> <td>-5788.383</td> <td> 1878.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>          <td> 4595.4329</td> <td> 2705.329</td> <td>    1.699</td> <td> 0.089</td> <td> -707.500</td> <td> 9898.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>           <td> 1041.0113</td> <td> 1941.638</td> <td>    0.536</td> <td> 0.592</td> <td>-2764.949</td> <td> 4846.971</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>           <td> 4725.5694</td> <td> 2050.409</td> <td>    2.305</td> <td> 0.021</td> <td>  706.399</td> <td> 8744.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>           <td> 5488.4050</td> <td> 3280.892</td> <td>    1.673</td> <td> 0.094</td> <td> -942.735</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>           <td> 3365.6623</td> <td> 2041.335</td> <td>    1.649</td> <td> 0.099</td> <td> -635.722</td> <td> 7367.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>           <td> -1.18e+04</td> <td> 2769.890</td> <td>   -4.259</td> <td> 0.000</td> <td>-1.72e+04</td> <td>-6368.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>           <td> 6942.6066</td> <td> 2376.659</td> <td>    2.921</td> <td> 0.003</td> <td> 2283.926</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>           <td> -1.43e+04</td> <td> 3080.299</td> <td>   -4.641</td> <td> 0.000</td> <td>-2.03e+04</td> <td>-8258.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>          <td> 3005.6705</td> <td> 2628.581</td> <td>    1.143</td> <td> 0.253</td> <td>-2146.823</td> <td> 8158.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>           <td> 4245.4049</td> <td> 1999.313</td> <td>    2.123</td> <td> 0.034</td> <td>  326.390</td> <td> 8164.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>           <td>-8981.9585</td> <td> 2585.904</td> <td>   -3.473</td> <td> 0.001</td> <td>-1.41e+04</td> <td>-3913.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>          <td> 4046.7267</td> <td> 3720.335</td> <td>    1.088</td> <td> 0.277</td> <td>-3245.801</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>           <td> 1552.2041</td> <td> 4059.869</td> <td>    0.382</td> <td> 0.702</td> <td>-6405.872</td> <td> 9510.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>           <td> 6470.9768</td> <td> 2276.025</td> <td>    2.843</td> <td> 0.004</td> <td> 2009.557</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>           <td>-8710.6213</td> <td> 2438.976</td> <td>   -3.571</td> <td> 0.000</td> <td>-1.35e+04</td> <td>-3929.789</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>           <td> 2432.2612</td> <td> 2055.952</td> <td>    1.183</td> <td> 0.237</td> <td>-1597.775</td> <td> 6462.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>           <td>-4474.2130</td> <td> 2085.969</td> <td>   -2.145</td> <td> 0.032</td> <td>-8563.089</td> <td> -385.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>           <td>-2935.0794</td> <td> 1950.524</td> <td>   -1.505</td> <td> 0.132</td> <td>-6758.459</td> <td>  888.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>           <td> 5963.6939</td> <td> 2023.911</td> <td>    2.947</td> <td> 0.003</td> <td> 1996.463</td> <td> 9930.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>           <td> 4683.9283</td> <td> 2029.882</td> <td>    2.307</td> <td> 0.021</td> <td>  704.994</td> <td> 8662.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>           <td> -1.85e+04</td> <td> 4016.193</td> <td>   -4.605</td> <td> 0.000</td> <td>-2.64e+04</td> <td>-1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>           <td>-3736.9631</td> <td> 1998.914</td> <td>   -1.869</td> <td> 0.062</td> <td>-7655.196</td> <td>  181.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>           <td> -472.6100</td> <td> 1877.034</td> <td>   -0.252</td> <td> 0.801</td> <td>-4151.935</td> <td> 3206.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>           <td> 3232.7325</td> <td> 2534.569</td> <td>    1.275</td> <td> 0.202</td> <td>-1735.480</td> <td> 8200.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>           <td> 2346.9978</td> <td> 1881.936</td> <td>    1.247</td> <td> 0.212</td> <td>-1341.936</td> <td> 6035.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>           <td> -219.9822</td> <td> 1864.523</td> <td>   -0.118</td> <td> 0.906</td> <td>-3874.783</td> <td> 3434.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>           <td> 7760.6254</td> <td> 2309.286</td> <td>    3.361</td> <td> 0.001</td> <td> 3234.008</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>          <td> 2.919e+04</td> <td> 2748.072</td> <td>   10.623</td> <td> 0.000</td> <td> 2.38e+04</td> <td> 3.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>           <td>-3940.1136</td> <td> 2059.564</td> <td>   -1.913</td> <td> 0.056</td> <td>-7977.230</td> <td>   97.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>           <td> -145.5963</td> <td> 1899.605</td> <td>   -0.077</td> <td> 0.939</td> <td>-3869.164</td> <td> 3577.971</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>           <td>  466.2951</td> <td> 1877.045</td> <td>    0.248</td> <td> 0.804</td> <td>-3213.052</td> <td> 4145.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>          <td> 8036.7814</td> <td> 2947.759</td> <td>    2.726</td> <td> 0.006</td> <td> 2258.642</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>          <td> 7581.8510</td> <td> 2866.455</td> <td>    2.645</td> <td> 0.008</td> <td> 1963.082</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>           <td>-2669.4119</td> <td> 2038.405</td> <td>   -1.310</td> <td> 0.190</td> <td>-6665.053</td> <td> 1326.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>           <td> 2.067e+04</td> <td> 3209.509</td> <td>    6.439</td> <td> 0.000</td> <td> 1.44e+04</td> <td>  2.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>          <td>-8858.3795</td> <td> 2967.467</td> <td>   -2.985</td> <td> 0.003</td> <td>-1.47e+04</td> <td>-3041.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>           <td> 2734.4145</td> <td> 1926.414</td> <td>    1.419</td> <td> 0.156</td> <td>-1041.703</td> <td> 6510.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>           <td> 3.583e+04</td> <td> 2612.940</td> <td>   13.712</td> <td> 0.000</td> <td> 3.07e+04</td> <td>  4.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>           <td> 7391.7326</td> <td> 2317.607</td> <td>    3.189</td> <td> 0.001</td> <td> 2848.806</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>           <td>-9159.3909</td> <td> 2082.316</td> <td>   -4.399</td> <td> 0.000</td> <td>-1.32e+04</td> <td>-5077.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>           <td>   55.8354</td> <td> 1864.970</td> <td>    0.030</td> <td> 0.976</td> <td>-3599.841</td> <td> 3711.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>          <td> 7774.0320</td> <td> 2919.055</td> <td>    2.663</td> <td> 0.008</td> <td> 2052.158</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>           <td> -838.6605</td> <td> 5742.865</td> <td>   -0.146</td> <td> 0.884</td> <td>-1.21e+04</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>           <td> 7228.9116</td> <td> 2408.790</td> <td>    3.001</td> <td> 0.003</td> <td> 2507.250</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>          <td>  581.7322</td> <td> 2640.211</td> <td>    0.220</td> <td> 0.826</td> <td>-4593.557</td> <td> 5757.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>           <td>   24.2625</td> <td> 1867.099</td> <td>    0.013</td> <td> 0.990</td> <td>-3635.588</td> <td> 3684.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>          <td> 2989.0819</td> <td> 2627.929</td> <td>    1.137</td> <td> 0.255</td> <td>-2162.132</td> <td> 8140.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>           <td>-4146.3035</td> <td> 2030.371</td> <td>   -2.042</td> <td> 0.041</td> <td>-8126.196</td> <td> -166.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>          <td> -501.2126</td> <td> 2571.563</td> <td>   -0.195</td> <td> 0.845</td> <td>-5541.940</td> <td> 4539.515</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>           <td>-1.987e+04</td> <td> 2336.845</td> <td>   -8.503</td> <td> 0.000</td> <td>-2.44e+04</td> <td>-1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>           <td>-3811.1880</td> <td> 1872.008</td> <td>   -2.036</td> <td> 0.042</td> <td>-7480.661</td> <td> -141.715</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>           <td> 8023.2436</td> <td> 3569.229</td> <td>    2.248</td> <td> 0.025</td> <td> 1026.911</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>           <td>-5957.3664</td> <td> 2297.220</td> <td>   -2.593</td> <td> 0.010</td> <td>-1.05e+04</td> <td>-1454.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>          <td> -776.8606</td> <td> 2705.056</td> <td>   -0.287</td> <td> 0.774</td> <td>-6079.258</td> <td> 4525.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>          <td> 5798.8798</td> <td> 2921.359</td> <td>    1.985</td> <td> 0.047</td> <td>   72.490</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>           <td> 1961.8790</td> <td> 4085.517</td> <td>    0.480</td> <td> 0.631</td> <td>-6046.471</td> <td> 9970.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>           <td>-5763.4410</td> <td> 2366.706</td> <td>   -2.435</td> <td> 0.015</td> <td>-1.04e+04</td> <td>-1124.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>          <td>-1.781e+04</td> <td> 4886.318</td> <td>   -3.644</td> <td> 0.000</td> <td>-2.74e+04</td> <td>-8227.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>           <td> 5753.9197</td> <td> 2112.482</td> <td>    2.724</td> <td> 0.006</td> <td> 1613.075</td> <td> 9894.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>           <td>-9773.2623</td> <td> 2597.402</td> <td>   -3.763</td> <td> 0.000</td> <td>-1.49e+04</td> <td>-4681.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>           <td> 1.911e+04</td> <td> 2130.281</td> <td>    8.970</td> <td> 0.000</td> <td> 1.49e+04</td> <td> 2.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>           <td> 9215.4939</td> <td> 2324.978</td> <td>    3.964</td> <td> 0.000</td> <td> 4658.117</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>           <td>-6328.3637</td> <td> 2156.028</td> <td>   -2.935</td> <td> 0.003</td> <td>-1.06e+04</td> <td>-2102.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>           <td>  871.9388</td> <td> 1867.170</td> <td>    0.467</td> <td> 0.641</td> <td>-2788.051</td> <td> 4531.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>           <td> 5063.4747</td> <td> 3756.950</td> <td>    1.348</td> <td> 0.178</td> <td>-2300.824</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>          <td> -406.9425</td> <td> 2870.067</td> <td>   -0.142</td> <td> 0.887</td> <td>-6032.791</td> <td> 5218.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>          <td>-2377.2485</td> <td> 3004.914</td> <td>   -0.791</td> <td> 0.429</td> <td>-8267.421</td> <td> 3512.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>           <td> 6137.1445</td> <td> 2146.212</td> <td>    2.860</td> <td> 0.004</td> <td> 1930.181</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>          <td> 5750.0595</td> <td> 3249.398</td> <td>    1.770</td> <td> 0.077</td> <td> -619.346</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>           <td>-2533.2457</td> <td> 2035.151</td> <td>   -1.245</td> <td> 0.213</td> <td>-6522.508</td> <td> 1456.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>           <td> -2.31e+04</td> <td> 3703.554</td> <td>   -6.236</td> <td> 0.000</td> <td>-3.04e+04</td> <td>-1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>           <td> 3084.6834</td> <td> 2165.915</td> <td>    1.424</td> <td> 0.154</td> <td>-1160.901</td> <td> 7330.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>          <td> 1564.7264</td> <td> 2879.122</td> <td>    0.543</td> <td> 0.587</td> <td>-4078.872</td> <td> 7208.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>           <td> 3667.7789</td> <td> 1981.681</td> <td>    1.851</td> <td> 0.064</td> <td> -216.674</td> <td> 7552.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>          <td> -519.5598</td> <td> 8121.496</td> <td>   -0.064</td> <td> 0.949</td> <td>-1.64e+04</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>          <td> 5893.9281</td> <td> 2883.611</td> <td>    2.044</td> <td> 0.041</td> <td>  241.530</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>           <td>-1929.2007</td> <td> 1955.004</td> <td>   -0.987</td> <td> 0.324</td> <td>-5761.361</td> <td> 1902.960</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>           <td> 6662.2041</td> <td> 3169.660</td> <td>    2.102</td> <td> 0.036</td> <td>  449.099</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>          <td> 5022.2105</td> <td> 4762.281</td> <td>    1.055</td> <td> 0.292</td> <td>-4312.720</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>           <td>-1.196e+04</td> <td> 2180.644</td> <td>   -5.485</td> <td> 0.000</td> <td>-1.62e+04</td> <td>-7687.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>           <td> 5107.4417</td> <td> 2062.228</td> <td>    2.477</td> <td> 0.013</td> <td> 1065.104</td> <td> 9149.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>           <td>-1729.5345</td> <td> 2589.240</td> <td>   -0.668</td> <td> 0.504</td> <td>-6804.912</td> <td> 3345.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>          <td> 1742.7951</td> <td> 3088.970</td> <td>    0.564</td> <td> 0.573</td> <td>-4312.143</td> <td> 7797.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>           <td>-2057.6351</td> <td> 1913.398</td> <td>   -1.075</td> <td> 0.282</td> <td>-5808.239</td> <td> 1692.969</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>           <td>-1671.0046</td> <td> 1878.124</td> <td>   -0.890</td> <td> 0.374</td> <td>-5352.467</td> <td> 2010.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>           <td> 7441.0482</td> <td> 2274.848</td> <td>    3.271</td> <td> 0.001</td> <td> 2981.936</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>           <td> 6347.2392</td> <td> 1932.433</td> <td>    3.285</td> <td> 0.001</td> <td> 2559.322</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>           <td> 7041.1013</td> <td> 2249.918</td> <td>    3.129</td> <td> 0.002</td> <td> 2630.856</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>           <td> 3473.5093</td> <td> 1967.063</td> <td>    1.766</td> <td> 0.077</td> <td> -382.290</td> <td> 7329.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>           <td>-8494.4128</td> <td> 2430.898</td> <td>   -3.494</td> <td> 0.000</td> <td>-1.33e+04</td> <td>-3729.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>           <td> 5804.6405</td> <td> 2119.524</td> <td>    2.739</td> <td> 0.006</td> <td> 1649.990</td> <td> 9959.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>           <td>-1.835e+04</td> <td> 3254.017</td> <td>   -5.640</td> <td> 0.000</td> <td>-2.47e+04</td> <td> -1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>          <td>-1.353e+04</td> <td> 3717.342</td> <td>   -3.641</td> <td> 0.000</td> <td>-2.08e+04</td> <td>-6246.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>          <td> 2415.7030</td> <td> 1923.666</td> <td>    1.256</td> <td> 0.209</td> <td>-1355.028</td> <td> 6186.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>          <td>-3268.2963</td> <td> 3086.129</td> <td>   -1.059</td> <td> 0.290</td> <td>-9317.665</td> <td> 2781.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>           <td>-1158.8006</td> <td> 2113.251</td> <td>   -0.548</td> <td> 0.583</td> <td>-5301.154</td> <td> 2983.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>           <td> 3246.1953</td> <td> 2742.116</td> <td>    1.184</td> <td> 0.237</td> <td>-2128.846</td> <td> 8621.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>           <td>-2440.2332</td> <td> 1926.409</td> <td>   -1.267</td> <td> 0.205</td> <td>-6216.342</td> <td> 1335.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>          <td>-1.796e+04</td> <td> 4127.779</td> <td>   -4.351</td> <td> 0.000</td> <td>-2.61e+04</td> <td>-9869.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>           <td>  116.3507</td> <td> 2708.210</td> <td>    0.043</td> <td> 0.966</td> <td>-5192.230</td> <td> 5424.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>           <td> 6504.8671</td> <td> 2621.410</td> <td>    2.481</td> <td> 0.013</td> <td> 1366.431</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>           <td> 2359.5234</td> <td> 1899.256</td> <td>    1.242</td> <td> 0.214</td> <td>-1363.360</td> <td> 6082.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>           <td>-3349.3851</td> <td> 2905.402</td> <td>   -1.153</td> <td> 0.249</td> <td>-9044.496</td> <td> 2345.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>           <td>-2023.7214</td> <td> 1902.975</td> <td>   -1.063</td> <td> 0.288</td> <td>-5753.895</td> <td> 1706.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>           <td> 4342.1583</td> <td> 2027.537</td> <td>    2.142</td> <td> 0.032</td> <td>  367.821</td> <td> 8316.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>           <td>-9953.4737</td> <td> 2672.549</td> <td>   -3.724</td> <td> 0.000</td> <td>-1.52e+04</td> <td>-4714.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>           <td> 1543.4197</td> <td> 2034.873</td> <td>    0.758</td> <td> 0.448</td> <td>-2445.299</td> <td> 5532.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>           <td> 7455.6922</td> <td> 2214.322</td> <td>    3.367</td> <td> 0.001</td> <td> 3115.222</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>           <td> 3128.6646</td> <td> 1975.593</td> <td>    1.584</td> <td> 0.113</td> <td> -743.854</td> <td> 7001.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>           <td>-6195.1370</td> <td> 3021.612</td> <td>   -2.050</td> <td> 0.040</td> <td>-1.21e+04</td> <td> -272.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>           <td>  221.5773</td> <td> 1988.875</td> <td>    0.111</td> <td> 0.911</td> <td>-3676.976</td> <td> 4120.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>           <td> 2571.3322</td> <td> 2193.116</td> <td>    1.172</td> <td> 0.241</td> <td>-1727.571</td> <td> 6870.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>           <td>-1262.3700</td> <td> 3629.311</td> <td>   -0.348</td> <td> 0.728</td> <td>-8376.473</td> <td> 5851.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>           <td> 4820.8076</td> <td> 1938.340</td> <td>    2.487</td> <td> 0.013</td> <td> 1021.313</td> <td> 8620.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>           <td>   50.8772</td> <td> 1864.582</td> <td>    0.027</td> <td> 0.978</td> <td>-3604.039</td> <td> 3705.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>           <td> 5.164e+04</td> <td> 2009.846</td> <td>   25.695</td> <td> 0.000</td> <td> 4.77e+04</td> <td> 5.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>           <td> 5528.8925</td> <td> 2513.145</td> <td>    2.200</td> <td> 0.028</td> <td>  602.675</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>           <td> 2613.5072</td> <td> 1914.202</td> <td>    1.365</td> <td> 0.172</td> <td>-1138.673</td> <td> 6365.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>           <td> 5740.7299</td> <td> 1904.419</td> <td>    3.014</td> <td> 0.003</td> <td> 2007.725</td> <td> 9473.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>           <td> 6033.3161</td> <td> 2242.532</td> <td>    2.690</td> <td> 0.007</td> <td> 1637.549</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>           <td>-3341.6269</td> <td> 2121.164</td> <td>   -1.575</td> <td> 0.115</td> <td>-7499.490</td> <td>  816.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>           <td> 5746.5719</td> <td> 2221.989</td> <td>    2.586</td> <td> 0.010</td> <td> 1391.074</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>           <td> -877.8615</td> <td> 2050.458</td> <td>   -0.428</td> <td> 0.669</td> <td>-4897.129</td> <td> 3141.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>           <td> 4639.7278</td> <td> 2100.036</td> <td>    2.209</td> <td> 0.027</td> <td>  523.278</td> <td> 8756.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>           <td>  752.8101</td> <td> 1867.693</td> <td>    0.403</td> <td> 0.687</td> <td>-2908.206</td> <td> 4413.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>           <td>-1.726e+04</td> <td> 2670.598</td> <td>   -6.462</td> <td> 0.000</td> <td>-2.25e+04</td> <td> -1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>           <td>-6278.6048</td> <td> 2254.498</td> <td>   -2.785</td> <td> 0.005</td> <td>-1.07e+04</td> <td>-1859.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>           <td> 2659.0910</td> <td> 2474.101</td> <td>    1.075</td> <td> 0.283</td> <td>-2190.594</td> <td> 7508.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>           <td> -619.6711</td> <td> 1876.424</td> <td>   -0.330</td> <td> 0.741</td> <td>-4297.801</td> <td> 3058.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>           <td>-8834.0753</td> <td> 2554.688</td> <td>   -3.458</td> <td> 0.001</td> <td>-1.38e+04</td> <td>-3826.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>           <td> -479.3749</td> <td> 1866.770</td> <td>   -0.257</td> <td> 0.797</td> <td>-4138.580</td> <td> 3179.831</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>           <td>-2477.5731</td> <td> 2731.185</td> <td>   -0.907</td> <td> 0.364</td> <td>-7831.188</td> <td> 2876.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>           <td>-5156.3375</td> <td> 2034.291</td> <td>   -2.535</td> <td> 0.011</td> <td>-9143.914</td> <td>-1168.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>           <td> 2706.2668</td> <td> 1879.355</td> <td>    1.440</td> <td> 0.150</td> <td> -977.607</td> <td> 6390.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>           <td> 5468.6194</td> <td> 4143.995</td> <td>    1.320</td> <td> 0.187</td> <td>-2654.357</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>           <td>-1666.0673</td> <td> 1876.899</td> <td>   -0.888</td> <td> 0.375</td> <td>-5345.128</td> <td> 2012.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>           <td> 8495.2292</td> <td> 2361.801</td> <td>    3.597</td> <td> 0.000</td> <td> 3865.673</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>           <td> 3211.5327</td> <td> 2025.667</td> <td>    1.585</td> <td> 0.113</td> <td> -759.140</td> <td> 7182.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>           <td> 5842.5102</td> <td> 2170.677</td> <td>    2.692</td> <td> 0.007</td> <td> 1587.592</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>           <td>-5569.8963</td> <td> 2126.935</td> <td>   -2.619</td> <td> 0.009</td> <td>-9739.072</td> <td>-1400.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>           <td>-1.393e+04</td> <td> 2714.286</td> <td>   -5.134</td> <td> 0.000</td> <td>-1.93e+04</td> <td>-8613.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>           <td>-8333.9959</td> <td> 2446.351</td> <td>   -3.407</td> <td> 0.001</td> <td>-1.31e+04</td> <td>-3538.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>           <td>-8281.2919</td> <td> 2120.677</td> <td>   -3.905</td> <td> 0.000</td> <td>-1.24e+04</td> <td>-4124.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>           <td>-9453.1677</td> <td> 2966.849</td> <td>   -3.186</td> <td> 0.001</td> <td>-1.53e+04</td> <td>-3637.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>           <td>-6582.6545</td> <td> 2223.313</td> <td>   -2.961</td> <td> 0.003</td> <td>-1.09e+04</td> <td>-2224.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>           <td> 1112.8960</td> <td> 1976.312</td> <td>    0.563</td> <td> 0.573</td> <td>-2761.032</td> <td> 4986.824</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>           <td> 3406.6073</td> <td> 2407.950</td> <td>    1.415</td> <td> 0.157</td> <td>-1313.409</td> <td> 8126.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>           <td>-7465.5064</td> <td> 2305.977</td> <td>   -3.237</td> <td> 0.001</td> <td> -1.2e+04</td> <td>-2945.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>           <td> 3988.5817</td> <td> 1984.616</td> <td>    2.010</td> <td> 0.044</td> <td>   98.377</td> <td> 7878.786</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>           <td> 1381.6824</td> <td> 2044.029</td> <td>    0.676</td> <td> 0.499</td> <td>-2624.982</td> <td> 5388.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>           <td> 5209.9044</td> <td> 2143.230</td> <td>    2.431</td> <td> 0.015</td> <td> 1008.787</td> <td> 9411.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>           <td>-4218.9592</td> <td> 1882.486</td> <td>   -2.241</td> <td> 0.025</td> <td>-7908.971</td> <td> -528.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>           <td>-9770.0399</td> <td> 3133.196</td> <td>   -3.118</td> <td> 0.002</td> <td>-1.59e+04</td> <td>-3628.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>           <td> 2927.0314</td> <td> 1930.956</td> <td>    1.516</td> <td> 0.130</td> <td> -857.991</td> <td> 6712.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>           <td> 4038.4655</td> <td> 1990.440</td> <td>    2.029</td> <td> 0.042</td> <td>  136.844</td> <td> 7940.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>           <td> 6138.5218</td> <td> 3219.795</td> <td>    1.906</td> <td> 0.057</td> <td> -172.857</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>           <td>-9596.9977</td> <td> 2507.910</td> <td>   -3.827</td> <td> 0.000</td> <td>-1.45e+04</td> <td>-4681.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>           <td> 5226.1949</td> <td> 2162.918</td> <td>    2.416</td> <td> 0.016</td> <td>  986.486</td> <td> 9465.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>           <td> 6973.0171</td> <td> 2206.800</td> <td>    3.160</td> <td> 0.002</td> <td> 2647.291</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>           <td> 1979.1902</td> <td> 1991.465</td> <td>    0.994</td> <td> 0.320</td> <td>-1924.440</td> <td> 5882.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>           <td> 1.202e+04</td> <td> 1932.033</td> <td>    6.220</td> <td> 0.000</td> <td> 8230.312</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>           <td>-5921.5990</td> <td> 2070.569</td> <td>   -2.860</td> <td> 0.004</td> <td>-9980.288</td> <td>-1862.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>           <td> 5115.5995</td> <td> 2046.651</td> <td>    2.499</td> <td> 0.012</td> <td> 1103.795</td> <td> 9127.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>           <td> 3206.5487</td> <td> 1921.949</td> <td>    1.668</td> <td> 0.095</td> <td> -560.817</td> <td> 6973.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>           <td> 3051.0613</td> <td> 1892.804</td> <td>    1.612</td> <td> 0.107</td> <td> -659.176</td> <td> 6761.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>           <td>-1.628e+04</td> <td> 2856.127</td> <td>   -5.701</td> <td> 0.000</td> <td>-2.19e+04</td> <td>-1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>           <td> 1.007e+04</td> <td> 2332.242</td> <td>    4.319</td> <td> 0.000</td> <td> 5502.034</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>           <td> 3780.6434</td> <td> 3349.737</td> <td>    1.129</td> <td> 0.259</td> <td>-2785.444</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>           <td>-2.322e+04</td> <td> 3294.134</td> <td>   -7.048</td> <td> 0.000</td> <td>-2.97e+04</td> <td>-1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>           <td> 1998.9224</td> <td> 2477.718</td> <td>    0.807</td> <td> 0.420</td> <td>-2857.851</td> <td> 6855.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>           <td>-3525.5756</td> <td> 1983.603</td> <td>   -1.777</td> <td> 0.076</td> <td>-7413.796</td> <td>  362.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>           <td> 5914.4417</td> <td> 2743.566</td> <td>    2.156</td> <td> 0.031</td> <td>  536.558</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>           <td> 2560.6378</td> <td> 2470.849</td> <td>    1.036</td> <td> 0.300</td> <td>-2282.673</td> <td> 7403.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>           <td> 5907.9693</td> <td> 2117.104</td> <td>    2.791</td> <td> 0.005</td> <td> 1758.063</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>           <td> 1285.6791</td> <td> 2336.059</td> <td>    0.550</td> <td> 0.582</td> <td>-3293.417</td> <td> 5864.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>           <td>-1.585e+04</td> <td> 3474.430</td> <td>   -4.562</td> <td> 0.000</td> <td>-2.27e+04</td> <td>-9038.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>           <td>  808.3674</td> <td> 1862.423</td> <td>    0.434</td> <td> 0.664</td> <td>-2842.317</td> <td> 4459.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>           <td>-3607.1406</td> <td> 1989.203</td> <td>   -1.813</td> <td> 0.070</td> <td>-7506.338</td> <td>  292.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>           <td> 1403.0178</td> <td> 1874.897</td> <td>    0.748</td> <td> 0.454</td> <td>-2272.119</td> <td> 5078.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>           <td> 5148.4199</td> <td> 2065.112</td> <td>    2.493</td> <td> 0.013</td> <td> 1100.428</td> <td> 9196.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>           <td> 3631.2777</td> <td> 2168.770</td> <td>    1.674</td> <td> 0.094</td> <td> -619.902</td> <td> 7882.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>           <td>-2806.8085</td> <td> 2047.358</td> <td>   -1.371</td> <td> 0.170</td> <td>-6819.999</td> <td> 1206.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>           <td>-4667.3963</td> <td> 2422.014</td> <td>   -1.927</td> <td> 0.054</td> <td>-9414.980</td> <td>   80.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>           <td> -608.2474</td> <td> 3314.435</td> <td>   -0.184</td> <td> 0.854</td> <td>-7105.138</td> <td> 5888.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>           <td>-1599.1975</td> <td> 1939.399</td> <td>   -0.825</td> <td> 0.410</td> <td>-5400.769</td> <td> 2202.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>           <td>-3398.3487</td> <td> 1976.182</td> <td>   -1.720</td> <td> 0.086</td> <td>-7272.021</td> <td>  475.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>           <td> 4653.8189</td> <td> 2130.264</td> <td>    2.185</td> <td> 0.029</td> <td>  478.117</td> <td> 8829.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>           <td> 4454.0494</td> <td> 2011.609</td> <td>    2.214</td> <td> 0.027</td> <td>  510.933</td> <td> 8397.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>           <td>-6248.1810</td> <td> 2401.134</td> <td>   -2.602</td> <td> 0.009</td> <td> -1.1e+04</td> <td>-1541.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>           <td>-1.049e+04</td> <td> 2418.176</td> <td>   -4.339</td> <td> 0.000</td> <td>-1.52e+04</td> <td>-5752.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>           <td>-3048.1121</td> <td> 1955.438</td> <td>   -1.559</td> <td> 0.119</td> <td>-6881.122</td> <td>  784.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>           <td> 3101.4769</td> <td> 8122.033</td> <td>    0.382</td> <td> 0.703</td> <td>-1.28e+04</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>           <td>  397.0851</td> <td> 1971.862</td> <td>    0.201</td> <td> 0.840</td> <td>-3468.119</td> <td> 4262.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>           <td> 8237.8741</td> <td> 2353.238</td> <td>    3.501</td> <td> 0.000</td> <td> 3625.102</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>           <td> 5651.0424</td> <td> 2106.274</td> <td>    2.683</td> <td> 0.007</td> <td> 1522.365</td> <td> 9779.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>           <td>  172.3713</td> <td> 1865.018</td> <td>    0.092</td> <td> 0.926</td> <td>-3483.401</td> <td> 3828.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>           <td>-1591.7797</td> <td> 1994.959</td> <td>   -0.798</td> <td> 0.425</td> <td>-5502.260</td> <td> 2318.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>           <td> 5341.7990</td> <td> 2081.956</td> <td>    2.566</td> <td> 0.010</td> <td> 1260.789</td> <td> 9422.809</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>           <td> 1066.2183</td> <td> 1867.881</td> <td>    0.571</td> <td> 0.568</td> <td>-2595.166</td> <td> 4727.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>           <td>-8825.8179</td> <td> 2440.966</td> <td>   -3.616</td> <td> 0.000</td> <td>-1.36e+04</td> <td>-4041.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>           <td> 4675.2103</td> <td> 2070.222</td> <td>    2.258</td> <td> 0.024</td> <td>  617.203</td> <td> 8733.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>           <td>  997.5575</td> <td> 1888.424</td> <td>    0.528</td> <td> 0.597</td> <td>-2704.093</td> <td> 4699.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>           <td> 6203.3110</td> <td> 2153.731</td> <td>    2.880</td> <td> 0.004</td> <td> 1981.610</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>           <td> 4722.3159</td> <td> 2136.453</td> <td>    2.210</td> <td> 0.027</td> <td>  534.482</td> <td> 8910.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>           <td> 4130.2512</td> <td> 1994.054</td> <td>    2.071</td> <td> 0.038</td> <td>  221.545</td> <td> 8038.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>           <td> 2675.3897</td> <td> 1965.555</td> <td>    1.361</td> <td> 0.173</td> <td>-1177.452</td> <td> 6528.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>           <td>-1.191e+05</td> <td> 4233.271</td> <td>  -28.133</td> <td> 0.000</td> <td>-1.27e+05</td> <td>-1.11e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>           <td> 4091.0901</td> <td> 1925.916</td> <td>    2.124</td> <td> 0.034</td> <td>  315.947</td> <td> 7866.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>           <td>-2251.8864</td> <td> 1918.174</td> <td>   -1.174</td> <td> 0.240</td> <td>-6011.853</td> <td> 1508.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>           <td> 4257.5069</td> <td> 1996.202</td> <td>    2.133</td> <td> 0.033</td> <td>  344.592</td> <td> 8170.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>           <td>-3476.2071</td> <td> 1957.953</td> <td>   -1.775</td> <td> 0.076</td> <td>-7314.148</td> <td>  361.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>           <td>  542.3435</td> <td> 1882.604</td> <td>    0.288</td> <td> 0.773</td> <td>-3147.900</td> <td> 4232.587</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>           <td>-1.128e+04</td> <td> 3590.856</td> <td>   -3.143</td> <td> 0.002</td> <td>-1.83e+04</td> <td>-4246.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>           <td> 1.212e+04</td> <td> 2166.411</td> <td>    5.596</td> <td> 0.000</td> <td> 7876.786</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>           <td> 8210.1591</td> <td> 2352.873</td> <td>    3.489</td> <td> 0.000</td> <td> 3598.104</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>           <td>-1.448e+04</td> <td> 3024.558</td> <td>   -4.788</td> <td> 0.000</td> <td>-2.04e+04</td> <td>-8552.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>           <td> 3930.1478</td> <td> 1869.322</td> <td>    2.102</td> <td> 0.036</td> <td>  265.939</td> <td> 7594.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>           <td>-2780.8939</td> <td> 1938.519</td> <td>   -1.435</td> <td> 0.151</td> <td>-6580.740</td> <td> 1018.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>           <td> 2085.8418</td> <td> 1902.032</td> <td>    1.097</td> <td> 0.273</td> <td>-1642.484</td> <td> 5814.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>           <td> -616.7749</td> <td> 1867.760</td> <td>   -0.330</td> <td> 0.741</td> <td>-4277.921</td> <td> 3044.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>           <td>-1.729e+04</td> <td> 3031.667</td> <td>   -5.704</td> <td> 0.000</td> <td>-2.32e+04</td> <td>-1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>           <td> 2.379e+04</td> <td> 4241.993</td> <td>    5.608</td> <td> 0.000</td> <td> 1.55e+04</td> <td> 3.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>           <td> 4805.6677</td> <td> 2390.351</td> <td>    2.010</td> <td> 0.044</td> <td>  120.148</td> <td> 9491.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>           <td> 4759.8373</td> <td> 2387.440</td> <td>    1.994</td> <td> 0.046</td> <td>   80.025</td> <td> 9439.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>           <td>-1.659e+05</td> <td> 6360.642</td> <td>  -26.085</td> <td> 0.000</td> <td>-1.78e+05</td> <td>-1.53e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>           <td>-1074.7682</td> <td> 1864.101</td> <td>   -0.577</td> <td> 0.564</td> <td>-4728.742</td> <td> 2579.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>           <td> 6156.8224</td> <td> 2173.231</td> <td>    2.833</td> <td> 0.005</td> <td> 1896.897</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>           <td>-1063.4051</td> <td> 2121.551</td> <td>   -0.501</td> <td> 0.616</td> <td>-5222.027</td> <td> 3095.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>           <td>-3169.4565</td> <td> 1952.593</td> <td>   -1.623</td> <td> 0.105</td> <td>-6996.892</td> <td>  657.979</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>           <td>-9160.0809</td> <td> 1987.931</td> <td>   -4.608</td> <td> 0.000</td> <td>-1.31e+04</td> <td>-5263.377</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>           <td>  318.4028</td> <td> 2710.019</td> <td>    0.117</td> <td> 0.906</td> <td>-4993.722</td> <td> 5630.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>           <td> 6750.8497</td> <td> 2559.942</td> <td>    2.637</td> <td> 0.008</td> <td> 1732.902</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>           <td> 1.511e+04</td> <td> 2001.234</td> <td>    7.550</td> <td> 0.000</td> <td> 1.12e+04</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>           <td> 1640.8492</td> <td> 2271.830</td> <td>    0.722</td> <td> 0.470</td> <td>-2812.347</td> <td> 6094.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>           <td> 5217.9213</td> <td> 2052.345</td> <td>    2.542</td> <td> 0.011</td> <td> 1194.956</td> <td> 9240.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>           <td> 5800.0679</td> <td> 2167.718</td> <td>    2.676</td> <td> 0.007</td> <td> 1550.950</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>           <td> 5893.9436</td> <td> 2526.090</td> <td>    2.333</td> <td> 0.020</td> <td>  942.352</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>           <td>-9469.7480</td> <td> 2405.312</td> <td>   -3.937</td> <td> 0.000</td> <td>-1.42e+04</td> <td>-4754.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>           <td>-1.212e+04</td> <td> 2089.843</td> <td>   -5.800</td> <td> 0.000</td> <td>-1.62e+04</td> <td>-8024.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>           <td> 3671.2141</td> <td> 1960.977</td> <td>    1.872</td> <td> 0.061</td> <td> -172.654</td> <td> 7515.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>           <td>  205.2720</td> <td> 1864.926</td> <td>    0.110</td> <td> 0.912</td> <td>-3450.320</td> <td> 3860.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>           <td>    1.5661</td> <td> 1866.155</td> <td>    0.001</td> <td> 0.999</td> <td>-3656.434</td> <td> 3659.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>           <td>-8164.5373</td> <td> 2537.338</td> <td>   -3.218</td> <td> 0.001</td> <td>-1.31e+04</td> <td>-3190.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>           <td> 2988.8536</td> <td> 1924.135</td> <td>    1.553</td> <td> 0.120</td> <td> -782.799</td> <td> 6760.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>           <td> 6499.7649</td> <td> 2207.361</td> <td>    2.945</td> <td> 0.003</td> <td> 2172.939</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>           <td> 4154.1516</td> <td> 2030.818</td> <td>    2.046</td> <td> 0.041</td> <td>  173.383</td> <td> 8134.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>           <td> 8040.7845</td> <td> 2345.467</td> <td>    3.428</td> <td> 0.001</td> <td> 3443.247</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>           <td> -408.0736</td> <td> 2448.885</td> <td>   -0.167</td> <td> 0.868</td> <td>-5208.330</td> <td> 4392.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>           <td> 8930.2376</td> <td> 2386.371</td> <td>    3.742</td> <td> 0.000</td> <td> 4252.521</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>           <td>-1.613e+04</td> <td> 3387.599</td> <td>   -4.762</td> <td> 0.000</td> <td>-2.28e+04</td> <td>-9490.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>           <td> 1502.5059</td> <td> 1881.197</td> <td>    0.799</td> <td> 0.424</td> <td>-2184.979</td> <td> 5189.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>           <td> 5540.7848</td> <td> 2130.770</td> <td>    2.600</td> <td> 0.009</td> <td> 1364.091</td> <td> 9717.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>           <td> 4055.3226</td> <td> 2328.914</td> <td>    1.741</td> <td> 0.082</td> <td> -509.768</td> <td> 8620.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>           <td> 1115.9623</td> <td> 1872.844</td> <td>    0.596</td> <td> 0.551</td> <td>-2555.149</td> <td> 4787.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>           <td> 3706.3908</td> <td> 2059.949</td> <td>    1.799</td> <td> 0.072</td> <td> -331.480</td> <td> 7744.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>           <td> 5191.3374</td> <td> 1903.722</td> <td>    2.727</td> <td> 0.006</td> <td> 1459.698</td> <td> 8922.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>           <td> 5268.3797</td> <td> 2121.742</td> <td>    2.483</td> <td> 0.013</td> <td> 1109.384</td> <td> 9427.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>           <td> 4546.5266</td> <td> 2004.631</td> <td>    2.268</td> <td> 0.023</td> <td>  617.088</td> <td> 8475.965</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>           <td> 3827.8583</td> <td> 1945.265</td> <td>    1.968</td> <td> 0.049</td> <td>   14.788</td> <td> 7640.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>           <td>-1.016e+04</td> <td> 2687.164</td> <td>   -3.782</td> <td> 0.000</td> <td>-1.54e+04</td> <td>-4894.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>           <td> 3641.4163</td> <td> 2590.049</td> <td>    1.406</td> <td> 0.160</td> <td>-1435.547</td> <td> 8718.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>           <td> -2.49e+04</td> <td> 2945.544</td> <td>   -8.452</td> <td> 0.000</td> <td>-3.07e+04</td> <td>-1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>           <td> 7378.7637</td> <td> 2191.166</td> <td>    3.368</td> <td> 0.001</td> <td> 3083.684</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>           <td> 8260.7074</td> <td> 2387.777</td> <td>    3.460</td> <td> 0.001</td> <td> 3580.234</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>           <td> 3985.0621</td> <td> 1977.021</td> <td>    2.016</td> <td> 0.044</td> <td>  109.744</td> <td> 7860.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>           <td> 3716.4396</td> <td> 2034.262</td> <td>    1.827</td> <td> 0.068</td> <td> -271.082</td> <td> 7703.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>           <td> 4996.0601</td> <td> 2035.830</td> <td>    2.454</td> <td> 0.014</td> <td> 1005.466</td> <td> 8986.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>           <td>-2603.1282</td> <td> 1992.133</td> <td>   -1.307</td> <td> 0.191</td> <td>-6508.068</td> <td> 1301.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>           <td> 1519.0392</td> <td> 1881.855</td> <td>    0.807</td> <td> 0.420</td> <td>-2169.737</td> <td> 5207.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>           <td>-2.109e+04</td> <td> 2637.276</td> <td>   -7.995</td> <td> 0.000</td> <td>-2.63e+04</td> <td>-1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>           <td> 1364.7944</td> <td> 1907.846</td> <td>    0.715</td> <td> 0.474</td> <td>-2374.927</td> <td> 5104.516</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>           <td> 2067.7321</td> <td> 1992.452</td> <td>    1.038</td> <td> 0.299</td> <td>-1837.833</td> <td> 5973.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>           <td>-1864.7784</td> <td> 1886.734</td> <td>   -0.988</td> <td> 0.323</td> <td>-5563.116</td> <td> 1833.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>           <td>-4990.6884</td> <td> 2036.050</td> <td>   -2.451</td> <td> 0.014</td> <td>-8981.714</td> <td> -999.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>           <td> 1.741e+04</td> <td> 2880.320</td> <td>    6.046</td> <td> 0.000</td> <td> 1.18e+04</td> <td> 2.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>           <td>-1.003e+04</td> <td> 2718.706</td> <td>   -3.690</td> <td> 0.000</td> <td>-1.54e+04</td> <td>-4703.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>           <td> 2579.2198</td> <td> 2486.002</td> <td>    1.037</td> <td> 0.300</td> <td>-2293.792</td> <td> 7452.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>           <td>-6898.4988</td> <td> 2196.252</td> <td>   -3.141</td> <td> 0.002</td> <td>-1.12e+04</td> <td>-2593.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>           <td> 3501.6223</td> <td> 1953.934</td> <td>    1.792</td> <td> 0.073</td> <td> -328.440</td> <td> 7331.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>           <td> 7077.4692</td> <td> 2465.742</td> <td>    2.870</td> <td> 0.004</td> <td> 2244.170</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>           <td>-9772.4102</td> <td> 4154.307</td> <td>   -2.352</td> <td> 0.019</td> <td>-1.79e+04</td> <td>-1629.220</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>           <td> 5496.2272</td> <td> 1920.170</td> <td>    2.862</td> <td> 0.004</td> <td> 1732.348</td> <td> 9260.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>           <td>-1.406e+04</td> <td> 2549.252</td> <td>   -5.516</td> <td> 0.000</td> <td>-1.91e+04</td> <td>-9065.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>          <td> 3239.1093</td> <td> 4129.186</td> <td>    0.784</td> <td> 0.433</td> <td>-4854.840</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>           <td> 7640.1809</td> <td> 2207.786</td> <td>    3.461</td> <td> 0.001</td> <td> 3312.522</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>           <td> 2843.4365</td> <td> 1996.673</td> <td>    1.424</td> <td> 0.154</td> <td>-1070.402</td> <td> 6757.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>           <td>-8300.1663</td> <td> 2037.445</td> <td>   -4.074</td> <td> 0.000</td> <td>-1.23e+04</td> <td>-4306.407</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>           <td>-6360.6720</td> <td> 2216.257</td> <td>   -2.870</td> <td> 0.004</td> <td>-1.07e+04</td> <td>-2016.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>           <td>-6787.9531</td> <td> 2240.150</td> <td>   -3.030</td> <td> 0.002</td> <td>-1.12e+04</td> <td>-2396.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>          <td>-1.149e+04</td> <td> 3882.144</td> <td>   -2.960</td> <td> 0.003</td> <td>-1.91e+04</td> <td>-3883.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>           <td> -916.8590</td> <td> 2043.647</td> <td>   -0.449</td> <td> 0.654</td> <td>-4922.776</td> <td> 3089.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>           <td> 3378.2117</td> <td> 1939.055</td> <td>    1.742</td> <td> 0.082</td> <td> -422.686</td> <td> 7179.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>          <td>-1362.8843</td> <td> 4057.286</td> <td>   -0.336</td> <td> 0.737</td> <td>-9315.897</td> <td> 6590.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>           <td> 4778.5766</td> <td> 2033.773</td> <td>    2.350</td> <td> 0.019</td> <td>  792.016</td> <td> 8765.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>           <td> 2860.6555</td> <td> 1924.860</td> <td>    1.486</td> <td> 0.137</td> <td> -912.416</td> <td> 6633.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>           <td> 7726.5050</td> <td> 2345.597</td> <td>    3.294</td> <td> 0.001</td> <td> 3128.712</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>          <td> 3663.3054</td> <td> 4132.258</td> <td>    0.887</td> <td> 0.375</td> <td>-4436.666</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>           <td> 5957.1127</td> <td> 2391.462</td> <td>    2.491</td> <td> 0.013</td> <td> 1269.416</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>          <td>  568.0216</td> <td> 5686.343</td> <td>    0.100</td> <td> 0.920</td> <td>-1.06e+04</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>           <td> 9467.6831</td> <td> 2313.372</td> <td>    4.093</td> <td> 0.000</td> <td> 4933.056</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>           <td>-8787.1572</td> <td> 2112.679</td> <td>   -4.159</td> <td> 0.000</td> <td>-1.29e+04</td> <td>-4645.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>           <td> -908.5221</td> <td> 1881.992</td> <td>   -0.483</td> <td> 0.629</td> <td>-4597.565</td> <td> 2780.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>           <td> 1464.9049</td> <td> 1982.188</td> <td>    0.739</td> <td> 0.460</td> <td>-2420.541</td> <td> 5350.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>           <td>-2.216e+04</td> <td> 2856.979</td> <td>   -7.757</td> <td> 0.000</td> <td>-2.78e+04</td> <td>-1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>           <td> 4618.1251</td> <td> 3427.161</td> <td>    1.348</td> <td> 0.178</td> <td>-2099.729</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>           <td> 6492.4201</td> <td> 2181.697</td> <td>    2.976</td> <td> 0.003</td> <td> 2215.901</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>           <td>-9508.0014</td> <td> 2508.589</td> <td>   -3.790</td> <td> 0.000</td> <td>-1.44e+04</td> <td>-4590.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>           <td> 2480.1364</td> <td> 1910.620</td> <td>    1.298</td> <td> 0.194</td> <td>-1265.024</td> <td> 6225.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>           <td> 5776.3456</td> <td> 2247.337</td> <td>    2.570</td> <td> 0.010</td> <td> 1371.159</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>           <td>  1.01e+04</td> <td> 2040.587</td> <td>    4.948</td> <td> 0.000</td> <td> 6097.595</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>           <td> 5614.1876</td> <td> 2140.060</td> <td>    2.623</td> <td> 0.009</td> <td> 1419.284</td> <td> 9809.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>           <td> 8938.8647</td> <td> 5781.832</td> <td>    1.546</td> <td> 0.122</td> <td>-2394.569</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>           <td> 3710.1340</td> <td> 1955.122</td> <td>    1.898</td> <td> 0.058</td> <td> -122.257</td> <td> 7542.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>           <td> 1056.7945</td> <td> 1959.891</td> <td>    0.539</td> <td> 0.590</td> <td>-2784.945</td> <td> 4898.534</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>           <td> 7553.1609</td> <td> 2284.474</td> <td>    3.306</td> <td> 0.001</td> <td> 3075.181</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>           <td> 1492.3880</td> <td> 1988.851</td> <td>    0.750</td> <td> 0.453</td> <td>-2406.119</td> <td> 5390.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>           <td> 5001.3026</td> <td> 2068.519</td> <td>    2.418</td> <td> 0.016</td> <td>  946.633</td> <td> 9055.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>           <td> 7686.9981</td> <td> 2299.971</td> <td>    3.342</td> <td> 0.001</td> <td> 3178.640</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>           <td> -737.8230</td> <td> 2057.950</td> <td>   -0.359</td> <td> 0.720</td> <td>-4771.777</td> <td> 3296.131</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>           <td>-6962.4962</td> <td> 2261.952</td> <td>   -3.078</td> <td> 0.002</td> <td>-1.14e+04</td> <td>-2528.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>           <td> 6239.3979</td> <td> 2154.347</td> <td>    2.896</td> <td> 0.004</td> <td> 2016.490</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>           <td> 7548.7349</td> <td> 8225.317</td> <td>    0.918</td> <td> 0.359</td> <td>-8574.370</td> <td> 2.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>           <td> 1273.6530</td> <td> 1874.999</td> <td>    0.679</td> <td> 0.497</td> <td>-2401.682</td> <td> 4948.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>           <td> 5996.2601</td> <td> 2494.329</td> <td>    2.404</td> <td> 0.016</td> <td> 1106.925</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>           <td> 4138.6189</td> <td> 2205.290</td> <td>    1.877</td> <td> 0.061</td> <td> -184.148</td> <td> 8461.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>           <td> -1.46e+04</td> <td> 2501.366</td> <td>   -5.836</td> <td> 0.000</td> <td>-1.95e+04</td> <td>-9693.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>           <td> -909.3400</td> <td> 1922.830</td> <td>   -0.473</td> <td> 0.636</td> <td>-4678.433</td> <td> 2859.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>           <td> 5667.3057</td> <td> 2107.568</td> <td>    2.689</td> <td> 0.007</td> <td> 1536.092</td> <td> 9798.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>           <td> 5442.1657</td> <td> 2102.652</td> <td>    2.588</td> <td> 0.010</td> <td> 1320.589</td> <td> 9563.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>           <td> 3292.2371</td> <td> 2176.446</td> <td>    1.513</td> <td> 0.130</td> <td> -973.990</td> <td> 7558.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>           <td> 5851.9235</td> <td> 2332.408</td> <td>    2.509</td> <td> 0.012</td> <td> 1279.984</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>           <td>-1.029e+04</td> <td> 2789.761</td> <td>   -3.689</td> <td> 0.000</td> <td>-1.58e+04</td> <td>-4821.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>           <td> 3479.5408</td> <td> 1963.291</td> <td>    1.772</td> <td> 0.076</td> <td> -368.864</td> <td> 7327.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>           <td> 2775.7816</td> <td> 1913.330</td> <td>    1.451</td> <td> 0.147</td> <td> -974.691</td> <td> 6526.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>           <td> 3055.6252</td> <td> 4081.419</td> <td>    0.749</td> <td> 0.454</td> <td>-4944.692</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>           <td> 6919.6160</td> <td> 2188.499</td> <td>    3.162</td> <td> 0.002</td> <td> 2629.763</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>           <td> 1.006e+04</td> <td> 2161.155</td> <td>    4.655</td> <td> 0.000</td> <td> 5824.690</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>           <td>-7019.9476</td> <td> 2270.362</td> <td>   -3.092</td> <td> 0.002</td> <td>-1.15e+04</td> <td>-2569.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>           <td> 7170.2119</td> <td> 4326.943</td> <td>    1.657</td> <td> 0.098</td> <td>-1311.377</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>           <td> 5044.4407</td> <td> 2056.453</td> <td>    2.453</td> <td> 0.014</td> <td> 1013.422</td> <td> 9075.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>           <td> 7115.0126</td> <td> 2232.742</td> <td>    3.187</td> <td> 0.001</td> <td> 2738.436</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>           <td>-1.086e+04</td> <td> 2485.280</td> <td>   -4.368</td> <td> 0.000</td> <td>-1.57e+04</td> <td>-5983.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>           <td> 9142.6171</td> <td> 2428.250</td> <td>    3.765</td> <td> 0.000</td> <td> 4382.809</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>           <td>-1.663e+04</td> <td> 2635.587</td> <td>   -6.311</td> <td> 0.000</td> <td>-2.18e+04</td> <td>-1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>           <td> 6532.6165</td> <td> 2636.852</td> <td>    2.477</td> <td> 0.013</td> <td> 1363.912</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>           <td> 6239.7276</td> <td> 2168.069</td> <td>    2.878</td> <td> 0.004</td> <td> 1989.922</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>           <td> 3571.1369</td> <td> 1971.184</td> <td>    1.812</td> <td> 0.070</td> <td> -292.740</td> <td> 7435.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>           <td> 5691.0431</td> <td> 2087.521</td> <td>    2.726</td> <td> 0.006</td> <td> 1599.125</td> <td> 9782.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>           <td>-7051.5009</td> <td> 2278.308</td> <td>   -3.095</td> <td> 0.002</td> <td>-1.15e+04</td> <td>-2585.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>           <td> 6681.3227</td> <td> 2396.308</td> <td>    2.788</td> <td> 0.005</td> <td> 1984.126</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>           <td>-3738.7082</td> <td> 2708.085</td> <td>   -1.381</td> <td> 0.167</td> <td>-9047.043</td> <td> 1569.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>           <td>-3750.3918</td> <td> 2002.203</td> <td>   -1.873</td> <td> 0.061</td> <td>-7675.071</td> <td>  174.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>           <td>-2916.1307</td> <td> 1957.450</td> <td>   -1.490</td> <td> 0.136</td> <td>-6753.086</td> <td>  920.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>           <td> 5542.6486</td> <td> 2083.600</td> <td>    2.660</td> <td> 0.008</td> <td> 1458.416</td> <td> 9626.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>           <td> 1363.0136</td> <td> 2484.971</td> <td>    0.549</td> <td> 0.583</td> <td>-3507.977</td> <td> 6234.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>           <td> 3498.3989</td> <td> 1873.146</td> <td>    1.868</td> <td> 0.062</td> <td> -173.305</td> <td> 7170.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>           <td> 3721.4889</td> <td> 1871.454</td> <td>    1.989</td> <td> 0.047</td> <td>   53.101</td> <td> 7389.877</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>           <td>-2262.4082</td> <td> 2076.355</td> <td>   -1.090</td> <td> 0.276</td> <td>-6332.438</td> <td> 1807.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>           <td> 8409.7314</td> <td> 2307.397</td> <td>    3.645</td> <td> 0.000</td> <td> 3886.817</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>           <td> 8374.1123</td> <td> 3393.680</td> <td>    2.468</td> <td> 0.014</td> <td> 1721.888</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>           <td> 3790.6464</td> <td> 1973.720</td> <td>    1.921</td> <td> 0.055</td> <td>  -78.200</td> <td> 7659.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>           <td> 6381.3813</td> <td> 2447.749</td> <td>    2.607</td> <td> 0.009</td> <td> 1583.352</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>           <td> 2511.3497</td> <td> 1916.834</td> <td>    1.310</td> <td> 0.190</td> <td>-1245.991</td> <td> 6268.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>           <td> 4706.9041</td> <td> 2030.512</td> <td>    2.318</td> <td> 0.020</td> <td>  726.734</td> <td> 8687.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>           <td> 7757.6182</td> <td> 2072.114</td> <td>    3.744</td> <td> 0.000</td> <td> 3695.902</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>           <td>-2421.7565</td> <td> 1874.454</td> <td>   -1.292</td> <td> 0.196</td> <td>-6096.024</td> <td> 1252.511</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>           <td>-5726.5396</td> <td> 2170.948</td> <td>   -2.638</td> <td> 0.008</td> <td>-9981.990</td> <td>-1471.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>           <td> 1.455e+04</td> <td> 2133.403</td> <td>    6.818</td> <td> 0.000</td> <td> 1.04e+04</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>           <td> 6731.6393</td> <td> 3255.395</td> <td>    2.068</td> <td> 0.039</td> <td>  350.477</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>           <td> 2138.3776</td> <td> 2197.752</td> <td>    0.973</td> <td> 0.331</td> <td>-2169.613</td> <td> 6446.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>           <td> 3.176e+04</td> <td> 2117.435</td> <td>   15.001</td> <td> 0.000</td> <td> 2.76e+04</td> <td> 3.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>           <td> 2408.8671</td> <td> 1897.741</td> <td>    1.269</td> <td> 0.204</td> <td>-1311.047</td> <td> 6128.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>           <td> 2360.5998</td> <td> 2342.473</td> <td>    1.008</td> <td> 0.314</td> <td>-2231.070</td> <td> 6952.269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>           <td> 5187.9468</td> <td> 2167.823</td> <td>    2.393</td> <td> 0.017</td> <td>  938.622</td> <td> 9437.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>           <td> 5486.3808</td> <td> 2996.105</td> <td>    1.831</td> <td> 0.067</td> <td> -386.526</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>           <td>-3585.6338</td> <td> 1998.396</td> <td>   -1.794</td> <td> 0.073</td> <td>-7502.850</td> <td>  331.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>           <td>-5333.1941</td> <td> 3491.925</td> <td>   -1.527</td> <td> 0.127</td> <td>-1.22e+04</td> <td> 1511.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>           <td>  651.5341</td> <td> 1867.455</td> <td>    0.349</td> <td> 0.727</td> <td>-3009.013</td> <td> 4312.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>           <td> 3041.9554</td> <td> 1931.405</td> <td>    1.575</td> <td> 0.115</td> <td> -743.946</td> <td> 6827.857</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>           <td>  310.1206</td> <td> 1864.417</td> <td>    0.166</td> <td> 0.868</td> <td>-3344.473</td> <td> 3964.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>           <td>-2069.2980</td> <td> 1904.095</td> <td>   -1.087</td> <td> 0.277</td> <td>-5801.668</td> <td> 1663.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>           <td>-4300.2979</td> <td> 3336.017</td> <td>   -1.289</td> <td> 0.197</td> <td>-1.08e+04</td> <td> 2238.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>           <td> 4575.6767</td> <td> 2162.039</td> <td>    2.116</td> <td> 0.034</td> <td>  337.691</td> <td> 8813.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>           <td> 3327.0591</td> <td> 1944.850</td> <td>    1.711</td> <td> 0.087</td> <td> -485.198</td> <td> 7139.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>           <td> 1110.9578</td> <td> 3629.557</td> <td>    0.306</td> <td> 0.760</td> <td>-6003.628</td> <td> 8225.543</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>           <td> 1681.1847</td> <td> 1864.510</td> <td>    0.902</td> <td> 0.367</td> <td>-1973.592</td> <td> 5335.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>           <td> 6068.4666</td> <td> 2125.652</td> <td>    2.855</td> <td> 0.004</td> <td> 1901.806</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>           <td>-3132.7330</td> <td> 1947.742</td> <td>   -1.608</td> <td> 0.108</td> <td>-6950.659</td> <td>  685.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>           <td> 5074.5529</td> <td> 2089.598</td> <td>    2.428</td> <td> 0.015</td> <td>  978.565</td> <td> 9170.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>           <td>-5768.0515</td> <td> 2234.127</td> <td>   -2.582</td> <td> 0.010</td> <td>-1.01e+04</td> <td>-1388.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>           <td>-8360.7510</td> <td> 3225.690</td> <td>   -2.592</td> <td> 0.010</td> <td>-1.47e+04</td> <td>-2037.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>           <td> 4115.2606</td> <td> 1990.738</td> <td>    2.067</td> <td> 0.039</td> <td>  213.056</td> <td> 8017.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>           <td> 5981.9239</td> <td> 2399.900</td> <td>    2.493</td> <td> 0.013</td> <td> 1277.687</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>           <td> 4996.1309</td> <td> 2052.820</td> <td>    2.434</td> <td> 0.015</td> <td>  972.233</td> <td> 9020.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>           <td> 5066.5700</td> <td> 2057.716</td> <td>    2.462</td> <td> 0.014</td> <td> 1033.076</td> <td> 9100.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>           <td>-6023.6247</td> <td> 2149.646</td> <td>   -2.802</td> <td> 0.005</td> <td>-1.02e+04</td> <td>-1809.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>           <td> 5132.9406</td> <td> 2204.662</td> <td>    2.328</td> <td> 0.020</td> <td>  811.407</td> <td> 9454.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>           <td> 1209.7216</td> <td> 2107.912</td> <td>    0.574</td> <td> 0.566</td> <td>-2922.165</td> <td> 5341.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>           <td>-2409.2113</td> <td> 1908.119</td> <td>   -1.263</td> <td> 0.207</td> <td>-6149.468</td> <td> 1331.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>           <td> 2404.9306</td> <td> 1897.249</td> <td>    1.268</td> <td> 0.205</td> <td>-1314.020</td> <td> 6123.881</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>           <td> -1.37e+04</td> <td> 3025.335</td> <td>   -4.530</td> <td> 0.000</td> <td>-1.96e+04</td> <td>-7774.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>           <td> 3049.8115</td> <td> 1931.994</td> <td>    1.579</td> <td> 0.114</td> <td> -737.245</td> <td> 6836.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>           <td>-3395.4030</td> <td> 1967.205</td> <td>   -1.726</td> <td> 0.084</td> <td>-7251.480</td> <td>  460.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>           <td> 4453.7623</td> <td> 2001.368</td> <td>    2.225</td> <td> 0.026</td> <td>  530.720</td> <td> 8376.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>           <td> 1314.2159</td> <td> 1876.490</td> <td>    0.700</td> <td> 0.484</td> <td>-2364.043</td> <td> 4992.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>           <td>-7385.6645</td> <td> 2326.273</td> <td>   -3.175</td> <td> 0.002</td> <td>-1.19e+04</td> <td>-2825.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>           <td>-7911.9946</td> <td> 2248.566</td> <td>   -3.519</td> <td> 0.000</td> <td>-1.23e+04</td> <td>-3504.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>           <td> 8457.4243</td> <td> 2177.531</td> <td>    3.884</td> <td> 0.000</td> <td> 4189.072</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>           <td> 1900.2675</td> <td> 1871.373</td> <td>    1.015</td> <td> 0.310</td> <td>-1767.961</td> <td> 5568.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>           <td> 5226.8772</td> <td> 2146.809</td> <td>    2.435</td> <td> 0.015</td> <td> 1018.744</td> <td> 9435.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>           <td>   40.7129</td> <td> 1864.053</td> <td>    0.022</td> <td> 0.983</td> <td>-3613.168</td> <td> 3694.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>           <td> -321.7520</td> <td> 1864.079</td> <td>   -0.173</td> <td> 0.863</td> <td>-3975.683</td> <td> 3332.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>           <td>-2.135e+04</td> <td> 3619.084</td> <td>   -5.900</td> <td> 0.000</td> <td>-2.84e+04</td> <td>-1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>           <td> 3426.7507</td> <td> 2056.518</td> <td>    1.666</td> <td> 0.096</td> <td> -604.394</td> <td> 7457.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>           <td> 7901.8265</td> <td> 2261.528</td> <td>    3.494</td> <td> 0.000</td> <td> 3468.824</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>           <td>  833.3317</td> <td> 1866.253</td> <td>    0.447</td> <td> 0.655</td> <td>-2824.860</td> <td> 4491.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>           <td> 1602.6486</td> <td> 2727.533</td> <td>    0.588</td> <td> 0.557</td> <td>-3743.808</td> <td> 6949.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>           <td>   78.5373</td> <td> 1915.560</td> <td>    0.041</td> <td> 0.967</td> <td>-3676.306</td> <td> 3833.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>           <td> -1.48e+04</td> <td> 2256.653</td> <td>   -6.560</td> <td> 0.000</td> <td>-1.92e+04</td> <td>-1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>           <td>-2521.2918</td> <td> 1928.370</td> <td>   -1.307</td> <td> 0.191</td> <td>-6301.244</td> <td> 1258.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>           <td> 1779.8670</td> <td> 1888.790</td> <td>    0.942</td> <td> 0.346</td> <td>-1922.503</td> <td> 5482.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>           <td> 6767.4474</td> <td> 2206.383</td> <td>    3.067</td> <td> 0.002</td> <td> 2442.538</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>           <td> 3670.7502</td> <td> 1965.696</td> <td>    1.867</td> <td> 0.062</td> <td> -182.368</td> <td> 7523.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>           <td>-1.084e+04</td> <td> 2689.266</td> <td>   -4.029</td> <td> 0.000</td> <td>-1.61e+04</td> <td>-5564.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>           <td> 5450.6132</td> <td> 2097.209</td> <td>    2.599</td> <td> 0.009</td> <td> 1339.705</td> <td> 9561.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>           <td>-6883.0058</td> <td> 2271.288</td> <td>   -3.030</td> <td> 0.002</td> <td>-1.13e+04</td> <td>-2430.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>           <td> 7829.9634</td> <td> 2649.656</td> <td>    2.955</td> <td> 0.003</td> <td> 2636.160</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>           <td>-3470.2149</td> <td> 2109.124</td> <td>   -1.645</td> <td> 0.100</td> <td>-7604.477</td> <td>  664.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>           <td>-4483.6951</td> <td> 1939.925</td> <td>   -2.311</td> <td> 0.021</td> <td>-8286.298</td> <td> -681.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>           <td>-9038.7901</td> <td> 2403.228</td> <td>   -3.761</td> <td> 0.000</td> <td>-1.37e+04</td> <td>-4328.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>           <td> 7162.4585</td> <td> 2269.938</td> <td>    3.155</td> <td> 0.002</td> <td> 2712.972</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>           <td>-4490.8986</td> <td> 2070.447</td> <td>   -2.169</td> <td> 0.030</td> <td>-8549.348</td> <td> -432.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>           <td>-6634.8697</td> <td> 2103.115</td> <td>   -3.155</td> <td> 0.002</td> <td>-1.08e+04</td> <td>-2512.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>           <td>   44.4631</td> <td> 2004.461</td> <td>    0.022</td> <td> 0.982</td> <td>-3884.642</td> <td> 3973.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>           <td>-5592.6582</td> <td> 2142.027</td> <td>   -2.611</td> <td> 0.009</td> <td>-9791.418</td> <td>-1393.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>           <td> 4343.8755</td> <td> 1928.676</td> <td>    2.252</td> <td> 0.024</td> <td>  563.323</td> <td> 8124.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>           <td> 8156.6130</td> <td> 2451.973</td> <td>    3.327</td> <td> 0.001</td> <td> 3350.304</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>           <td> 1859.8477</td> <td> 1887.968</td> <td>    0.985</td> <td> 0.325</td> <td>-1840.910</td> <td> 5560.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>           <td> 1391.0125</td> <td> 1873.915</td> <td>    0.742</td> <td> 0.458</td> <td>-2282.200</td> <td> 5064.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>           <td>-2972.1496</td> <td> 1941.167</td> <td>   -1.531</td> <td> 0.126</td> <td>-6777.187</td> <td>  832.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>           <td>-9765.3587</td> <td> 2303.586</td> <td>   -4.239</td> <td> 0.000</td> <td>-1.43e+04</td> <td>-5249.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>           <td> 8313.1316</td> <td> 2684.159</td> <td>    3.097</td> <td> 0.002</td> <td> 3051.696</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>           <td> 3540.6440</td> <td> 1979.017</td> <td>    1.789</td> <td> 0.074</td> <td> -338.586</td> <td> 7419.875</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>           <td> 7069.6698</td> <td> 2777.372</td> <td>    2.545</td> <td> 0.011</td> <td> 1625.520</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>           <td> 1.527e+04</td> <td> 2143.195</td> <td>    7.127</td> <td> 0.000</td> <td> 1.11e+04</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>           <td>-6067.7336</td> <td> 2128.180</td> <td>   -2.851</td> <td> 0.004</td> <td>-1.02e+04</td> <td>-1896.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>           <td> 2.431e+04</td> <td> 2339.891</td> <td>   10.391</td> <td> 0.000</td> <td> 1.97e+04</td> <td> 2.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>           <td>-8491.4525</td> <td> 2144.141</td> <td>   -3.960</td> <td> 0.000</td> <td>-1.27e+04</td> <td>-4288.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>           <td> 6317.6608</td> <td> 2201.517</td> <td>    2.870</td> <td> 0.004</td> <td> 2002.290</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>           <td> 5102.2092</td> <td> 2202.085</td> <td>    2.317</td> <td> 0.021</td> <td>  785.726</td> <td> 9418.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>           <td> 5362.8151</td> <td> 2180.010</td> <td>    2.460</td> <td> 0.014</td> <td> 1089.602</td> <td> 9636.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>           <td> 2441.4458</td> <td> 1864.788</td> <td>    1.309</td> <td> 0.190</td> <td>-1213.874</td> <td> 6096.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>           <td> 7058.7426</td> <td> 2257.207</td> <td>    3.127</td> <td> 0.002</td> <td> 2634.210</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>           <td> 1606.9304</td> <td> 1882.540</td> <td>    0.854</td> <td> 0.393</td> <td>-2083.187</td> <td> 5297.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>           <td> -273.1365</td> <td> 1864.424</td> <td>   -0.146</td> <td> 0.884</td> <td>-3927.743</td> <td> 3381.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>           <td> 6256.4555</td> <td> 3809.101</td> <td>    1.643</td> <td> 0.101</td> <td>-1210.069</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>           <td>-3098.7461</td> <td> 1958.469</td> <td>   -1.582</td> <td> 0.114</td> <td>-6937.698</td> <td>  740.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>           <td>-2061.6502</td> <td> 2250.261</td> <td>   -0.916</td> <td> 0.360</td> <td>-6472.568</td> <td> 2349.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>           <td>-2354.8354</td> <td> 1895.725</td> <td>   -1.242</td> <td> 0.214</td> <td>-6070.798</td> <td> 1361.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>           <td> 3125.8315</td> <td> 1938.059</td> <td>    1.613</td> <td> 0.107</td> <td> -673.113</td> <td> 6924.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>           <td> 5612.2962</td> <td> 2087.698</td> <td>    2.688</td> <td> 0.007</td> <td> 1520.033</td> <td> 9704.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>           <td> 5593.7671</td> <td> 2114.288</td> <td>    2.646</td> <td> 0.008</td> <td> 1449.381</td> <td> 9738.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>           <td>  1.02e+04</td> <td> 1999.661</td> <td>    5.101</td> <td> 0.000</td> <td> 6280.065</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>           <td> 8386.7682</td> <td> 2377.400</td> <td>    3.528</td> <td> 0.000</td> <td> 3726.636</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>           <td> 2716.9956</td> <td> 1920.984</td> <td>    1.414</td> <td> 0.157</td> <td>-1048.480</td> <td> 6482.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>           <td> 3050.3855</td> <td> 1931.187</td> <td>    1.580</td> <td> 0.114</td> <td> -735.090</td> <td> 6835.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>           <td> 7119.6062</td> <td> 2269.908</td> <td>    3.137</td> <td> 0.002</td> <td> 2670.177</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>           <td>-4092.6895</td> <td> 2053.430</td> <td>   -1.993</td> <td> 0.046</td> <td>-8117.782</td> <td>  -67.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>           <td>  525.8274</td> <td> 1866.275</td> <td>    0.282</td> <td> 0.778</td> <td>-3132.409</td> <td> 4184.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>           <td> 2252.9193</td> <td> 1900.170</td> <td>    1.186</td> <td> 0.236</td> <td>-1471.756</td> <td> 5977.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>           <td>-1.706e+04</td> <td> 3205.600</td> <td>   -5.323</td> <td> 0.000</td> <td>-2.33e+04</td> <td>-1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>           <td>-2852.1868</td> <td> 2100.602</td> <td>   -1.358</td> <td> 0.175</td> <td>-6969.746</td> <td> 1265.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>           <td> 6122.1074</td> <td> 2452.259</td> <td>    2.497</td> <td> 0.013</td> <td> 1315.237</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>           <td> -156.8318</td> <td> 1866.832</td> <td>   -0.084</td> <td> 0.933</td> <td>-3816.160</td> <td> 3502.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>           <td>-4123.7657</td> <td> 2024.008</td> <td>   -2.037</td> <td> 0.042</td> <td>-8091.186</td> <td> -156.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>           <td>-2111.4475</td> <td> 2288.481</td> <td>   -0.923</td> <td> 0.356</td> <td>-6597.282</td> <td> 2374.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>           <td>-1.034e+04</td> <td> 2692.856</td> <td>   -3.841</td> <td> 0.000</td> <td>-1.56e+04</td> <td>-5064.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>           <td> -489.4159</td> <td> 1866.396</td> <td>   -0.262</td> <td> 0.793</td> <td>-4147.888</td> <td> 3169.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>           <td>-8444.7733</td> <td> 2140.807</td> <td>   -3.945</td> <td> 0.000</td> <td>-1.26e+04</td> <td>-4248.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>           <td> 8487.8115</td> <td> 3631.523</td> <td>    2.337</td> <td> 0.019</td> <td> 1369.372</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>           <td>  326.4350</td> <td> 2255.147</td> <td>    0.145</td> <td> 0.885</td> <td>-4094.060</td> <td> 4746.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>           <td> 5704.1023</td> <td> 2133.434</td> <td>    2.674</td> <td> 0.008</td> <td> 1522.187</td> <td> 9886.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>           <td>  419.3959</td> <td> 1865.732</td> <td>    0.225</td> <td> 0.822</td> <td>-3237.775</td> <td> 4076.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>           <td>-7372.7748</td> <td> 3463.407</td> <td>   -2.129</td> <td> 0.033</td> <td>-1.42e+04</td> <td> -583.872</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>           <td>-1.098e+04</td> <td> 2757.147</td> <td>   -3.983</td> <td> 0.000</td> <td>-1.64e+04</td> <td>-5576.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>           <td>-1.158e+04</td> <td> 2798.937</td> <td>   -4.136</td> <td> 0.000</td> <td>-1.71e+04</td> <td>-6090.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>           <td> 3196.0518</td> <td> 1924.982</td> <td>    1.660</td> <td> 0.097</td> <td> -577.260</td> <td> 6969.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>           <td> 7182.7628</td> <td> 2765.582</td> <td>    2.597</td> <td> 0.009</td> <td> 1761.724</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>           <td> 8354.7013</td> <td> 2371.940</td> <td>    3.522</td> <td> 0.000</td> <td> 3705.272</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>           <td> 5061.0236</td> <td> 2042.829</td> <td>    2.477</td> <td> 0.013</td> <td> 1056.711</td> <td> 9065.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>           <td>-4470.4230</td> <td> 1902.325</td> <td>   -2.350</td> <td> 0.019</td> <td>-8199.324</td> <td> -741.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>           <td> 6085.0890</td> <td> 2144.445</td> <td>    2.838</td> <td> 0.005</td> <td> 1881.590</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>           <td>  723.9273</td> <td> 3068.441</td> <td>    0.236</td> <td> 0.813</td> <td>-5290.771</td> <td> 6738.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>           <td>-1736.7159</td> <td> 1865.669</td> <td>   -0.931</td> <td> 0.352</td> <td>-5393.764</td> <td> 1920.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>           <td> 1145.8660</td> <td> 1923.806</td> <td>    0.596</td> <td> 0.551</td> <td>-2625.141</td> <td> 4916.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>           <td>-2386.5728</td> <td> 1918.073</td> <td>   -1.244</td> <td> 0.213</td> <td>-6146.341</td> <td> 1373.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>           <td> -1.06e+04</td> <td> 3467.553</td> <td>   -3.057</td> <td> 0.002</td> <td>-1.74e+04</td> <td>-3803.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>           <td> 7808.8527</td> <td> 2560.984</td> <td>    3.049</td> <td> 0.002</td> <td> 2788.862</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>           <td> -528.3712</td> <td> 1999.930</td> <td>   -0.264</td> <td> 0.792</td> <td>-4448.594</td> <td> 3391.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>           <td> 4272.7861</td> <td> 2153.015</td> <td>    1.985</td> <td> 0.047</td> <td>   52.489</td> <td> 8493.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>           <td> 3829.1320</td> <td> 2035.049</td> <td>    1.882</td> <td> 0.060</td> <td> -159.930</td> <td> 7818.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>           <td>-3461.9009</td> <td> 1980.006</td> <td>   -1.748</td> <td> 0.080</td> <td>-7343.069</td> <td>  419.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>           <td>-2966.5377</td> <td> 1954.795</td> <td>   -1.518</td> <td> 0.129</td> <td>-6798.289</td> <td>  865.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>           <td>-2.178e+04</td> <td> 3477.567</td> <td>   -6.263</td> <td> 0.000</td> <td>-2.86e+04</td> <td> -1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>           <td> 5556.6143</td> <td> 2095.883</td> <td>    2.651</td> <td> 0.008</td> <td> 1448.306</td> <td> 9664.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>           <td>-3000.3150</td> <td> 2815.493</td> <td>   -1.066</td> <td> 0.287</td> <td>-8519.190</td> <td> 2518.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>           <td>   96.4325</td> <td> 1866.180</td> <td>    0.052</td> <td> 0.959</td> <td>-3561.616</td> <td> 3754.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>           <td>-3209.1877</td> <td> 3345.415</td> <td>   -0.959</td> <td> 0.337</td> <td>-9766.804</td> <td> 3348.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>           <td> 8362.0997</td> <td> 2368.635</td> <td>    3.530</td> <td> 0.000</td> <td> 3719.148</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>           <td>-7569.9362</td> <td> 2290.434</td> <td>   -3.305</td> <td> 0.001</td> <td>-1.21e+04</td> <td>-3080.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>           <td>-1.023e+04</td> <td> 2649.183</td> <td>   -3.862</td> <td> 0.000</td> <td>-1.54e+04</td> <td>-5038.780</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>           <td>-1.725e+04</td> <td> 4717.428</td> <td>   -3.657</td> <td> 0.000</td> <td>-2.65e+04</td> <td>-8003.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>           <td>-7987.2772</td> <td> 2362.354</td> <td>   -3.381</td> <td> 0.001</td> <td>-1.26e+04</td> <td>-3356.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>           <td> 5019.9025</td> <td> 2055.389</td> <td>    2.442</td> <td> 0.015</td> <td>  990.970</td> <td> 9048.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>           <td> 4179.4211</td> <td> 1917.850</td> <td>    2.179</td> <td> 0.029</td> <td>  420.090</td> <td> 7938.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>           <td>-6450.6652</td> <td> 2229.140</td> <td>   -2.894</td> <td> 0.004</td> <td>-1.08e+04</td> <td>-2081.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>           <td>-1060.9540</td> <td> 1878.750</td> <td>   -0.565</td> <td> 0.572</td> <td>-4743.643</td> <td> 2621.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>           <td> 1719.9734</td> <td> 1885.152</td> <td>    0.912</td> <td> 0.362</td> <td>-1975.264</td> <td> 5415.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>           <td>-7657.2718</td> <td> 2282.093</td> <td>   -3.355</td> <td> 0.001</td> <td>-1.21e+04</td> <td>-3183.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>           <td> 6542.7727</td> <td> 2160.521</td> <td>    3.028</td> <td> 0.002</td> <td> 2307.762</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>           <td>-2362.7993</td> <td> 1948.249</td> <td>   -1.213</td> <td> 0.225</td> <td>-6181.719</td> <td> 1456.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>           <td>-1.118e+04</td> <td> 2973.308</td> <td>   -3.760</td> <td> 0.000</td> <td> -1.7e+04</td> <td>-5350.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>           <td> 1308.4978</td> <td> 1970.360</td> <td>    0.664</td> <td> 0.507</td> <td>-2553.762</td> <td> 5170.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>           <td>-2.906e+04</td> <td> 2163.126</td> <td>  -13.433</td> <td> 0.000</td> <td>-3.33e+04</td> <td>-2.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>           <td> 7678.9303</td> <td> 2307.083</td> <td>    3.328</td> <td> 0.001</td> <td> 3156.632</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>           <td>-2248.8226</td> <td> 1916.817</td> <td>   -1.173</td> <td> 0.241</td> <td>-6006.129</td> <td> 1508.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>           <td>-1.144e+04</td> <td> 3612.246</td> <td>   -3.167</td> <td> 0.002</td> <td>-1.85e+04</td> <td>-4359.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>           <td>-2221.9814</td> <td> 1883.297</td> <td>   -1.180</td> <td> 0.238</td> <td>-5913.583</td> <td> 1469.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>           <td> 7509.6774</td> <td> 2345.541</td> <td>    3.202</td> <td> 0.001</td> <td> 2911.995</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9999.0</th>           <td>  -1.1e+04</td> <td> 2734.749</td> <td>   -4.023</td> <td> 0.000</td> <td>-1.64e+04</td> <td>-5641.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1982</th> <td>    0.0155</td> <td>    0.052</td> <td>    0.299</td> <td> 0.765</td> <td>   -0.086</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1983</th> <td>   -0.0057</td> <td>    0.051</td> <td>   -0.110</td> <td> 0.912</td> <td>   -0.106</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1984</th> <td>   -0.0692</td> <td>    0.051</td> <td>   -1.359</td> <td> 0.174</td> <td>   -0.169</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1985</th> <td>   -0.1076</td> <td>    0.051</td> <td>   -2.103</td> <td> 0.036</td> <td>   -0.208</td> <td>   -0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1986</th> <td>   -0.1549</td> <td>    0.052</td> <td>   -2.990</td> <td> 0.003</td> <td>   -0.256</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1987</th> <td>   -0.1737</td> <td>    0.053</td> <td>   -3.301</td> <td> 0.001</td> <td>   -0.277</td> <td>   -0.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1988</th> <td>   -0.2116</td> <td>    0.053</td> <td>   -3.958</td> <td> 0.000</td> <td>   -0.316</td> <td>   -0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1989</th> <td>   -0.2163</td> <td>    0.054</td> <td>   -3.979</td> <td> 0.000</td> <td>   -0.323</td> <td>   -0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1990</th> <td>   -0.2536</td> <td>    0.055</td> <td>   -4.569</td> <td> 0.000</td> <td>   -0.362</td> <td>   -0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1991</th> <td>   -0.2393</td> <td>    0.056</td> <td>   -4.241</td> <td> 0.000</td> <td>   -0.350</td> <td>   -0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1992</th> <td>   -0.2451</td> <td>    0.057</td> <td>   -4.269</td> <td> 0.000</td> <td>   -0.358</td> <td>   -0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1993</th> <td>   -0.2314</td> <td>    0.059</td> <td>   -3.940</td> <td> 0.000</td> <td>   -0.346</td> <td>   -0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1994</th> <td>   -0.2438</td> <td>    0.060</td> <td>   -4.064</td> <td> 0.000</td> <td>   -0.361</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1995</th> <td>   -0.2358</td> <td>    0.062</td> <td>   -3.823</td> <td> 0.000</td> <td>   -0.357</td> <td>   -0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1996</th> <td>   -0.2342</td> <td>    0.064</td> <td>   -3.667</td> <td> 0.000</td> <td>   -0.359</td> <td>   -0.109</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1997</th> <td>   -0.2241</td> <td>    0.066</td> <td>   -3.382</td> <td> 0.001</td> <td>   -0.354</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1998</th> <td>   -0.1955</td> <td>    0.069</td> <td>   -2.850</td> <td> 0.004</td> <td>   -0.330</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1999</th> <td>   -0.1094</td> <td>    0.071</td> <td>   -1.551</td> <td> 0.121</td> <td>   -0.248</td> <td>    0.029</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22347.423</td> <th>  Durbin-Watson:     </th>   <td>   0.741</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>128268599.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>14.218</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>514.370</td>  <th>  Cond. No.          </th>   <td>8.09e+18</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.32e-25. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &       0.666    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &       0.643    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &       28.50    \\\\\n",
       "\\textbf{Date:}             & Mon, 14 Oct 2024 & \\textbf{  Prob (F-statistic):} &       0.00     \\\\\n",
       "\\textbf{Time:}             &     16:57:50     & \\textbf{  Log-Likelihood:    } &  -1.2190e+05   \\\\\n",
       "\\textbf{No. Observations:} &       11736      & \\textbf{  AIC:               } &   2.453e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       10968      & \\textbf{  BIC:               } &   2.510e+05    \\\\\n",
       "\\textbf{Df Model:}         &         767      & \\textbf{                     } &                \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &                \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &   -9211.7705  &     1638.776     &    -5.621  &         0.000        &    -1.24e+04    &    -5999.475     \\\\\n",
       "\\textbf{gspilltecIV}      &       0.5889  &        0.115     &     5.124  &         0.000        &        0.364    &        0.814     \\\\\n",
       "\\textbf{pat\\_count}       &     -25.4471  &        1.772     &   -14.360  &         0.000        &      -28.921    &      -21.973     \\\\\n",
       "\\textbf{rsales}           &       0.9838  &        0.041     &    23.821  &         0.000        &        0.903    &        1.065     \\\\\n",
       "\\textbf{rppent}           &       0.4660  &        0.086     &     5.390  &         0.000        &        0.297    &        0.636     \\\\\n",
       "\\textbf{emp}              &      -6.8225  &        7.044     &    -0.969  &         0.333        &      -20.629    &        6.984     \\\\\n",
       "\\textbf{rxrd}             &      10.1307  &        0.655     &    15.470  &         0.000        &        8.847    &       11.414     \\\\\n",
       "\\textbf{1982}             &    -302.7136  &      894.280     &    -0.338  &         0.735        &    -2055.664    &     1450.237     \\\\\n",
       "\\textbf{1983}             &    -188.3216  &      886.018     &    -0.213  &         0.832        &    -1925.076    &     1548.433     \\\\\n",
       "\\textbf{1984}             &      61.4814  &      880.418     &     0.070  &         0.944        &    -1664.297    &     1787.259     \\\\\n",
       "\\textbf{1985}             &     310.8801  &      876.108     &     0.355  &         0.723        &    -1406.450    &     2028.211     \\\\\n",
       "\\textbf{1986}             &     702.0587  &      870.202     &     0.807  &         0.420        &    -1003.693    &     2407.811     \\\\\n",
       "\\textbf{1987}             &     656.7606  &      866.805     &     0.758  &         0.449        &    -1042.333    &     2355.854     \\\\\n",
       "\\textbf{1988}             &     933.6216  &      864.888     &     1.079  &         0.280        &     -761.715    &     2628.958     \\\\\n",
       "\\textbf{1989}             &    1082.0033  &      862.114     &     1.255  &         0.209        &     -607.896    &     2771.903     \\\\\n",
       "\\textbf{1990}             &    1284.3781  &      856.621     &     1.499  &         0.134        &     -394.754    &     2963.510     \\\\\n",
       "\\textbf{1991}             &    1326.5463  &      855.595     &     1.550  &         0.121        &     -350.574    &     3003.666     \\\\\n",
       "\\textbf{1992}             &    1228.9372  &      855.768     &     1.436  &         0.151        &     -448.523    &     2906.398     \\\\\n",
       "\\textbf{1993}             &     891.3080  &      852.779     &     1.045  &         0.296        &     -780.293    &     2562.909     \\\\\n",
       "\\textbf{1994}             &     808.1006  &      853.938     &     0.946  &         0.344        &     -865.773    &     2481.974     \\\\\n",
       "\\textbf{1995}             &    1054.1039  &      852.975     &     1.236  &         0.217        &     -617.882    &     2726.089     \\\\\n",
       "\\textbf{1996}             &    1059.0999  &      854.487     &     1.239  &         0.215        &     -615.849    &     2734.049     \\\\\n",
       "\\textbf{1997}             &    1111.7247  &      858.487     &     1.295  &         0.195        &     -571.065    &     2794.514     \\\\\n",
       "\\textbf{1998}             &     382.0384  &      861.634     &     0.443  &         0.657        &    -1306.919    &     2070.996     \\\\\n",
       "\\textbf{1999}             &   -1789.5647  &      870.771     &    -2.055  &         0.040        &    -3496.433    &      -82.696     \\\\\n",
       "\\textbf{10005.0}          &    1471.8278  &     1876.597     &     0.784  &         0.433        &    -2206.641    &     5150.297     \\\\\n",
       "\\textbf{10006.0}          &    -747.7524  &     2257.891     &    -0.331  &         0.741        &    -5173.627    &     3678.122     \\\\\n",
       "\\textbf{10008.0}          &   -1482.7238  &     1888.179     &    -0.785  &         0.432        &    -5183.894    &     2218.447     \\\\\n",
       "\\textbf{10016.0}          &    -487.2088  &     1868.785     &    -0.261  &         0.794        &    -4150.365    &     3175.947     \\\\\n",
       "\\textbf{10030.0}          &    5259.6555  &     2071.946     &     2.539  &         0.011        &     1198.268    &     9321.043     \\\\\n",
       "\\textbf{1004.0}           &    2805.5935  &     1921.966     &     1.460  &         0.144        &     -961.807    &     6572.994     \\\\\n",
       "\\textbf{10056.0}          &     902.2280  &     1873.077     &     0.482  &         0.630        &    -2769.340    &     4573.796     \\\\\n",
       "\\textbf{10085.0}          &   -2309.5248  &     1919.226     &    -1.203  &         0.229        &    -6071.554    &     1452.504     \\\\\n",
       "\\textbf{10092.0}          &    5069.6464  &     4737.149     &     1.070  &         0.285        &    -4216.020    &     1.44e+04     \\\\\n",
       "\\textbf{10097.0}          &   -5499.5467  &     2000.689     &    -2.749  &         0.006        &    -9421.259    &    -1577.835     \\\\\n",
       "\\textbf{1010.0}           &     476.9711  &     4683.548     &     0.102  &         0.919        &    -8703.627    &     9657.569     \\\\\n",
       "\\textbf{10109.0}          &    8482.8358  &     2362.111     &     3.591  &         0.000        &     3852.673    &     1.31e+04     \\\\\n",
       "\\textbf{10115.0}          &    3202.1094  &     1895.146     &     1.690  &         0.091        &     -512.719    &     6916.937     \\\\\n",
       "\\textbf{10124.0}          &    8671.0722  &     2386.396     &     3.634  &         0.000        &     3993.305    &     1.33e+04     \\\\\n",
       "\\textbf{1013.0}           &   -3756.1988  &     2034.674     &    -1.846  &         0.065        &    -7744.526    &      232.129     \\\\\n",
       "\\textbf{10150.0}          &   -3698.4862  &     2480.895     &    -1.491  &         0.136        &    -8561.488    &     1164.516     \\\\\n",
       "\\textbf{10159.0}          &    5904.5710  &     3439.553     &     1.717  &         0.086        &     -837.574    &     1.26e+04     \\\\\n",
       "\\textbf{10174.0}          &    4789.7900  &     2252.427     &     2.127  &         0.033        &      374.627    &     9204.953     \\\\\n",
       "\\textbf{10185.0}          &    5912.9151  &     2354.896     &     2.511  &         0.012        &     1296.895    &     1.05e+04     \\\\\n",
       "\\textbf{10195.0}          &    3689.4278  &     1957.111     &     1.885  &         0.059        &     -146.863    &     7525.718     \\\\\n",
       "\\textbf{10198.0}          &    4939.9188  &     2049.954     &     2.410  &         0.016        &      921.640    &     8958.197     \\\\\n",
       "\\textbf{10215.0}          &    8174.7392  &     2356.187     &     3.469  &         0.001        &     3556.188    &     1.28e+04     \\\\\n",
       "\\textbf{10232.0}          &    7734.8066  &     2430.117     &     3.183  &         0.001        &     2971.339    &     1.25e+04     \\\\\n",
       "\\textbf{10236.0}          &    5551.7698  &     2096.550     &     2.648  &         0.008        &     1442.153    &     9661.386     \\\\\n",
       "\\textbf{10286.0}          &   -1179.8527  &     1881.783     &    -0.627  &         0.531        &    -4868.486    &     2508.781     \\\\\n",
       "\\textbf{10301.0}          &   -2.113e+04  &     3215.118     &    -6.571  &         0.000        &    -2.74e+04    &    -1.48e+04     \\\\\n",
       "\\textbf{10312.0}          &    2071.0779  &     1894.980     &     1.093  &         0.274        &    -1643.425    &     5785.581     \\\\\n",
       "\\textbf{10332.0}          &   -1.848e+04  &     4178.152     &    -4.422  &         0.000        &    -2.67e+04    &    -1.03e+04     \\\\\n",
       "\\textbf{1036.0}           &    3944.3640  &     2135.332     &     1.847  &         0.065        &     -241.271    &     8129.999     \\\\\n",
       "\\textbf{10374.0}          &    1614.5948  &     1879.868     &     0.859  &         0.390        &    -2070.286    &     5299.476     \\\\\n",
       "\\textbf{10386.0}          &   -8535.2040  &     2431.070     &    -3.511  &         0.000        &    -1.33e+04    &    -3769.868     \\\\\n",
       "\\textbf{10391.0}          &   -1.279e+04  &     2888.661     &    -4.427  &         0.000        &    -1.85e+04    &    -7127.228     \\\\\n",
       "\\textbf{10407.0}          &    -283.7100  &     1869.500     &    -0.152  &         0.879        &    -3948.267    &     3380.847     \\\\\n",
       "\\textbf{10420.0}          &    5890.4125  &     1973.724     &     2.984  &         0.003        &     2021.558    &     9759.267     \\\\\n",
       "\\textbf{10422.0}          &     499.2547  &     1974.630     &     0.253  &         0.800        &    -3371.376    &     4369.885     \\\\\n",
       "\\textbf{10426.0}          &    7179.3354  &     2454.691     &     2.925  &         0.003        &     2367.698    &      1.2e+04     \\\\\n",
       "\\textbf{10441.0}          &    6884.8418  &     2214.192     &     3.109  &         0.002        &     2544.626    &     1.12e+04     \\\\\n",
       "\\textbf{1045.0}           &   -9701.5817  &     2043.591     &    -4.747  &         0.000        &    -1.37e+04    &    -5695.776     \\\\\n",
       "\\textbf{10453.0}          &   -8024.8045  &     2395.586     &    -3.350  &         0.001        &    -1.27e+04    &    -3329.024     \\\\\n",
       "\\textbf{10482.0}          &   -2.291e+04  &     2417.172     &    -9.476  &         0.000        &    -2.76e+04    &    -1.82e+04     \\\\\n",
       "\\textbf{10498.0}          &    4666.9864  &     2050.562     &     2.276  &         0.023        &      647.514    &     8686.458     \\\\\n",
       "\\textbf{10499.0}          &   -1.268e+04  &     3642.486     &    -3.482  &         0.001        &    -1.98e+04    &    -5542.064     \\\\\n",
       "\\textbf{10511.0}          &    8076.3474  &     2407.548     &     3.355  &         0.001        &     3357.120    &     1.28e+04     \\\\\n",
       "\\textbf{10519.0}          &   -1.221e+04  &     2496.373     &    -4.891  &         0.000        &    -1.71e+04    &    -7316.646     \\\\\n",
       "\\textbf{10530.0}          &   -7471.0255  &     2339.416     &    -3.194  &         0.001        &    -1.21e+04    &    -2885.349     \\\\\n",
       "\\textbf{10537.0}          &   -4822.3296  &     2439.002     &    -1.977  &         0.048        &    -9603.213    &      -41.446     \\\\\n",
       "\\textbf{10540.0}          &   -1142.9396  &     1894.488     &    -0.603  &         0.546        &    -4856.479    &     2570.599     \\\\\n",
       "\\textbf{10541.0}          &    1274.9540  &     1977.196     &     0.645  &         0.519        &    -2600.706    &     5150.614     \\\\\n",
       "\\textbf{10550.0}          &   -2711.8146  &     5781.834     &    -0.469  &         0.639        &     -1.4e+04    &     8621.623     \\\\\n",
       "\\textbf{10553.0}          &   -4014.2561  &     2194.630     &    -1.829  &         0.067        &    -8316.126    &      287.614     \\\\\n",
       "\\textbf{10565.0}          &    5677.5052  &     2070.467     &     2.742  &         0.006        &     1619.016    &     9735.994     \\\\\n",
       "\\textbf{10580.0}          &    7527.5576  &     2163.965     &     3.479  &         0.001        &     3285.796    &     1.18e+04     \\\\\n",
       "\\textbf{10581.0}          &    2587.6587  &     1957.315     &     1.322  &         0.186        &    -1249.031    &     6424.348     \\\\\n",
       "\\textbf{10588.0}          &   -6487.9890  &     2226.401     &    -2.914  &         0.004        &    -1.09e+04    &    -2123.842     \\\\\n",
       "\\textbf{10597.0}          &    2716.4309  &     1925.974     &     1.410  &         0.158        &    -1058.826    &     6491.688     \\\\\n",
       "\\textbf{10599.0}          &    3047.7307  &     1940.657     &     1.570  &         0.116        &     -756.306    &     6851.767     \\\\\n",
       "\\textbf{10618.0}          &    2574.6693  &     1923.722     &     1.338  &         0.181        &    -1196.172    &     6345.511     \\\\\n",
       "\\textbf{10656.0}          &    -319.6551  &     1868.298     &    -0.171  &         0.864        &    -3981.856    &     3342.546     \\\\\n",
       "\\textbf{10658.0}          &    -658.2818  &     1871.850     &    -0.352  &         0.725        &    -4327.445    &     3010.881     \\\\\n",
       "\\textbf{10726.0}          &    8846.5101  &     2330.883     &     3.795  &         0.000        &     4277.559    &     1.34e+04     \\\\\n",
       "\\textbf{10734.0}          &    3514.8776  &     2637.144     &     1.333  &         0.183        &    -1654.399    &     8684.154     \\\\\n",
       "\\textbf{10735.0}          &    7127.5181  &     2272.095     &     3.137  &         0.002        &     2673.803    &     1.16e+04     \\\\\n",
       "\\textbf{10764.0}          &    7560.5283  &     2349.973     &     3.217  &         0.001        &     2954.157    &     1.22e+04     \\\\\n",
       "\\textbf{10777.0}          &    -715.5421  &     1872.912     &    -0.382  &         0.702        &    -4386.788    &     2955.704     \\\\\n",
       "\\textbf{1078.0}           &    6633.3340  &     2287.959     &     2.899  &         0.004        &     2148.521    &     1.11e+04     \\\\\n",
       "\\textbf{10793.0}          &    4093.5238  &     2067.320     &     1.980  &         0.048        &       41.204    &     8145.843     \\\\\n",
       "\\textbf{10816.0}          &     354.3631  &     1878.627     &     0.189  &         0.850        &    -3328.085    &     4036.812     \\\\\n",
       "\\textbf{10839.0}          &    2645.9482  &     1906.353     &     1.388  &         0.165        &    -1090.847    &     6382.744     \\\\\n",
       "\\textbf{10857.0}          &   -1.515e+04  &     2814.102     &    -5.383  &         0.000        &    -2.07e+04    &    -9631.001     \\\\\n",
       "\\textbf{10867.0}          &    1529.7769  &     2279.365     &     0.671  &         0.502        &    -2938.190    &     5997.744     \\\\\n",
       "\\textbf{10906.0}          &    3342.7403  &     1949.156     &     1.715  &         0.086        &     -477.957    &     7163.437     \\\\\n",
       "\\textbf{10950.0}          &    2926.4519  &     3106.433     &     0.942  &         0.346        &    -3162.717    &     9015.620     \\\\\n",
       "\\textbf{10983.0}          &   -2.684e+04  &     2824.291     &    -9.505  &         0.000        &    -3.24e+04    &    -2.13e+04     \\\\\n",
       "\\textbf{1099.0}           &    1191.0221  &     1874.388     &     0.635  &         0.525        &    -2483.116    &     4865.160     \\\\\n",
       "\\textbf{10991.0}          &    6376.8999  &     2710.799     &     2.352  &         0.019        &     1063.245    &     1.17e+04     \\\\\n",
       "\\textbf{11012.0}          &    4334.3376  &     2063.583     &     2.100  &         0.036        &      289.343    &     8379.332     \\\\\n",
       "\\textbf{11038.0}          &     791.9569  &     2159.054     &     0.367  &         0.714        &    -3440.178    &     5024.091     \\\\\n",
       "\\textbf{1104.0}           &    5941.4394  &     2128.434     &     2.791  &         0.005        &     1769.325    &     1.01e+04     \\\\\n",
       "\\textbf{11060.0}          &    4935.8143  &     2053.018     &     2.404  &         0.016        &      911.529    &     8960.100     \\\\\n",
       "\\textbf{11094.0}          &    2368.7159  &     1909.567     &     1.240  &         0.215        &    -1374.381    &     6111.812     \\\\\n",
       "\\textbf{11096.0}          &   -2293.5691  &     1914.395     &    -1.198  &         0.231        &    -6046.128    &     1458.990     \\\\\n",
       "\\textbf{11113.0}          &    4232.9568  &     2159.918     &     1.960  &         0.050        &       -0.871    &     8466.785     \\\\\n",
       "\\textbf{1115.0}           &     762.7175  &     1867.675     &     0.408  &         0.683        &    -2898.262    &     4423.697     \\\\\n",
       "\\textbf{11161.0}          &    1303.1616  &     1875.641     &     0.695  &         0.487        &    -2373.432    &     4979.755     \\\\\n",
       "\\textbf{11225.0}          &    6798.8464  &     2204.200     &     3.084  &         0.002        &     2478.217    &     1.11e+04     \\\\\n",
       "\\textbf{11228.0}          &    4349.6694  &     1967.917     &     2.210  &         0.027        &      492.198    &     8207.141     \\\\\n",
       "\\textbf{11236.0}          &   -1.066e+04  &     3625.783     &    -2.939  &         0.003        &    -1.78e+04    &    -3549.729     \\\\\n",
       "\\textbf{11288.0}          &    1158.9606  &     2222.013     &     0.522  &         0.602        &    -3196.586    &     5514.508     \\\\\n",
       "\\textbf{11312.0}          &   -6500.3203  &     2290.356     &    -2.838  &         0.005        &     -1.1e+04    &    -2010.810     \\\\\n",
       "\\textbf{11361.0}          &   -2480.1543  &     1927.470     &    -1.287  &         0.198        &    -6258.343    &     1298.035     \\\\\n",
       "\\textbf{11399.0}          &   -6519.7565  &     2038.346     &    -3.199  &         0.001        &    -1.05e+04    &    -2524.231     \\\\\n",
       "\\textbf{114303.0}         &   -2.474e+04  &     6159.589     &    -4.017  &         0.000        &    -3.68e+04    &    -1.27e+04     \\\\\n",
       "\\textbf{11456.0}          &   -7977.8559  &     2147.430     &    -3.715  &         0.000        &    -1.22e+04    &    -3768.505     \\\\\n",
       "\\textbf{11465.0}          &    -815.8950  &     1911.551     &    -0.427  &         0.670        &    -4562.880    &     2931.090     \\\\\n",
       "\\textbf{11502.0}          &    6925.3259  &     2279.743     &     3.038  &         0.002        &     2456.618    &     1.14e+04     \\\\\n",
       "\\textbf{11506.0}          &   -8431.3839  &     2306.817     &    -3.655  &         0.000        &     -1.3e+04    &    -3909.606     \\\\\n",
       "\\textbf{11537.0}          &    1942.3990  &     1891.341     &     1.027  &         0.304        &    -1764.970    &     5649.768     \\\\\n",
       "\\textbf{11566.0}          &    8088.9736  &     2341.947     &     3.454  &         0.001        &     3498.335    &     1.27e+04     \\\\\n",
       "\\textbf{11573.0}          &    -940.3841  &     1877.924     &    -0.501  &         0.617        &    -4621.454    &     2740.685     \\\\\n",
       "\\textbf{11580.0}          &   -1.033e+04  &     2902.474     &    -3.558  &         0.000        &     -1.6e+04    &    -4638.242     \\\\\n",
       "\\textbf{11600.0}          &    6702.4017  &     2178.355     &     3.077  &         0.002        &     2432.434    &      1.1e+04     \\\\\n",
       "\\textbf{11609.0}          &    8594.4081  &     2102.710     &     4.087  &         0.000        &     4472.717    &     1.27e+04     \\\\\n",
       "\\textbf{1161.0}           &   -1.068e+04  &     2823.182     &    -3.783  &         0.000        &    -1.62e+04    &    -5147.136     \\\\\n",
       "\\textbf{11636.0}          &   -9834.1861  &     2059.968     &    -4.774  &         0.000        &    -1.39e+04    &    -5796.278     \\\\\n",
       "\\textbf{11670.0}          &    8372.7072  &     2385.777     &     3.509  &         0.000        &     3696.154    &      1.3e+04     \\\\\n",
       "\\textbf{11678.0}          &   -4858.3619  &     2105.617     &    -2.307  &         0.021        &    -8985.750    &     -730.973     \\\\\n",
       "\\textbf{11682.0}          &   -3380.1413  &     2121.584     &    -1.593  &         0.111        &    -7538.829    &      778.547     \\\\\n",
       "\\textbf{11694.0}          &    3266.4616  &     2103.246     &     1.553  &         0.120        &     -856.280    &     7389.203     \\\\\n",
       "\\textbf{11720.0}          &    1007.7269  &     3326.172     &     0.303  &         0.762        &    -5512.170    &     7527.624     \\\\\n",
       "\\textbf{11721.0}          &   -2154.7270  &     2200.443     &    -0.979  &         0.327        &    -6467.991    &     2158.537     \\\\\n",
       "\\textbf{11722.0}          &    -815.4558  &     2180.736     &    -0.374  &         0.708        &    -5090.092    &     3459.180     \\\\\n",
       "\\textbf{11793.0}          &   -7904.9485  &     6007.426     &    -1.316  &         0.188        &    -1.97e+04    &     3870.690     \\\\\n",
       "\\textbf{11797.0}          &    7199.2400  &     2612.880     &     2.755  &         0.006        &     2077.523    &     1.23e+04     \\\\\n",
       "\\textbf{11914.0}          &    3458.9714  &     2782.924     &     1.243  &         0.214        &    -1996.061    &     8914.004     \\\\\n",
       "\\textbf{1209.0}           &   -1930.9294  &     1982.619     &    -0.974  &         0.330        &    -5817.220    &     1955.362     \\\\\n",
       "\\textbf{12136.0}          &    -1.37e+04  &     3093.214     &    -4.428  &         0.000        &    -1.98e+04    &    -7632.949     \\\\\n",
       "\\textbf{12141.0}          &    6.067e+04  &     2436.892     &    24.897  &         0.000        &     5.59e+04    &     6.54e+04     \\\\\n",
       "\\textbf{12181.0}          &   -3135.3163  &     3409.956     &    -0.919  &         0.358        &    -9819.446    &     3548.813     \\\\\n",
       "\\textbf{12215.0}          &   -8042.6444  &     2659.261     &    -3.024  &         0.002        &    -1.33e+04    &    -2830.014     \\\\\n",
       "\\textbf{12216.0}          &   -2506.4794  &     2388.160     &    -1.050  &         0.294        &    -7187.704    &     2174.746     \\\\\n",
       "\\textbf{12256.0}          &   -3633.0708  &     2284.675     &    -1.590  &         0.112        &    -8111.446    &      845.305     \\\\\n",
       "\\textbf{12262.0}          &    7013.6473  &     2505.314     &     2.800  &         0.005        &     2102.780    &     1.19e+04     \\\\\n",
       "\\textbf{12389.0}          &   -5095.6956  &     2456.326     &    -2.075  &         0.038        &    -9910.538    &     -280.854     \\\\\n",
       "\\textbf{1239.0}           &    -443.2119  &     1865.901     &    -0.238  &         0.812        &    -4100.713    &     3214.290     \\\\\n",
       "\\textbf{12390.0}          &    -114.9577  &     2568.587     &    -0.045  &         0.964        &    -5149.852    &     4919.936     \\\\\n",
       "\\textbf{12397.0}          &   -2089.4157  &     4713.861     &    -0.443  &         0.658        &    -1.13e+04    &     7150.602     \\\\\n",
       "\\textbf{1243.0}           &     401.2082  &     2043.999     &     0.196  &         0.844        &    -3605.398    &     4407.814     \\\\\n",
       "\\textbf{12548.0}          &    2141.4505  &     2480.742     &     0.863  &         0.388        &    -2721.251    &     7004.152     \\\\\n",
       "\\textbf{12570.0}          &    4855.7868  &     2406.932     &     2.017  &         0.044        &      137.766    &     9573.808     \\\\\n",
       "\\textbf{12581.0}          &   -3887.0089  &     2508.536     &    -1.550  &         0.121        &    -8804.193    &     1030.175     \\\\\n",
       "\\textbf{12592.0}          &    3392.2388  &     2241.324     &     1.513  &         0.130        &    -1001.160    &     7785.638     \\\\\n",
       "\\textbf{12604.0}          &    3168.3128  &     4791.460     &     0.661  &         0.508        &    -6223.813    &     1.26e+04     \\\\\n",
       "\\textbf{12656.0}          &    8282.4899  &     2634.929     &     3.143  &         0.002        &     3117.553    &     1.34e+04     \\\\\n",
       "\\textbf{12679.0}          &   -9569.3139  &     2636.971     &    -3.629  &         0.000        &    -1.47e+04    &    -4400.376     \\\\\n",
       "\\textbf{1278.0}           &    7338.8150  &     2354.335     &     3.117  &         0.002        &     2723.893    &      1.2e+04     \\\\\n",
       "\\textbf{12788.0}          &    5392.7961  &     2512.729     &     2.146  &         0.032        &      467.395    &     1.03e+04     \\\\\n",
       "\\textbf{1283.0}           &    8550.3272  &     2388.220     &     3.580  &         0.000        &     3868.986    &     1.32e+04     \\\\\n",
       "\\textbf{1297.0}           &    2782.6322  &     1925.496     &     1.445  &         0.148        &     -991.688    &     6556.952     \\\\\n",
       "\\textbf{12992.0}          &    5535.6962  &     2467.240     &     2.244  &         0.025        &      699.461    &     1.04e+04     \\\\\n",
       "\\textbf{13135.0}          &     571.3451  &     2356.870     &     0.242  &         0.808        &    -4048.546    &     5191.236     \\\\\n",
       "\\textbf{1327.0}           &    2979.3791  &     1975.994     &     1.508  &         0.132        &     -893.926    &     6852.684     \\\\\n",
       "\\textbf{13282.0}          &   -5447.0197  &     4709.511     &    -1.157  &         0.247        &    -1.47e+04    &     3784.470     \\\\\n",
       "\\textbf{1334.0}           &    8299.7785  &     2411.836     &     3.441  &         0.001        &     3572.145    &      1.3e+04     \\\\\n",
       "\\textbf{13351.0}          &   -7870.7942  &     3149.175     &    -2.499  &         0.012        &     -1.4e+04    &    -1697.843     \\\\\n",
       "\\textbf{13365.0}          &   -7295.5691  &     2618.681     &    -2.786  &         0.005        &    -1.24e+04    &    -2162.483     \\\\\n",
       "\\textbf{13369.0}          &   -6398.1879  &     2531.195     &    -2.528  &         0.011        &    -1.14e+04    &    -1436.590     \\\\\n",
       "\\textbf{13406.0}          &    3302.4052  &     2314.075     &     1.427  &         0.154        &    -1233.600    &     7838.410     \\\\\n",
       "\\textbf{13407.0}          &   -2036.4176  &     2277.634     &    -0.894  &         0.371        &    -6500.990    &     2428.155     \\\\\n",
       "\\textbf{13417.0}          &    7199.1504  &     2683.288     &     2.683  &         0.007        &     1939.422    &     1.25e+04     \\\\\n",
       "\\textbf{13525.0}          &    6702.3576  &     2559.975     &     2.618  &         0.009        &     1684.345    &     1.17e+04     \\\\\n",
       "\\textbf{13554.0}          &    7946.3676  &     2745.679     &     2.894  &         0.004        &     2564.341    &     1.33e+04     \\\\\n",
       "\\textbf{1359.0}           &    3362.1599  &     1873.562     &     1.795  &         0.073        &     -310.360    &     7034.680     \\\\\n",
       "\\textbf{13623.0}          &    2844.3078  &     2398.032     &     1.186  &         0.236        &    -1856.268    &     7544.883     \\\\\n",
       "\\textbf{1372.0}           &    2336.6983  &     1906.538     &     1.226  &         0.220        &    -1400.460    &     6073.856     \\\\\n",
       "\\textbf{1380.0}           &   -2063.6147  &     1916.954     &    -1.077  &         0.282        &    -5821.190    &     1693.961     \\\\\n",
       "\\textbf{13923.0}          &    1691.7022  &     2586.023     &     0.654  &         0.513        &    -3377.369    &     6760.774     \\\\\n",
       "\\textbf{13932.0}          &    6047.0934  &     3496.398     &     1.730  &         0.084        &     -806.477    &     1.29e+04     \\\\\n",
       "\\textbf{13941.0}          &   -1.013e+04  &     3024.694     &    -3.348  &         0.001        &    -1.61e+04    &    -4196.607     \\\\\n",
       "\\textbf{1397.0}           &   -1937.1929  &     2238.879     &    -0.865  &         0.387        &    -6325.799    &     2451.414     \\\\\n",
       "\\textbf{14064.0}          &    4789.3911  &     2416.382     &     1.982  &         0.047        &       52.846    &     9525.936     \\\\\n",
       "\\textbf{14084.0}          &   -4685.8260  &     2401.465     &    -1.951  &         0.051        &    -9393.130    &       21.478     \\\\\n",
       "\\textbf{14324.0}          &    -786.4427  &     2397.801     &    -0.328  &         0.743        &    -5486.564    &     3913.679     \\\\\n",
       "\\textbf{14462.0}          &   -7785.3253  &     2723.311     &    -2.859  &         0.004        &    -1.31e+04    &    -2447.144     \\\\\n",
       "\\textbf{1447.0}           &    2800.1178  &     4103.530     &     0.682  &         0.495        &    -5243.540    &     1.08e+04     \\\\\n",
       "\\textbf{14593.0}          &    5546.5657  &     2688.525     &     2.063  &         0.039        &      276.571    &     1.08e+04     \\\\\n",
       "\\textbf{14622.0}          &   -1770.3156  &     8138.760     &    -0.218  &         0.828        &    -1.77e+04    &     1.42e+04     \\\\\n",
       "\\textbf{1465.0}           &    5863.8852  &     2799.345     &     2.095  &         0.036        &      376.665    &     1.14e+04     \\\\\n",
       "\\textbf{1468.0}           &    5080.1273  &     2579.898     &     1.969  &         0.049        &       23.063    &     1.01e+04     \\\\\n",
       "\\textbf{14897.0}          &    4556.2190  &     4744.669     &     0.960  &         0.337        &    -4744.188    &     1.39e+04     \\\\\n",
       "\\textbf{14954.0}          &    4844.4134  &     2598.074     &     1.865  &         0.062        &     -248.281    &     9937.108     \\\\\n",
       "\\textbf{1496.0}           &    5418.8534  &     2084.683     &     2.599  &         0.009        &     1332.499    &     9505.207     \\\\\n",
       "\\textbf{15267.0}          &    2383.3239  &     2481.831     &     0.960  &         0.337        &    -2481.512    &     7248.160     \\\\\n",
       "\\textbf{15354.0}          &   -3644.3899  &     2654.559     &    -1.373  &         0.170        &    -8847.804    &     1559.024     \\\\\n",
       "\\textbf{1542.0}           &     389.3899  &     1865.507     &     0.209  &         0.835        &    -3267.341    &     4046.121     \\\\\n",
       "\\textbf{15459.0}          &   -9981.6539  &     3067.375     &    -3.254  &         0.001        &     -1.6e+04    &    -3969.047     \\\\\n",
       "\\textbf{1554.0}           &    5423.0598  &     2081.390     &     2.605  &         0.009        &     1343.159    &     9502.960     \\\\\n",
       "\\textbf{15708.0}          &   -1.002e+04  &     3171.947     &    -3.158  &         0.002        &    -1.62e+04    &    -3798.165     \\\\\n",
       "\\textbf{15711.0}          &   -1616.3195  &     2448.562     &    -0.660  &         0.509        &    -6415.941    &     3183.303     \\\\\n",
       "\\textbf{15761.0}          &    3099.4708  &     2931.098     &     1.057  &         0.290        &    -2646.009    &     8844.950     \\\\\n",
       "\\textbf{1581.0}           &   -2.374e+04  &     4109.964     &    -5.777  &         0.000        &    -3.18e+04    &    -1.57e+04     \\\\\n",
       "\\textbf{1593.0}           &   -2831.8659  &     1933.262     &    -1.465  &         0.143        &    -6621.408    &      957.677     \\\\\n",
       "\\textbf{1602.0}           &    4666.7591  &     2105.899     &     2.216  &         0.027        &      538.818    &     8794.700     \\\\\n",
       "\\textbf{1613.0}           &    5439.1842  &     2095.574     &     2.596  &         0.009        &     1331.481    &     9546.887     \\\\\n",
       "\\textbf{16188.0}          &    1729.2905  &     2717.624     &     0.636  &         0.525        &    -3597.742    &     7056.323     \\\\\n",
       "\\textbf{1632.0}           &   -4807.0435  &     2132.455     &    -2.254  &         0.024        &    -8987.040    &     -627.047     \\\\\n",
       "\\textbf{1633.0}           &    2565.6965  &     1911.232     &     1.342  &         0.179        &    -1180.662    &     6312.055     \\\\\n",
       "\\textbf{1635.0}           &    -126.0121  &     1866.961     &    -0.067  &         0.946        &    -3785.592    &     3533.568     \\\\\n",
       "\\textbf{16401.0}          &   -1.629e+04  &     3721.600     &    -4.378  &         0.000        &    -2.36e+04    &    -8997.019     \\\\\n",
       "\\textbf{16437.0}          &   -7581.4867  &     3293.674     &    -2.302  &         0.021        &     -1.4e+04    &    -1125.293     \\\\\n",
       "\\textbf{1651.0}           &    -464.6488  &     1875.442     &    -0.248  &         0.804        &    -4140.853    &     3211.555     \\\\\n",
       "\\textbf{1655.0}           &    3893.3420  &     1976.949     &     1.969  &         0.049        &       18.165    &     7768.519     \\\\\n",
       "\\textbf{1663.0}           &    4451.3397  &     1925.366     &     2.312  &         0.021        &      677.276    &     8225.404     \\\\\n",
       "\\textbf{16710.0}          &   -2151.0226  &     2719.089     &    -0.791  &         0.429        &    -7480.928    &     3178.883     \\\\\n",
       "\\textbf{16729.0}          &    -843.8305  &     2568.706     &    -0.329  &         0.743        &    -5878.957    &     4191.296     \\\\\n",
       "\\textbf{1690.0}           &   -1.168e+04  &     2193.646     &    -5.324  &         0.000        &     -1.6e+04    &    -7379.093     \\\\\n",
       "\\textbf{1703.0}           &   -1955.1330  &     1955.560     &    -1.000  &         0.317        &    -5788.383    &     1878.117     \\\\\n",
       "\\textbf{17202.0}          &    4595.4329  &     2705.329     &     1.699  &         0.089        &     -707.500    &     9898.366     \\\\\n",
       "\\textbf{1722.0}           &    1041.0113  &     1941.638     &     0.536  &         0.592        &    -2764.949    &     4846.971     \\\\\n",
       "\\textbf{1728.0}           &    4725.5694  &     2050.409     &     2.305  &         0.021        &      706.399    &     8744.740     \\\\\n",
       "\\textbf{1743.0}           &    5488.4050  &     3280.892     &     1.673  &         0.094        &     -942.735    &     1.19e+04     \\\\\n",
       "\\textbf{1754.0}           &    3365.6623  &     2041.335     &     1.649  &         0.099        &     -635.722    &     7367.046     \\\\\n",
       "\\textbf{1762.0}           &    -1.18e+04  &     2769.890     &    -4.259  &         0.000        &    -1.72e+04    &    -6368.685     \\\\\n",
       "\\textbf{1773.0}           &    6942.6066  &     2376.659     &     2.921  &         0.003        &     2283.926    &     1.16e+04     \\\\\n",
       "\\textbf{1786.0}           &    -1.43e+04  &     3080.299     &    -4.641  &         0.000        &    -2.03e+04    &    -8258.858     \\\\\n",
       "\\textbf{18100.0}          &    3005.6705  &     2628.581     &     1.143  &         0.253        &    -2146.823    &     8158.164     \\\\\n",
       "\\textbf{1820.0}           &    4245.4049  &     1999.313     &     2.123  &         0.034        &      326.390    &     8164.419     \\\\\n",
       "\\textbf{1848.0}           &   -8981.9585  &     2585.904     &    -3.473  &         0.001        &    -1.41e+04    &    -3913.121     \\\\\n",
       "\\textbf{18654.0}          &    4046.7267  &     3720.335     &     1.088  &         0.277        &    -3245.801    &     1.13e+04     \\\\\n",
       "\\textbf{1875.0}           &    1552.2041  &     4059.869     &     0.382  &         0.702        &    -6405.872    &     9510.280     \\\\\n",
       "\\textbf{1884.0}           &    6470.9768  &     2276.025     &     2.843  &         0.004        &     2009.557    &     1.09e+04     \\\\\n",
       "\\textbf{1913.0}           &   -8710.6213  &     2438.976     &    -3.571  &         0.000        &    -1.35e+04    &    -3929.789     \\\\\n",
       "\\textbf{1919.0}           &    2432.2612  &     2055.952     &     1.183  &         0.237        &    -1597.775    &     6462.298     \\\\\n",
       "\\textbf{1920.0}           &   -4474.2130  &     2085.969     &    -2.145  &         0.032        &    -8563.089    &     -385.337     \\\\\n",
       "\\textbf{1968.0}           &   -2935.0794  &     1950.524     &    -1.505  &         0.132        &    -6758.459    &      888.300     \\\\\n",
       "\\textbf{1976.0}           &    5963.6939  &     2023.911     &     2.947  &         0.003        &     1996.463    &     9930.925     \\\\\n",
       "\\textbf{1981.0}           &    4683.9283  &     2029.882     &     2.307  &         0.021        &      704.994    &     8662.862     \\\\\n",
       "\\textbf{1988.0}           &    -1.85e+04  &     4016.193     &    -4.605  &         0.000        &    -2.64e+04    &    -1.06e+04     \\\\\n",
       "\\textbf{1992.0}           &   -3736.9631  &     1998.914     &    -1.869  &         0.062        &    -7655.196    &      181.270     \\\\\n",
       "\\textbf{2008.0}           &    -472.6100  &     1877.034     &    -0.252  &         0.801        &    -4151.935    &     3206.715     \\\\\n",
       "\\textbf{2033.0}           &    3232.7325  &     2534.569     &     1.275  &         0.202        &    -1735.480    &     8200.945     \\\\\n",
       "\\textbf{2044.0}           &    2346.9978  &     1881.936     &     1.247  &         0.212        &    -1341.936    &     6035.932     \\\\\n",
       "\\textbf{2049.0}           &    -219.9822  &     1864.523     &    -0.118  &         0.906        &    -3874.783    &     3434.819     \\\\\n",
       "\\textbf{2061.0}           &    7760.6254  &     2309.286     &     3.361  &         0.001        &     3234.008    &     1.23e+04     \\\\\n",
       "\\textbf{20779.0}          &    2.919e+04  &     2748.072     &    10.623  &         0.000        &     2.38e+04    &     3.46e+04     \\\\\n",
       "\\textbf{2085.0}           &   -3940.1136  &     2059.564     &    -1.913  &         0.056        &    -7977.230    &       97.003     \\\\\n",
       "\\textbf{2086.0}           &    -145.5963  &     1899.605     &    -0.077  &         0.939        &    -3869.164    &     3577.971     \\\\\n",
       "\\textbf{2111.0}           &     466.2951  &     1877.045     &     0.248  &         0.804        &    -3213.052    &     4145.642     \\\\\n",
       "\\textbf{21204.0}          &    8036.7814  &     2947.759     &     2.726  &         0.006        &     2258.642    &     1.38e+04     \\\\\n",
       "\\textbf{21238.0}          &    7581.8510  &     2866.455     &     2.645  &         0.008        &     1963.082    &     1.32e+04     \\\\\n",
       "\\textbf{2124.0}           &   -2669.4119  &     2038.405     &    -1.310  &         0.190        &    -6665.053    &     1326.229     \\\\\n",
       "\\textbf{2146.0}           &    2.067e+04  &     3209.509     &     6.439  &         0.000        &     1.44e+04    &      2.7e+04     \\\\\n",
       "\\textbf{21496.0}          &   -8858.3795  &     2967.467     &    -2.985  &         0.003        &    -1.47e+04    &    -3041.609     \\\\\n",
       "\\textbf{2154.0}           &    2734.4145  &     1926.414     &     1.419  &         0.156        &    -1041.703    &     6510.532     \\\\\n",
       "\\textbf{2176.0}           &    3.583e+04  &     2612.940     &    13.712  &         0.000        &     3.07e+04    &      4.1e+04     \\\\\n",
       "\\textbf{2188.0}           &    7391.7326  &     2317.607     &     3.189  &         0.001        &     2848.806    &     1.19e+04     \\\\\n",
       "\\textbf{2189.0}           &   -9159.3909  &     2082.316     &    -4.399  &         0.000        &    -1.32e+04    &    -5077.677     \\\\\n",
       "\\textbf{2220.0}           &      55.8354  &     1864.970     &     0.030  &         0.976        &    -3599.841    &     3711.512     \\\\\n",
       "\\textbf{22205.0}          &    7774.0320  &     2919.055     &     2.663  &         0.008        &     2052.158    &     1.35e+04     \\\\\n",
       "\\textbf{2226.0}           &    -838.6605  &     5742.865     &    -0.146  &         0.884        &    -1.21e+04    &     1.04e+04     \\\\\n",
       "\\textbf{2230.0}           &    7228.9116  &     2408.790     &     3.001  &         0.003        &     2507.250    &      1.2e+04     \\\\\n",
       "\\textbf{22325.0}          &     581.7322  &     2640.211     &     0.220  &         0.826        &    -4593.557    &     5757.021     \\\\\n",
       "\\textbf{2255.0}           &      24.2625  &     1867.099     &     0.013  &         0.990        &    -3635.588    &     3684.113     \\\\\n",
       "\\textbf{22619.0}          &    2989.0819  &     2627.929     &     1.137  &         0.255        &    -2162.132    &     8140.296     \\\\\n",
       "\\textbf{2267.0}           &   -4146.3035  &     2030.371     &    -2.042  &         0.041        &    -8126.196    &     -166.411     \\\\\n",
       "\\textbf{22815.0}          &    -501.2126  &     2571.563     &    -0.195  &         0.845        &    -5541.940    &     4539.515     \\\\\n",
       "\\textbf{2285.0}           &   -1.987e+04  &     2336.845     &    -8.503  &         0.000        &    -2.44e+04    &    -1.53e+04     \\\\\n",
       "\\textbf{2290.0}           &   -3811.1880  &     1872.008     &    -2.036  &         0.042        &    -7480.661    &     -141.715     \\\\\n",
       "\\textbf{2295.0}           &    8023.2436  &     3569.229     &     2.248  &         0.025        &     1026.911    &      1.5e+04     \\\\\n",
       "\\textbf{2316.0}           &   -5957.3664  &     2297.220     &    -2.593  &         0.010        &    -1.05e+04    &    -1454.401     \\\\\n",
       "\\textbf{23220.0}          &    -776.8606  &     2705.056     &    -0.287  &         0.774        &    -6079.258    &     4525.536     \\\\\n",
       "\\textbf{23224.0}          &    5798.8798  &     2921.359     &     1.985  &         0.047        &       72.490    &     1.15e+04     \\\\\n",
       "\\textbf{2343.0}           &    1961.8790  &     4085.517     &     0.480  &         0.631        &    -6046.471    &     9970.229     \\\\\n",
       "\\textbf{2352.0}           &   -5763.4410  &     2366.706     &    -2.435  &         0.015        &    -1.04e+04    &    -1124.271     \\\\\n",
       "\\textbf{23700.0}          &   -1.781e+04  &     4886.318     &    -3.644  &         0.000        &    -2.74e+04    &    -8227.811     \\\\\n",
       "\\textbf{2390.0}           &    5753.9197  &     2112.482     &     2.724  &         0.006        &     1613.075    &     9894.764     \\\\\n",
       "\\textbf{2393.0}           &   -9773.2623  &     2597.402     &    -3.763  &         0.000        &    -1.49e+04    &    -4681.886     \\\\\n",
       "\\textbf{2403.0}           &    1.911e+04  &     2130.281     &     8.970  &         0.000        &     1.49e+04    &     2.33e+04     \\\\\n",
       "\\textbf{2435.0}           &    9215.4939  &     2324.978     &     3.964  &         0.000        &     4658.117    &     1.38e+04     \\\\\n",
       "\\textbf{2444.0}           &   -6328.3637  &     2156.028     &    -2.935  &         0.003        &    -1.06e+04    &    -2102.160     \\\\\n",
       "\\textbf{2448.0}           &     871.9388  &     1867.170     &     0.467  &         0.641        &    -2788.051    &     4531.928     \\\\\n",
       "\\textbf{2469.0}           &    5063.4747  &     3756.950     &     1.348  &         0.178        &    -2300.824    &     1.24e+04     \\\\\n",
       "\\textbf{24720.0}          &    -406.9425  &     2870.067     &    -0.142  &         0.887        &    -6032.791    &     5218.906     \\\\\n",
       "\\textbf{24800.0}          &   -2377.2485  &     3004.914     &    -0.791  &         0.429        &    -8267.421    &     3512.924     \\\\\n",
       "\\textbf{2482.0}           &    6137.1445  &     2146.212     &     2.860  &         0.004        &     1930.181    &     1.03e+04     \\\\\n",
       "\\textbf{24969.0}          &    5750.0595  &     3249.398     &     1.770  &         0.077        &     -619.346    &     1.21e+04     \\\\\n",
       "\\textbf{2498.0}           &   -2533.2457  &     2035.151     &    -1.245  &         0.213        &    -6522.508    &     1456.016     \\\\\n",
       "\\textbf{2504.0}           &    -2.31e+04  &     3703.554     &    -6.236  &         0.000        &    -3.04e+04    &    -1.58e+04     \\\\\n",
       "\\textbf{2508.0}           &    3084.6834  &     2165.915     &     1.424  &         0.154        &    -1160.901    &     7330.268     \\\\\n",
       "\\textbf{25124.0}          &    1564.7264  &     2879.122     &     0.543  &         0.587        &    -4078.872    &     7208.325     \\\\\n",
       "\\textbf{2518.0}           &    3667.7789  &     1981.681     &     1.851  &         0.064        &     -216.674    &     7552.232     \\\\\n",
       "\\textbf{25224.0}          &    -519.5598  &     8121.496     &    -0.064  &         0.949        &    -1.64e+04    &     1.54e+04     \\\\\n",
       "\\textbf{25279.0}          &    5893.9281  &     2883.611     &     2.044  &         0.041        &      241.530    &     1.15e+04     \\\\\n",
       "\\textbf{2537.0}           &   -1929.2007  &     1955.004     &    -0.987  &         0.324        &    -5761.361    &     1902.960     \\\\\n",
       "\\textbf{2538.0}           &    6662.2041  &     3169.660     &     2.102  &         0.036        &      449.099    &     1.29e+04     \\\\\n",
       "\\textbf{25389.0}          &    5022.2105  &     4762.281     &     1.055  &         0.292        &    -4312.720    &     1.44e+04     \\\\\n",
       "\\textbf{2547.0}           &   -1.196e+04  &     2180.644     &    -5.485  &         0.000        &    -1.62e+04    &    -7687.227     \\\\\n",
       "\\textbf{2553.0}           &    5107.4417  &     2062.228     &     2.477  &         0.013        &     1065.104    &     9149.780     \\\\\n",
       "\\textbf{2574.0}           &   -1729.5345  &     2589.240     &    -0.668  &         0.504        &    -6804.912    &     3345.843     \\\\\n",
       "\\textbf{25747.0}          &    1742.7951  &     3088.970     &     0.564  &         0.573        &    -4312.143    &     7797.733     \\\\\n",
       "\\textbf{2577.0}           &   -2057.6351  &     1913.398     &    -1.075  &         0.282        &    -5808.239    &     1692.969     \\\\\n",
       "\\textbf{2593.0}           &   -1671.0046  &     1878.124     &    -0.890  &         0.374        &    -5352.467    &     2010.458     \\\\\n",
       "\\textbf{2596.0}           &    7441.0482  &     2274.848     &     3.271  &         0.001        &     2981.936    &     1.19e+04     \\\\\n",
       "\\textbf{2663.0}           &    6347.2392  &     1932.433     &     3.285  &         0.001        &     2559.322    &     1.01e+04     \\\\\n",
       "\\textbf{2771.0}           &    7041.1013  &     2249.918     &     3.129  &         0.002        &     2630.856    &     1.15e+04     \\\\\n",
       "\\textbf{2787.0}           &    3473.5093  &     1967.063     &     1.766  &         0.077        &     -382.290    &     7329.308     \\\\\n",
       "\\textbf{2797.0}           &   -8494.4128  &     2430.898     &    -3.494  &         0.000        &    -1.33e+04    &    -3729.415     \\\\\n",
       "\\textbf{2802.0}           &    5804.6405  &     2119.524     &     2.739  &         0.006        &     1649.990    &     9959.291     \\\\\n",
       "\\textbf{2817.0}           &   -1.835e+04  &     3254.017     &    -5.640  &         0.000        &    -2.47e+04    &     -1.2e+04     \\\\\n",
       "\\textbf{28678.0}          &   -1.353e+04  &     3717.342     &    -3.641  &         0.000        &    -2.08e+04    &    -6246.472     \\\\\n",
       "\\textbf{28701.0}          &    2415.7030  &     1923.666     &     1.256  &         0.209        &    -1355.028    &     6186.434     \\\\\n",
       "\\textbf{28742.0}          &   -3268.2963  &     3086.129     &    -1.059  &         0.290        &    -9317.665    &     2781.073     \\\\\n",
       "\\textbf{2888.0}           &   -1158.8006  &     2113.251     &    -0.548  &         0.583        &    -5301.154    &     2983.552     \\\\\n",
       "\\textbf{2897.0}           &    3246.1953  &     2742.116     &     1.184  &         0.237        &    -2128.846    &     8621.237     \\\\\n",
       "\\textbf{2917.0}           &   -2440.2332  &     1926.409     &    -1.267  &         0.205        &    -6216.342    &     1335.876     \\\\\n",
       "\\textbf{29392.0}          &   -1.796e+04  &     4127.779     &    -4.351  &         0.000        &    -2.61e+04    &    -9869.383     \\\\\n",
       "\\textbf{2950.0}           &     116.3507  &     2708.210     &     0.043  &         0.966        &    -5192.230    &     5424.931     \\\\\n",
       "\\textbf{2951.0}           &    6504.8671  &     2621.410     &     2.481  &         0.013        &     1366.431    &     1.16e+04     \\\\\n",
       "\\textbf{2953.0}           &    2359.5234  &     1899.256     &     1.242  &         0.214        &    -1363.360    &     6082.407     \\\\\n",
       "\\textbf{2960.0}           &   -3349.3851  &     2905.402     &    -1.153  &         0.249        &    -9044.496    &     2345.726     \\\\\n",
       "\\textbf{2975.0}           &   -2023.7214  &     1902.975     &    -1.063  &         0.288        &    -5753.895    &     1706.453     \\\\\n",
       "\\textbf{2982.0}           &    4342.1583  &     2027.537     &     2.142  &         0.032        &      367.821    &     8316.496     \\\\\n",
       "\\textbf{2991.0}           &   -9953.4737  &     2672.549     &    -3.724  &         0.000        &    -1.52e+04    &    -4714.795     \\\\\n",
       "\\textbf{3011.0}           &    1543.4197  &     2034.873     &     0.758  &         0.448        &    -2445.299    &     5532.138     \\\\\n",
       "\\textbf{3015.0}           &    7455.6922  &     2214.322     &     3.367  &         0.001        &     3115.222    &     1.18e+04     \\\\\n",
       "\\textbf{3026.0}           &    3128.6646  &     1975.593     &     1.584  &         0.113        &     -743.854    &     7001.183     \\\\\n",
       "\\textbf{3031.0}           &   -6195.1370  &     3021.612     &    -2.050  &         0.040        &    -1.21e+04    &     -272.233     \\\\\n",
       "\\textbf{3062.0}           &     221.5773  &     1988.875     &     0.111  &         0.911        &    -3676.976    &     4120.130     \\\\\n",
       "\\textbf{3093.0}           &    2571.3322  &     2193.116     &     1.172  &         0.241        &    -1727.571    &     6870.236     \\\\\n",
       "\\textbf{3107.0}           &   -1262.3700  &     3629.311     &    -0.348  &         0.728        &    -8376.473    &     5851.733     \\\\\n",
       "\\textbf{3121.0}           &    4820.8076  &     1938.340     &     2.487  &         0.013        &     1021.313    &     8620.303     \\\\\n",
       "\\textbf{3126.0}           &      50.8772  &     1864.582     &     0.027  &         0.978        &    -3604.039    &     3705.794     \\\\\n",
       "\\textbf{3144.0}           &    5.164e+04  &     2009.846     &    25.695  &         0.000        &     4.77e+04    &     5.56e+04     \\\\\n",
       "\\textbf{3156.0}           &    5528.8925  &     2513.145     &     2.200  &         0.028        &      602.675    &     1.05e+04     \\\\\n",
       "\\textbf{3157.0}           &    2613.5072  &     1914.202     &     1.365  &         0.172        &    -1138.673    &     6365.687     \\\\\n",
       "\\textbf{3170.0}           &    5740.7299  &     1904.419     &     3.014  &         0.003        &     2007.725    &     9473.735     \\\\\n",
       "\\textbf{3178.0}           &    6033.3161  &     2242.532     &     2.690  &         0.007        &     1637.549    &     1.04e+04     \\\\\n",
       "\\textbf{3206.0}           &   -3341.6269  &     2121.164     &    -1.575  &         0.115        &    -7499.490    &      816.236     \\\\\n",
       "\\textbf{3229.0}           &    5746.5719  &     2221.989     &     2.586  &         0.010        &     1391.074    &     1.01e+04     \\\\\n",
       "\\textbf{3235.0}           &    -877.8615  &     2050.458     &    -0.428  &         0.669        &    -4897.129    &     3141.406     \\\\\n",
       "\\textbf{3246.0}           &    4639.7278  &     2100.036     &     2.209  &         0.027        &      523.278    &     8756.177     \\\\\n",
       "\\textbf{3248.0}           &     752.8101  &     1867.693     &     0.403  &         0.687        &    -2908.206    &     4413.826     \\\\\n",
       "\\textbf{3282.0}           &   -1.726e+04  &     2670.598     &    -6.462  &         0.000        &    -2.25e+04    &     -1.2e+04     \\\\\n",
       "\\textbf{3362.0}           &   -6278.6048  &     2254.498     &    -2.785  &         0.005        &    -1.07e+04    &    -1859.382     \\\\\n",
       "\\textbf{3372.0}           &    2659.0910  &     2474.101     &     1.075  &         0.283        &    -2190.594    &     7508.776     \\\\\n",
       "\\textbf{3422.0}           &    -619.6711  &     1876.424     &    -0.330  &         0.741        &    -4297.801    &     3058.459     \\\\\n",
       "\\textbf{3497.0}           &   -8834.0753  &     2554.688     &    -3.458  &         0.001        &    -1.38e+04    &    -3826.426     \\\\\n",
       "\\textbf{3502.0}           &    -479.3749  &     1866.770     &    -0.257  &         0.797        &    -4138.580    &     3179.831     \\\\\n",
       "\\textbf{3504.0}           &   -2477.5731  &     2731.185     &    -0.907  &         0.364        &    -7831.188    &     2876.041     \\\\\n",
       "\\textbf{3505.0}           &   -5156.3375  &     2034.291     &    -2.535  &         0.011        &    -9143.914    &    -1168.761     \\\\\n",
       "\\textbf{3532.0}           &    2706.2668  &     1879.355     &     1.440  &         0.150        &     -977.607    &     6390.141     \\\\\n",
       "\\textbf{3574.0}           &    5468.6194  &     4143.995     &     1.320  &         0.187        &    -2654.357    &     1.36e+04     \\\\\n",
       "\\textbf{3580.0}           &   -1666.0673  &     1876.899     &    -0.888  &         0.375        &    -5345.128    &     2012.994     \\\\\n",
       "\\textbf{3612.0}           &    8495.2292  &     2361.801     &     3.597  &         0.000        &     3865.673    &     1.31e+04     \\\\\n",
       "\\textbf{3619.0}           &    3211.5327  &     2025.667     &     1.585  &         0.113        &     -759.140    &     7182.205     \\\\\n",
       "\\textbf{3622.0}           &    5842.5102  &     2170.677     &     2.692  &         0.007        &     1587.592    &     1.01e+04     \\\\\n",
       "\\textbf{3639.0}           &   -5569.8963  &     2126.935     &    -2.619  &         0.009        &    -9739.072    &    -1400.721     \\\\\n",
       "\\textbf{3650.0}           &   -1.393e+04  &     2714.286     &    -5.134  &         0.000        &    -1.93e+04    &    -8613.380     \\\\\n",
       "\\textbf{3662.0}           &   -8333.9959  &     2446.351     &    -3.407  &         0.001        &    -1.31e+04    &    -3538.707     \\\\\n",
       "\\textbf{3734.0}           &   -8281.2919  &     2120.677     &    -3.905  &         0.000        &    -1.24e+04    &    -4124.382     \\\\\n",
       "\\textbf{3735.0}           &   -9453.1677  &     2966.849     &    -3.186  &         0.001        &    -1.53e+04    &    -3637.608     \\\\\n",
       "\\textbf{3761.0}           &   -6582.6545  &     2223.313     &    -2.961  &         0.003        &    -1.09e+04    &    -2224.560     \\\\\n",
       "\\textbf{3779.0}           &    1112.8960  &     1976.312     &     0.563  &         0.573        &    -2761.032    &     4986.824     \\\\\n",
       "\\textbf{3781.0}           &    3406.6073  &     2407.950     &     1.415  &         0.157        &    -1313.409    &     8126.624     \\\\\n",
       "\\textbf{3782.0}           &   -7465.5064  &     2305.977     &    -3.237  &         0.001        &     -1.2e+04    &    -2945.375     \\\\\n",
       "\\textbf{3786.0}           &    3988.5817  &     1984.616     &     2.010  &         0.044        &       98.377    &     7878.786     \\\\\n",
       "\\textbf{3796.0}           &    1381.6824  &     2044.029     &     0.676  &         0.499        &    -2624.982    &     5388.347     \\\\\n",
       "\\textbf{3821.0}           &    5209.9044  &     2143.230     &     2.431  &         0.015        &     1008.787    &     9411.021     \\\\\n",
       "\\textbf{3835.0}           &   -4218.9592  &     1882.486     &    -2.241  &         0.025        &    -7908.971    &     -528.948     \\\\\n",
       "\\textbf{3839.0}           &   -9770.0399  &     3133.196     &    -3.118  &         0.002        &    -1.59e+04    &    -3628.412     \\\\\n",
       "\\textbf{3840.0}           &    2927.0314  &     1930.956     &     1.516  &         0.130        &     -857.991    &     6712.054     \\\\\n",
       "\\textbf{3895.0}           &    4038.4655  &     1990.440     &     2.029  &         0.042        &      136.844    &     7940.087     \\\\\n",
       "\\textbf{3908.0}           &    6138.5218  &     3219.795     &     1.906  &         0.057        &     -172.857    &     1.24e+04     \\\\\n",
       "\\textbf{3911.0}           &   -9596.9977  &     2507.910     &    -3.827  &         0.000        &    -1.45e+04    &    -4681.042     \\\\\n",
       "\\textbf{3917.0}           &    5226.1949  &     2162.918     &     2.416  &         0.016        &      986.486    &     9465.904     \\\\\n",
       "\\textbf{3946.0}           &    6973.0171  &     2206.800     &     3.160  &         0.002        &     2647.291    &     1.13e+04     \\\\\n",
       "\\textbf{3971.0}           &    1979.1902  &     1991.465     &     0.994  &         0.320        &    -1924.440    &     5882.820     \\\\\n",
       "\\textbf{3980.0}           &    1.202e+04  &     1932.033     &     6.220  &         0.000        &     8230.312    &     1.58e+04     \\\\\n",
       "\\textbf{4034.0}           &   -5921.5990  &     2070.569     &    -2.860  &         0.004        &    -9980.288    &    -1862.910     \\\\\n",
       "\\textbf{4036.0}           &    5115.5995  &     2046.651     &     2.499  &         0.012        &     1103.795    &     9127.404     \\\\\n",
       "\\textbf{4040.0}           &    3206.5487  &     1921.949     &     1.668  &         0.095        &     -560.817    &     6973.914     \\\\\n",
       "\\textbf{4058.0}           &    3051.0613  &     1892.804     &     1.612  &         0.107        &     -659.176    &     6761.299     \\\\\n",
       "\\textbf{4060.0}           &   -1.628e+04  &     2856.127     &    -5.701  &         0.000        &    -2.19e+04    &    -1.07e+04     \\\\\n",
       "\\textbf{4062.0}           &    1.007e+04  &     2332.242     &     4.319  &         0.000        &     5502.034    &     1.46e+04     \\\\\n",
       "\\textbf{4077.0}           &    3780.6434  &     3349.737     &     1.129  &         0.259        &    -2785.444    &     1.03e+04     \\\\\n",
       "\\textbf{4087.0}           &   -2.322e+04  &     3294.134     &    -7.048  &         0.000        &    -2.97e+04    &    -1.68e+04     \\\\\n",
       "\\textbf{4091.0}           &    1998.9224  &     2477.718     &     0.807  &         0.420        &    -2857.851    &     6855.696     \\\\\n",
       "\\textbf{4127.0}           &   -3525.5756  &     1983.603     &    -1.777  &         0.076        &    -7413.796    &      362.644     \\\\\n",
       "\\textbf{4138.0}           &    5914.4417  &     2743.566     &     2.156  &         0.031        &      536.558    &     1.13e+04     \\\\\n",
       "\\textbf{4162.0}           &    2560.6378  &     2470.849     &     1.036  &         0.300        &    -2282.673    &     7403.948     \\\\\n",
       "\\textbf{4186.0}           &    5907.9693  &     2117.104     &     2.791  &         0.005        &     1758.063    &     1.01e+04     \\\\\n",
       "\\textbf{4194.0}           &    1285.6791  &     2336.059     &     0.550  &         0.582        &    -3293.417    &     5864.776     \\\\\n",
       "\\textbf{4199.0}           &   -1.585e+04  &     3474.430     &    -4.562  &         0.000        &    -2.27e+04    &    -9038.799     \\\\\n",
       "\\textbf{4213.0}           &     808.3674  &     1862.423     &     0.434  &         0.664        &    -2842.317    &     4459.052     \\\\\n",
       "\\textbf{4222.0}           &   -3607.1406  &     1989.203     &    -1.813  &         0.070        &    -7506.338    &      292.057     \\\\\n",
       "\\textbf{4223.0}           &    1403.0178  &     1874.897     &     0.748  &         0.454        &    -2272.119    &     5078.154     \\\\\n",
       "\\textbf{4251.0}           &    5148.4199  &     2065.112     &     2.493  &         0.013        &     1100.428    &     9196.412     \\\\\n",
       "\\textbf{4265.0}           &    3631.2777  &     2168.770     &     1.674  &         0.094        &     -619.902    &     7882.457     \\\\\n",
       "\\textbf{4274.0}           &   -2806.8085  &     2047.358     &    -1.371  &         0.170        &    -6819.999    &     1206.382     \\\\\n",
       "\\textbf{4321.0}           &   -4667.3963  &     2422.014     &    -1.927  &         0.054        &    -9414.980    &       80.188     \\\\\n",
       "\\textbf{4335.0}           &    -608.2474  &     3314.435     &    -0.184  &         0.854        &    -7105.138    &     5888.643     \\\\\n",
       "\\textbf{4340.0}           &   -1599.1975  &     1939.399     &    -0.825  &         0.410        &    -5400.769    &     2202.374     \\\\\n",
       "\\textbf{4371.0}           &   -3398.3487  &     1976.182     &    -1.720  &         0.086        &    -7272.021    &      475.324     \\\\\n",
       "\\textbf{4415.0}           &    4653.8189  &     2130.264     &     2.185  &         0.029        &      478.117    &     8829.521     \\\\\n",
       "\\textbf{4450.0}           &    4454.0494  &     2011.609     &     2.214  &         0.027        &      510.933    &     8397.166     \\\\\n",
       "\\textbf{4476.0}           &   -6248.1810  &     2401.134     &    -2.602  &         0.009        &     -1.1e+04    &    -1541.525     \\\\\n",
       "\\textbf{4510.0}           &   -1.049e+04  &     2418.176     &    -4.339  &         0.000        &    -1.52e+04    &    -5752.472     \\\\\n",
       "\\textbf{4520.0}           &   -3048.1121  &     1955.438     &    -1.559  &         0.119        &    -6881.122    &      784.898     \\\\\n",
       "\\textbf{4551.0}           &    3101.4769  &     8122.033     &     0.382  &         0.703        &    -1.28e+04    &      1.9e+04     \\\\\n",
       "\\textbf{4568.0}           &     397.0851  &     1971.862     &     0.201  &         0.840        &    -3468.119    &     4262.290     \\\\\n",
       "\\textbf{4579.0}           &    8237.8741  &     2353.238     &     3.501  &         0.000        &     3625.102    &     1.29e+04     \\\\\n",
       "\\textbf{4585.0}           &    5651.0424  &     2106.274     &     2.683  &         0.007        &     1522.365    &     9779.719     \\\\\n",
       "\\textbf{4595.0}           &     172.3713  &     1865.018     &     0.092  &         0.926        &    -3483.401    &     3828.144     \\\\\n",
       "\\textbf{4600.0}           &   -1591.7797  &     1994.959     &    -0.798  &         0.425        &    -5502.260    &     2318.700     \\\\\n",
       "\\textbf{4607.0}           &    5341.7990  &     2081.956     &     2.566  &         0.010        &     1260.789    &     9422.809     \\\\\n",
       "\\textbf{4608.0}           &    1066.2183  &     1867.881     &     0.571  &         0.568        &    -2595.166    &     4727.602     \\\\\n",
       "\\textbf{4622.0}           &   -8825.8179  &     2440.966     &    -3.616  &         0.000        &    -1.36e+04    &    -4041.085     \\\\\n",
       "\\textbf{4623.0}           &    4675.2103  &     2070.222     &     2.258  &         0.024        &      617.203    &     8733.218     \\\\\n",
       "\\textbf{4768.0}           &     997.5575  &     1888.424     &     0.528  &         0.597        &    -2704.093    &     4699.208     \\\\\n",
       "\\textbf{4771.0}           &    6203.3110  &     2153.731     &     2.880  &         0.004        &     1981.610    &     1.04e+04     \\\\\n",
       "\\textbf{4800.0}           &    4722.3159  &     2136.453     &     2.210  &         0.027        &      534.482    &     8910.150     \\\\\n",
       "\\textbf{4802.0}           &    4130.2512  &     1994.054     &     2.071  &         0.038        &      221.545    &     8038.957     \\\\\n",
       "\\textbf{4807.0}           &    2675.3897  &     1965.555     &     1.361  &         0.173        &    -1177.452    &     6528.231     \\\\\n",
       "\\textbf{4839.0}           &   -1.191e+05  &     4233.271     &   -28.133  &         0.000        &    -1.27e+05    &    -1.11e+05     \\\\\n",
       "\\textbf{4843.0}           &    4091.0901  &     1925.916     &     2.124  &         0.034        &      315.947    &     7866.233     \\\\\n",
       "\\textbf{4881.0}           &   -2251.8864  &     1918.174     &    -1.174  &         0.240        &    -6011.853    &     1508.080     \\\\\n",
       "\\textbf{4900.0}           &    4257.5069  &     1996.202     &     2.133  &         0.033        &      344.592    &     8170.422     \\\\\n",
       "\\textbf{4926.0}           &   -3476.2071  &     1957.953     &    -1.775  &         0.076        &    -7314.148    &      361.734     \\\\\n",
       "\\textbf{4941.0}           &     542.3435  &     1882.604     &     0.288  &         0.773        &    -3147.900    &     4232.587     \\\\\n",
       "\\textbf{4961.0}           &   -1.128e+04  &     3590.856     &    -3.143  &         0.002        &    -1.83e+04    &    -4246.042     \\\\\n",
       "\\textbf{4988.0}           &    1.212e+04  &     2166.411     &     5.596  &         0.000        &     7876.786    &     1.64e+04     \\\\\n",
       "\\textbf{4993.0}           &    8210.1591  &     2352.873     &     3.489  &         0.000        &     3598.104    &     1.28e+04     \\\\\n",
       "\\textbf{5018.0}           &   -1.448e+04  &     3024.558     &    -4.788  &         0.000        &    -2.04e+04    &    -8552.920     \\\\\n",
       "\\textbf{5020.0}           &    3930.1478  &     1869.322     &     2.102  &         0.036        &      265.939    &     7594.357     \\\\\n",
       "\\textbf{5027.0}           &   -2780.8939  &     1938.519     &    -1.435  &         0.151        &    -6580.740    &     1018.952     \\\\\n",
       "\\textbf{5032.0}           &    2085.8418  &     1902.032     &     1.097  &         0.273        &    -1642.484    &     5814.168     \\\\\n",
       "\\textbf{5043.0}           &    -616.7749  &     1867.760     &    -0.330  &         0.741        &    -4277.921    &     3044.371     \\\\\n",
       "\\textbf{5046.0}           &   -1.729e+04  &     3031.667     &    -5.704  &         0.000        &    -2.32e+04    &    -1.14e+04     \\\\\n",
       "\\textbf{5047.0}           &    2.379e+04  &     4241.993     &     5.608  &         0.000        &     1.55e+04    &     3.21e+04     \\\\\n",
       "\\textbf{5065.0}           &    4805.6677  &     2390.351     &     2.010  &         0.044        &      120.148    &     9491.187     \\\\\n",
       "\\textbf{5071.0}           &    4759.8373  &     2387.440     &     1.994  &         0.046        &       80.025    &     9439.649     \\\\\n",
       "\\textbf{5073.0}           &   -1.659e+05  &     6360.642     &   -26.085  &         0.000        &    -1.78e+05    &    -1.53e+05     \\\\\n",
       "\\textbf{5087.0}           &   -1074.7682  &     1864.101     &    -0.577  &         0.564        &    -4728.742    &     2579.206     \\\\\n",
       "\\textbf{5109.0}           &    6156.8224  &     2173.231     &     2.833  &         0.005        &     1896.897    &     1.04e+04     \\\\\n",
       "\\textbf{5116.0}           &   -1063.4051  &     2121.551     &    -0.501  &         0.616        &    -5222.027    &     3095.217     \\\\\n",
       "\\textbf{5122.0}           &   -3169.4565  &     1952.593     &    -1.623  &         0.105        &    -6996.892    &      657.979     \\\\\n",
       "\\textbf{5134.0}           &   -9160.0809  &     1987.931     &    -4.608  &         0.000        &    -1.31e+04    &    -5263.377     \\\\\n",
       "\\textbf{5142.0}           &     318.4028  &     2710.019     &     0.117  &         0.906        &    -4993.722    &     5630.528     \\\\\n",
       "\\textbf{5165.0}           &    6750.8497  &     2559.942     &     2.637  &         0.008        &     1732.902    &     1.18e+04     \\\\\n",
       "\\textbf{5169.0}           &    1.511e+04  &     2001.234     &     7.550  &         0.000        &     1.12e+04    &      1.9e+04     \\\\\n",
       "\\textbf{5174.0}           &    1640.8492  &     2271.830     &     0.722  &         0.470        &    -2812.347    &     6094.045     \\\\\n",
       "\\textbf{5179.0}           &    5217.9213  &     2052.345     &     2.542  &         0.011        &     1194.956    &     9240.887     \\\\\n",
       "\\textbf{5181.0}           &    5800.0679  &     2167.718     &     2.676  &         0.007        &     1550.950    &        1e+04     \\\\\n",
       "\\textbf{5187.0}           &    5893.9436  &     2526.090     &     2.333  &         0.020        &      942.352    &     1.08e+04     \\\\\n",
       "\\textbf{5229.0}           &   -9469.7480  &     2405.312     &    -3.937  &         0.000        &    -1.42e+04    &    -4754.904     \\\\\n",
       "\\textbf{5234.0}           &   -1.212e+04  &     2089.843     &    -5.800  &         0.000        &    -1.62e+04    &    -8024.707     \\\\\n",
       "\\textbf{5237.0}           &    3671.2141  &     1960.977     &     1.872  &         0.061        &     -172.654    &     7515.082     \\\\\n",
       "\\textbf{5252.0}           &     205.2720  &     1864.926     &     0.110  &         0.912        &    -3450.320    &     3860.864     \\\\\n",
       "\\textbf{5254.0}           &       1.5661  &     1866.155     &     0.001  &         0.999        &    -3656.434    &     3659.566     \\\\\n",
       "\\textbf{5306.0}           &   -8164.5373  &     2537.338     &    -3.218  &         0.001        &    -1.31e+04    &    -3190.898     \\\\\n",
       "\\textbf{5338.0}           &    2988.8536  &     1924.135     &     1.553  &         0.120        &     -782.799    &     6760.506     \\\\\n",
       "\\textbf{5377.0}           &    6499.7649  &     2207.361     &     2.945  &         0.003        &     2172.939    &     1.08e+04     \\\\\n",
       "\\textbf{5439.0}           &    4154.1516  &     2030.818     &     2.046  &         0.041        &      173.383    &     8134.920     \\\\\n",
       "\\textbf{5456.0}           &    8040.7845  &     2345.467     &     3.428  &         0.001        &     3443.247    &     1.26e+04     \\\\\n",
       "\\textbf{5464.0}           &    -408.0736  &     2448.885     &    -0.167  &         0.868        &    -5208.330    &     4392.182     \\\\\n",
       "\\textbf{5476.0}           &    8930.2376  &     2386.371     &     3.742  &         0.000        &     4252.521    &     1.36e+04     \\\\\n",
       "\\textbf{5492.0}           &   -1.613e+04  &     3387.599     &    -4.762  &         0.000        &    -2.28e+04    &    -9490.202     \\\\\n",
       "\\textbf{5496.0}           &    1502.5059  &     1881.197     &     0.799  &         0.424        &    -2184.979    &     5189.991     \\\\\n",
       "\\textbf{5505.0}           &    5540.7848  &     2130.770     &     2.600  &         0.009        &     1364.091    &     9717.479     \\\\\n",
       "\\textbf{5518.0}           &    4055.3226  &     2328.914     &     1.741  &         0.082        &     -509.768    &     8620.413     \\\\\n",
       "\\textbf{5520.0}           &    1115.9623  &     1872.844     &     0.596  &         0.551        &    -2555.149    &     4787.074     \\\\\n",
       "\\textbf{5545.0}           &    3706.3908  &     2059.949     &     1.799  &         0.072        &     -331.480    &     7744.262     \\\\\n",
       "\\textbf{5568.0}           &    5191.3374  &     1903.722     &     2.727  &         0.006        &     1459.698    &     8922.976     \\\\\n",
       "\\textbf{5569.0}           &    5268.3797  &     2121.742     &     2.483  &         0.013        &     1109.384    &     9427.376     \\\\\n",
       "\\textbf{5578.0}           &    4546.5266  &     2004.631     &     2.268  &         0.023        &      617.088    &     8475.965     \\\\\n",
       "\\textbf{5581.0}           &    3827.8583  &     1945.265     &     1.968  &         0.049        &       14.788    &     7640.928     \\\\\n",
       "\\textbf{5589.0}           &   -1.016e+04  &     2687.164     &    -3.782  &         0.000        &    -1.54e+04    &    -4894.780     \\\\\n",
       "\\textbf{5597.0}           &    3641.4163  &     2590.049     &     1.406  &         0.160        &    -1435.547    &     8718.380     \\\\\n",
       "\\textbf{5606.0}           &    -2.49e+04  &     2945.544     &    -8.452  &         0.000        &    -3.07e+04    &    -1.91e+04     \\\\\n",
       "\\textbf{5639.0}           &    7378.7637  &     2191.166     &     3.368  &         0.001        &     3083.684    &     1.17e+04     \\\\\n",
       "\\textbf{5667.0}           &    8260.7074  &     2387.777     &     3.460  &         0.001        &     3580.234    &     1.29e+04     \\\\\n",
       "\\textbf{5690.0}           &    3985.0621  &     1977.021     &     2.016  &         0.044        &      109.744    &     7860.380     \\\\\n",
       "\\textbf{5709.0}           &    3716.4396  &     2034.262     &     1.827  &         0.068        &     -271.082    &     7703.961     \\\\\n",
       "\\textbf{5726.0}           &    4996.0601  &     2035.830     &     2.454  &         0.014        &     1005.466    &     8986.654     \\\\\n",
       "\\textbf{5764.0}           &   -2603.1282  &     1992.133     &    -1.307  &         0.191        &    -6508.068    &     1301.812     \\\\\n",
       "\\textbf{5772.0}           &    1519.0392  &     1881.855     &     0.807  &         0.420        &    -2169.737    &     5207.815     \\\\\n",
       "\\textbf{5860.0}           &   -2.109e+04  &     2637.276     &    -7.995  &         0.000        &    -2.63e+04    &    -1.59e+04     \\\\\n",
       "\\textbf{5878.0}           &    1364.7944  &     1907.846     &     0.715  &         0.474        &    -2374.927    &     5104.516     \\\\\n",
       "\\textbf{5903.0}           &    2067.7321  &     1992.452     &     1.038  &         0.299        &    -1837.833    &     5973.297     \\\\\n",
       "\\textbf{5905.0}           &   -1864.7784  &     1886.734     &    -0.988  &         0.323        &    -5563.116    &     1833.560     \\\\\n",
       "\\textbf{5959.0}           &   -4990.6884  &     2036.050     &    -2.451  &         0.014        &    -8981.714    &     -999.663     \\\\\n",
       "\\textbf{6008.0}           &    1.741e+04  &     2880.320     &     6.046  &         0.000        &     1.18e+04    &     2.31e+04     \\\\\n",
       "\\textbf{6034.0}           &   -1.003e+04  &     2718.706     &    -3.690  &         0.000        &    -1.54e+04    &    -4703.137     \\\\\n",
       "\\textbf{6035.0}           &    2579.2198  &     2486.002     &     1.037  &         0.300        &    -2293.792    &     7452.232     \\\\\n",
       "\\textbf{6036.0}           &   -6898.4988  &     2196.252     &    -3.141  &         0.002        &    -1.12e+04    &    -2593.448     \\\\\n",
       "\\textbf{6039.0}           &    3501.6223  &     1953.934     &     1.792  &         0.073        &     -328.440    &     7331.685     \\\\\n",
       "\\textbf{6044.0}           &    7077.4692  &     2465.742     &     2.870  &         0.004        &     2244.170    &     1.19e+04     \\\\\n",
       "\\textbf{6066.0}           &   -9772.4102  &     4154.307     &    -2.352  &         0.019        &    -1.79e+04    &    -1629.220     \\\\\n",
       "\\textbf{6078.0}           &    5496.2272  &     1920.170     &     2.862  &         0.004        &     1732.348    &     9260.106     \\\\\n",
       "\\textbf{6081.0}           &   -1.406e+04  &     2549.252     &    -5.516  &         0.000        &    -1.91e+04    &    -9065.544     \\\\\n",
       "\\textbf{60893.0}          &    3239.1093  &     4129.186     &     0.784  &         0.433        &    -4854.840    &     1.13e+04     \\\\\n",
       "\\textbf{6097.0}           &    7640.1809  &     2207.786     &     3.461  &         0.001        &     3312.522    &      1.2e+04     \\\\\n",
       "\\textbf{6102.0}           &    2843.4365  &     1996.673     &     1.424  &         0.154        &    -1070.402    &     6757.275     \\\\\n",
       "\\textbf{6104.0}           &   -8300.1663  &     2037.445     &    -4.074  &         0.000        &    -1.23e+04    &    -4306.407     \\\\\n",
       "\\textbf{6109.0}           &   -6360.6720  &     2216.257     &    -2.870  &         0.004        &    -1.07e+04    &    -2016.410     \\\\\n",
       "\\textbf{6127.0}           &   -6787.9531  &     2240.150     &    -3.030  &         0.002        &    -1.12e+04    &    -2396.854     \\\\\n",
       "\\textbf{61552.0}          &   -1.149e+04  &     3882.144     &    -2.960  &         0.003        &    -1.91e+04    &    -3883.247     \\\\\n",
       "\\textbf{6158.0}           &    -916.8590  &     2043.647     &    -0.449  &         0.654        &    -4922.776    &     3089.058     \\\\\n",
       "\\textbf{6171.0}           &    3378.2117  &     1939.055     &     1.742  &         0.082        &     -422.686    &     7179.109     \\\\\n",
       "\\textbf{61780.0}          &   -1362.8843  &     4057.286     &    -0.336  &         0.737        &    -9315.897    &     6590.129     \\\\\n",
       "\\textbf{6207.0}           &    4778.5766  &     2033.773     &     2.350  &         0.019        &      792.016    &     8765.138     \\\\\n",
       "\\textbf{6214.0}           &    2860.6555  &     1924.860     &     1.486  &         0.137        &     -912.416    &     6633.727     \\\\\n",
       "\\textbf{6216.0}           &    7726.5050  &     2345.597     &     3.294  &         0.001        &     3128.712    &     1.23e+04     \\\\\n",
       "\\textbf{62221.0}          &    3663.3054  &     4132.258     &     0.887  &         0.375        &    -4436.666    &     1.18e+04     \\\\\n",
       "\\textbf{6259.0}           &    5957.1127  &     2391.462     &     2.491  &         0.013        &     1269.416    &     1.06e+04     \\\\\n",
       "\\textbf{62599.0}          &     568.0216  &     5686.343     &     0.100  &         0.920        &    -1.06e+04    &     1.17e+04     \\\\\n",
       "\\textbf{6266.0}           &    9467.6831  &     2313.372     &     4.093  &         0.000        &     4933.056    &      1.4e+04     \\\\\n",
       "\\textbf{6268.0}           &   -8787.1572  &     2112.679     &    -4.159  &         0.000        &    -1.29e+04    &    -4645.925     \\\\\n",
       "\\textbf{6288.0}           &    -908.5221  &     1881.992     &    -0.483  &         0.629        &    -4597.565    &     2780.521     \\\\\n",
       "\\textbf{6297.0}           &    1464.9049  &     1982.188     &     0.739  &         0.460        &    -2420.541    &     5350.351     \\\\\n",
       "\\textbf{6307.0}           &   -2.216e+04  &     2856.979     &    -7.757  &         0.000        &    -2.78e+04    &    -1.66e+04     \\\\\n",
       "\\textbf{6313.0}           &    4618.1251  &     3427.161     &     1.348  &         0.178        &    -2099.729    &     1.13e+04     \\\\\n",
       "\\textbf{6314.0}           &    6492.4201  &     2181.697     &     2.976  &         0.003        &     2215.901    &     1.08e+04     \\\\\n",
       "\\textbf{6326.0}           &   -9508.0014  &     2508.589     &    -3.790  &         0.000        &    -1.44e+04    &    -4590.714     \\\\\n",
       "\\textbf{6349.0}           &    2480.1364  &     1910.620     &     1.298  &         0.194        &    -1265.024    &     6225.296     \\\\\n",
       "\\textbf{6357.0}           &    5776.3456  &     2247.337     &     2.570  &         0.010        &     1371.159    &     1.02e+04     \\\\\n",
       "\\textbf{6375.0}           &     1.01e+04  &     2040.587     &     4.948  &         0.000        &     6097.595    &     1.41e+04     \\\\\n",
       "\\textbf{6376.0}           &    5614.1876  &     2140.060     &     2.623  &         0.009        &     1419.284    &     9809.091     \\\\\n",
       "\\textbf{6379.0}           &    8938.8647  &     5781.832     &     1.546  &         0.122        &    -2394.569    &     2.03e+04     \\\\\n",
       "\\textbf{6386.0}           &    3710.1340  &     1955.122     &     1.898  &         0.058        &     -122.257    &     7542.525     \\\\\n",
       "\\textbf{6403.0}           &    1056.7945  &     1959.891     &     0.539  &         0.590        &    -2784.945    &     4898.534     \\\\\n",
       "\\textbf{6410.0}           &    7553.1609  &     2284.474     &     3.306  &         0.001        &     3075.181    &      1.2e+04     \\\\\n",
       "\\textbf{6416.0}           &    1492.3880  &     1988.851     &     0.750  &         0.453        &    -2406.119    &     5390.895     \\\\\n",
       "\\textbf{6424.0}           &    5001.3026  &     2068.519     &     2.418  &         0.016        &      946.633    &     9055.973     \\\\\n",
       "\\textbf{6433.0}           &    7686.9981  &     2299.971     &     3.342  &         0.001        &     3178.640    &     1.22e+04     \\\\\n",
       "\\textbf{6435.0}           &    -737.8230  &     2057.950     &    -0.359  &         0.720        &    -4771.777    &     3296.131     \\\\\n",
       "\\textbf{6492.0}           &   -6962.4962  &     2261.952     &    -3.078  &         0.002        &    -1.14e+04    &    -2528.662     \\\\\n",
       "\\textbf{6497.0}           &    6239.3979  &     2154.347     &     2.896  &         0.004        &     2016.490    &     1.05e+04     \\\\\n",
       "\\textbf{6500.0}           &    7548.7349  &     8225.317     &     0.918  &         0.359        &    -8574.370    &     2.37e+04     \\\\\n",
       "\\textbf{6509.0}           &    1273.6530  &     1874.999     &     0.679  &         0.497        &    -2401.682    &     4948.988     \\\\\n",
       "\\textbf{6527.0}           &    5996.2601  &     2494.329     &     2.404  &         0.016        &     1106.925    &     1.09e+04     \\\\\n",
       "\\textbf{6528.0}           &    4138.6189  &     2205.290     &     1.877  &         0.061        &     -184.148    &     8461.386     \\\\\n",
       "\\textbf{6531.0}           &    -1.46e+04  &     2501.366     &    -5.836  &         0.000        &    -1.95e+04    &    -9693.994     \\\\\n",
       "\\textbf{6532.0}           &    -909.3400  &     1922.830     &    -0.473  &         0.636        &    -4678.433    &     2859.753     \\\\\n",
       "\\textbf{6543.0}           &    5667.3057  &     2107.568     &     2.689  &         0.007        &     1536.092    &     9798.519     \\\\\n",
       "\\textbf{6548.0}           &    5442.1657  &     2102.652     &     2.588  &         0.010        &     1320.589    &     9563.742     \\\\\n",
       "\\textbf{6550.0}           &    3292.2371  &     2176.446     &     1.513  &         0.130        &     -973.990    &     7558.464     \\\\\n",
       "\\textbf{6552.0}           &    5851.9235  &     2332.408     &     2.509  &         0.012        &     1279.984    &     1.04e+04     \\\\\n",
       "\\textbf{6565.0}           &   -1.029e+04  &     2789.761     &    -3.689  &         0.000        &    -1.58e+04    &    -4821.628     \\\\\n",
       "\\textbf{6571.0}           &    3479.5408  &     1963.291     &     1.772  &         0.076        &     -368.864    &     7327.945     \\\\\n",
       "\\textbf{6573.0}           &    2775.7816  &     1913.330     &     1.451  &         0.147        &     -974.691    &     6526.254     \\\\\n",
       "\\textbf{6641.0}           &    3055.6252  &     4081.419     &     0.749  &         0.454        &    -4944.692    &     1.11e+04     \\\\\n",
       "\\textbf{6649.0}           &    6919.6160  &     2188.499     &     3.162  &         0.002        &     2629.763    &     1.12e+04     \\\\\n",
       "\\textbf{6730.0}           &    1.006e+04  &     2161.155     &     4.655  &         0.000        &     5824.690    &     1.43e+04     \\\\\n",
       "\\textbf{6731.0}           &   -7019.9476  &     2270.362     &    -3.092  &         0.002        &    -1.15e+04    &    -2569.628     \\\\\n",
       "\\textbf{6742.0}           &    7170.2119  &     4326.943     &     1.657  &         0.098        &    -1311.377    &     1.57e+04     \\\\\n",
       "\\textbf{6745.0}           &    5044.4407  &     2056.453     &     2.453  &         0.014        &     1013.422    &     9075.459     \\\\\n",
       "\\textbf{6756.0}           &    7115.0126  &     2232.742     &     3.187  &         0.001        &     2738.436    &     1.15e+04     \\\\\n",
       "\\textbf{6765.0}           &   -1.086e+04  &     2485.280     &    -4.368  &         0.000        &    -1.57e+04    &    -5983.673     \\\\\n",
       "\\textbf{6768.0}           &    9142.6171  &     2428.250     &     3.765  &         0.000        &     4382.809    &     1.39e+04     \\\\\n",
       "\\textbf{6774.0}           &   -1.663e+04  &     2635.587     &    -6.311  &         0.000        &    -2.18e+04    &    -1.15e+04     \\\\\n",
       "\\textbf{6797.0}           &    6532.6165  &     2636.852     &     2.477  &         0.013        &     1363.912    &     1.17e+04     \\\\\n",
       "\\textbf{6803.0}           &    6239.7276  &     2168.069     &     2.878  &         0.004        &     1989.922    &     1.05e+04     \\\\\n",
       "\\textbf{6821.0}           &    3571.1369  &     1971.184     &     1.812  &         0.070        &     -292.740    &     7435.014     \\\\\n",
       "\\textbf{6830.0}           &    5691.0431  &     2087.521     &     2.726  &         0.006        &     1599.125    &     9782.961     \\\\\n",
       "\\textbf{6845.0}           &   -7051.5009  &     2278.308     &    -3.095  &         0.002        &    -1.15e+04    &    -2585.606     \\\\\n",
       "\\textbf{6848.0}           &    6681.3227  &     2396.308     &     2.788  &         0.005        &     1984.126    &     1.14e+04     \\\\\n",
       "\\textbf{6873.0}           &   -3738.7082  &     2708.085     &    -1.381  &         0.167        &    -9047.043    &     1569.627     \\\\\n",
       "\\textbf{6900.0}           &   -3750.3918  &     2002.203     &    -1.873  &         0.061        &    -7675.071    &      174.288     \\\\\n",
       "\\textbf{6908.0}           &   -2916.1307  &     1957.450     &    -1.490  &         0.136        &    -6753.086    &      920.825     \\\\\n",
       "\\textbf{6994.0}           &    5542.6486  &     2083.600     &     2.660  &         0.008        &     1458.416    &     9626.881     \\\\\n",
       "\\textbf{7045.0}           &    1363.0136  &     2484.971     &     0.549  &         0.583        &    -3507.977    &     6234.004     \\\\\n",
       "\\textbf{7065.0}           &    3498.3989  &     1873.146     &     1.868  &         0.062        &     -173.305    &     7170.103     \\\\\n",
       "\\textbf{7085.0}           &    3721.4889  &     1871.454     &     1.989  &         0.047        &       53.101    &     7389.877     \\\\\n",
       "\\textbf{7107.0}           &   -2262.4082  &     2076.355     &    -1.090  &         0.276        &    -6332.438    &     1807.622     \\\\\n",
       "\\textbf{7116.0}           &    8409.7314  &     2307.397     &     3.645  &         0.000        &     3886.817    &     1.29e+04     \\\\\n",
       "\\textbf{7117.0}           &    8374.1123  &     3393.680     &     2.468  &         0.014        &     1721.888    &      1.5e+04     \\\\\n",
       "\\textbf{7121.0}           &    3790.6464  &     1973.720     &     1.921  &         0.055        &      -78.200    &     7659.493     \\\\\n",
       "\\textbf{7127.0}           &    6381.3813  &     2447.749     &     2.607  &         0.009        &     1583.352    &     1.12e+04     \\\\\n",
       "\\textbf{7139.0}           &    2511.3497  &     1916.834     &     1.310  &         0.190        &    -1245.991    &     6268.690     \\\\\n",
       "\\textbf{7146.0}           &    4706.9041  &     2030.512     &     2.318  &         0.020        &      726.734    &     8687.074     \\\\\n",
       "\\textbf{7163.0}           &    7757.6182  &     2072.114     &     3.744  &         0.000        &     3695.902    &     1.18e+04     \\\\\n",
       "\\textbf{7180.0}           &   -2421.7565  &     1874.454     &    -1.292  &         0.196        &    -6096.024    &     1252.511     \\\\\n",
       "\\textbf{7183.0}           &   -5726.5396  &     2170.948     &    -2.638  &         0.008        &    -9981.990    &    -1471.090     \\\\\n",
       "\\textbf{7228.0}           &    1.455e+04  &     2133.403     &     6.818  &         0.000        &     1.04e+04    &     1.87e+04     \\\\\n",
       "\\textbf{7232.0}           &    6731.6393  &     3255.395     &     2.068  &         0.039        &      350.477    &     1.31e+04     \\\\\n",
       "\\textbf{7250.0}           &    2138.3776  &     2197.752     &     0.973  &         0.331        &    -2169.613    &     6446.369     \\\\\n",
       "\\textbf{7257.0}           &    3.176e+04  &     2117.435     &    15.001  &         0.000        &     2.76e+04    &     3.59e+04     \\\\\n",
       "\\textbf{7260.0}           &    2408.8671  &     1897.741     &     1.269  &         0.204        &    -1311.047    &     6128.781     \\\\\n",
       "\\textbf{7267.0}           &    2360.5998  &     2342.473     &     1.008  &         0.314        &    -2231.070    &     6952.269     \\\\\n",
       "\\textbf{7268.0}           &    5187.9468  &     2167.823     &     2.393  &         0.017        &      938.622    &     9437.272     \\\\\n",
       "\\textbf{7281.0}           &    5486.3808  &     2996.105     &     1.831  &         0.067        &     -386.526    &     1.14e+04     \\\\\n",
       "\\textbf{7291.0}           &   -3585.6338  &     1998.396     &    -1.794  &         0.073        &    -7502.850    &      331.582     \\\\\n",
       "\\textbf{7343.0}           &   -5333.1941  &     3491.925     &    -1.527  &         0.127        &    -1.22e+04    &     1511.608     \\\\\n",
       "\\textbf{7346.0}           &     651.5341  &     1867.455     &     0.349  &         0.727        &    -3009.013    &     4312.082     \\\\\n",
       "\\textbf{7401.0}           &    3041.9554  &     1931.405     &     1.575  &         0.115        &     -743.946    &     6827.857     \\\\\n",
       "\\textbf{7409.0}           &     310.1206  &     1864.417     &     0.166  &         0.868        &    -3344.473    &     3964.714     \\\\\n",
       "\\textbf{7420.0}           &   -2069.2980  &     1904.095     &    -1.087  &         0.277        &    -5801.668    &     1663.072     \\\\\n",
       "\\textbf{7435.0}           &   -4300.2979  &     3336.017     &    -1.289  &         0.197        &    -1.08e+04    &     2238.898     \\\\\n",
       "\\textbf{7466.0}           &    4575.6767  &     2162.039     &     2.116  &         0.034        &      337.691    &     8813.662     \\\\\n",
       "\\textbf{7486.0}           &    3327.0591  &     1944.850     &     1.711  &         0.087        &     -485.198    &     7139.317     \\\\\n",
       "\\textbf{7503.0}           &    1110.9578  &     3629.557     &     0.306  &         0.760        &    -6003.628    &     8225.543     \\\\\n",
       "\\textbf{7506.0}           &    1681.1847  &     1864.510     &     0.902  &         0.367        &    -1973.592    &     5335.961     \\\\\n",
       "\\textbf{7537.0}           &    6068.4666  &     2125.652     &     2.855  &         0.004        &     1901.806    &     1.02e+04     \\\\\n",
       "\\textbf{7549.0}           &   -3132.7330  &     1947.742     &    -1.608  &         0.108        &    -6950.659    &      685.193     \\\\\n",
       "\\textbf{7554.0}           &    5074.5529  &     2089.598     &     2.428  &         0.015        &      978.565    &     9170.541     \\\\\n",
       "\\textbf{7557.0}           &   -5768.0515  &     2234.127     &    -2.582  &         0.010        &    -1.01e+04    &    -1388.759     \\\\\n",
       "\\textbf{7585.0}           &   -8360.7510  &     3225.690     &    -2.592  &         0.010        &    -1.47e+04    &    -2037.817     \\\\\n",
       "\\textbf{7602.0}           &    4115.2606  &     1990.738     &     2.067  &         0.039        &      213.056    &     8017.465     \\\\\n",
       "\\textbf{7620.0}           &    5981.9239  &     2399.900     &     2.493  &         0.013        &     1277.687    &     1.07e+04     \\\\\n",
       "\\textbf{7636.0}           &    4996.1309  &     2052.820     &     2.434  &         0.015        &      972.233    &     9020.029     \\\\\n",
       "\\textbf{7646.0}           &    5066.5700  &     2057.716     &     2.462  &         0.014        &     1033.076    &     9100.064     \\\\\n",
       "\\textbf{7658.0}           &   -6023.6247  &     2149.646     &    -2.802  &         0.005        &    -1.02e+04    &    -1809.930     \\\\\n",
       "\\textbf{7683.0}           &    5132.9406  &     2204.662     &     2.328  &         0.020        &      811.407    &     9454.475     \\\\\n",
       "\\textbf{7685.0}           &    1209.7216  &     2107.912     &     0.574  &         0.566        &    -2922.165    &     5341.608     \\\\\n",
       "\\textbf{7692.0}           &   -2409.2113  &     1908.119     &    -1.263  &         0.207        &    -6149.468    &     1331.045     \\\\\n",
       "\\textbf{7762.0}           &    2404.9306  &     1897.249     &     1.268  &         0.205        &    -1314.020    &     6123.881     \\\\\n",
       "\\textbf{7772.0}           &    -1.37e+04  &     3025.335     &    -4.530  &         0.000        &    -1.96e+04    &    -7774.656     \\\\\n",
       "\\textbf{7773.0}           &    3049.8115  &     1931.994     &     1.579  &         0.114        &     -737.245    &     6836.868     \\\\\n",
       "\\textbf{7777.0}           &   -3395.4030  &     1967.205     &    -1.726  &         0.084        &    -7251.480    &      460.674     \\\\\n",
       "\\textbf{7835.0}           &    4453.7623  &     2001.368     &     2.225  &         0.026        &      530.720    &     8376.804     \\\\\n",
       "\\textbf{7873.0}           &    1314.2159  &     1876.490     &     0.700  &         0.484        &    -2364.043    &     4992.474     \\\\\n",
       "\\textbf{7883.0}           &   -7385.6645  &     2326.273     &    -3.175  &         0.002        &    -1.19e+04    &    -2825.750     \\\\\n",
       "\\textbf{7904.0}           &   -7911.9946  &     2248.566     &    -3.519  &         0.000        &    -1.23e+04    &    -3504.400     \\\\\n",
       "\\textbf{7906.0}           &    8457.4243  &     2177.531     &     3.884  &         0.000        &     4189.072    &     1.27e+04     \\\\\n",
       "\\textbf{7921.0}           &    1900.2675  &     1871.373     &     1.015  &         0.310        &    -1767.961    &     5568.496     \\\\\n",
       "\\textbf{7923.0}           &    5226.8772  &     2146.809     &     2.435  &         0.015        &     1018.744    &     9435.010     \\\\\n",
       "\\textbf{7935.0}           &      40.7129  &     1864.053     &     0.022  &         0.983        &    -3613.168    &     3694.594     \\\\\n",
       "\\textbf{7938.0}           &    -321.7520  &     1864.079     &    -0.173  &         0.863        &    -3975.683    &     3332.179     \\\\\n",
       "\\textbf{7985.0}           &   -2.135e+04  &     3619.084     &    -5.900  &         0.000        &    -2.84e+04    &    -1.43e+04     \\\\\n",
       "\\textbf{8014.0}           &    3426.7507  &     2056.518     &     1.666  &         0.096        &     -604.394    &     7457.896     \\\\\n",
       "\\textbf{8030.0}           &    7901.8265  &     2261.528     &     3.494  &         0.000        &     3468.824    &     1.23e+04     \\\\\n",
       "\\textbf{8046.0}           &     833.3317  &     1866.253     &     0.447  &         0.655        &    -2824.860    &     4491.523     \\\\\n",
       "\\textbf{8047.0}           &    1602.6486  &     2727.533     &     0.588  &         0.557        &    -3743.808    &     6949.105     \\\\\n",
       "\\textbf{8062.0}           &      78.5373  &     1915.560     &     0.041  &         0.967        &    -3676.306    &     3833.381     \\\\\n",
       "\\textbf{8068.0}           &    -1.48e+04  &     2256.653     &    -6.560  &         0.000        &    -1.92e+04    &    -1.04e+04     \\\\\n",
       "\\textbf{8087.0}           &   -2521.2918  &     1928.370     &    -1.307  &         0.191        &    -6301.244    &     1258.660     \\\\\n",
       "\\textbf{8095.0}           &    1779.8670  &     1888.790     &     0.942  &         0.346        &    -1922.503    &     5482.237     \\\\\n",
       "\\textbf{8096.0}           &    6767.4474  &     2206.383     &     3.067  &         0.002        &     2442.538    &     1.11e+04     \\\\\n",
       "\\textbf{8109.0}           &    3670.7502  &     1965.696     &     1.867  &         0.062        &     -182.368    &     7523.868     \\\\\n",
       "\\textbf{8123.0}           &   -1.084e+04  &     2689.266     &    -4.029  &         0.000        &    -1.61e+04    &    -5564.030     \\\\\n",
       "\\textbf{8150.0}           &    5450.6132  &     2097.209     &     2.599  &         0.009        &     1339.705    &     9561.521     \\\\\n",
       "\\textbf{8163.0}           &   -6883.0058  &     2271.288     &    -3.030  &         0.002        &    -1.13e+04    &    -2430.871     \\\\\n",
       "\\textbf{8176.0}           &    7829.9634  &     2649.656     &     2.955  &         0.003        &     2636.160    &      1.3e+04     \\\\\n",
       "\\textbf{8202.0}           &   -3470.2149  &     2109.124     &    -1.645  &         0.100        &    -7604.477    &      664.048     \\\\\n",
       "\\textbf{8214.0}           &   -4483.6951  &     1939.925     &    -2.311  &         0.021        &    -8286.298    &     -681.092     \\\\\n",
       "\\textbf{8215.0}           &   -9038.7901  &     2403.228     &    -3.761  &         0.000        &    -1.37e+04    &    -4328.030     \\\\\n",
       "\\textbf{8219.0}           &    7162.4585  &     2269.938     &     3.155  &         0.002        &     2712.972    &     1.16e+04     \\\\\n",
       "\\textbf{8247.0}           &   -4490.8986  &     2070.447     &    -2.169  &         0.030        &    -8549.348    &     -432.449     \\\\\n",
       "\\textbf{8253.0}           &   -6634.8697  &     2103.115     &    -3.155  &         0.002        &    -1.08e+04    &    -2512.384     \\\\\n",
       "\\textbf{8290.0}           &      44.4631  &     2004.461     &     0.022  &         0.982        &    -3884.642    &     3973.568     \\\\\n",
       "\\textbf{8293.0}           &   -5592.6582  &     2142.027     &    -2.611  &         0.009        &    -9791.418    &    -1393.898     \\\\\n",
       "\\textbf{8304.0}           &    4343.8755  &     1928.676     &     2.252  &         0.024        &      563.323    &     8124.428     \\\\\n",
       "\\textbf{8334.0}           &    8156.6130  &     2451.973     &     3.327  &         0.001        &     3350.304    &      1.3e+04     \\\\\n",
       "\\textbf{8348.0}           &    1859.8477  &     1887.968     &     0.985  &         0.325        &    -1840.910    &     5560.605     \\\\\n",
       "\\textbf{8357.0}           &    1391.0125  &     1873.915     &     0.742  &         0.458        &    -2282.200    &     5064.225     \\\\\n",
       "\\textbf{8358.0}           &   -2972.1496  &     1941.167     &    -1.531  &         0.126        &    -6777.187    &      832.888     \\\\\n",
       "\\textbf{8446.0}           &   -9765.3587  &     2303.586     &    -4.239  &         0.000        &    -1.43e+04    &    -5249.915     \\\\\n",
       "\\textbf{8460.0}           &    8313.1316  &     2684.159     &     3.097  &         0.002        &     3051.696    &     1.36e+04     \\\\\n",
       "\\textbf{8463.0}           &    3540.6440  &     1979.017     &     1.789  &         0.074        &     -338.586    &     7419.875     \\\\\n",
       "\\textbf{8479.0}           &    7069.6698  &     2777.372     &     2.545  &         0.011        &     1625.520    &     1.25e+04     \\\\\n",
       "\\textbf{8530.0}           &    1.527e+04  &     2143.195     &     7.127  &         0.000        &     1.11e+04    &     1.95e+04     \\\\\n",
       "\\textbf{8536.0}           &   -6067.7336  &     2128.180     &    -2.851  &         0.004        &    -1.02e+04    &    -1896.118     \\\\\n",
       "\\textbf{8543.0}           &    2.431e+04  &     2339.891     &    10.391  &         0.000        &     1.97e+04    &     2.89e+04     \\\\\n",
       "\\textbf{8549.0}           &   -8491.4525  &     2144.141     &    -3.960  &         0.000        &    -1.27e+04    &    -4288.549     \\\\\n",
       "\\textbf{8551.0}           &    6317.6608  &     2201.517     &     2.870  &         0.004        &     2002.290    &     1.06e+04     \\\\\n",
       "\\textbf{8559.0}           &    5102.2092  &     2202.085     &     2.317  &         0.021        &      785.726    &     9418.693     \\\\\n",
       "\\textbf{8573.0}           &    5362.8151  &     2180.010     &     2.460  &         0.014        &     1089.602    &     9636.029     \\\\\n",
       "\\textbf{8606.0}           &    2441.4458  &     1864.788     &     1.309  &         0.190        &    -1213.874    &     6096.766     \\\\\n",
       "\\textbf{8607.0}           &    7058.7426  &     2257.207     &     3.127  &         0.002        &     2634.210    &     1.15e+04     \\\\\n",
       "\\textbf{8648.0}           &    1606.9304  &     1882.540     &     0.854  &         0.393        &    -2083.187    &     5297.048     \\\\\n",
       "\\textbf{8657.0}           &    -273.1365  &     1864.424     &    -0.146  &         0.884        &    -3927.743    &     3381.470     \\\\\n",
       "\\textbf{8675.0}           &    6256.4555  &     3809.101     &     1.643  &         0.101        &    -1210.069    &     1.37e+04     \\\\\n",
       "\\textbf{8681.0}           &   -3098.7461  &     1958.469     &    -1.582  &         0.114        &    -6937.698    &      740.206     \\\\\n",
       "\\textbf{8687.0}           &   -2061.6502  &     2250.261     &    -0.916  &         0.360        &    -6472.568    &     2349.267     \\\\\n",
       "\\textbf{8692.0}           &   -2354.8354  &     1895.725     &    -1.242  &         0.214        &    -6070.798    &     1361.127     \\\\\n",
       "\\textbf{8699.0}           &    3125.8315  &     1938.059     &     1.613  &         0.107        &     -673.113    &     6924.776     \\\\\n",
       "\\textbf{8717.0}           &    5612.2962  &     2087.698     &     2.688  &         0.007        &     1520.033    &     9704.560     \\\\\n",
       "\\textbf{8759.0}           &    5593.7671  &     2114.288     &     2.646  &         0.008        &     1449.381    &     9738.153     \\\\\n",
       "\\textbf{8762.0}           &     1.02e+04  &     1999.661     &     5.101  &         0.000        &     6280.065    &     1.41e+04     \\\\\n",
       "\\textbf{8819.0}           &    8386.7682  &     2377.400     &     3.528  &         0.000        &     3726.636    &      1.3e+04     \\\\\n",
       "\\textbf{8850.0}           &    2716.9956  &     1920.984     &     1.414  &         0.157        &    -1048.480    &     6482.471     \\\\\n",
       "\\textbf{8852.0}           &    3050.3855  &     1931.187     &     1.580  &         0.114        &     -735.090    &     6835.861     \\\\\n",
       "\\textbf{8859.0}           &    7119.6062  &     2269.908     &     3.137  &         0.002        &     2670.177    &     1.16e+04     \\\\\n",
       "\\textbf{8867.0}           &   -4092.6895  &     2053.430     &    -1.993  &         0.046        &    -8117.782    &      -67.597     \\\\\n",
       "\\textbf{8881.0}           &     525.8274  &     1866.275     &     0.282  &         0.778        &    -3132.409    &     4184.064     \\\\\n",
       "\\textbf{8958.0}           &    2252.9193  &     1900.170     &     1.186  &         0.236        &    -1471.756    &     5977.594     \\\\\n",
       "\\textbf{8972.0}           &   -1.706e+04  &     3205.600     &    -5.323  &         0.000        &    -2.33e+04    &    -1.08e+04     \\\\\n",
       "\\textbf{8990.0}           &   -2852.1868  &     2100.602     &    -1.358  &         0.175        &    -6969.746    &     1265.372     \\\\\n",
       "\\textbf{9004.0}           &    6122.1074  &     2452.259     &     2.497  &         0.013        &     1315.237    &     1.09e+04     \\\\\n",
       "\\textbf{9016.0}           &    -156.8318  &     1866.832     &    -0.084  &         0.933        &    -3816.160    &     3502.496     \\\\\n",
       "\\textbf{9048.0}           &   -4123.7657  &     2024.008     &    -2.037  &         0.042        &    -8091.186    &     -156.346     \\\\\n",
       "\\textbf{9051.0}           &   -2111.4475  &     2288.481     &    -0.923  &         0.356        &    -6597.282    &     2374.387     \\\\\n",
       "\\textbf{9071.0}           &   -1.034e+04  &     2692.856     &    -3.841  &         0.000        &    -1.56e+04    &    -5064.121     \\\\\n",
       "\\textbf{9112.0}           &    -489.4159  &     1866.396     &    -0.262  &         0.793        &    -4147.888    &     3169.057     \\\\\n",
       "\\textbf{9114.0}           &   -8444.7733  &     2140.807     &    -3.945  &         0.000        &    -1.26e+04    &    -4248.406     \\\\\n",
       "\\textbf{9132.0}           &    8487.8115  &     3631.523     &     2.337  &         0.019        &     1369.372    &     1.56e+04     \\\\\n",
       "\\textbf{9173.0}           &     326.4350  &     2255.147     &     0.145  &         0.885        &    -4094.060    &     4746.930     \\\\\n",
       "\\textbf{9180.0}           &    5704.1023  &     2133.434     &     2.674  &         0.008        &     1522.187    &     9886.018     \\\\\n",
       "\\textbf{9186.0}           &     419.3959  &     1865.732     &     0.225  &         0.822        &    -3237.775    &     4076.566     \\\\\n",
       "\\textbf{9191.0}           &   -7372.7748  &     3463.407     &    -2.129  &         0.033        &    -1.42e+04    &     -583.872     \\\\\n",
       "\\textbf{9216.0}           &   -1.098e+04  &     2757.147     &    -3.983  &         0.000        &    -1.64e+04    &    -5576.156     \\\\\n",
       "\\textbf{9217.0}           &   -1.158e+04  &     2798.937     &    -4.136  &         0.000        &    -1.71e+04    &    -6090.930     \\\\\n",
       "\\textbf{9225.0}           &    3196.0518  &     1924.982     &     1.660  &         0.097        &     -577.260    &     6969.364     \\\\\n",
       "\\textbf{9230.0}           &    7182.7628  &     2765.582     &     2.597  &         0.009        &     1761.724    &     1.26e+04     \\\\\n",
       "\\textbf{9259.0}           &    8354.7013  &     2371.940     &     3.522  &         0.000        &     3705.272    &      1.3e+04     \\\\\n",
       "\\textbf{9293.0}           &    5061.0236  &     2042.829     &     2.477  &         0.013        &     1056.711    &     9065.336     \\\\\n",
       "\\textbf{9299.0}           &   -4470.4230  &     1902.325     &    -2.350  &         0.019        &    -8199.324    &     -741.522     \\\\\n",
       "\\textbf{9308.0}           &    6085.0890  &     2144.445     &     2.838  &         0.005        &     1881.590    &     1.03e+04     \\\\\n",
       "\\textbf{9311.0}           &     723.9273  &     3068.441     &     0.236  &         0.813        &    -5290.771    &     6738.625     \\\\\n",
       "\\textbf{9313.0}           &   -1736.7159  &     1865.669     &    -0.931  &         0.352        &    -5393.764    &     1920.332     \\\\\n",
       "\\textbf{9325.0}           &    1145.8660  &     1923.806     &     0.596  &         0.551        &    -2625.141    &     4916.873     \\\\\n",
       "\\textbf{9332.0}           &   -2386.5728  &     1918.073     &    -1.244  &         0.213        &    -6146.341    &     1373.195     \\\\\n",
       "\\textbf{9340.0}           &    -1.06e+04  &     3467.553     &    -3.057  &         0.002        &    -1.74e+04    &    -3803.699     \\\\\n",
       "\\textbf{9372.0}           &    7808.8527  &     2560.984     &     3.049  &         0.002        &     2788.862    &     1.28e+04     \\\\\n",
       "\\textbf{9411.0}           &    -528.3712  &     1999.930     &    -0.264  &         0.792        &    -4448.594    &     3391.852     \\\\\n",
       "\\textbf{9459.0}           &    4272.7861  &     2153.015     &     1.985  &         0.047        &       52.489    &     8493.083     \\\\\n",
       "\\textbf{9465.0}           &    3829.1320  &     2035.049     &     1.882  &         0.060        &     -159.930    &     7818.194     \\\\\n",
       "\\textbf{9472.0}           &   -3461.9009  &     1980.006     &    -1.748  &         0.080        &    -7343.069    &      419.267     \\\\\n",
       "\\textbf{9483.0}           &   -2966.5377  &     1954.795     &    -1.518  &         0.129        &    -6798.289    &      865.213     \\\\\n",
       "\\textbf{9563.0}           &   -2.178e+04  &     3477.567     &    -6.263  &         0.000        &    -2.86e+04    &     -1.5e+04     \\\\\n",
       "\\textbf{9590.0}           &    5556.6143  &     2095.883     &     2.651  &         0.008        &     1448.306    &     9664.923     \\\\\n",
       "\\textbf{9598.0}           &   -3000.3150  &     2815.493     &    -1.066  &         0.287        &    -8519.190    &     2518.560     \\\\\n",
       "\\textbf{9599.0}           &      96.4325  &     1866.180     &     0.052  &         0.959        &    -3561.616    &     3754.481     \\\\\n",
       "\\textbf{9602.0}           &   -3209.1877  &     3345.415     &    -0.959  &         0.337        &    -9766.804    &     3348.428     \\\\\n",
       "\\textbf{9619.0}           &    8362.0997  &     2368.635     &     3.530  &         0.000        &     3719.148    &      1.3e+04     \\\\\n",
       "\\textbf{9643.0}           &   -7569.9362  &     2290.434     &    -3.305  &         0.001        &    -1.21e+04    &    -3080.273     \\\\\n",
       "\\textbf{9650.0}           &   -1.023e+04  &     2649.183     &    -3.862  &         0.000        &    -1.54e+04    &    -5038.780     \\\\\n",
       "\\textbf{9653.0}           &   -1.725e+04  &     4717.428     &    -3.657  &         0.000        &    -2.65e+04    &    -8003.553     \\\\\n",
       "\\textbf{9667.0}           &   -7987.2772  &     2362.354     &    -3.381  &         0.001        &    -1.26e+04    &    -3356.638     \\\\\n",
       "\\textbf{9698.0}           &    5019.9025  &     2055.389     &     2.442  &         0.015        &      990.970    &     9048.835     \\\\\n",
       "\\textbf{9699.0}           &    4179.4211  &     1917.850     &     2.179  &         0.029        &      420.090    &     7938.753     \\\\\n",
       "\\textbf{9719.0}           &   -6450.6652  &     2229.140     &    -2.894  &         0.004        &    -1.08e+04    &    -2081.149     \\\\\n",
       "\\textbf{9742.0}           &   -1060.9540  &     1878.750     &    -0.565  &         0.572        &    -4743.643    &     2621.735     \\\\\n",
       "\\textbf{9761.0}           &    1719.9734  &     1885.152     &     0.912  &         0.362        &    -1975.264    &     5415.211     \\\\\n",
       "\\textbf{9771.0}           &   -7657.2718  &     2282.093     &    -3.355  &         0.001        &    -1.21e+04    &    -3183.957     \\\\\n",
       "\\textbf{9772.0}           &    6542.7727  &     2160.521     &     3.028  &         0.002        &     2307.762    &     1.08e+04     \\\\\n",
       "\\textbf{9778.0}           &   -2362.7993  &     1948.249     &    -1.213  &         0.225        &    -6181.719    &     1456.120     \\\\\n",
       "\\textbf{9799.0}           &   -1.118e+04  &     2973.308     &    -3.760  &         0.000        &     -1.7e+04    &    -5350.162     \\\\\n",
       "\\textbf{9815.0}           &    1308.4978  &     1970.360     &     0.664  &         0.507        &    -2553.762    &     5170.758     \\\\\n",
       "\\textbf{9818.0}           &   -2.906e+04  &     2163.126     &   -13.433  &         0.000        &    -3.33e+04    &    -2.48e+04     \\\\\n",
       "\\textbf{9837.0}           &    7678.9303  &     2307.083     &     3.328  &         0.001        &     3156.632    &     1.22e+04     \\\\\n",
       "\\textbf{9922.0}           &   -2248.8226  &     1916.817     &    -1.173  &         0.241        &    -6006.129    &     1508.484     \\\\\n",
       "\\textbf{9954.0}           &   -1.144e+04  &     3612.246     &    -3.167  &         0.002        &    -1.85e+04    &    -4359.659     \\\\\n",
       "\\textbf{9963.0}           &   -2221.9814  &     1883.297     &    -1.180  &         0.238        &    -5913.583    &     1469.620     \\\\\n",
       "\\textbf{9988.0}           &    7509.6774  &     2345.541     &     3.202  &         0.001        &     2911.995    &     1.21e+04     \\\\\n",
       "\\textbf{9999.0}           &     -1.1e+04  &     2734.749     &    -4.023  &         0.000        &    -1.64e+04    &    -5641.594     \\\\\n",
       "\\textbf{gspilltecIVX1982} &       0.0155  &        0.052     &     0.299  &         0.765        &       -0.086    &        0.118     \\\\\n",
       "\\textbf{gspilltecIVX1983} &      -0.0057  &        0.051     &    -0.110  &         0.912        &       -0.106    &        0.095     \\\\\n",
       "\\textbf{gspilltecIVX1984} &      -0.0692  &        0.051     &    -1.359  &         0.174        &       -0.169    &        0.031     \\\\\n",
       "\\textbf{gspilltecIVX1985} &      -0.1076  &        0.051     &    -2.103  &         0.036        &       -0.208    &       -0.007     \\\\\n",
       "\\textbf{gspilltecIVX1986} &      -0.1549  &        0.052     &    -2.990  &         0.003        &       -0.256    &       -0.053     \\\\\n",
       "\\textbf{gspilltecIVX1987} &      -0.1737  &        0.053     &    -3.301  &         0.001        &       -0.277    &       -0.071     \\\\\n",
       "\\textbf{gspilltecIVX1988} &      -0.2116  &        0.053     &    -3.958  &         0.000        &       -0.316    &       -0.107     \\\\\n",
       "\\textbf{gspilltecIVX1989} &      -0.2163  &        0.054     &    -3.979  &         0.000        &       -0.323    &       -0.110     \\\\\n",
       "\\textbf{gspilltecIVX1990} &      -0.2536  &        0.055     &    -4.569  &         0.000        &       -0.362    &       -0.145     \\\\\n",
       "\\textbf{gspilltecIVX1991} &      -0.2393  &        0.056     &    -4.241  &         0.000        &       -0.350    &       -0.129     \\\\\n",
       "\\textbf{gspilltecIVX1992} &      -0.2451  &        0.057     &    -4.269  &         0.000        &       -0.358    &       -0.133     \\\\\n",
       "\\textbf{gspilltecIVX1993} &      -0.2314  &        0.059     &    -3.940  &         0.000        &       -0.346    &       -0.116     \\\\\n",
       "\\textbf{gspilltecIVX1994} &      -0.2438  &        0.060     &    -4.064  &         0.000        &       -0.361    &       -0.126     \\\\\n",
       "\\textbf{gspilltecIVX1995} &      -0.2358  &        0.062     &    -3.823  &         0.000        &       -0.357    &       -0.115     \\\\\n",
       "\\textbf{gspilltecIVX1996} &      -0.2342  &        0.064     &    -3.667  &         0.000        &       -0.359    &       -0.109     \\\\\n",
       "\\textbf{gspilltecIVX1997} &      -0.2241  &        0.066     &    -3.382  &         0.001        &       -0.354    &       -0.094     \\\\\n",
       "\\textbf{gspilltecIVX1998} &      -0.1955  &        0.069     &    -2.850  &         0.004        &       -0.330    &       -0.061     \\\\\n",
       "\\textbf{gspilltecIVX1999} &      -0.1094  &        0.071     &    -1.551  &         0.121        &       -0.248    &        0.029     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 22347.423 & \\textbf{  Durbin-Watson:     } &       0.741    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 128268599.681  \\\\\n",
       "\\textbf{Skew:}          &   14.218  & \\textbf{  Prob(JB):          } &        0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  514.370  & \\textbf{  Cond. No.          } &    8.09e+18    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 1.32e-25. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.666\n",
       "Model:                            OLS   Adj. R-squared:                  0.643\n",
       "Method:                 Least Squares   F-statistic:                     28.50\n",
       "Date:                Mon, 14 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        16:57:50   Log-Likelihood:            -1.2190e+05\n",
       "No. Observations:               11736   AIC:                         2.453e+05\n",
       "Df Residuals:                   10968   BIC:                         2.510e+05\n",
       "Df Model:                         767                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -9211.7705   1638.776     -5.621      0.000   -1.24e+04   -5999.475\n",
       "gspilltecIV          0.5889      0.115      5.124      0.000       0.364       0.814\n",
       "pat_count          -25.4471      1.772    -14.360      0.000     -28.921     -21.973\n",
       "rsales               0.9838      0.041     23.821      0.000       0.903       1.065\n",
       "rppent               0.4660      0.086      5.390      0.000       0.297       0.636\n",
       "emp                 -6.8225      7.044     -0.969      0.333     -20.629       6.984\n",
       "rxrd                10.1307      0.655     15.470      0.000       8.847      11.414\n",
       "1982              -302.7136    894.280     -0.338      0.735   -2055.664    1450.237\n",
       "1983              -188.3216    886.018     -0.213      0.832   -1925.076    1548.433\n",
       "1984                61.4814    880.418      0.070      0.944   -1664.297    1787.259\n",
       "1985               310.8801    876.108      0.355      0.723   -1406.450    2028.211\n",
       "1986               702.0587    870.202      0.807      0.420   -1003.693    2407.811\n",
       "1987               656.7606    866.805      0.758      0.449   -1042.333    2355.854\n",
       "1988               933.6216    864.888      1.079      0.280    -761.715    2628.958\n",
       "1989              1082.0033    862.114      1.255      0.209    -607.896    2771.903\n",
       "1990              1284.3781    856.621      1.499      0.134    -394.754    2963.510\n",
       "1991              1326.5463    855.595      1.550      0.121    -350.574    3003.666\n",
       "1992              1228.9372    855.768      1.436      0.151    -448.523    2906.398\n",
       "1993               891.3080    852.779      1.045      0.296    -780.293    2562.909\n",
       "1994               808.1006    853.938      0.946      0.344    -865.773    2481.974\n",
       "1995              1054.1039    852.975      1.236      0.217    -617.882    2726.089\n",
       "1996              1059.0999    854.487      1.239      0.215    -615.849    2734.049\n",
       "1997              1111.7247    858.487      1.295      0.195    -571.065    2794.514\n",
       "1998               382.0384    861.634      0.443      0.657   -1306.919    2070.996\n",
       "1999             -1789.5647    870.771     -2.055      0.040   -3496.433     -82.696\n",
       "10005.0           1471.8278   1876.597      0.784      0.433   -2206.641    5150.297\n",
       "10006.0           -747.7524   2257.891     -0.331      0.741   -5173.627    3678.122\n",
       "10008.0          -1482.7238   1888.179     -0.785      0.432   -5183.894    2218.447\n",
       "10016.0           -487.2088   1868.785     -0.261      0.794   -4150.365    3175.947\n",
       "10030.0           5259.6555   2071.946      2.539      0.011    1198.268    9321.043\n",
       "1004.0            2805.5935   1921.966      1.460      0.144    -961.807    6572.994\n",
       "10056.0            902.2280   1873.077      0.482      0.630   -2769.340    4573.796\n",
       "10085.0          -2309.5248   1919.226     -1.203      0.229   -6071.554    1452.504\n",
       "10092.0           5069.6464   4737.149      1.070      0.285   -4216.020    1.44e+04\n",
       "10097.0          -5499.5467   2000.689     -2.749      0.006   -9421.259   -1577.835\n",
       "1010.0             476.9711   4683.548      0.102      0.919   -8703.627    9657.569\n",
       "10109.0           8482.8358   2362.111      3.591      0.000    3852.673    1.31e+04\n",
       "10115.0           3202.1094   1895.146      1.690      0.091    -512.719    6916.937\n",
       "10124.0           8671.0722   2386.396      3.634      0.000    3993.305    1.33e+04\n",
       "1013.0           -3756.1988   2034.674     -1.846      0.065   -7744.526     232.129\n",
       "10150.0          -3698.4862   2480.895     -1.491      0.136   -8561.488    1164.516\n",
       "10159.0           5904.5710   3439.553      1.717      0.086    -837.574    1.26e+04\n",
       "10174.0           4789.7900   2252.427      2.127      0.033     374.627    9204.953\n",
       "10185.0           5912.9151   2354.896      2.511      0.012    1296.895    1.05e+04\n",
       "10195.0           3689.4278   1957.111      1.885      0.059    -146.863    7525.718\n",
       "10198.0           4939.9188   2049.954      2.410      0.016     921.640    8958.197\n",
       "10215.0           8174.7392   2356.187      3.469      0.001    3556.188    1.28e+04\n",
       "10232.0           7734.8066   2430.117      3.183      0.001    2971.339    1.25e+04\n",
       "10236.0           5551.7698   2096.550      2.648      0.008    1442.153    9661.386\n",
       "10286.0          -1179.8527   1881.783     -0.627      0.531   -4868.486    2508.781\n",
       "10301.0          -2.113e+04   3215.118     -6.571      0.000   -2.74e+04   -1.48e+04\n",
       "10312.0           2071.0779   1894.980      1.093      0.274   -1643.425    5785.581\n",
       "10332.0          -1.848e+04   4178.152     -4.422      0.000   -2.67e+04   -1.03e+04\n",
       "1036.0            3944.3640   2135.332      1.847      0.065    -241.271    8129.999\n",
       "10374.0           1614.5948   1879.868      0.859      0.390   -2070.286    5299.476\n",
       "10386.0          -8535.2040   2431.070     -3.511      0.000   -1.33e+04   -3769.868\n",
       "10391.0          -1.279e+04   2888.661     -4.427      0.000   -1.85e+04   -7127.228\n",
       "10407.0           -283.7100   1869.500     -0.152      0.879   -3948.267    3380.847\n",
       "10420.0           5890.4125   1973.724      2.984      0.003    2021.558    9759.267\n",
       "10422.0            499.2547   1974.630      0.253      0.800   -3371.376    4369.885\n",
       "10426.0           7179.3354   2454.691      2.925      0.003    2367.698     1.2e+04\n",
       "10441.0           6884.8418   2214.192      3.109      0.002    2544.626    1.12e+04\n",
       "1045.0           -9701.5817   2043.591     -4.747      0.000   -1.37e+04   -5695.776\n",
       "10453.0          -8024.8045   2395.586     -3.350      0.001   -1.27e+04   -3329.024\n",
       "10482.0          -2.291e+04   2417.172     -9.476      0.000   -2.76e+04   -1.82e+04\n",
       "10498.0           4666.9864   2050.562      2.276      0.023     647.514    8686.458\n",
       "10499.0          -1.268e+04   3642.486     -3.482      0.001   -1.98e+04   -5542.064\n",
       "10511.0           8076.3474   2407.548      3.355      0.001    3357.120    1.28e+04\n",
       "10519.0          -1.221e+04   2496.373     -4.891      0.000   -1.71e+04   -7316.646\n",
       "10530.0          -7471.0255   2339.416     -3.194      0.001   -1.21e+04   -2885.349\n",
       "10537.0          -4822.3296   2439.002     -1.977      0.048   -9603.213     -41.446\n",
       "10540.0          -1142.9396   1894.488     -0.603      0.546   -4856.479    2570.599\n",
       "10541.0           1274.9540   1977.196      0.645      0.519   -2600.706    5150.614\n",
       "10550.0          -2711.8146   5781.834     -0.469      0.639    -1.4e+04    8621.623\n",
       "10553.0          -4014.2561   2194.630     -1.829      0.067   -8316.126     287.614\n",
       "10565.0           5677.5052   2070.467      2.742      0.006    1619.016    9735.994\n",
       "10580.0           7527.5576   2163.965      3.479      0.001    3285.796    1.18e+04\n",
       "10581.0           2587.6587   1957.315      1.322      0.186   -1249.031    6424.348\n",
       "10588.0          -6487.9890   2226.401     -2.914      0.004   -1.09e+04   -2123.842\n",
       "10597.0           2716.4309   1925.974      1.410      0.158   -1058.826    6491.688\n",
       "10599.0           3047.7307   1940.657      1.570      0.116    -756.306    6851.767\n",
       "10618.0           2574.6693   1923.722      1.338      0.181   -1196.172    6345.511\n",
       "10656.0           -319.6551   1868.298     -0.171      0.864   -3981.856    3342.546\n",
       "10658.0           -658.2818   1871.850     -0.352      0.725   -4327.445    3010.881\n",
       "10726.0           8846.5101   2330.883      3.795      0.000    4277.559    1.34e+04\n",
       "10734.0           3514.8776   2637.144      1.333      0.183   -1654.399    8684.154\n",
       "10735.0           7127.5181   2272.095      3.137      0.002    2673.803    1.16e+04\n",
       "10764.0           7560.5283   2349.973      3.217      0.001    2954.157    1.22e+04\n",
       "10777.0           -715.5421   1872.912     -0.382      0.702   -4386.788    2955.704\n",
       "1078.0            6633.3340   2287.959      2.899      0.004    2148.521    1.11e+04\n",
       "10793.0           4093.5238   2067.320      1.980      0.048      41.204    8145.843\n",
       "10816.0            354.3631   1878.627      0.189      0.850   -3328.085    4036.812\n",
       "10839.0           2645.9482   1906.353      1.388      0.165   -1090.847    6382.744\n",
       "10857.0          -1.515e+04   2814.102     -5.383      0.000   -2.07e+04   -9631.001\n",
       "10867.0           1529.7769   2279.365      0.671      0.502   -2938.190    5997.744\n",
       "10906.0           3342.7403   1949.156      1.715      0.086    -477.957    7163.437\n",
       "10950.0           2926.4519   3106.433      0.942      0.346   -3162.717    9015.620\n",
       "10983.0          -2.684e+04   2824.291     -9.505      0.000   -3.24e+04   -2.13e+04\n",
       "1099.0            1191.0221   1874.388      0.635      0.525   -2483.116    4865.160\n",
       "10991.0           6376.8999   2710.799      2.352      0.019    1063.245    1.17e+04\n",
       "11012.0           4334.3376   2063.583      2.100      0.036     289.343    8379.332\n",
       "11038.0            791.9569   2159.054      0.367      0.714   -3440.178    5024.091\n",
       "1104.0            5941.4394   2128.434      2.791      0.005    1769.325    1.01e+04\n",
       "11060.0           4935.8143   2053.018      2.404      0.016     911.529    8960.100\n",
       "11094.0           2368.7159   1909.567      1.240      0.215   -1374.381    6111.812\n",
       "11096.0          -2293.5691   1914.395     -1.198      0.231   -6046.128    1458.990\n",
       "11113.0           4232.9568   2159.918      1.960      0.050      -0.871    8466.785\n",
       "1115.0             762.7175   1867.675      0.408      0.683   -2898.262    4423.697\n",
       "11161.0           1303.1616   1875.641      0.695      0.487   -2373.432    4979.755\n",
       "11225.0           6798.8464   2204.200      3.084      0.002    2478.217    1.11e+04\n",
       "11228.0           4349.6694   1967.917      2.210      0.027     492.198    8207.141\n",
       "11236.0          -1.066e+04   3625.783     -2.939      0.003   -1.78e+04   -3549.729\n",
       "11288.0           1158.9606   2222.013      0.522      0.602   -3196.586    5514.508\n",
       "11312.0          -6500.3203   2290.356     -2.838      0.005    -1.1e+04   -2010.810\n",
       "11361.0          -2480.1543   1927.470     -1.287      0.198   -6258.343    1298.035\n",
       "11399.0          -6519.7565   2038.346     -3.199      0.001   -1.05e+04   -2524.231\n",
       "114303.0         -2.474e+04   6159.589     -4.017      0.000   -3.68e+04   -1.27e+04\n",
       "11456.0          -7977.8559   2147.430     -3.715      0.000   -1.22e+04   -3768.505\n",
       "11465.0           -815.8950   1911.551     -0.427      0.670   -4562.880    2931.090\n",
       "11502.0           6925.3259   2279.743      3.038      0.002    2456.618    1.14e+04\n",
       "11506.0          -8431.3839   2306.817     -3.655      0.000    -1.3e+04   -3909.606\n",
       "11537.0           1942.3990   1891.341      1.027      0.304   -1764.970    5649.768\n",
       "11566.0           8088.9736   2341.947      3.454      0.001    3498.335    1.27e+04\n",
       "11573.0           -940.3841   1877.924     -0.501      0.617   -4621.454    2740.685\n",
       "11580.0          -1.033e+04   2902.474     -3.558      0.000    -1.6e+04   -4638.242\n",
       "11600.0           6702.4017   2178.355      3.077      0.002    2432.434     1.1e+04\n",
       "11609.0           8594.4081   2102.710      4.087      0.000    4472.717    1.27e+04\n",
       "1161.0           -1.068e+04   2823.182     -3.783      0.000   -1.62e+04   -5147.136\n",
       "11636.0          -9834.1861   2059.968     -4.774      0.000   -1.39e+04   -5796.278\n",
       "11670.0           8372.7072   2385.777      3.509      0.000    3696.154     1.3e+04\n",
       "11678.0          -4858.3619   2105.617     -2.307      0.021   -8985.750    -730.973\n",
       "11682.0          -3380.1413   2121.584     -1.593      0.111   -7538.829     778.547\n",
       "11694.0           3266.4616   2103.246      1.553      0.120    -856.280    7389.203\n",
       "11720.0           1007.7269   3326.172      0.303      0.762   -5512.170    7527.624\n",
       "11721.0          -2154.7270   2200.443     -0.979      0.327   -6467.991    2158.537\n",
       "11722.0           -815.4558   2180.736     -0.374      0.708   -5090.092    3459.180\n",
       "11793.0          -7904.9485   6007.426     -1.316      0.188   -1.97e+04    3870.690\n",
       "11797.0           7199.2400   2612.880      2.755      0.006    2077.523    1.23e+04\n",
       "11914.0           3458.9714   2782.924      1.243      0.214   -1996.061    8914.004\n",
       "1209.0           -1930.9294   1982.619     -0.974      0.330   -5817.220    1955.362\n",
       "12136.0           -1.37e+04   3093.214     -4.428      0.000   -1.98e+04   -7632.949\n",
       "12141.0           6.067e+04   2436.892     24.897      0.000    5.59e+04    6.54e+04\n",
       "12181.0          -3135.3163   3409.956     -0.919      0.358   -9819.446    3548.813\n",
       "12215.0          -8042.6444   2659.261     -3.024      0.002   -1.33e+04   -2830.014\n",
       "12216.0          -2506.4794   2388.160     -1.050      0.294   -7187.704    2174.746\n",
       "12256.0          -3633.0708   2284.675     -1.590      0.112   -8111.446     845.305\n",
       "12262.0           7013.6473   2505.314      2.800      0.005    2102.780    1.19e+04\n",
       "12389.0          -5095.6956   2456.326     -2.075      0.038   -9910.538    -280.854\n",
       "1239.0            -443.2119   1865.901     -0.238      0.812   -4100.713    3214.290\n",
       "12390.0           -114.9577   2568.587     -0.045      0.964   -5149.852    4919.936\n",
       "12397.0          -2089.4157   4713.861     -0.443      0.658   -1.13e+04    7150.602\n",
       "1243.0             401.2082   2043.999      0.196      0.844   -3605.398    4407.814\n",
       "12548.0           2141.4505   2480.742      0.863      0.388   -2721.251    7004.152\n",
       "12570.0           4855.7868   2406.932      2.017      0.044     137.766    9573.808\n",
       "12581.0          -3887.0089   2508.536     -1.550      0.121   -8804.193    1030.175\n",
       "12592.0           3392.2388   2241.324      1.513      0.130   -1001.160    7785.638\n",
       "12604.0           3168.3128   4791.460      0.661      0.508   -6223.813    1.26e+04\n",
       "12656.0           8282.4899   2634.929      3.143      0.002    3117.553    1.34e+04\n",
       "12679.0          -9569.3139   2636.971     -3.629      0.000   -1.47e+04   -4400.376\n",
       "1278.0            7338.8150   2354.335      3.117      0.002    2723.893     1.2e+04\n",
       "12788.0           5392.7961   2512.729      2.146      0.032     467.395    1.03e+04\n",
       "1283.0            8550.3272   2388.220      3.580      0.000    3868.986    1.32e+04\n",
       "1297.0            2782.6322   1925.496      1.445      0.148    -991.688    6556.952\n",
       "12992.0           5535.6962   2467.240      2.244      0.025     699.461    1.04e+04\n",
       "13135.0            571.3451   2356.870      0.242      0.808   -4048.546    5191.236\n",
       "1327.0            2979.3791   1975.994      1.508      0.132    -893.926    6852.684\n",
       "13282.0          -5447.0197   4709.511     -1.157      0.247   -1.47e+04    3784.470\n",
       "1334.0            8299.7785   2411.836      3.441      0.001    3572.145     1.3e+04\n",
       "13351.0          -7870.7942   3149.175     -2.499      0.012    -1.4e+04   -1697.843\n",
       "13365.0          -7295.5691   2618.681     -2.786      0.005   -1.24e+04   -2162.483\n",
       "13369.0          -6398.1879   2531.195     -2.528      0.011   -1.14e+04   -1436.590\n",
       "13406.0           3302.4052   2314.075      1.427      0.154   -1233.600    7838.410\n",
       "13407.0          -2036.4176   2277.634     -0.894      0.371   -6500.990    2428.155\n",
       "13417.0           7199.1504   2683.288      2.683      0.007    1939.422    1.25e+04\n",
       "13525.0           6702.3576   2559.975      2.618      0.009    1684.345    1.17e+04\n",
       "13554.0           7946.3676   2745.679      2.894      0.004    2564.341    1.33e+04\n",
       "1359.0            3362.1599   1873.562      1.795      0.073    -310.360    7034.680\n",
       "13623.0           2844.3078   2398.032      1.186      0.236   -1856.268    7544.883\n",
       "1372.0            2336.6983   1906.538      1.226      0.220   -1400.460    6073.856\n",
       "1380.0           -2063.6147   1916.954     -1.077      0.282   -5821.190    1693.961\n",
       "13923.0           1691.7022   2586.023      0.654      0.513   -3377.369    6760.774\n",
       "13932.0           6047.0934   3496.398      1.730      0.084    -806.477    1.29e+04\n",
       "13941.0          -1.013e+04   3024.694     -3.348      0.001   -1.61e+04   -4196.607\n",
       "1397.0           -1937.1929   2238.879     -0.865      0.387   -6325.799    2451.414\n",
       "14064.0           4789.3911   2416.382      1.982      0.047      52.846    9525.936\n",
       "14084.0          -4685.8260   2401.465     -1.951      0.051   -9393.130      21.478\n",
       "14324.0           -786.4427   2397.801     -0.328      0.743   -5486.564    3913.679\n",
       "14462.0          -7785.3253   2723.311     -2.859      0.004   -1.31e+04   -2447.144\n",
       "1447.0            2800.1178   4103.530      0.682      0.495   -5243.540    1.08e+04\n",
       "14593.0           5546.5657   2688.525      2.063      0.039     276.571    1.08e+04\n",
       "14622.0          -1770.3156   8138.760     -0.218      0.828   -1.77e+04    1.42e+04\n",
       "1465.0            5863.8852   2799.345      2.095      0.036     376.665    1.14e+04\n",
       "1468.0            5080.1273   2579.898      1.969      0.049      23.063    1.01e+04\n",
       "14897.0           4556.2190   4744.669      0.960      0.337   -4744.188    1.39e+04\n",
       "14954.0           4844.4134   2598.074      1.865      0.062    -248.281    9937.108\n",
       "1496.0            5418.8534   2084.683      2.599      0.009    1332.499    9505.207\n",
       "15267.0           2383.3239   2481.831      0.960      0.337   -2481.512    7248.160\n",
       "15354.0          -3644.3899   2654.559     -1.373      0.170   -8847.804    1559.024\n",
       "1542.0             389.3899   1865.507      0.209      0.835   -3267.341    4046.121\n",
       "15459.0          -9981.6539   3067.375     -3.254      0.001    -1.6e+04   -3969.047\n",
       "1554.0            5423.0598   2081.390      2.605      0.009    1343.159    9502.960\n",
       "15708.0          -1.002e+04   3171.947     -3.158      0.002   -1.62e+04   -3798.165\n",
       "15711.0          -1616.3195   2448.562     -0.660      0.509   -6415.941    3183.303\n",
       "15761.0           3099.4708   2931.098      1.057      0.290   -2646.009    8844.950\n",
       "1581.0           -2.374e+04   4109.964     -5.777      0.000   -3.18e+04   -1.57e+04\n",
       "1593.0           -2831.8659   1933.262     -1.465      0.143   -6621.408     957.677\n",
       "1602.0            4666.7591   2105.899      2.216      0.027     538.818    8794.700\n",
       "1613.0            5439.1842   2095.574      2.596      0.009    1331.481    9546.887\n",
       "16188.0           1729.2905   2717.624      0.636      0.525   -3597.742    7056.323\n",
       "1632.0           -4807.0435   2132.455     -2.254      0.024   -8987.040    -627.047\n",
       "1633.0            2565.6965   1911.232      1.342      0.179   -1180.662    6312.055\n",
       "1635.0            -126.0121   1866.961     -0.067      0.946   -3785.592    3533.568\n",
       "16401.0          -1.629e+04   3721.600     -4.378      0.000   -2.36e+04   -8997.019\n",
       "16437.0          -7581.4867   3293.674     -2.302      0.021    -1.4e+04   -1125.293\n",
       "1651.0            -464.6488   1875.442     -0.248      0.804   -4140.853    3211.555\n",
       "1655.0            3893.3420   1976.949      1.969      0.049      18.165    7768.519\n",
       "1663.0            4451.3397   1925.366      2.312      0.021     677.276    8225.404\n",
       "16710.0          -2151.0226   2719.089     -0.791      0.429   -7480.928    3178.883\n",
       "16729.0           -843.8305   2568.706     -0.329      0.743   -5878.957    4191.296\n",
       "1690.0           -1.168e+04   2193.646     -5.324      0.000    -1.6e+04   -7379.093\n",
       "1703.0           -1955.1330   1955.560     -1.000      0.317   -5788.383    1878.117\n",
       "17202.0           4595.4329   2705.329      1.699      0.089    -707.500    9898.366\n",
       "1722.0            1041.0113   1941.638      0.536      0.592   -2764.949    4846.971\n",
       "1728.0            4725.5694   2050.409      2.305      0.021     706.399    8744.740\n",
       "1743.0            5488.4050   3280.892      1.673      0.094    -942.735    1.19e+04\n",
       "1754.0            3365.6623   2041.335      1.649      0.099    -635.722    7367.046\n",
       "1762.0            -1.18e+04   2769.890     -4.259      0.000   -1.72e+04   -6368.685\n",
       "1773.0            6942.6066   2376.659      2.921      0.003    2283.926    1.16e+04\n",
       "1786.0            -1.43e+04   3080.299     -4.641      0.000   -2.03e+04   -8258.858\n",
       "18100.0           3005.6705   2628.581      1.143      0.253   -2146.823    8158.164\n",
       "1820.0            4245.4049   1999.313      2.123      0.034     326.390    8164.419\n",
       "1848.0           -8981.9585   2585.904     -3.473      0.001   -1.41e+04   -3913.121\n",
       "18654.0           4046.7267   3720.335      1.088      0.277   -3245.801    1.13e+04\n",
       "1875.0            1552.2041   4059.869      0.382      0.702   -6405.872    9510.280\n",
       "1884.0            6470.9768   2276.025      2.843      0.004    2009.557    1.09e+04\n",
       "1913.0           -8710.6213   2438.976     -3.571      0.000   -1.35e+04   -3929.789\n",
       "1919.0            2432.2612   2055.952      1.183      0.237   -1597.775    6462.298\n",
       "1920.0           -4474.2130   2085.969     -2.145      0.032   -8563.089    -385.337\n",
       "1968.0           -2935.0794   1950.524     -1.505      0.132   -6758.459     888.300\n",
       "1976.0            5963.6939   2023.911      2.947      0.003    1996.463    9930.925\n",
       "1981.0            4683.9283   2029.882      2.307      0.021     704.994    8662.862\n",
       "1988.0            -1.85e+04   4016.193     -4.605      0.000   -2.64e+04   -1.06e+04\n",
       "1992.0           -3736.9631   1998.914     -1.869      0.062   -7655.196     181.270\n",
       "2008.0            -472.6100   1877.034     -0.252      0.801   -4151.935    3206.715\n",
       "2033.0            3232.7325   2534.569      1.275      0.202   -1735.480    8200.945\n",
       "2044.0            2346.9978   1881.936      1.247      0.212   -1341.936    6035.932\n",
       "2049.0            -219.9822   1864.523     -0.118      0.906   -3874.783    3434.819\n",
       "2061.0            7760.6254   2309.286      3.361      0.001    3234.008    1.23e+04\n",
       "20779.0           2.919e+04   2748.072     10.623      0.000    2.38e+04    3.46e+04\n",
       "2085.0           -3940.1136   2059.564     -1.913      0.056   -7977.230      97.003\n",
       "2086.0            -145.5963   1899.605     -0.077      0.939   -3869.164    3577.971\n",
       "2111.0             466.2951   1877.045      0.248      0.804   -3213.052    4145.642\n",
       "21204.0           8036.7814   2947.759      2.726      0.006    2258.642    1.38e+04\n",
       "21238.0           7581.8510   2866.455      2.645      0.008    1963.082    1.32e+04\n",
       "2124.0           -2669.4119   2038.405     -1.310      0.190   -6665.053    1326.229\n",
       "2146.0            2.067e+04   3209.509      6.439      0.000    1.44e+04     2.7e+04\n",
       "21496.0          -8858.3795   2967.467     -2.985      0.003   -1.47e+04   -3041.609\n",
       "2154.0            2734.4145   1926.414      1.419      0.156   -1041.703    6510.532\n",
       "2176.0            3.583e+04   2612.940     13.712      0.000    3.07e+04     4.1e+04\n",
       "2188.0            7391.7326   2317.607      3.189      0.001    2848.806    1.19e+04\n",
       "2189.0           -9159.3909   2082.316     -4.399      0.000   -1.32e+04   -5077.677\n",
       "2220.0              55.8354   1864.970      0.030      0.976   -3599.841    3711.512\n",
       "22205.0           7774.0320   2919.055      2.663      0.008    2052.158    1.35e+04\n",
       "2226.0            -838.6605   5742.865     -0.146      0.884   -1.21e+04    1.04e+04\n",
       "2230.0            7228.9116   2408.790      3.001      0.003    2507.250     1.2e+04\n",
       "22325.0            581.7322   2640.211      0.220      0.826   -4593.557    5757.021\n",
       "2255.0              24.2625   1867.099      0.013      0.990   -3635.588    3684.113\n",
       "22619.0           2989.0819   2627.929      1.137      0.255   -2162.132    8140.296\n",
       "2267.0           -4146.3035   2030.371     -2.042      0.041   -8126.196    -166.411\n",
       "22815.0           -501.2126   2571.563     -0.195      0.845   -5541.940    4539.515\n",
       "2285.0           -1.987e+04   2336.845     -8.503      0.000   -2.44e+04   -1.53e+04\n",
       "2290.0           -3811.1880   1872.008     -2.036      0.042   -7480.661    -141.715\n",
       "2295.0            8023.2436   3569.229      2.248      0.025    1026.911     1.5e+04\n",
       "2316.0           -5957.3664   2297.220     -2.593      0.010   -1.05e+04   -1454.401\n",
       "23220.0           -776.8606   2705.056     -0.287      0.774   -6079.258    4525.536\n",
       "23224.0           5798.8798   2921.359      1.985      0.047      72.490    1.15e+04\n",
       "2343.0            1961.8790   4085.517      0.480      0.631   -6046.471    9970.229\n",
       "2352.0           -5763.4410   2366.706     -2.435      0.015   -1.04e+04   -1124.271\n",
       "23700.0          -1.781e+04   4886.318     -3.644      0.000   -2.74e+04   -8227.811\n",
       "2390.0            5753.9197   2112.482      2.724      0.006    1613.075    9894.764\n",
       "2393.0           -9773.2623   2597.402     -3.763      0.000   -1.49e+04   -4681.886\n",
       "2403.0            1.911e+04   2130.281      8.970      0.000    1.49e+04    2.33e+04\n",
       "2435.0            9215.4939   2324.978      3.964      0.000    4658.117    1.38e+04\n",
       "2444.0           -6328.3637   2156.028     -2.935      0.003   -1.06e+04   -2102.160\n",
       "2448.0             871.9388   1867.170      0.467      0.641   -2788.051    4531.928\n",
       "2469.0            5063.4747   3756.950      1.348      0.178   -2300.824    1.24e+04\n",
       "24720.0           -406.9425   2870.067     -0.142      0.887   -6032.791    5218.906\n",
       "24800.0          -2377.2485   3004.914     -0.791      0.429   -8267.421    3512.924\n",
       "2482.0            6137.1445   2146.212      2.860      0.004    1930.181    1.03e+04\n",
       "24969.0           5750.0595   3249.398      1.770      0.077    -619.346    1.21e+04\n",
       "2498.0           -2533.2457   2035.151     -1.245      0.213   -6522.508    1456.016\n",
       "2504.0            -2.31e+04   3703.554     -6.236      0.000   -3.04e+04   -1.58e+04\n",
       "2508.0            3084.6834   2165.915      1.424      0.154   -1160.901    7330.268\n",
       "25124.0           1564.7264   2879.122      0.543      0.587   -4078.872    7208.325\n",
       "2518.0            3667.7789   1981.681      1.851      0.064    -216.674    7552.232\n",
       "25224.0           -519.5598   8121.496     -0.064      0.949   -1.64e+04    1.54e+04\n",
       "25279.0           5893.9281   2883.611      2.044      0.041     241.530    1.15e+04\n",
       "2537.0           -1929.2007   1955.004     -0.987      0.324   -5761.361    1902.960\n",
       "2538.0            6662.2041   3169.660      2.102      0.036     449.099    1.29e+04\n",
       "25389.0           5022.2105   4762.281      1.055      0.292   -4312.720    1.44e+04\n",
       "2547.0           -1.196e+04   2180.644     -5.485      0.000   -1.62e+04   -7687.227\n",
       "2553.0            5107.4417   2062.228      2.477      0.013    1065.104    9149.780\n",
       "2574.0           -1729.5345   2589.240     -0.668      0.504   -6804.912    3345.843\n",
       "25747.0           1742.7951   3088.970      0.564      0.573   -4312.143    7797.733\n",
       "2577.0           -2057.6351   1913.398     -1.075      0.282   -5808.239    1692.969\n",
       "2593.0           -1671.0046   1878.124     -0.890      0.374   -5352.467    2010.458\n",
       "2596.0            7441.0482   2274.848      3.271      0.001    2981.936    1.19e+04\n",
       "2663.0            6347.2392   1932.433      3.285      0.001    2559.322    1.01e+04\n",
       "2771.0            7041.1013   2249.918      3.129      0.002    2630.856    1.15e+04\n",
       "2787.0            3473.5093   1967.063      1.766      0.077    -382.290    7329.308\n",
       "2797.0           -8494.4128   2430.898     -3.494      0.000   -1.33e+04   -3729.415\n",
       "2802.0            5804.6405   2119.524      2.739      0.006    1649.990    9959.291\n",
       "2817.0           -1.835e+04   3254.017     -5.640      0.000   -2.47e+04    -1.2e+04\n",
       "28678.0          -1.353e+04   3717.342     -3.641      0.000   -2.08e+04   -6246.472\n",
       "28701.0           2415.7030   1923.666      1.256      0.209   -1355.028    6186.434\n",
       "28742.0          -3268.2963   3086.129     -1.059      0.290   -9317.665    2781.073\n",
       "2888.0           -1158.8006   2113.251     -0.548      0.583   -5301.154    2983.552\n",
       "2897.0            3246.1953   2742.116      1.184      0.237   -2128.846    8621.237\n",
       "2917.0           -2440.2332   1926.409     -1.267      0.205   -6216.342    1335.876\n",
       "29392.0          -1.796e+04   4127.779     -4.351      0.000   -2.61e+04   -9869.383\n",
       "2950.0             116.3507   2708.210      0.043      0.966   -5192.230    5424.931\n",
       "2951.0            6504.8671   2621.410      2.481      0.013    1366.431    1.16e+04\n",
       "2953.0            2359.5234   1899.256      1.242      0.214   -1363.360    6082.407\n",
       "2960.0           -3349.3851   2905.402     -1.153      0.249   -9044.496    2345.726\n",
       "2975.0           -2023.7214   1902.975     -1.063      0.288   -5753.895    1706.453\n",
       "2982.0            4342.1583   2027.537      2.142      0.032     367.821    8316.496\n",
       "2991.0           -9953.4737   2672.549     -3.724      0.000   -1.52e+04   -4714.795\n",
       "3011.0            1543.4197   2034.873      0.758      0.448   -2445.299    5532.138\n",
       "3015.0            7455.6922   2214.322      3.367      0.001    3115.222    1.18e+04\n",
       "3026.0            3128.6646   1975.593      1.584      0.113    -743.854    7001.183\n",
       "3031.0           -6195.1370   3021.612     -2.050      0.040   -1.21e+04    -272.233\n",
       "3062.0             221.5773   1988.875      0.111      0.911   -3676.976    4120.130\n",
       "3093.0            2571.3322   2193.116      1.172      0.241   -1727.571    6870.236\n",
       "3107.0           -1262.3700   3629.311     -0.348      0.728   -8376.473    5851.733\n",
       "3121.0            4820.8076   1938.340      2.487      0.013    1021.313    8620.303\n",
       "3126.0              50.8772   1864.582      0.027      0.978   -3604.039    3705.794\n",
       "3144.0            5.164e+04   2009.846     25.695      0.000    4.77e+04    5.56e+04\n",
       "3156.0            5528.8925   2513.145      2.200      0.028     602.675    1.05e+04\n",
       "3157.0            2613.5072   1914.202      1.365      0.172   -1138.673    6365.687\n",
       "3170.0            5740.7299   1904.419      3.014      0.003    2007.725    9473.735\n",
       "3178.0            6033.3161   2242.532      2.690      0.007    1637.549    1.04e+04\n",
       "3206.0           -3341.6269   2121.164     -1.575      0.115   -7499.490     816.236\n",
       "3229.0            5746.5719   2221.989      2.586      0.010    1391.074    1.01e+04\n",
       "3235.0            -877.8615   2050.458     -0.428      0.669   -4897.129    3141.406\n",
       "3246.0            4639.7278   2100.036      2.209      0.027     523.278    8756.177\n",
       "3248.0             752.8101   1867.693      0.403      0.687   -2908.206    4413.826\n",
       "3282.0           -1.726e+04   2670.598     -6.462      0.000   -2.25e+04    -1.2e+04\n",
       "3362.0           -6278.6048   2254.498     -2.785      0.005   -1.07e+04   -1859.382\n",
       "3372.0            2659.0910   2474.101      1.075      0.283   -2190.594    7508.776\n",
       "3422.0            -619.6711   1876.424     -0.330      0.741   -4297.801    3058.459\n",
       "3497.0           -8834.0753   2554.688     -3.458      0.001   -1.38e+04   -3826.426\n",
       "3502.0            -479.3749   1866.770     -0.257      0.797   -4138.580    3179.831\n",
       "3504.0           -2477.5731   2731.185     -0.907      0.364   -7831.188    2876.041\n",
       "3505.0           -5156.3375   2034.291     -2.535      0.011   -9143.914   -1168.761\n",
       "3532.0            2706.2668   1879.355      1.440      0.150    -977.607    6390.141\n",
       "3574.0            5468.6194   4143.995      1.320      0.187   -2654.357    1.36e+04\n",
       "3580.0           -1666.0673   1876.899     -0.888      0.375   -5345.128    2012.994\n",
       "3612.0            8495.2292   2361.801      3.597      0.000    3865.673    1.31e+04\n",
       "3619.0            3211.5327   2025.667      1.585      0.113    -759.140    7182.205\n",
       "3622.0            5842.5102   2170.677      2.692      0.007    1587.592    1.01e+04\n",
       "3639.0           -5569.8963   2126.935     -2.619      0.009   -9739.072   -1400.721\n",
       "3650.0           -1.393e+04   2714.286     -5.134      0.000   -1.93e+04   -8613.380\n",
       "3662.0           -8333.9959   2446.351     -3.407      0.001   -1.31e+04   -3538.707\n",
       "3734.0           -8281.2919   2120.677     -3.905      0.000   -1.24e+04   -4124.382\n",
       "3735.0           -9453.1677   2966.849     -3.186      0.001   -1.53e+04   -3637.608\n",
       "3761.0           -6582.6545   2223.313     -2.961      0.003   -1.09e+04   -2224.560\n",
       "3779.0            1112.8960   1976.312      0.563      0.573   -2761.032    4986.824\n",
       "3781.0            3406.6073   2407.950      1.415      0.157   -1313.409    8126.624\n",
       "3782.0           -7465.5064   2305.977     -3.237      0.001    -1.2e+04   -2945.375\n",
       "3786.0            3988.5817   1984.616      2.010      0.044      98.377    7878.786\n",
       "3796.0            1381.6824   2044.029      0.676      0.499   -2624.982    5388.347\n",
       "3821.0            5209.9044   2143.230      2.431      0.015    1008.787    9411.021\n",
       "3835.0           -4218.9592   1882.486     -2.241      0.025   -7908.971    -528.948\n",
       "3839.0           -9770.0399   3133.196     -3.118      0.002   -1.59e+04   -3628.412\n",
       "3840.0            2927.0314   1930.956      1.516      0.130    -857.991    6712.054\n",
       "3895.0            4038.4655   1990.440      2.029      0.042     136.844    7940.087\n",
       "3908.0            6138.5218   3219.795      1.906      0.057    -172.857    1.24e+04\n",
       "3911.0           -9596.9977   2507.910     -3.827      0.000   -1.45e+04   -4681.042\n",
       "3917.0            5226.1949   2162.918      2.416      0.016     986.486    9465.904\n",
       "3946.0            6973.0171   2206.800      3.160      0.002    2647.291    1.13e+04\n",
       "3971.0            1979.1902   1991.465      0.994      0.320   -1924.440    5882.820\n",
       "3980.0            1.202e+04   1932.033      6.220      0.000    8230.312    1.58e+04\n",
       "4034.0           -5921.5990   2070.569     -2.860      0.004   -9980.288   -1862.910\n",
       "4036.0            5115.5995   2046.651      2.499      0.012    1103.795    9127.404\n",
       "4040.0            3206.5487   1921.949      1.668      0.095    -560.817    6973.914\n",
       "4058.0            3051.0613   1892.804      1.612      0.107    -659.176    6761.299\n",
       "4060.0           -1.628e+04   2856.127     -5.701      0.000   -2.19e+04   -1.07e+04\n",
       "4062.0            1.007e+04   2332.242      4.319      0.000    5502.034    1.46e+04\n",
       "4077.0            3780.6434   3349.737      1.129      0.259   -2785.444    1.03e+04\n",
       "4087.0           -2.322e+04   3294.134     -7.048      0.000   -2.97e+04   -1.68e+04\n",
       "4091.0            1998.9224   2477.718      0.807      0.420   -2857.851    6855.696\n",
       "4127.0           -3525.5756   1983.603     -1.777      0.076   -7413.796     362.644\n",
       "4138.0            5914.4417   2743.566      2.156      0.031     536.558    1.13e+04\n",
       "4162.0            2560.6378   2470.849      1.036      0.300   -2282.673    7403.948\n",
       "4186.0            5907.9693   2117.104      2.791      0.005    1758.063    1.01e+04\n",
       "4194.0            1285.6791   2336.059      0.550      0.582   -3293.417    5864.776\n",
       "4199.0           -1.585e+04   3474.430     -4.562      0.000   -2.27e+04   -9038.799\n",
       "4213.0             808.3674   1862.423      0.434      0.664   -2842.317    4459.052\n",
       "4222.0           -3607.1406   1989.203     -1.813      0.070   -7506.338     292.057\n",
       "4223.0            1403.0178   1874.897      0.748      0.454   -2272.119    5078.154\n",
       "4251.0            5148.4199   2065.112      2.493      0.013    1100.428    9196.412\n",
       "4265.0            3631.2777   2168.770      1.674      0.094    -619.902    7882.457\n",
       "4274.0           -2806.8085   2047.358     -1.371      0.170   -6819.999    1206.382\n",
       "4321.0           -4667.3963   2422.014     -1.927      0.054   -9414.980      80.188\n",
       "4335.0            -608.2474   3314.435     -0.184      0.854   -7105.138    5888.643\n",
       "4340.0           -1599.1975   1939.399     -0.825      0.410   -5400.769    2202.374\n",
       "4371.0           -3398.3487   1976.182     -1.720      0.086   -7272.021     475.324\n",
       "4415.0            4653.8189   2130.264      2.185      0.029     478.117    8829.521\n",
       "4450.0            4454.0494   2011.609      2.214      0.027     510.933    8397.166\n",
       "4476.0           -6248.1810   2401.134     -2.602      0.009    -1.1e+04   -1541.525\n",
       "4510.0           -1.049e+04   2418.176     -4.339      0.000   -1.52e+04   -5752.472\n",
       "4520.0           -3048.1121   1955.438     -1.559      0.119   -6881.122     784.898\n",
       "4551.0            3101.4769   8122.033      0.382      0.703   -1.28e+04     1.9e+04\n",
       "4568.0             397.0851   1971.862      0.201      0.840   -3468.119    4262.290\n",
       "4579.0            8237.8741   2353.238      3.501      0.000    3625.102    1.29e+04\n",
       "4585.0            5651.0424   2106.274      2.683      0.007    1522.365    9779.719\n",
       "4595.0             172.3713   1865.018      0.092      0.926   -3483.401    3828.144\n",
       "4600.0           -1591.7797   1994.959     -0.798      0.425   -5502.260    2318.700\n",
       "4607.0            5341.7990   2081.956      2.566      0.010    1260.789    9422.809\n",
       "4608.0            1066.2183   1867.881      0.571      0.568   -2595.166    4727.602\n",
       "4622.0           -8825.8179   2440.966     -3.616      0.000   -1.36e+04   -4041.085\n",
       "4623.0            4675.2103   2070.222      2.258      0.024     617.203    8733.218\n",
       "4768.0             997.5575   1888.424      0.528      0.597   -2704.093    4699.208\n",
       "4771.0            6203.3110   2153.731      2.880      0.004    1981.610    1.04e+04\n",
       "4800.0            4722.3159   2136.453      2.210      0.027     534.482    8910.150\n",
       "4802.0            4130.2512   1994.054      2.071      0.038     221.545    8038.957\n",
       "4807.0            2675.3897   1965.555      1.361      0.173   -1177.452    6528.231\n",
       "4839.0           -1.191e+05   4233.271    -28.133      0.000   -1.27e+05   -1.11e+05\n",
       "4843.0            4091.0901   1925.916      2.124      0.034     315.947    7866.233\n",
       "4881.0           -2251.8864   1918.174     -1.174      0.240   -6011.853    1508.080\n",
       "4900.0            4257.5069   1996.202      2.133      0.033     344.592    8170.422\n",
       "4926.0           -3476.2071   1957.953     -1.775      0.076   -7314.148     361.734\n",
       "4941.0             542.3435   1882.604      0.288      0.773   -3147.900    4232.587\n",
       "4961.0           -1.128e+04   3590.856     -3.143      0.002   -1.83e+04   -4246.042\n",
       "4988.0            1.212e+04   2166.411      5.596      0.000    7876.786    1.64e+04\n",
       "4993.0            8210.1591   2352.873      3.489      0.000    3598.104    1.28e+04\n",
       "5018.0           -1.448e+04   3024.558     -4.788      0.000   -2.04e+04   -8552.920\n",
       "5020.0            3930.1478   1869.322      2.102      0.036     265.939    7594.357\n",
       "5027.0           -2780.8939   1938.519     -1.435      0.151   -6580.740    1018.952\n",
       "5032.0            2085.8418   1902.032      1.097      0.273   -1642.484    5814.168\n",
       "5043.0            -616.7749   1867.760     -0.330      0.741   -4277.921    3044.371\n",
       "5046.0           -1.729e+04   3031.667     -5.704      0.000   -2.32e+04   -1.14e+04\n",
       "5047.0            2.379e+04   4241.993      5.608      0.000    1.55e+04    3.21e+04\n",
       "5065.0            4805.6677   2390.351      2.010      0.044     120.148    9491.187\n",
       "5071.0            4759.8373   2387.440      1.994      0.046      80.025    9439.649\n",
       "5073.0           -1.659e+05   6360.642    -26.085      0.000   -1.78e+05   -1.53e+05\n",
       "5087.0           -1074.7682   1864.101     -0.577      0.564   -4728.742    2579.206\n",
       "5109.0            6156.8224   2173.231      2.833      0.005    1896.897    1.04e+04\n",
       "5116.0           -1063.4051   2121.551     -0.501      0.616   -5222.027    3095.217\n",
       "5122.0           -3169.4565   1952.593     -1.623      0.105   -6996.892     657.979\n",
       "5134.0           -9160.0809   1987.931     -4.608      0.000   -1.31e+04   -5263.377\n",
       "5142.0             318.4028   2710.019      0.117      0.906   -4993.722    5630.528\n",
       "5165.0            6750.8497   2559.942      2.637      0.008    1732.902    1.18e+04\n",
       "5169.0            1.511e+04   2001.234      7.550      0.000    1.12e+04     1.9e+04\n",
       "5174.0            1640.8492   2271.830      0.722      0.470   -2812.347    6094.045\n",
       "5179.0            5217.9213   2052.345      2.542      0.011    1194.956    9240.887\n",
       "5181.0            5800.0679   2167.718      2.676      0.007    1550.950       1e+04\n",
       "5187.0            5893.9436   2526.090      2.333      0.020     942.352    1.08e+04\n",
       "5229.0           -9469.7480   2405.312     -3.937      0.000   -1.42e+04   -4754.904\n",
       "5234.0           -1.212e+04   2089.843     -5.800      0.000   -1.62e+04   -8024.707\n",
       "5237.0            3671.2141   1960.977      1.872      0.061    -172.654    7515.082\n",
       "5252.0             205.2720   1864.926      0.110      0.912   -3450.320    3860.864\n",
       "5254.0               1.5661   1866.155      0.001      0.999   -3656.434    3659.566\n",
       "5306.0           -8164.5373   2537.338     -3.218      0.001   -1.31e+04   -3190.898\n",
       "5338.0            2988.8536   1924.135      1.553      0.120    -782.799    6760.506\n",
       "5377.0            6499.7649   2207.361      2.945      0.003    2172.939    1.08e+04\n",
       "5439.0            4154.1516   2030.818      2.046      0.041     173.383    8134.920\n",
       "5456.0            8040.7845   2345.467      3.428      0.001    3443.247    1.26e+04\n",
       "5464.0            -408.0736   2448.885     -0.167      0.868   -5208.330    4392.182\n",
       "5476.0            8930.2376   2386.371      3.742      0.000    4252.521    1.36e+04\n",
       "5492.0           -1.613e+04   3387.599     -4.762      0.000   -2.28e+04   -9490.202\n",
       "5496.0            1502.5059   1881.197      0.799      0.424   -2184.979    5189.991\n",
       "5505.0            5540.7848   2130.770      2.600      0.009    1364.091    9717.479\n",
       "5518.0            4055.3226   2328.914      1.741      0.082    -509.768    8620.413\n",
       "5520.0            1115.9623   1872.844      0.596      0.551   -2555.149    4787.074\n",
       "5545.0            3706.3908   2059.949      1.799      0.072    -331.480    7744.262\n",
       "5568.0            5191.3374   1903.722      2.727      0.006    1459.698    8922.976\n",
       "5569.0            5268.3797   2121.742      2.483      0.013    1109.384    9427.376\n",
       "5578.0            4546.5266   2004.631      2.268      0.023     617.088    8475.965\n",
       "5581.0            3827.8583   1945.265      1.968      0.049      14.788    7640.928\n",
       "5589.0           -1.016e+04   2687.164     -3.782      0.000   -1.54e+04   -4894.780\n",
       "5597.0            3641.4163   2590.049      1.406      0.160   -1435.547    8718.380\n",
       "5606.0            -2.49e+04   2945.544     -8.452      0.000   -3.07e+04   -1.91e+04\n",
       "5639.0            7378.7637   2191.166      3.368      0.001    3083.684    1.17e+04\n",
       "5667.0            8260.7074   2387.777      3.460      0.001    3580.234    1.29e+04\n",
       "5690.0            3985.0621   1977.021      2.016      0.044     109.744    7860.380\n",
       "5709.0            3716.4396   2034.262      1.827      0.068    -271.082    7703.961\n",
       "5726.0            4996.0601   2035.830      2.454      0.014    1005.466    8986.654\n",
       "5764.0           -2603.1282   1992.133     -1.307      0.191   -6508.068    1301.812\n",
       "5772.0            1519.0392   1881.855      0.807      0.420   -2169.737    5207.815\n",
       "5860.0           -2.109e+04   2637.276     -7.995      0.000   -2.63e+04   -1.59e+04\n",
       "5878.0            1364.7944   1907.846      0.715      0.474   -2374.927    5104.516\n",
       "5903.0            2067.7321   1992.452      1.038      0.299   -1837.833    5973.297\n",
       "5905.0           -1864.7784   1886.734     -0.988      0.323   -5563.116    1833.560\n",
       "5959.0           -4990.6884   2036.050     -2.451      0.014   -8981.714    -999.663\n",
       "6008.0            1.741e+04   2880.320      6.046      0.000    1.18e+04    2.31e+04\n",
       "6034.0           -1.003e+04   2718.706     -3.690      0.000   -1.54e+04   -4703.137\n",
       "6035.0            2579.2198   2486.002      1.037      0.300   -2293.792    7452.232\n",
       "6036.0           -6898.4988   2196.252     -3.141      0.002   -1.12e+04   -2593.448\n",
       "6039.0            3501.6223   1953.934      1.792      0.073    -328.440    7331.685\n",
       "6044.0            7077.4692   2465.742      2.870      0.004    2244.170    1.19e+04\n",
       "6066.0           -9772.4102   4154.307     -2.352      0.019   -1.79e+04   -1629.220\n",
       "6078.0            5496.2272   1920.170      2.862      0.004    1732.348    9260.106\n",
       "6081.0           -1.406e+04   2549.252     -5.516      0.000   -1.91e+04   -9065.544\n",
       "60893.0           3239.1093   4129.186      0.784      0.433   -4854.840    1.13e+04\n",
       "6097.0            7640.1809   2207.786      3.461      0.001    3312.522     1.2e+04\n",
       "6102.0            2843.4365   1996.673      1.424      0.154   -1070.402    6757.275\n",
       "6104.0           -8300.1663   2037.445     -4.074      0.000   -1.23e+04   -4306.407\n",
       "6109.0           -6360.6720   2216.257     -2.870      0.004   -1.07e+04   -2016.410\n",
       "6127.0           -6787.9531   2240.150     -3.030      0.002   -1.12e+04   -2396.854\n",
       "61552.0          -1.149e+04   3882.144     -2.960      0.003   -1.91e+04   -3883.247\n",
       "6158.0            -916.8590   2043.647     -0.449      0.654   -4922.776    3089.058\n",
       "6171.0            3378.2117   1939.055      1.742      0.082    -422.686    7179.109\n",
       "61780.0          -1362.8843   4057.286     -0.336      0.737   -9315.897    6590.129\n",
       "6207.0            4778.5766   2033.773      2.350      0.019     792.016    8765.138\n",
       "6214.0            2860.6555   1924.860      1.486      0.137    -912.416    6633.727\n",
       "6216.0            7726.5050   2345.597      3.294      0.001    3128.712    1.23e+04\n",
       "62221.0           3663.3054   4132.258      0.887      0.375   -4436.666    1.18e+04\n",
       "6259.0            5957.1127   2391.462      2.491      0.013    1269.416    1.06e+04\n",
       "62599.0            568.0216   5686.343      0.100      0.920   -1.06e+04    1.17e+04\n",
       "6266.0            9467.6831   2313.372      4.093      0.000    4933.056     1.4e+04\n",
       "6268.0           -8787.1572   2112.679     -4.159      0.000   -1.29e+04   -4645.925\n",
       "6288.0            -908.5221   1881.992     -0.483      0.629   -4597.565    2780.521\n",
       "6297.0            1464.9049   1982.188      0.739      0.460   -2420.541    5350.351\n",
       "6307.0           -2.216e+04   2856.979     -7.757      0.000   -2.78e+04   -1.66e+04\n",
       "6313.0            4618.1251   3427.161      1.348      0.178   -2099.729    1.13e+04\n",
       "6314.0            6492.4201   2181.697      2.976      0.003    2215.901    1.08e+04\n",
       "6326.0           -9508.0014   2508.589     -3.790      0.000   -1.44e+04   -4590.714\n",
       "6349.0            2480.1364   1910.620      1.298      0.194   -1265.024    6225.296\n",
       "6357.0            5776.3456   2247.337      2.570      0.010    1371.159    1.02e+04\n",
       "6375.0             1.01e+04   2040.587      4.948      0.000    6097.595    1.41e+04\n",
       "6376.0            5614.1876   2140.060      2.623      0.009    1419.284    9809.091\n",
       "6379.0            8938.8647   5781.832      1.546      0.122   -2394.569    2.03e+04\n",
       "6386.0            3710.1340   1955.122      1.898      0.058    -122.257    7542.525\n",
       "6403.0            1056.7945   1959.891      0.539      0.590   -2784.945    4898.534\n",
       "6410.0            7553.1609   2284.474      3.306      0.001    3075.181     1.2e+04\n",
       "6416.0            1492.3880   1988.851      0.750      0.453   -2406.119    5390.895\n",
       "6424.0            5001.3026   2068.519      2.418      0.016     946.633    9055.973\n",
       "6433.0            7686.9981   2299.971      3.342      0.001    3178.640    1.22e+04\n",
       "6435.0            -737.8230   2057.950     -0.359      0.720   -4771.777    3296.131\n",
       "6492.0           -6962.4962   2261.952     -3.078      0.002   -1.14e+04   -2528.662\n",
       "6497.0            6239.3979   2154.347      2.896      0.004    2016.490    1.05e+04\n",
       "6500.0            7548.7349   8225.317      0.918      0.359   -8574.370    2.37e+04\n",
       "6509.0            1273.6530   1874.999      0.679      0.497   -2401.682    4948.988\n",
       "6527.0            5996.2601   2494.329      2.404      0.016    1106.925    1.09e+04\n",
       "6528.0            4138.6189   2205.290      1.877      0.061    -184.148    8461.386\n",
       "6531.0            -1.46e+04   2501.366     -5.836      0.000   -1.95e+04   -9693.994\n",
       "6532.0            -909.3400   1922.830     -0.473      0.636   -4678.433    2859.753\n",
       "6543.0            5667.3057   2107.568      2.689      0.007    1536.092    9798.519\n",
       "6548.0            5442.1657   2102.652      2.588      0.010    1320.589    9563.742\n",
       "6550.0            3292.2371   2176.446      1.513      0.130    -973.990    7558.464\n",
       "6552.0            5851.9235   2332.408      2.509      0.012    1279.984    1.04e+04\n",
       "6565.0           -1.029e+04   2789.761     -3.689      0.000   -1.58e+04   -4821.628\n",
       "6571.0            3479.5408   1963.291      1.772      0.076    -368.864    7327.945\n",
       "6573.0            2775.7816   1913.330      1.451      0.147    -974.691    6526.254\n",
       "6641.0            3055.6252   4081.419      0.749      0.454   -4944.692    1.11e+04\n",
       "6649.0            6919.6160   2188.499      3.162      0.002    2629.763    1.12e+04\n",
       "6730.0            1.006e+04   2161.155      4.655      0.000    5824.690    1.43e+04\n",
       "6731.0           -7019.9476   2270.362     -3.092      0.002   -1.15e+04   -2569.628\n",
       "6742.0            7170.2119   4326.943      1.657      0.098   -1311.377    1.57e+04\n",
       "6745.0            5044.4407   2056.453      2.453      0.014    1013.422    9075.459\n",
       "6756.0            7115.0126   2232.742      3.187      0.001    2738.436    1.15e+04\n",
       "6765.0           -1.086e+04   2485.280     -4.368      0.000   -1.57e+04   -5983.673\n",
       "6768.0            9142.6171   2428.250      3.765      0.000    4382.809    1.39e+04\n",
       "6774.0           -1.663e+04   2635.587     -6.311      0.000   -2.18e+04   -1.15e+04\n",
       "6797.0            6532.6165   2636.852      2.477      0.013    1363.912    1.17e+04\n",
       "6803.0            6239.7276   2168.069      2.878      0.004    1989.922    1.05e+04\n",
       "6821.0            3571.1369   1971.184      1.812      0.070    -292.740    7435.014\n",
       "6830.0            5691.0431   2087.521      2.726      0.006    1599.125    9782.961\n",
       "6845.0           -7051.5009   2278.308     -3.095      0.002   -1.15e+04   -2585.606\n",
       "6848.0            6681.3227   2396.308      2.788      0.005    1984.126    1.14e+04\n",
       "6873.0           -3738.7082   2708.085     -1.381      0.167   -9047.043    1569.627\n",
       "6900.0           -3750.3918   2002.203     -1.873      0.061   -7675.071     174.288\n",
       "6908.0           -2916.1307   1957.450     -1.490      0.136   -6753.086     920.825\n",
       "6994.0            5542.6486   2083.600      2.660      0.008    1458.416    9626.881\n",
       "7045.0            1363.0136   2484.971      0.549      0.583   -3507.977    6234.004\n",
       "7065.0            3498.3989   1873.146      1.868      0.062    -173.305    7170.103\n",
       "7085.0            3721.4889   1871.454      1.989      0.047      53.101    7389.877\n",
       "7107.0           -2262.4082   2076.355     -1.090      0.276   -6332.438    1807.622\n",
       "7116.0            8409.7314   2307.397      3.645      0.000    3886.817    1.29e+04\n",
       "7117.0            8374.1123   3393.680      2.468      0.014    1721.888     1.5e+04\n",
       "7121.0            3790.6464   1973.720      1.921      0.055     -78.200    7659.493\n",
       "7127.0            6381.3813   2447.749      2.607      0.009    1583.352    1.12e+04\n",
       "7139.0            2511.3497   1916.834      1.310      0.190   -1245.991    6268.690\n",
       "7146.0            4706.9041   2030.512      2.318      0.020     726.734    8687.074\n",
       "7163.0            7757.6182   2072.114      3.744      0.000    3695.902    1.18e+04\n",
       "7180.0           -2421.7565   1874.454     -1.292      0.196   -6096.024    1252.511\n",
       "7183.0           -5726.5396   2170.948     -2.638      0.008   -9981.990   -1471.090\n",
       "7228.0            1.455e+04   2133.403      6.818      0.000    1.04e+04    1.87e+04\n",
       "7232.0            6731.6393   3255.395      2.068      0.039     350.477    1.31e+04\n",
       "7250.0            2138.3776   2197.752      0.973      0.331   -2169.613    6446.369\n",
       "7257.0            3.176e+04   2117.435     15.001      0.000    2.76e+04    3.59e+04\n",
       "7260.0            2408.8671   1897.741      1.269      0.204   -1311.047    6128.781\n",
       "7267.0            2360.5998   2342.473      1.008      0.314   -2231.070    6952.269\n",
       "7268.0            5187.9468   2167.823      2.393      0.017     938.622    9437.272\n",
       "7281.0            5486.3808   2996.105      1.831      0.067    -386.526    1.14e+04\n",
       "7291.0           -3585.6338   1998.396     -1.794      0.073   -7502.850     331.582\n",
       "7343.0           -5333.1941   3491.925     -1.527      0.127   -1.22e+04    1511.608\n",
       "7346.0             651.5341   1867.455      0.349      0.727   -3009.013    4312.082\n",
       "7401.0            3041.9554   1931.405      1.575      0.115    -743.946    6827.857\n",
       "7409.0             310.1206   1864.417      0.166      0.868   -3344.473    3964.714\n",
       "7420.0           -2069.2980   1904.095     -1.087      0.277   -5801.668    1663.072\n",
       "7435.0           -4300.2979   3336.017     -1.289      0.197   -1.08e+04    2238.898\n",
       "7466.0            4575.6767   2162.039      2.116      0.034     337.691    8813.662\n",
       "7486.0            3327.0591   1944.850      1.711      0.087    -485.198    7139.317\n",
       "7503.0            1110.9578   3629.557      0.306      0.760   -6003.628    8225.543\n",
       "7506.0            1681.1847   1864.510      0.902      0.367   -1973.592    5335.961\n",
       "7537.0            6068.4666   2125.652      2.855      0.004    1901.806    1.02e+04\n",
       "7549.0           -3132.7330   1947.742     -1.608      0.108   -6950.659     685.193\n",
       "7554.0            5074.5529   2089.598      2.428      0.015     978.565    9170.541\n",
       "7557.0           -5768.0515   2234.127     -2.582      0.010   -1.01e+04   -1388.759\n",
       "7585.0           -8360.7510   3225.690     -2.592      0.010   -1.47e+04   -2037.817\n",
       "7602.0            4115.2606   1990.738      2.067      0.039     213.056    8017.465\n",
       "7620.0            5981.9239   2399.900      2.493      0.013    1277.687    1.07e+04\n",
       "7636.0            4996.1309   2052.820      2.434      0.015     972.233    9020.029\n",
       "7646.0            5066.5700   2057.716      2.462      0.014    1033.076    9100.064\n",
       "7658.0           -6023.6247   2149.646     -2.802      0.005   -1.02e+04   -1809.930\n",
       "7683.0            5132.9406   2204.662      2.328      0.020     811.407    9454.475\n",
       "7685.0            1209.7216   2107.912      0.574      0.566   -2922.165    5341.608\n",
       "7692.0           -2409.2113   1908.119     -1.263      0.207   -6149.468    1331.045\n",
       "7762.0            2404.9306   1897.249      1.268      0.205   -1314.020    6123.881\n",
       "7772.0            -1.37e+04   3025.335     -4.530      0.000   -1.96e+04   -7774.656\n",
       "7773.0            3049.8115   1931.994      1.579      0.114    -737.245    6836.868\n",
       "7777.0           -3395.4030   1967.205     -1.726      0.084   -7251.480     460.674\n",
       "7835.0            4453.7623   2001.368      2.225      0.026     530.720    8376.804\n",
       "7873.0            1314.2159   1876.490      0.700      0.484   -2364.043    4992.474\n",
       "7883.0           -7385.6645   2326.273     -3.175      0.002   -1.19e+04   -2825.750\n",
       "7904.0           -7911.9946   2248.566     -3.519      0.000   -1.23e+04   -3504.400\n",
       "7906.0            8457.4243   2177.531      3.884      0.000    4189.072    1.27e+04\n",
       "7921.0            1900.2675   1871.373      1.015      0.310   -1767.961    5568.496\n",
       "7923.0            5226.8772   2146.809      2.435      0.015    1018.744    9435.010\n",
       "7935.0              40.7129   1864.053      0.022      0.983   -3613.168    3694.594\n",
       "7938.0            -321.7520   1864.079     -0.173      0.863   -3975.683    3332.179\n",
       "7985.0           -2.135e+04   3619.084     -5.900      0.000   -2.84e+04   -1.43e+04\n",
       "8014.0            3426.7507   2056.518      1.666      0.096    -604.394    7457.896\n",
       "8030.0            7901.8265   2261.528      3.494      0.000    3468.824    1.23e+04\n",
       "8046.0             833.3317   1866.253      0.447      0.655   -2824.860    4491.523\n",
       "8047.0            1602.6486   2727.533      0.588      0.557   -3743.808    6949.105\n",
       "8062.0              78.5373   1915.560      0.041      0.967   -3676.306    3833.381\n",
       "8068.0            -1.48e+04   2256.653     -6.560      0.000   -1.92e+04   -1.04e+04\n",
       "8087.0           -2521.2918   1928.370     -1.307      0.191   -6301.244    1258.660\n",
       "8095.0            1779.8670   1888.790      0.942      0.346   -1922.503    5482.237\n",
       "8096.0            6767.4474   2206.383      3.067      0.002    2442.538    1.11e+04\n",
       "8109.0            3670.7502   1965.696      1.867      0.062    -182.368    7523.868\n",
       "8123.0           -1.084e+04   2689.266     -4.029      0.000   -1.61e+04   -5564.030\n",
       "8150.0            5450.6132   2097.209      2.599      0.009    1339.705    9561.521\n",
       "8163.0           -6883.0058   2271.288     -3.030      0.002   -1.13e+04   -2430.871\n",
       "8176.0            7829.9634   2649.656      2.955      0.003    2636.160     1.3e+04\n",
       "8202.0           -3470.2149   2109.124     -1.645      0.100   -7604.477     664.048\n",
       "8214.0           -4483.6951   1939.925     -2.311      0.021   -8286.298    -681.092\n",
       "8215.0           -9038.7901   2403.228     -3.761      0.000   -1.37e+04   -4328.030\n",
       "8219.0            7162.4585   2269.938      3.155      0.002    2712.972    1.16e+04\n",
       "8247.0           -4490.8986   2070.447     -2.169      0.030   -8549.348    -432.449\n",
       "8253.0           -6634.8697   2103.115     -3.155      0.002   -1.08e+04   -2512.384\n",
       "8290.0              44.4631   2004.461      0.022      0.982   -3884.642    3973.568\n",
       "8293.0           -5592.6582   2142.027     -2.611      0.009   -9791.418   -1393.898\n",
       "8304.0            4343.8755   1928.676      2.252      0.024     563.323    8124.428\n",
       "8334.0            8156.6130   2451.973      3.327      0.001    3350.304     1.3e+04\n",
       "8348.0            1859.8477   1887.968      0.985      0.325   -1840.910    5560.605\n",
       "8357.0            1391.0125   1873.915      0.742      0.458   -2282.200    5064.225\n",
       "8358.0           -2972.1496   1941.167     -1.531      0.126   -6777.187     832.888\n",
       "8446.0           -9765.3587   2303.586     -4.239      0.000   -1.43e+04   -5249.915\n",
       "8460.0            8313.1316   2684.159      3.097      0.002    3051.696    1.36e+04\n",
       "8463.0            3540.6440   1979.017      1.789      0.074    -338.586    7419.875\n",
       "8479.0            7069.6698   2777.372      2.545      0.011    1625.520    1.25e+04\n",
       "8530.0            1.527e+04   2143.195      7.127      0.000    1.11e+04    1.95e+04\n",
       "8536.0           -6067.7336   2128.180     -2.851      0.004   -1.02e+04   -1896.118\n",
       "8543.0            2.431e+04   2339.891     10.391      0.000    1.97e+04    2.89e+04\n",
       "8549.0           -8491.4525   2144.141     -3.960      0.000   -1.27e+04   -4288.549\n",
       "8551.0            6317.6608   2201.517      2.870      0.004    2002.290    1.06e+04\n",
       "8559.0            5102.2092   2202.085      2.317      0.021     785.726    9418.693\n",
       "8573.0            5362.8151   2180.010      2.460      0.014    1089.602    9636.029\n",
       "8606.0            2441.4458   1864.788      1.309      0.190   -1213.874    6096.766\n",
       "8607.0            7058.7426   2257.207      3.127      0.002    2634.210    1.15e+04\n",
       "8648.0            1606.9304   1882.540      0.854      0.393   -2083.187    5297.048\n",
       "8657.0            -273.1365   1864.424     -0.146      0.884   -3927.743    3381.470\n",
       "8675.0            6256.4555   3809.101      1.643      0.101   -1210.069    1.37e+04\n",
       "8681.0           -3098.7461   1958.469     -1.582      0.114   -6937.698     740.206\n",
       "8687.0           -2061.6502   2250.261     -0.916      0.360   -6472.568    2349.267\n",
       "8692.0           -2354.8354   1895.725     -1.242      0.214   -6070.798    1361.127\n",
       "8699.0            3125.8315   1938.059      1.613      0.107    -673.113    6924.776\n",
       "8717.0            5612.2962   2087.698      2.688      0.007    1520.033    9704.560\n",
       "8759.0            5593.7671   2114.288      2.646      0.008    1449.381    9738.153\n",
       "8762.0             1.02e+04   1999.661      5.101      0.000    6280.065    1.41e+04\n",
       "8819.0            8386.7682   2377.400      3.528      0.000    3726.636     1.3e+04\n",
       "8850.0            2716.9956   1920.984      1.414      0.157   -1048.480    6482.471\n",
       "8852.0            3050.3855   1931.187      1.580      0.114    -735.090    6835.861\n",
       "8859.0            7119.6062   2269.908      3.137      0.002    2670.177    1.16e+04\n",
       "8867.0           -4092.6895   2053.430     -1.993      0.046   -8117.782     -67.597\n",
       "8881.0             525.8274   1866.275      0.282      0.778   -3132.409    4184.064\n",
       "8958.0            2252.9193   1900.170      1.186      0.236   -1471.756    5977.594\n",
       "8972.0           -1.706e+04   3205.600     -5.323      0.000   -2.33e+04   -1.08e+04\n",
       "8990.0           -2852.1868   2100.602     -1.358      0.175   -6969.746    1265.372\n",
       "9004.0            6122.1074   2452.259      2.497      0.013    1315.237    1.09e+04\n",
       "9016.0            -156.8318   1866.832     -0.084      0.933   -3816.160    3502.496\n",
       "9048.0           -4123.7657   2024.008     -2.037      0.042   -8091.186    -156.346\n",
       "9051.0           -2111.4475   2288.481     -0.923      0.356   -6597.282    2374.387\n",
       "9071.0           -1.034e+04   2692.856     -3.841      0.000   -1.56e+04   -5064.121\n",
       "9112.0            -489.4159   1866.396     -0.262      0.793   -4147.888    3169.057\n",
       "9114.0           -8444.7733   2140.807     -3.945      0.000   -1.26e+04   -4248.406\n",
       "9132.0            8487.8115   3631.523      2.337      0.019    1369.372    1.56e+04\n",
       "9173.0             326.4350   2255.147      0.145      0.885   -4094.060    4746.930\n",
       "9180.0            5704.1023   2133.434      2.674      0.008    1522.187    9886.018\n",
       "9186.0             419.3959   1865.732      0.225      0.822   -3237.775    4076.566\n",
       "9191.0           -7372.7748   3463.407     -2.129      0.033   -1.42e+04    -583.872\n",
       "9216.0           -1.098e+04   2757.147     -3.983      0.000   -1.64e+04   -5576.156\n",
       "9217.0           -1.158e+04   2798.937     -4.136      0.000   -1.71e+04   -6090.930\n",
       "9225.0            3196.0518   1924.982      1.660      0.097    -577.260    6969.364\n",
       "9230.0            7182.7628   2765.582      2.597      0.009    1761.724    1.26e+04\n",
       "9259.0            8354.7013   2371.940      3.522      0.000    3705.272     1.3e+04\n",
       "9293.0            5061.0236   2042.829      2.477      0.013    1056.711    9065.336\n",
       "9299.0           -4470.4230   1902.325     -2.350      0.019   -8199.324    -741.522\n",
       "9308.0            6085.0890   2144.445      2.838      0.005    1881.590    1.03e+04\n",
       "9311.0             723.9273   3068.441      0.236      0.813   -5290.771    6738.625\n",
       "9313.0           -1736.7159   1865.669     -0.931      0.352   -5393.764    1920.332\n",
       "9325.0            1145.8660   1923.806      0.596      0.551   -2625.141    4916.873\n",
       "9332.0           -2386.5728   1918.073     -1.244      0.213   -6146.341    1373.195\n",
       "9340.0            -1.06e+04   3467.553     -3.057      0.002   -1.74e+04   -3803.699\n",
       "9372.0            7808.8527   2560.984      3.049      0.002    2788.862    1.28e+04\n",
       "9411.0            -528.3712   1999.930     -0.264      0.792   -4448.594    3391.852\n",
       "9459.0            4272.7861   2153.015      1.985      0.047      52.489    8493.083\n",
       "9465.0            3829.1320   2035.049      1.882      0.060    -159.930    7818.194\n",
       "9472.0           -3461.9009   1980.006     -1.748      0.080   -7343.069     419.267\n",
       "9483.0           -2966.5377   1954.795     -1.518      0.129   -6798.289     865.213\n",
       "9563.0           -2.178e+04   3477.567     -6.263      0.000   -2.86e+04    -1.5e+04\n",
       "9590.0            5556.6143   2095.883      2.651      0.008    1448.306    9664.923\n",
       "9598.0           -3000.3150   2815.493     -1.066      0.287   -8519.190    2518.560\n",
       "9599.0              96.4325   1866.180      0.052      0.959   -3561.616    3754.481\n",
       "9602.0           -3209.1877   3345.415     -0.959      0.337   -9766.804    3348.428\n",
       "9619.0            8362.0997   2368.635      3.530      0.000    3719.148     1.3e+04\n",
       "9643.0           -7569.9362   2290.434     -3.305      0.001   -1.21e+04   -3080.273\n",
       "9650.0           -1.023e+04   2649.183     -3.862      0.000   -1.54e+04   -5038.780\n",
       "9653.0           -1.725e+04   4717.428     -3.657      0.000   -2.65e+04   -8003.553\n",
       "9667.0           -7987.2772   2362.354     -3.381      0.001   -1.26e+04   -3356.638\n",
       "9698.0            5019.9025   2055.389      2.442      0.015     990.970    9048.835\n",
       "9699.0            4179.4211   1917.850      2.179      0.029     420.090    7938.753\n",
       "9719.0           -6450.6652   2229.140     -2.894      0.004   -1.08e+04   -2081.149\n",
       "9742.0           -1060.9540   1878.750     -0.565      0.572   -4743.643    2621.735\n",
       "9761.0            1719.9734   1885.152      0.912      0.362   -1975.264    5415.211\n",
       "9771.0           -7657.2718   2282.093     -3.355      0.001   -1.21e+04   -3183.957\n",
       "9772.0            6542.7727   2160.521      3.028      0.002    2307.762    1.08e+04\n",
       "9778.0           -2362.7993   1948.249     -1.213      0.225   -6181.719    1456.120\n",
       "9799.0           -1.118e+04   2973.308     -3.760      0.000    -1.7e+04   -5350.162\n",
       "9815.0            1308.4978   1970.360      0.664      0.507   -2553.762    5170.758\n",
       "9818.0           -2.906e+04   2163.126    -13.433      0.000   -3.33e+04   -2.48e+04\n",
       "9837.0            7678.9303   2307.083      3.328      0.001    3156.632    1.22e+04\n",
       "9922.0           -2248.8226   1916.817     -1.173      0.241   -6006.129    1508.484\n",
       "9954.0           -1.144e+04   3612.246     -3.167      0.002   -1.85e+04   -4359.659\n",
       "9963.0           -2221.9814   1883.297     -1.180      0.238   -5913.583    1469.620\n",
       "9988.0            7509.6774   2345.541      3.202      0.001    2911.995    1.21e+04\n",
       "9999.0             -1.1e+04   2734.749     -4.023      0.000   -1.64e+04   -5641.594\n",
       "gspilltecIVX1982     0.0155      0.052      0.299      0.765      -0.086       0.118\n",
       "gspilltecIVX1983    -0.0057      0.051     -0.110      0.912      -0.106       0.095\n",
       "gspilltecIVX1984    -0.0692      0.051     -1.359      0.174      -0.169       0.031\n",
       "gspilltecIVX1985    -0.1076      0.051     -2.103      0.036      -0.208      -0.007\n",
       "gspilltecIVX1986    -0.1549      0.052     -2.990      0.003      -0.256      -0.053\n",
       "gspilltecIVX1987    -0.1737      0.053     -3.301      0.001      -0.277      -0.071\n",
       "gspilltecIVX1988    -0.2116      0.053     -3.958      0.000      -0.316      -0.107\n",
       "gspilltecIVX1989    -0.2163      0.054     -3.979      0.000      -0.323      -0.110\n",
       "gspilltecIVX1990    -0.2536      0.055     -4.569      0.000      -0.362      -0.145\n",
       "gspilltecIVX1991    -0.2393      0.056     -4.241      0.000      -0.350      -0.129\n",
       "gspilltecIVX1992    -0.2451      0.057     -4.269      0.000      -0.358      -0.133\n",
       "gspilltecIVX1993    -0.2314      0.059     -3.940      0.000      -0.346      -0.116\n",
       "gspilltecIVX1994    -0.2438      0.060     -4.064      0.000      -0.361      -0.126\n",
       "gspilltecIVX1995    -0.2358      0.062     -3.823      0.000      -0.357      -0.115\n",
       "gspilltecIVX1996    -0.2342      0.064     -3.667      0.000      -0.359      -0.109\n",
       "gspilltecIVX1997    -0.2241      0.066     -3.382      0.001      -0.354      -0.094\n",
       "gspilltecIVX1998    -0.1955      0.069     -2.850      0.004      -0.330      -0.061\n",
       "gspilltecIVX1999    -0.1094      0.071     -1.551      0.121      -0.248       0.029\n",
       "==============================================================================\n",
       "Omnibus:                    22347.423   Durbin-Watson:                   0.741\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        128268599.681\n",
       "Skew:                          14.218   Prob(JB):                         0.00\n",
       "Kurtosis:                     514.370   Cond. No.                     8.09e+18\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.32e-25. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tech spillovers model, firm FE's\n",
    "x_vars_fe = x_vars.drop(columns=drop_columns)\n",
    "\n",
    "year_model3 = sm.OLS(y_var,x_vars_fe).fit()\n",
    "year_model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db840de-c87c-4b76-8a30-c3cd1c80f6af",
   "metadata": {},
   "source": [
    "Results highly depend on the firm FE's. Wrong sign for tech, insignificant for product market. Unobserved firm-related variables play a role?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ad88ded-6932-4961-82f2-53719a5b2d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.366</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.364</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   281.1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:57:56</td>     <th>  Log-Likelihood:    </th> <td>-1.2566e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 11736</td>      <th>  AIC:               </th>  <td>2.514e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 11711</td>      <th>  BIC:               </th>  <td>2.516e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>  675.9096</td> <td>  129.793</td> <td>    5.208</td> <td> 0.000</td> <td>  421.493</td> <td>  930.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th>      <td>   -0.0859</td> <td>    0.079</td> <td>   -1.083</td> <td> 0.279</td> <td>   -0.241</td> <td>    0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>    0.6219</td> <td>    1.536</td> <td>    0.405</td> <td> 0.686</td> <td>   -2.388</td> <td>    3.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.5655</td> <td>    0.034</td> <td>   16.551</td> <td> 0.000</td> <td>    0.499</td> <td>    0.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.5194</td> <td>    0.044</td> <td>   11.673</td> <td> 0.000</td> <td>    0.432</td> <td>    0.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>  -32.3321</td> <td>    3.285</td> <td>   -9.844</td> <td> 0.000</td> <td>  -38.770</td> <td>  -25.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>    5.9989</td> <td>    0.453</td> <td>   13.253</td> <td> 0.000</td> <td>    5.112</td> <td>    6.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1982</th> <td>    0.0296</td> <td>    0.106</td> <td>    0.279</td> <td> 0.781</td> <td>   -0.179</td> <td>    0.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1983</th> <td>    0.0531</td> <td>    0.104</td> <td>    0.512</td> <td> 0.608</td> <td>   -0.150</td> <td>    0.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1984</th> <td>    0.0302</td> <td>    0.100</td> <td>    0.301</td> <td> 0.764</td> <td>   -0.166</td> <td>    0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1985</th> <td>    0.0483</td> <td>    0.098</td> <td>    0.493</td> <td> 0.622</td> <td>   -0.144</td> <td>    0.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1986</th> <td>    0.0643</td> <td>    0.096</td> <td>    0.671</td> <td> 0.503</td> <td>   -0.124</td> <td>    0.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1987</th> <td>    0.0622</td> <td>    0.093</td> <td>    0.667</td> <td> 0.504</td> <td>   -0.120</td> <td>    0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1988</th> <td>    0.0486</td> <td>    0.092</td> <td>    0.526</td> <td> 0.599</td> <td>   -0.133</td> <td>    0.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1989</th> <td>    0.0831</td> <td>    0.091</td> <td>    0.912</td> <td> 0.362</td> <td>   -0.096</td> <td>    0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1990</th> <td>    0.0701</td> <td>    0.090</td> <td>    0.782</td> <td> 0.434</td> <td>   -0.106</td> <td>    0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1991</th> <td>    0.1178</td> <td>    0.089</td> <td>    1.330</td> <td> 0.184</td> <td>   -0.056</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1992</th> <td>    0.0942</td> <td>    0.088</td> <td>    1.072</td> <td> 0.284</td> <td>   -0.078</td> <td>    0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1993</th> <td>    0.1047</td> <td>    0.087</td> <td>    1.205</td> <td> 0.228</td> <td>   -0.066</td> <td>    0.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1994</th> <td>    0.1002</td> <td>    0.087</td> <td>    1.157</td> <td> 0.247</td> <td>   -0.070</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1995</th> <td>    0.1659</td> <td>    0.086</td> <td>    1.929</td> <td> 0.054</td> <td>   -0.003</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1996</th> <td>    0.2082</td> <td>    0.085</td> <td>    2.440</td> <td> 0.015</td> <td>    0.041</td> <td>    0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1997</th> <td>    0.3148</td> <td>    0.085</td> <td>    3.712</td> <td> 0.000</td> <td>    0.149</td> <td>    0.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1998</th> <td>    0.4530</td> <td>    0.084</td> <td>    5.373</td> <td> 0.000</td> <td>    0.288</td> <td>    0.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1999</th> <td>    0.5612</td> <td>    0.084</td> <td>    6.670</td> <td> 0.000</td> <td>    0.396</td> <td>    0.726</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22461.761</td> <th>  Durbin-Watson:     </th>   <td>   0.570</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>95291527.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>14.541</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>443.482</td>  <th>  Cond. No.          </th>   <td>1.51e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.51e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.366    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.364    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      281.1    \\\\\n",
       "\\textbf{Date:}             & Mon, 14 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     16:57:56     & \\textbf{  Log-Likelihood:    } & -1.2566e+05   \\\\\n",
       "\\textbf{No. Observations:} &       11736      & \\textbf{  AIC:               } &  2.514e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       11711      & \\textbf{  BIC:               } &  2.516e+05    \\\\\n",
       "\\textbf{Df Model:}         &          24      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &     675.9096  &      129.793     &     5.208  &         0.000        &      421.493    &      930.326     \\\\\n",
       "\\textbf{gspillsicIV}      &      -0.0859  &        0.079     &    -1.083  &         0.279        &       -0.241    &        0.070     \\\\\n",
       "\\textbf{pat\\_count}       &       0.6219  &        1.536     &     0.405  &         0.686        &       -2.388    &        3.632     \\\\\n",
       "\\textbf{rsales}           &       0.5655  &        0.034     &    16.551  &         0.000        &        0.499    &        0.632     \\\\\n",
       "\\textbf{rppent}           &       0.5194  &        0.044     &    11.673  &         0.000        &        0.432    &        0.607     \\\\\n",
       "\\textbf{emp}              &     -32.3321  &        3.285     &    -9.844  &         0.000        &      -38.770    &      -25.894     \\\\\n",
       "\\textbf{rxrd}             &       5.9989  &        0.453     &    13.253  &         0.000        &        5.112    &        6.886     \\\\\n",
       "\\textbf{gspillsicIVX1982} &       0.0296  &        0.106     &     0.279  &         0.781        &       -0.179    &        0.238     \\\\\n",
       "\\textbf{gspillsicIVX1983} &       0.0531  &        0.104     &     0.512  &         0.608        &       -0.150    &        0.256     \\\\\n",
       "\\textbf{gspillsicIVX1984} &       0.0302  &        0.100     &     0.301  &         0.764        &       -0.166    &        0.227     \\\\\n",
       "\\textbf{gspillsicIVX1985} &       0.0483  &        0.098     &     0.493  &         0.622        &       -0.144    &        0.241     \\\\\n",
       "\\textbf{gspillsicIVX1986} &       0.0643  &        0.096     &     0.671  &         0.503        &       -0.124    &        0.252     \\\\\n",
       "\\textbf{gspillsicIVX1987} &       0.0622  &        0.093     &     0.667  &         0.504        &       -0.120    &        0.245     \\\\\n",
       "\\textbf{gspillsicIVX1988} &       0.0486  &        0.092     &     0.526  &         0.599        &       -0.133    &        0.230     \\\\\n",
       "\\textbf{gspillsicIVX1989} &       0.0831  &        0.091     &     0.912  &         0.362        &       -0.096    &        0.262     \\\\\n",
       "\\textbf{gspillsicIVX1990} &       0.0701  &        0.090     &     0.782  &         0.434        &       -0.106    &        0.246     \\\\\n",
       "\\textbf{gspillsicIVX1991} &       0.1178  &        0.089     &     1.330  &         0.184        &       -0.056    &        0.292     \\\\\n",
       "\\textbf{gspillsicIVX1992} &       0.0942  &        0.088     &     1.072  &         0.284        &       -0.078    &        0.266     \\\\\n",
       "\\textbf{gspillsicIVX1993} &       0.1047  &        0.087     &     1.205  &         0.228        &       -0.066    &        0.275     \\\\\n",
       "\\textbf{gspillsicIVX1994} &       0.1002  &        0.087     &     1.157  &         0.247        &       -0.070    &        0.270     \\\\\n",
       "\\textbf{gspillsicIVX1995} &       0.1659  &        0.086     &     1.929  &         0.054        &       -0.003    &        0.335     \\\\\n",
       "\\textbf{gspillsicIVX1996} &       0.2082  &        0.085     &     2.440  &         0.015        &        0.041    &        0.375     \\\\\n",
       "\\textbf{gspillsicIVX1997} &       0.3148  &        0.085     &     3.712  &         0.000        &        0.149    &        0.481     \\\\\n",
       "\\textbf{gspillsicIVX1998} &       0.4530  &        0.084     &     5.373  &         0.000        &        0.288    &        0.618     \\\\\n",
       "\\textbf{gspillsicIVX1999} &       0.5612  &        0.084     &     6.670  &         0.000        &        0.396    &        0.726     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 22461.761 & \\textbf{  Durbin-Watson:     } &      0.570    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 95291527.573  \\\\\n",
       "\\textbf{Skew:}          &   14.541  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  443.482  & \\textbf{  Cond. No.          } &   1.51e+04    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.51e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.366\n",
       "Model:                            OLS   Adj. R-squared:                  0.364\n",
       "Method:                 Least Squares   F-statistic:                     281.1\n",
       "Date:                Mon, 14 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        16:57:56   Log-Likelihood:            -1.2566e+05\n",
       "No. Observations:               11736   AIC:                         2.514e+05\n",
       "Df Residuals:                   11711   BIC:                         2.516e+05\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const              675.9096    129.793      5.208      0.000     421.493     930.326\n",
       "gspillsicIV         -0.0859      0.079     -1.083      0.279      -0.241       0.070\n",
       "pat_count            0.6219      1.536      0.405      0.686      -2.388       3.632\n",
       "rsales               0.5655      0.034     16.551      0.000       0.499       0.632\n",
       "rppent               0.5194      0.044     11.673      0.000       0.432       0.607\n",
       "emp                -32.3321      3.285     -9.844      0.000     -38.770     -25.894\n",
       "rxrd                 5.9989      0.453     13.253      0.000       5.112       6.886\n",
       "gspillsicIVX1982     0.0296      0.106      0.279      0.781      -0.179       0.238\n",
       "gspillsicIVX1983     0.0531      0.104      0.512      0.608      -0.150       0.256\n",
       "gspillsicIVX1984     0.0302      0.100      0.301      0.764      -0.166       0.227\n",
       "gspillsicIVX1985     0.0483      0.098      0.493      0.622      -0.144       0.241\n",
       "gspillsicIVX1986     0.0643      0.096      0.671      0.503      -0.124       0.252\n",
       "gspillsicIVX1987     0.0622      0.093      0.667      0.504      -0.120       0.245\n",
       "gspillsicIVX1988     0.0486      0.092      0.526      0.599      -0.133       0.230\n",
       "gspillsicIVX1989     0.0831      0.091      0.912      0.362      -0.096       0.262\n",
       "gspillsicIVX1990     0.0701      0.090      0.782      0.434      -0.106       0.246\n",
       "gspillsicIVX1991     0.1178      0.089      1.330      0.184      -0.056       0.292\n",
       "gspillsicIVX1992     0.0942      0.088      1.072      0.284      -0.078       0.266\n",
       "gspillsicIVX1993     0.1047      0.087      1.205      0.228      -0.066       0.275\n",
       "gspillsicIVX1994     0.1002      0.087      1.157      0.247      -0.070       0.270\n",
       "gspillsicIVX1995     0.1659      0.086      1.929      0.054      -0.003       0.335\n",
       "gspillsicIVX1996     0.2082      0.085      2.440      0.015       0.041       0.375\n",
       "gspillsicIVX1997     0.3148      0.085      3.712      0.000       0.149       0.481\n",
       "gspillsicIVX1998     0.4530      0.084      5.373      0.000       0.288       0.618\n",
       "gspillsicIVX1999     0.5612      0.084      6.670      0.000       0.396       0.726\n",
       "==============================================================================\n",
       "Omnibus:                    22461.761   Durbin-Watson:                   0.570\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         95291527.573\n",
       "Skew:                          14.541   Prob(JB):                         0.00\n",
       "Kurtosis:                     443.482   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product spillovers model, no firm FE's\n",
    "drop_columns = [col for col in x_vars.columns if 'gspilltecIV' in col]\n",
    "x_vars_nofe = x_vars.drop(columns=fixed_effects)\n",
    "x_vars_nofe = x_vars_nofe.drop(columns=drop_columns)\n",
    "\n",
    "year_model4 = sm.OLS(y_var,x_vars_nofe).fit()\n",
    "year_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bc5fbf8-de34-465f-8097-71775a7443d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.665</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.641</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   28.37</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 14 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:58:05</td>     <th>  Log-Likelihood:    </th> <td>-1.2191e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 11736</td>      <th>  AIC:               </th>  <td>2.454e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 10968</td>      <th>  BIC:               </th>  <td>2.510e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   767</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-6154.3893</td> <td>  840.819</td> <td>   -7.320</td> <td> 0.000</td> <td>-7802.546</td> <td>-4506.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th>      <td>    1.2284</td> <td>    0.186</td> <td>    6.618</td> <td> 0.000</td> <td>    0.865</td> <td>    1.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -28.2387</td> <td>    1.740</td> <td>  -16.228</td> <td> 0.000</td> <td>  -31.650</td> <td>  -24.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    1.0403</td> <td>    0.041</td> <td>   25.468</td> <td> 0.000</td> <td>    0.960</td> <td>    1.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.5611</td> <td>    0.087</td> <td>    6.461</td> <td> 0.000</td> <td>    0.391</td> <td>    0.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>  -11.1408</td> <td>    6.898</td> <td>   -1.615</td> <td> 0.106</td> <td>  -24.663</td> <td>    2.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>    8.6120</td> <td>    0.670</td> <td>   12.852</td> <td> 0.000</td> <td>    7.299</td> <td>    9.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>             <td>  115.9480</td> <td>  620.546</td> <td>    0.187</td> <td> 0.852</td> <td>-1100.434</td> <td> 1332.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>             <td>  214.6565</td> <td>  615.322</td> <td>    0.349</td> <td> 0.727</td> <td> -991.485</td> <td> 1420.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>             <td>  -58.6229</td> <td>  611.072</td> <td>   -0.096</td> <td> 0.924</td> <td>-1256.434</td> <td> 1139.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>             <td>  -38.8501</td> <td>  609.748</td> <td>   -0.064</td> <td> 0.949</td> <td>-1234.065</td> <td> 1156.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>             <td>  -33.9845</td> <td>  606.213</td> <td>   -0.056</td> <td> 0.955</td> <td>-1222.270</td> <td> 1154.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>             <td> -146.6154</td> <td>  603.157</td> <td>   -0.243</td> <td> 0.808</td> <td>-1328.911</td> <td> 1035.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>             <td> -282.7993</td> <td>  602.981</td> <td>   -0.469</td> <td> 0.639</td> <td>-1464.750</td> <td>  899.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>             <td> -153.5184</td> <td>  600.503</td> <td>   -0.256</td> <td> 0.798</td> <td>-1330.613</td> <td> 1023.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>             <td> -482.0750</td> <td>  598.185</td> <td>   -0.806</td> <td> 0.420</td> <td>-1654.626</td> <td>  690.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>             <td> -175.7135</td> <td>  596.118</td> <td>   -0.295</td> <td> 0.768</td> <td>-1344.212</td> <td>  992.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>             <td>  -21.7112</td> <td>  595.147</td> <td>   -0.036</td> <td> 0.971</td> <td>-1188.307</td> <td> 1144.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>             <td>  103.0065</td> <td>  593.189</td> <td>    0.174</td> <td> 0.862</td> <td>-1059.752</td> <td> 1265.765</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>             <td> -115.0479</td> <td>  594.848</td> <td>   -0.193</td> <td> 0.847</td> <td>-1281.058</td> <td> 1050.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>             <td>  272.6655</td> <td>  595.318</td> <td>    0.458</td> <td> 0.647</td> <td> -894.265</td> <td> 1439.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>             <td>  710.9281</td> <td>  597.093</td> <td>    1.191</td> <td> 0.234</td> <td> -459.482</td> <td> 1881.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>             <td> 1004.6717</td> <td>  599.536</td> <td>    1.676</td> <td> 0.094</td> <td> -170.528</td> <td> 2179.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>             <td>  779.1160</td> <td>  603.511</td> <td>    1.291</td> <td> 0.197</td> <td> -403.874</td> <td> 1962.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>             <td> 1039.8341</td> <td>  609.204</td> <td>    1.707</td> <td> 0.088</td> <td> -154.315</td> <td> 2233.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>          <td> 4813.4949</td> <td> 1951.882</td> <td>    2.466</td> <td> 0.014</td> <td>  987.455</td> <td> 8639.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>          <td> 4797.8946</td> <td> 2330.607</td> <td>    2.059</td> <td> 0.040</td> <td>  229.485</td> <td> 9366.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>          <td> 3837.7388</td> <td> 1921.166</td> <td>    1.998</td> <td> 0.046</td> <td>   71.907</td> <td> 7603.570</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>          <td> 5633.0729</td> <td> 1974.935</td> <td>    2.852</td> <td> 0.004</td> <td> 1761.845</td> <td> 9504.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>          <td> 5704.5362</td> <td> 1987.517</td> <td>    2.870</td> <td> 0.004</td> <td> 1808.645</td> <td> 9600.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>           <td> 5673.8775</td> <td> 1989.429</td> <td>    2.852</td> <td> 0.004</td> <td> 1774.237</td> <td> 9573.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>          <td> 2609.4076</td> <td> 1897.755</td> <td>    1.375</td> <td> 0.169</td> <td>-1110.534</td> <td> 6329.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>          <td>-5372.5156</td> <td> 1996.211</td> <td>   -2.691</td> <td> 0.007</td> <td>-9285.449</td> <td>-1459.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>          <td> 5091.1028</td> <td> 4720.797</td> <td>    1.078</td> <td> 0.281</td> <td>-4162.511</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>          <td>-2417.5921</td> <td> 1871.431</td> <td>   -1.292</td> <td> 0.196</td> <td>-6085.934</td> <td> 1250.750</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>           <td> 5474.0561</td> <td> 4741.603</td> <td>    1.154</td> <td> 0.248</td> <td>-3820.341</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>          <td> 6035.9979</td> <td> 1994.555</td> <td>    3.026</td> <td> 0.002</td> <td> 2126.311</td> <td> 9945.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>          <td>  179.9615</td> <td> 1873.196</td> <td>    0.096</td> <td> 0.923</td> <td>-3491.840</td> <td> 3851.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>          <td> 6155.4676</td> <td> 2001.421</td> <td>    3.076</td> <td> 0.002</td> <td> 2232.322</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>           <td> -779.2988</td> <td> 1875.376</td> <td>   -0.416</td> <td> 0.678</td> <td>-4455.374</td> <td> 2896.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>          <td>-4203.7456</td> <td> 2471.997</td> <td>   -1.701</td> <td> 0.089</td> <td>-9049.305</td> <td>  641.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>          <td>-1.589e+04</td> <td> 4053.978</td> <td>   -3.921</td> <td> 0.000</td> <td>-2.38e+04</td> <td>-7947.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>          <td> 6019.0041</td> <td> 2219.889</td> <td>    2.711</td> <td> 0.007</td> <td> 1667.621</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>          <td>  741.4345</td> <td> 2103.257</td> <td>    0.353</td> <td> 0.724</td> <td>-3381.328</td> <td> 4864.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>          <td> -1.34e+04</td> <td> 2547.329</td> <td>   -5.260</td> <td> 0.000</td> <td>-1.84e+04</td> <td>-8404.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>          <td> 5775.5626</td> <td> 1991.524</td> <td>    2.900</td> <td> 0.004</td> <td> 1871.817</td> <td> 9679.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>          <td> 5892.5705</td> <td> 1999.611</td> <td>    2.947</td> <td> 0.003</td> <td> 1972.972</td> <td> 9812.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>          <td>-4786.0836</td> <td> 2217.613</td> <td>   -2.158</td> <td> 0.031</td> <td>-9133.006</td> <td> -439.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>          <td> 4748.5453</td> <td> 1952.213</td> <td>    2.432</td> <td> 0.015</td> <td>  921.856</td> <td> 8575.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>          <td> 4941.3511</td> <td> 1955.985</td> <td>    2.526</td> <td> 0.012</td> <td> 1107.267</td> <td> 8775.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>          <td>-2.169e+04</td> <td> 2733.118</td> <td>   -7.936</td> <td> 0.000</td> <td> -2.7e+04</td> <td>-1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>          <td> 5687.1815</td> <td> 1989.389</td> <td>    2.859</td> <td> 0.004</td> <td> 1787.620</td> <td> 9586.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>          <td> 5449.6824</td> <td> 3161.561</td> <td>    1.724</td> <td> 0.085</td> <td> -747.547</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>           <td>  136.4403</td> <td> 1974.368</td> <td>    0.069</td> <td> 0.945</td> <td>-3733.677</td> <td> 4006.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>          <td> 3180.1289</td> <td> 1901.625</td> <td>    1.672</td> <td> 0.094</td> <td> -547.398</td> <td> 6907.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>          <td>  159.6047</td> <td> 1864.898</td> <td>    0.086</td> <td> 0.932</td> <td>-3495.932</td> <td> 3815.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>          <td>-1304.6837</td> <td> 1874.653</td> <td>   -0.696</td> <td> 0.486</td> <td>-4979.341</td> <td> 2369.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>          <td>-4510.2417</td> <td> 1971.110</td> <td>   -2.288</td> <td> 0.022</td> <td>-8373.973</td> <td> -646.510</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>          <td> 2147.9748</td> <td> 1868.417</td> <td>    1.150</td> <td> 0.250</td> <td>-1514.459</td> <td> 5810.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>          <td>  827.9390</td> <td> 1978.150</td> <td>    0.419</td> <td> 0.676</td> <td>-3049.591</td> <td> 4705.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>          <td> 4101.8945</td> <td> 2116.924</td> <td>    1.938</td> <td> 0.053</td> <td>  -47.657</td> <td> 8251.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>          <td> 5431.0637</td> <td> 1976.162</td> <td>    2.748</td> <td> 0.006</td> <td> 1557.431</td> <td> 9304.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>           <td>-4396.3123</td> <td> 2140.993</td> <td>   -2.053</td> <td> 0.040</td> <td>-8593.045</td> <td> -199.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>          <td>  890.5197</td> <td> 1868.292</td> <td>    0.477</td> <td> 0.634</td> <td>-2771.669</td> <td> 4552.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>          <td>-2.681e+04</td> <td> 2430.093</td> <td>  -11.033</td> <td> 0.000</td> <td>-3.16e+04</td> <td> -2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>          <td> 4526.9593</td> <td> 1955.572</td> <td>    2.315</td> <td> 0.021</td> <td>  693.685</td> <td> 8360.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>          <td>-2289.7921</td> <td> 2219.374</td> <td>   -1.032</td> <td> 0.302</td> <td>-6640.164</td> <td> 2060.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>          <td> 5658.7336</td> <td> 2037.868</td> <td>    2.777</td> <td> 0.005</td> <td> 1664.146</td> <td> 9653.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>          <td>-1.549e+04</td> <td> 2455.731</td> <td>   -6.308</td> <td> 0.000</td> <td>-2.03e+04</td> <td>-1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>          <td> 2896.2869</td> <td> 1891.579</td> <td>    1.531</td> <td> 0.126</td> <td> -811.549</td> <td> 6604.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>          <td> 1897.1505</td> <td> 2266.096</td> <td>    0.837</td> <td> 0.403</td> <td>-2544.806</td> <td> 6339.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>          <td> 3762.2070</td> <td> 1901.412</td> <td>    1.979</td> <td> 0.048</td> <td>   35.097</td> <td> 7489.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>          <td> 5467.4940</td> <td> 2079.565</td> <td>    2.629</td> <td> 0.009</td> <td> 1391.172</td> <td> 9543.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>          <td>  893.7277</td> <td> 5745.727</td> <td>    0.156</td> <td> 0.876</td> <td>-1.04e+04</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>          <td>  548.1139</td> <td> 2037.324</td> <td>    0.269</td> <td> 0.788</td> <td>-3445.408</td> <td> 4541.636</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>          <td> 6318.3297</td> <td> 1997.411</td> <td>    3.163</td> <td> 0.002</td> <td> 2403.044</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>          <td> 5871.5698</td> <td> 1951.311</td> <td>    3.009</td> <td> 0.003</td> <td> 2046.647</td> <td> 9696.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>          <td> 4012.5149</td> <td> 1958.587</td> <td>    2.049</td> <td> 0.041</td> <td>  173.331</td> <td> 7851.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>          <td>-9773.7159</td> <td> 2249.288</td> <td>   -4.345</td> <td> 0.000</td> <td>-1.42e+04</td> <td>-5364.705</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>          <td> 4744.7244</td> <td> 1958.252</td> <td>    2.423</td> <td> 0.015</td> <td>  906.196</td> <td> 8583.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>          <td> 5741.7786</td> <td> 1997.394</td> <td>    2.875</td> <td> 0.004</td> <td> 1826.527</td> <td> 9657.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>          <td> 4077.2832</td> <td> 1936.830</td> <td>    2.105</td> <td> 0.035</td> <td>  280.747</td> <td> 7873.820</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>          <td> 5841.9447</td> <td> 1995.542</td> <td>    2.927</td> <td> 0.003</td> <td> 1930.324</td> <td> 9753.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>          <td> 5848.1400</td> <td> 1996.184</td> <td>    2.930</td> <td> 0.003</td> <td> 1935.260</td> <td> 9761.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>          <td> 6950.3536</td> <td> 2107.448</td> <td>    3.298</td> <td> 0.001</td> <td> 2819.375</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>          <td> 1463.9948</td> <td> 2575.943</td> <td>    0.568</td> <td> 0.570</td> <td>-3585.318</td> <td> 6513.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>          <td> 4998.1236</td> <td> 1972.215</td> <td>    2.534</td> <td> 0.011</td> <td> 1132.227</td> <td> 8864.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>          <td> 5669.5216</td> <td> 2036.622</td> <td>    2.784</td> <td> 0.005</td> <td> 1677.375</td> <td> 9661.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>          <td> 5816.7134</td> <td> 1996.922</td> <td>    2.913</td> <td> 0.004</td> <td> 1902.387</td> <td> 9731.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>           <td>-7730.8735</td> <td> 3358.565</td> <td>   -2.302</td> <td> 0.021</td> <td>-1.43e+04</td> <td>-1147.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>          <td> 4647.0994</td> <td> 1992.190</td> <td>    2.333</td> <td> 0.020</td> <td>  742.049</td> <td> 8552.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>          <td> 4439.0498</td> <td> 1989.563</td> <td>    2.231</td> <td> 0.026</td> <td>  539.147</td> <td> 8338.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>          <td> 5976.4480</td> <td> 1992.750</td> <td>    2.999</td> <td> 0.003</td> <td> 2070.298</td> <td> 9882.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>          <td>-4676.9149</td> <td> 1903.763</td> <td>   -2.457</td> <td> 0.014</td> <td>-8408.633</td> <td> -945.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>          <td>  999.9244</td> <td> 2248.159</td> <td>    0.445</td> <td> 0.656</td> <td>-3406.874</td> <td> 5406.722</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>          <td> 4250.9368</td> <td> 1935.088</td> <td>    2.197</td> <td> 0.028</td> <td>  457.816</td> <td> 8044.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>          <td> 5458.2452</td> <td> 3152.203</td> <td>    1.732</td> <td> 0.083</td> <td> -720.641</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>          <td>-1.863e+04</td> <td> 2228.504</td> <td>   -8.358</td> <td> 0.000</td> <td> -2.3e+04</td> <td>-1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>           <td> 4642.8669</td> <td> 1949.594</td> <td>    2.381</td> <td> 0.017</td> <td>  821.311</td> <td> 8464.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>          <td>-3086.1868</td> <td> 2474.978</td> <td>   -1.247</td> <td> 0.212</td> <td>-7937.590</td> <td> 1765.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>          <td>  871.2078</td> <td> 1920.437</td> <td>    0.454</td> <td> 0.650</td> <td>-2893.195</td> <td> 4635.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>          <td>-5938.0545</td> <td> 1948.190</td> <td>   -3.048</td> <td> 0.002</td> <td>-9756.858</td> <td>-2119.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>           <td> 3785.6276</td> <td> 1920.141</td> <td>    1.972</td> <td> 0.049</td> <td>   21.806</td> <td> 7549.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>          <td> 5844.4640</td> <td> 1996.187</td> <td>    2.928</td> <td> 0.003</td> <td> 1931.577</td> <td> 9757.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>          <td> 4253.5271</td> <td> 1939.561</td> <td>    2.193</td> <td> 0.028</td> <td>  451.637</td> <td> 8055.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>          <td> 2662.2524</td> <td> 1894.882</td> <td>    1.405</td> <td> 0.160</td> <td>-1052.058</td> <td> 6376.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>          <td> 4155.5135</td> <td> 2093.983</td> <td>    1.985</td> <td> 0.047</td> <td>   50.928</td> <td> 8260.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>           <td> 2710.8281</td> <td> 1893.928</td> <td>    1.431</td> <td> 0.152</td> <td>-1001.612</td> <td> 6423.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>          <td> -730.9916</td> <td> 1872.544</td> <td>   -0.390</td> <td> 0.696</td> <td>-4401.516</td> <td> 2939.533</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>          <td> 5313.3116</td> <td> 1972.168</td> <td>    2.694</td> <td> 0.007</td> <td> 1447.507</td> <td> 9179.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>          <td> 5737.7176</td> <td> 1966.824</td> <td>    2.917</td> <td> 0.004</td> <td> 1882.388</td> <td> 9593.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>          <td> 3217.8327</td> <td> 3357.319</td> <td>    0.958</td> <td> 0.338</td> <td>-3363.118</td> <td> 9798.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>          <td>-1.427e+04</td> <td> 3389.850</td> <td>   -4.208</td> <td> 0.000</td> <td>-2.09e+04</td> <td>-7621.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>          <td>-1.228e+04</td> <td> 2504.240</td> <td>   -4.904</td> <td> 0.000</td> <td>-1.72e+04</td> <td>-7372.243</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>          <td> 5916.8535</td> <td> 1999.738</td> <td>    2.959</td> <td> 0.003</td> <td> 1997.007</td> <td> 9836.700</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>          <td>-4233.5510</td> <td> 1889.819</td> <td>   -2.240</td> <td> 0.025</td> <td>-7937.936</td> <td> -529.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>         <td>-2.405e+04</td> <td> 5975.944</td> <td>   -4.024</td> <td> 0.000</td> <td>-3.58e+04</td> <td>-1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>          <td>  957.2408</td> <td> 1997.016</td> <td>    0.479</td> <td> 0.632</td> <td>-2957.270</td> <td> 4871.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>          <td>  517.2102</td> <td> 1922.049</td> <td>    0.269</td> <td> 0.788</td> <td>-3250.352</td> <td> 4284.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>          <td> 3431.2725</td> <td> 1932.551</td> <td>    1.776</td> <td> 0.076</td> <td> -356.876</td> <td> 7219.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>          <td>  947.6186</td> <td> 1932.201</td> <td>    0.490</td> <td> 0.624</td> <td>-2839.843</td> <td> 4735.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>          <td> 5523.6159</td> <td> 1984.032</td> <td>    2.784</td> <td> 0.005</td> <td> 1634.555</td> <td> 9412.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>          <td> 5765.0706</td> <td> 1990.728</td> <td>    2.896</td> <td> 0.004</td> <td> 1862.885</td> <td> 9667.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>          <td> 4834.3260</td> <td> 1954.235</td> <td>    2.474</td> <td> 0.013</td> <td> 1003.672</td> <td> 8664.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>          <td> 2486.7400</td> <td> 2370.999</td> <td>    1.049</td> <td> 0.294</td> <td>-2160.846</td> <td> 7134.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>          <td> 4550.4275</td> <td> 1936.905</td> <td>    2.349</td> <td> 0.019</td> <td>  753.745</td> <td> 8347.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>          <td> 8343.8116</td> <td> 1974.738</td> <td>    4.225</td> <td> 0.000</td> <td> 4472.969</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>           <td>-5580.3198</td> <td> 2088.450</td> <td>   -2.672</td> <td> 0.008</td> <td>-9674.059</td> <td>-1486.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>          <td>-1.131e+04</td> <td> 2113.276</td> <td>   -5.353</td> <td> 0.000</td> <td>-1.55e+04</td> <td>-7169.760</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>          <td> 4993.7166</td> <td> 1966.121</td> <td>    2.540</td> <td> 0.011</td> <td> 1139.765</td> <td> 8847.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>          <td>-1.354e+04</td> <td> 2600.128</td> <td>   -5.209</td> <td> 0.000</td> <td>-1.86e+04</td> <td>-8447.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>          <td> 5831.6292</td> <td> 2083.335</td> <td>    2.799</td> <td> 0.005</td> <td> 1747.917</td> <td> 9915.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>          <td> 5737.1473</td> <td> 2145.189</td> <td>    2.674</td> <td> 0.007</td> <td> 1532.190</td> <td> 9942.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>          <td>-1.091e+04</td> <td> 3498.580</td> <td>   -3.118</td> <td> 0.002</td> <td>-1.78e+04</td> <td>-4051.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>          <td>-2.458e+04</td> <td> 3700.651</td> <td>   -6.642</td> <td> 0.000</td> <td>-3.18e+04</td> <td>-1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>          <td> 4070.8052</td> <td> 2226.642</td> <td>    1.828</td> <td> 0.068</td> <td> -293.815</td> <td> 8435.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>          <td> 5046.0590</td> <td> 5772.818</td> <td>    0.874</td> <td> 0.382</td> <td>-6269.706</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>          <td> 5953.8702</td> <td> 2368.831</td> <td>    2.513</td> <td> 0.012</td> <td> 1310.535</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>          <td> 4550.5342</td> <td> 2774.512</td> <td>    1.640</td> <td> 0.101</td> <td> -888.010</td> <td> 9989.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>           <td> 3185.2717</td> <td> 1883.937</td> <td>    1.691</td> <td> 0.091</td> <td> -507.585</td> <td> 6878.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>          <td>-2.301e+04</td> <td> 3462.213</td> <td>   -6.645</td> <td> 0.000</td> <td>-2.98e+04</td> <td>-1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>          <td> 5.769e+04</td> <td> 2498.058</td> <td>   23.094</td> <td> 0.000</td> <td> 5.28e+04</td> <td> 6.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>          <td> 1660.9963</td> <td> 3321.432</td> <td>    0.500</td> <td> 0.617</td> <td>-4849.610</td> <td> 8171.602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>          <td>-1.083e+04</td> <td> 2576.714</td> <td>   -4.203</td> <td> 0.000</td> <td>-1.59e+04</td> <td>-5778.209</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>          <td>-9691.8845</td> <td> 2653.284</td> <td>   -3.653</td> <td> 0.000</td> <td>-1.49e+04</td> <td>-4490.970</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>          <td>-5456.4670</td> <td> 2285.410</td> <td>   -2.388</td> <td> 0.017</td> <td>-9936.282</td> <td> -976.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>          <td> 2818.3197</td> <td> 2200.443</td> <td>    1.281</td> <td> 0.200</td> <td>-1494.946</td> <td> 7131.585</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>          <td> 6567.9309</td> <td> 2276.230</td> <td>    2.885</td> <td> 0.004</td> <td> 2106.110</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>           <td>  536.9656</td> <td> 1867.287</td> <td>    0.288</td> <td> 0.774</td> <td>-3123.254</td> <td> 4197.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>          <td> 2560.0940</td> <td> 2595.506</td> <td>    0.986</td> <td> 0.324</td> <td>-2527.565</td> <td> 7647.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>          <td>  674.1276</td> <td> 4691.398</td> <td>    0.144</td> <td> 0.886</td> <td>-8521.858</td> <td> 9870.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>           <td> 1018.4517</td> <td> 2012.040</td> <td>    0.506</td> <td> 0.613</td> <td>-2925.510</td> <td> 4962.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>          <td> 4523.6422</td> <td> 2516.921</td> <td>    1.797</td> <td> 0.072</td> <td> -409.977</td> <td> 9457.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>          <td> 3242.1694</td> <td> 2287.571</td> <td>    1.417</td> <td> 0.156</td> <td>-1241.883</td> <td> 7726.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>          <td> 2985.4678</td> <td> 2491.175</td> <td>    1.198</td> <td> 0.231</td> <td>-1897.685</td> <td> 7868.621</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>          <td>  756.2421</td> <td> 2175.712</td> <td>    0.348</td> <td> 0.728</td> <td>-3508.545</td> <td> 5021.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>          <td> -444.7947</td> <td> 4695.812</td> <td>   -0.095</td> <td> 0.925</td> <td>-9649.432</td> <td> 8759.843</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>          <td> 4983.3418</td> <td> 2258.664</td> <td>    2.206</td> <td> 0.027</td> <td>  555.953</td> <td> 9410.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>          <td> -2.38e+04</td> <td> 3491.395</td> <td>   -6.816</td> <td> 0.000</td> <td>-3.06e+04</td> <td> -1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>           <td> 4713.1060</td> <td> 2015.046</td> <td>    2.339</td> <td> 0.019</td> <td>  763.253</td> <td> 8662.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>          <td>-2.537e+04</td> <td> 3749.737</td> <td>   -6.767</td> <td> 0.000</td> <td>-3.27e+04</td> <td> -1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>           <td> 3009.1830</td> <td> 1901.024</td> <td>    1.583</td> <td> 0.113</td> <td> -717.166</td> <td> 6735.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>           <td> 4155.4428</td> <td> 1934.994</td> <td>    2.148</td> <td> 0.032</td> <td>  362.505</td> <td> 7948.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>          <td> 5565.0936</td> <td> 2359.113</td> <td>    2.359</td> <td> 0.018</td> <td>  940.807</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>          <td> -562.5675</td> <td> 2346.922</td> <td>   -0.240</td> <td> 0.811</td> <td>-5162.958</td> <td> 4037.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>           <td>-1.224e+04</td> <td> 2470.053</td> <td>   -4.956</td> <td> 0.000</td> <td>-1.71e+04</td> <td>-7399.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>          <td>-2721.2310</td> <td> 4692.654</td> <td>   -0.580</td> <td> 0.562</td> <td>-1.19e+04</td> <td> 6477.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>           <td>-1.811e+04</td> <td> 2965.443</td> <td>   -6.107</td> <td> 0.000</td> <td>-2.39e+04</td> <td>-1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>          <td> 1502.3731</td> <td> 3123.311</td> <td>    0.481</td> <td> 0.631</td> <td>-4619.879</td> <td> 7624.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>          <td>-3.106e+04</td> <td> 4404.453</td> <td>   -7.052</td> <td> 0.000</td> <td>-3.97e+04</td> <td>-2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>          <td> 4498.0546</td> <td> 2323.950</td> <td>    1.936</td> <td> 0.053</td> <td>  -57.307</td> <td> 9053.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>          <td> 5145.0388</td> <td> 2332.358</td> <td>    2.206</td> <td> 0.027</td> <td>  573.197</td> <td> 9716.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>          <td>-5843.5719</td> <td> 2354.762</td> <td>   -2.482</td> <td> 0.013</td> <td>-1.05e+04</td> <td>-1227.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>          <td> 4967.9392</td> <td> 2428.751</td> <td>    2.045</td> <td> 0.041</td> <td>  207.149</td> <td> 9728.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>          <td>-1.318e+04</td> <td> 2754.299</td> <td>   -4.784</td> <td> 0.000</td> <td>-1.86e+04</td> <td>-7778.331</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>          <td> 5611.4352</td> <td> 2447.232</td> <td>    2.293</td> <td> 0.022</td> <td>  814.420</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>           <td>-2.247e+04</td> <td> 3632.149</td> <td>   -6.185</td> <td> 0.000</td> <td>-2.96e+04</td> <td>-1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>          <td> 1210.0618</td> <td> 2353.506</td> <td>    0.514</td> <td> 0.607</td> <td>-3403.234</td> <td> 5823.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>           <td>-1.121e+04</td> <td> 2350.270</td> <td>   -4.769</td> <td> 0.000</td> <td>-1.58e+04</td> <td>-6602.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>           <td>-7498.9924</td> <td> 1945.758</td> <td>   -3.854</td> <td> 0.000</td> <td>-1.13e+04</td> <td>-3684.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>          <td> 4515.7166</td> <td> 2631.671</td> <td>    1.716</td> <td> 0.086</td> <td> -642.833</td> <td> 9674.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>          <td> 5289.0955</td> <td> 3397.307</td> <td>    1.557</td> <td> 0.120</td> <td>-1370.239</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>          <td>-1.168e+04</td> <td> 2772.846</td> <td>   -4.213</td> <td> 0.000</td> <td>-1.71e+04</td> <td>-6246.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>           <td> 3250.9457</td> <td> 2199.079</td> <td>    1.478</td> <td> 0.139</td> <td>-1059.646</td> <td> 7561.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>          <td>   50.2213</td> <td> 2256.777</td> <td>    0.022</td> <td> 0.982</td> <td>-4373.468</td> <td> 4473.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>          <td> 5613.9947</td> <td> 2360.812</td> <td>    2.378</td> <td> 0.017</td> <td>  986.377</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>          <td>-1.166e+04</td> <td> 2867.437</td> <td>   -4.067</td> <td> 0.000</td> <td>-1.73e+04</td> <td>-6039.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>          <td> 4648.7560</td> <td> 2418.044</td> <td>    1.923</td> <td> 0.055</td> <td>  -91.046</td> <td> 9388.558</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>           <td> 5310.8958</td> <td> 4144.973</td> <td>    1.281</td> <td> 0.200</td> <td>-2813.998</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>          <td> 4720.7038</td> <td> 2542.214</td> <td>    1.857</td> <td> 0.063</td> <td> -262.495</td> <td> 9703.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>          <td> 5168.6326</td> <td> 8141.182</td> <td>    0.635</td> <td> 0.526</td> <td>-1.08e+04</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>           <td> 1920.7060</td> <td> 2586.558</td> <td>    0.743</td> <td> 0.458</td> <td>-3149.413</td> <td> 6990.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>           <td> 6453.7636</td> <td> 2553.611</td> <td>    2.527</td> <td> 0.012</td> <td> 1448.225</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>          <td> 2124.2078</td> <td> 4695.488</td> <td>    0.452</td> <td> 0.651</td> <td>-7079.795</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>          <td> 3534.6047</td> <td> 2492.565</td> <td>    1.418</td> <td> 0.156</td> <td>-1351.272</td> <td> 8420.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>           <td> 5981.1779</td> <td> 2001.187</td> <td>    2.989</td> <td> 0.003</td> <td> 2058.491</td> <td> 9903.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>          <td> 3324.8172</td> <td> 2484.970</td> <td>    1.338</td> <td> 0.181</td> <td>-1546.172</td> <td> 8195.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>          <td>-1237.4126</td> <td> 2579.186</td> <td>   -0.480</td> <td> 0.631</td> <td>-6293.083</td> <td> 3818.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>           <td> 3916.4533</td> <td> 1923.050</td> <td>    2.037</td> <td> 0.042</td> <td>  146.928</td> <td> 7685.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>          <td> 3000.6691</td> <td> 2604.382</td> <td>    1.152</td> <td> 0.249</td> <td>-2104.389</td> <td> 8105.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>           <td> 5694.0640</td> <td> 1986.883</td> <td>    2.866</td> <td> 0.004</td> <td> 1799.414</td> <td> 9588.714</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>          <td>-3.218e+04</td> <td> 4715.431</td> <td>   -6.824</td> <td> 0.000</td> <td>-4.14e+04</td> <td>-2.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>          <td> 3230.4275</td> <td> 2509.289</td> <td>    1.287</td> <td> 0.198</td> <td>-1688.231</td> <td> 8149.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>          <td> 5213.4306</td> <td> 2949.338</td> <td>    1.768</td> <td> 0.077</td> <td> -567.803</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>           <td>-4841.1692</td> <td> 3034.031</td> <td>   -1.596</td> <td> 0.111</td> <td>-1.08e+04</td> <td> 1106.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>           <td> 4321.2884</td> <td> 1943.873</td> <td>    2.223</td> <td> 0.026</td> <td>  510.946</td> <td> 8131.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>           <td> 1.022e+04</td> <td> 2061.778</td> <td>    4.959</td> <td> 0.000</td> <td> 6183.141</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>           <td> 4387.9538</td> <td> 1942.787</td> <td>    2.259</td> <td> 0.024</td> <td>  579.741</td> <td> 8196.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>          <td>-2185.7033</td> <td> 2725.376</td> <td>   -0.802</td> <td> 0.423</td> <td>-7527.931</td> <td> 3156.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>           <td>-7123.0045</td> <td> 2129.102</td> <td>   -3.346</td> <td> 0.001</td> <td>-1.13e+04</td> <td>-2949.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>           <td> 1749.7199</td> <td> 1877.093</td> <td>    0.932</td> <td> 0.351</td> <td>-1929.720</td> <td> 5429.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>           <td>-2.416e+04</td> <td> 3591.521</td> <td>   -6.726</td> <td> 0.000</td> <td>-3.12e+04</td> <td>-1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>          <td>-7854.3757</td> <td> 2707.195</td> <td>   -2.901</td> <td> 0.004</td> <td>-1.32e+04</td> <td>-2547.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>          <td>-5618.1399</td> <td> 2978.211</td> <td>   -1.886</td> <td> 0.059</td> <td>-1.15e+04</td> <td>  219.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>           <td>-2359.9296</td> <td> 1906.348</td> <td>   -1.238</td> <td> 0.216</td> <td>-6096.715</td> <td> 1376.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>           <td> 5918.5046</td> <td> 1996.516</td> <td>    2.964</td> <td> 0.003</td> <td> 2004.973</td> <td> 9832.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>           <td> 7869.2121</td> <td> 2017.388</td> <td>    3.901</td> <td> 0.000</td> <td> 3914.768</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>          <td>-2160.6819</td> <td> 2715.903</td> <td>   -0.796</td> <td> 0.426</td> <td>-7484.341</td> <td> 3162.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>          <td>-4426.9439</td> <td> 2610.724</td> <td>   -1.696</td> <td> 0.090</td> <td>-9544.433</td> <td>  690.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>           <td>-2.176e+04</td> <td> 2799.319</td> <td>   -7.773</td> <td> 0.000</td> <td>-2.72e+04</td> <td>-1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>           <td> 3466.1648</td> <td> 1961.927</td> <td>    1.767</td> <td> 0.077</td> <td> -379.566</td> <td> 7311.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>          <td> 2006.6420</td> <td> 2588.229</td> <td>    0.775</td> <td> 0.438</td> <td>-3066.754</td> <td> 7080.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>           <td> 3116.2628</td> <td> 1993.580</td> <td>    1.563</td> <td> 0.118</td> <td> -791.513</td> <td> 7024.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>           <td> 5633.9310</td> <td> 1996.403</td> <td>    2.822</td> <td> 0.005</td> <td> 1720.621</td> <td> 9547.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>           <td> 4381.9496</td> <td> 3153.520</td> <td>    1.390</td> <td> 0.165</td> <td>-1799.518</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>           <td> 4698.8460</td> <td> 2045.572</td> <td>    2.297</td> <td> 0.022</td> <td>  689.156</td> <td> 8708.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>           <td> 2996.5292</td> <td> 1967.402</td> <td>    1.523</td> <td> 0.128</td> <td> -859.933</td> <td> 6852.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>           <td> 3990.7601</td> <td> 1983.464</td> <td>    2.012</td> <td> 0.044</td> <td>  102.813</td> <td> 7878.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>           <td> -1.51e+04</td> <td> 2600.786</td> <td>   -5.807</td> <td> 0.000</td> <td>-2.02e+04</td> <td>   -1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>          <td>  684.5639</td> <td> 2575.351</td> <td>    0.266</td> <td> 0.790</td> <td>-4363.588</td> <td> 5732.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>           <td>-1537.7552</td> <td> 1881.683</td> <td>   -0.817</td> <td> 0.414</td> <td>-5226.194</td> <td> 2150.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>           <td>-1.003e+04</td> <td> 2495.059</td> <td>   -4.022</td> <td> 0.000</td> <td>-1.49e+04</td> <td>-5143.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>          <td> 5071.6705</td> <td> 3705.654</td> <td>    1.369</td> <td> 0.171</td> <td>-2192.080</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>           <td>-3179.7872</td> <td> 4102.927</td> <td>   -0.775</td> <td> 0.438</td> <td>-1.12e+04</td> <td> 4862.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>           <td> 3490.0985</td> <td> 2017.841</td> <td>    1.730</td> <td> 0.084</td> <td> -465.233</td> <td> 7445.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>           <td> 2787.5754</td> <td> 1896.846</td> <td>    1.470</td> <td> 0.142</td> <td> -930.584</td> <td> 6505.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>           <td> 4405.2188</td> <td> 2080.640</td> <td>    2.117</td> <td> 0.034</td> <td>  326.790</td> <td> 8483.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>           <td>  630.2583</td> <td> 1868.894</td> <td>    0.337</td> <td> 0.736</td> <td>-3033.110</td> <td> 4293.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>           <td> 4876.8374</td> <td> 1956.488</td> <td>    2.493</td> <td> 0.013</td> <td> 1041.769</td> <td> 8711.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>           <td> 5531.6252</td> <td> 1926.111</td> <td>    2.872</td> <td> 0.004</td> <td> 1756.100</td> <td> 9307.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>           <td> 4132.7220</td> <td> 1930.521</td> <td>    2.141</td> <td> 0.032</td> <td>  348.553</td> <td> 7916.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>           <td>-2144.8429</td> <td> 3071.256</td> <td>   -0.698</td> <td> 0.485</td> <td>-8165.059</td> <td> 3875.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>           <td> 5729.4776</td> <td> 1989.753</td> <td>    2.879</td> <td> 0.004</td> <td> 1829.202</td> <td> 9629.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>           <td> 4293.8972</td> <td> 1920.835</td> <td>    2.235</td> <td> 0.025</td> <td>  528.714</td> <td> 8059.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>           <td> 4877.5272</td> <td> 2537.714</td> <td>    1.922</td> <td> 0.055</td> <td>  -96.850</td> <td> 9851.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>           <td> 1056.4120</td> <td> 1866.200</td> <td>    0.566</td> <td> 0.571</td> <td>-2601.676</td> <td> 4714.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>           <td> 4119.6311</td> <td> 1935.321</td> <td>    2.129</td> <td> 0.033</td> <td>  326.052</td> <td> 7913.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>           <td> 5957.2341</td> <td> 2001.011</td> <td>    2.977</td> <td> 0.003</td> <td> 2034.892</td> <td> 9879.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>          <td> 2.845e+04</td> <td> 2755.046</td> <td>   10.327</td> <td> 0.000</td> <td> 2.31e+04</td> <td> 3.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>           <td>-2.111e+04</td> <td> 3328.065</td> <td>   -6.342</td> <td> 0.000</td> <td>-2.76e+04</td> <td>-1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>           <td> 3094.3038</td> <td> 1884.339</td> <td>    1.642</td> <td> 0.101</td> <td> -599.341</td> <td> 6787.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>           <td> 2882.4529</td> <td> 1869.712</td> <td>    1.542</td> <td> 0.123</td> <td> -782.519</td> <td> 6547.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>          <td>-7461.7244</td> <td> 2690.133</td> <td>   -2.774</td> <td> 0.006</td> <td>-1.27e+04</td> <td>-2188.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>          <td> 2419.0385</td> <td> 2586.317</td> <td>    0.935</td> <td> 0.350</td> <td>-2650.609</td> <td> 7488.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>           <td> 5101.5536</td> <td> 2065.963</td> <td>    2.469</td> <td> 0.014</td> <td> 1051.894</td> <td> 9151.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>           <td> 4601.4461</td> <td> 3060.832</td> <td>    1.503</td> <td> 0.133</td> <td>-1398.337</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>          <td>-3.386e+04</td> <td> 4726.783</td> <td>   -7.164</td> <td> 0.000</td> <td>-4.31e+04</td> <td>-2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>           <td> 4044.6641</td> <td> 1933.674</td> <td>    2.092</td> <td> 0.036</td> <td>  254.314</td> <td> 7835.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>           <td> 3.574e+04</td> <td> 2599.419</td> <td>   13.749</td> <td> 0.000</td> <td> 3.06e+04</td> <td> 4.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>           <td> 5672.6793</td> <td> 2033.203</td> <td>    2.790</td> <td> 0.005</td> <td> 1687.234</td> <td> 9658.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>           <td>  148.4369</td> <td> 1959.794</td> <td>    0.076</td> <td> 0.940</td> <td>-3693.113</td> <td> 3989.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>           <td> 5015.6190</td> <td> 1965.133</td> <td>    2.552</td> <td> 0.011</td> <td> 1163.605</td> <td> 8867.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>          <td> 5646.3189</td> <td> 2667.620</td> <td>    2.117</td> <td> 0.034</td> <td>  417.303</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>           <td>  814.4474</td> <td> 5745.805</td> <td>    0.142</td> <td> 0.887</td> <td>-1.04e+04</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>           <td> 1301.6681</td> <td> 2174.922</td> <td>    0.598</td> <td> 0.550</td> <td>-2961.572</td> <td> 5564.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>          <td>-1.485e+04</td> <td> 3411.237</td> <td>   -4.355</td> <td> 0.000</td> <td>-2.15e+04</td> <td>-8167.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>           <td> 4579.6258</td> <td> 1972.396</td> <td>    2.322</td> <td> 0.020</td> <td>  713.374</td> <td> 8445.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>          <td> 5145.3920</td> <td> 2652.925</td> <td>    1.940</td> <td> 0.052</td> <td>  -54.819</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>           <td>-1.155e+04</td> <td> 2384.995</td> <td>   -4.844</td> <td> 0.000</td> <td>-1.62e+04</td> <td>-6878.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>          <td>-3121.7529</td> <td> 2602.541</td> <td>   -1.200</td> <td> 0.230</td> <td>-8223.202</td> <td> 1979.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>           <td>-1.429e+04</td> <td> 2085.478</td> <td>   -6.854</td> <td> 0.000</td> <td>-1.84e+04</td> <td>-1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>           <td>  468.8118</td> <td> 1942.062</td> <td>    0.241</td> <td> 0.809</td> <td>-3337.979</td> <td> 4275.603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>           <td> 4937.7627</td> <td> 3362.044</td> <td>    1.469</td> <td> 0.142</td> <td>-1652.450</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>           <td>-1179.7652</td> <td> 2105.972</td> <td>   -0.560</td> <td> 0.575</td> <td>-5307.850</td> <td> 2948.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>          <td> 3943.3174</td> <td> 2769.146</td> <td>    1.424</td> <td> 0.154</td> <td>-1484.709</td> <td> 9371.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>          <td>-2.318e+04</td> <td> 3768.793</td> <td>   -6.151</td> <td> 0.000</td> <td>-3.06e+04</td> <td>-1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>           <td>-1.672e+04</td> <td> 4434.085</td> <td>   -3.771</td> <td> 0.000</td> <td>-2.54e+04</td> <td>-8030.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>           <td> 5331.8550</td> <td> 2194.582</td> <td>    2.430</td> <td> 0.015</td> <td> 1030.078</td> <td> 9633.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>          <td> -1.45e+04</td> <td> 3833.444</td> <td>   -3.784</td> <td> 0.000</td> <td> -2.2e+04</td> <td>-6990.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>           <td> 5837.7262</td> <td> 1994.917</td> <td>    2.926</td> <td> 0.003</td> <td> 1927.330</td> <td> 9748.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>           <td> 3446.0993</td> <td> 1910.184</td> <td>    1.804</td> <td> 0.071</td> <td> -298.206</td> <td> 7190.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>           <td> 2761.3152</td> <td> 3389.110</td> <td>    0.815</td> <td> 0.415</td> <td>-3881.951</td> <td> 9404.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>           <td> 7046.0132</td> <td> 1992.542</td> <td>    3.536</td> <td> 0.000</td> <td> 3140.271</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>           <td> 4135.4846</td> <td> 1943.015</td> <td>    2.128</td> <td> 0.033</td> <td>  326.825</td> <td> 7944.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>           <td> 2405.0247</td> <td> 1886.047</td> <td>    1.275</td> <td> 0.202</td> <td>-1291.967</td> <td> 6102.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>           <td> 4508.6059</td> <td> 3694.083</td> <td>    1.220</td> <td> 0.222</td> <td>-2732.462</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>          <td> 5465.7697</td> <td> 2965.238</td> <td>    1.843</td> <td> 0.065</td> <td> -346.632</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>          <td>-2.171e+04</td> <td> 4068.270</td> <td>   -5.338</td> <td> 0.000</td> <td>-2.97e+04</td> <td>-1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>           <td> 5953.1001</td> <td> 2000.598</td> <td>    2.976</td> <td> 0.003</td> <td> 2031.568</td> <td> 9874.633</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>          <td> 4786.0951</td> <td> 3134.337</td> <td>    1.527</td> <td> 0.127</td> <td>-1357.770</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>           <td>-8638.2868</td> <td> 2263.003</td> <td>   -3.817</td> <td> 0.000</td> <td>-1.31e+04</td> <td>-4202.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>           <td>-2210.6970</td> <td> 1925.030</td> <td>   -1.148</td> <td> 0.251</td> <td>-5984.103</td> <td> 1562.709</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>           <td> 5573.8713</td> <td> 2207.837</td> <td>    2.525</td> <td> 0.012</td> <td> 1246.113</td> <td> 9901.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>          <td> 5829.1780</td> <td> 2947.556</td> <td>    1.978</td> <td> 0.048</td> <td>   51.438</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>           <td> 5596.9129</td> <td> 1996.457</td> <td>    2.803</td> <td> 0.005</td> <td> 1683.498</td> <td> 9510.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>          <td> 6375.0727</td> <td> 8147.603</td> <td>    0.782</td> <td> 0.434</td> <td>-9595.697</td> <td> 2.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>          <td> 2037.8755</td> <td> 2884.776</td> <td>    0.706</td> <td> 0.480</td> <td>-3616.805</td> <td> 7692.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>           <td>-1.968e+04</td> <td> 3137.493</td> <td>   -6.274</td> <td> 0.000</td> <td>-2.58e+04</td> <td>-1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>           <td> 4746.4598</td> <td> 2942.806</td> <td>    1.613</td> <td> 0.107</td> <td>-1021.970</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>          <td> 4692.3411</td> <td> 4720.530</td> <td>    0.994</td> <td> 0.320</td> <td>-4560.748</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>           <td>-5269.2420</td> <td> 2189.996</td> <td>   -2.406</td> <td> 0.016</td> <td>-9562.029</td> <td> -976.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>           <td> 3522.0549</td> <td> 1913.312</td> <td>    1.841</td> <td> 0.066</td> <td> -228.381</td> <td> 7272.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>           <td>-1609.3684</td> <td> 2578.575</td> <td>   -0.624</td> <td> 0.533</td> <td>-6663.840</td> <td> 3445.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>          <td> 4900.2402</td> <td> 3135.869</td> <td>    1.563</td> <td> 0.118</td> <td>-1246.628</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>           <td> 3835.8963</td> <td> 1915.962</td> <td>    2.002</td> <td> 0.045</td> <td>   80.266</td> <td> 7591.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>           <td> 4531.3516</td> <td> 1964.152</td> <td>    2.307</td> <td> 0.021</td> <td>  681.259</td> <td> 8381.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>           <td>-3239.4189</td> <td> 1914.573</td> <td>   -1.692</td> <td> 0.091</td> <td>-6992.327</td> <td>  513.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>           <td> 9161.1645</td> <td> 1997.545</td> <td>    4.586</td> <td> 0.000</td> <td> 5245.617</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>           <td>-2662.9682</td> <td> 1897.529</td> <td>   -1.403</td> <td> 0.161</td> <td>-6382.467</td> <td> 1056.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>           <td> 4574.8892</td> <td> 1953.949</td> <td>    2.341</td> <td> 0.019</td> <td>  744.797</td> <td> 8404.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>           <td>-1.562e+04</td> <td> 2718.260</td> <td>   -5.746</td> <td> 0.000</td> <td>-2.09e+04</td> <td>-1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>           <td> 5276.4913</td> <td> 1972.838</td> <td>    2.675</td> <td> 0.007</td> <td> 1409.374</td> <td> 9143.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>           <td>  129.1254</td> <td> 1954.573</td> <td>    0.066</td> <td> 0.947</td> <td>-3702.190</td> <td> 3960.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>          <td>-2.352e+04</td> <td> 4034.940</td> <td>   -5.828</td> <td> 0.000</td> <td>-3.14e+04</td> <td>-1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>          <td> 2830.2390</td> <td> 1909.723</td> <td>    1.482</td> <td> 0.138</td> <td> -913.163</td> <td> 6573.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>          <td>-2.231e+04</td> <td> 3907.568</td> <td>   -5.710</td> <td> 0.000</td> <td>   -3e+04</td> <td>-1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>           <td> 5250.5375</td> <td> 2194.333</td> <td>    2.393</td> <td> 0.017</td> <td>  949.249</td> <td> 9551.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>           <td> 5865.7959</td> <td> 2791.469</td> <td>    2.101</td> <td> 0.036</td> <td>  394.014</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>           <td> -488.6654</td> <td> 1970.596</td> <td>   -0.248</td> <td> 0.804</td> <td>-4351.390</td> <td> 3374.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>          <td>-9938.5533</td> <td> 3427.430</td> <td>   -2.900</td> <td> 0.004</td> <td>-1.67e+04</td> <td>-3220.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>           <td>-3.926e+04</td> <td> 5294.553</td> <td>   -7.415</td> <td> 0.000</td> <td>-4.96e+04</td> <td>-2.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>           <td> 5535.6905</td> <td> 2445.824</td> <td>    2.263</td> <td> 0.024</td> <td>  741.434</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>           <td> 4124.3201</td> <td> 1926.336</td> <td>    2.141</td> <td> 0.032</td> <td>  348.353</td> <td> 7900.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>           <td> 3711.7939</td> <td> 2919.533</td> <td>    1.271</td> <td> 0.204</td> <td>-2011.017</td> <td> 9434.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>           <td>-5691.6089</td> <td> 2000.969</td> <td>   -2.844</td> <td> 0.004</td> <td>-9613.868</td> <td>-1769.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>           <td> 2898.8181</td> <td> 1904.850</td> <td>    1.522</td> <td> 0.128</td> <td> -835.031</td> <td> 6632.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>           <td>-1.469e+04</td> <td> 2700.464</td> <td>   -5.440</td> <td> 0.000</td> <td>   -2e+04</td> <td>-9396.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>           <td> -1.54e+04</td> <td> 2892.609</td> <td>   -5.325</td> <td> 0.000</td> <td>-2.11e+04</td> <td>-9732.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>           <td> 5202.8797</td> <td> 1946.551</td> <td>    2.673</td> <td> 0.008</td> <td> 1387.288</td> <td> 9018.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>           <td> 2363.1203</td> <td> 1933.109</td> <td>    1.222</td> <td> 0.222</td> <td>-1426.122</td> <td> 6152.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>           <td>-2.915e+04</td> <td> 4370.172</td> <td>   -6.670</td> <td> 0.000</td> <td>-3.77e+04</td> <td>-2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>           <td> 7463.7255</td> <td> 2096.541</td> <td>    3.560</td> <td> 0.000</td> <td> 3354.127</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>           <td>-8806.6966</td> <td> 2541.664</td> <td>   -3.465</td> <td> 0.001</td> <td>-1.38e+04</td> <td>-3824.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>           <td> 5301.6018</td> <td> 3708.550</td> <td>    1.430</td> <td> 0.153</td> <td>-1967.826</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>           <td> 6374.5636</td> <td> 1949.454</td> <td>    3.270</td> <td> 0.001</td> <td> 2553.282</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>           <td> 5718.2200</td> <td> 1997.710</td> <td>    2.862</td> <td> 0.004</td> <td> 1802.348</td> <td> 9634.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>           <td> 5.252e+04</td> <td> 2001.423</td> <td>   26.242</td> <td> 0.000</td> <td> 4.86e+04</td> <td> 5.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>           <td> 2456.8442</td> <td> 2357.562</td> <td>    1.042</td> <td> 0.297</td> <td>-2164.401</td> <td> 7078.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>           <td> 3843.2398</td> <td> 1921.376</td> <td>    2.000</td> <td> 0.045</td> <td>   76.997</td> <td> 7609.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>           <td> 3315.0700</td> <td> 1871.969</td> <td>    1.771</td> <td> 0.077</td> <td> -354.326</td> <td> 6984.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>           <td>-9202.2841</td> <td> 2296.026</td> <td>   -4.008</td> <td> 0.000</td> <td>-1.37e+04</td> <td>-4701.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>           <td> 1499.4128</td> <td> 2197.761</td> <td>    0.682</td> <td> 0.495</td> <td>-2808.596</td> <td> 5807.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>           <td>-1595.4902</td> <td> 1981.822</td> <td>   -0.805</td> <td> 0.421</td> <td>-5480.219</td> <td> 2289.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>           <td> 5792.5791</td> <td> 2148.702</td> <td>    2.696</td> <td> 0.007</td> <td> 1580.736</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>           <td> 3978.7541</td> <td> 1960.246</td> <td>    2.030</td> <td> 0.042</td> <td>  136.318</td> <td> 7821.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>           <td> 5437.7381</td> <td> 1981.647</td> <td>    2.744</td> <td> 0.006</td> <td> 1553.353</td> <td> 9322.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>           <td>-2.539e+04</td> <td> 2993.946</td> <td>   -8.480</td> <td> 0.000</td> <td>-3.13e+04</td> <td>-1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>           <td>-4424.8740</td> <td> 2290.165</td> <td>   -1.932</td> <td> 0.053</td> <td>-8914.011</td> <td>   64.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>           <td> 4318.9152</td> <td> 2496.575</td> <td>    1.730</td> <td> 0.084</td> <td> -574.822</td> <td> 9212.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>           <td> 4207.4331</td> <td> 1922.122</td> <td>    2.189</td> <td> 0.029</td> <td>  439.727</td> <td> 7975.139</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>           <td>-1240.7329</td> <td> 1938.259</td> <td>   -0.640</td> <td> 0.522</td> <td>-5040.069</td> <td> 2558.603</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>           <td>-4236.9220</td> <td> 1947.227</td> <td>   -2.176</td> <td> 0.030</td> <td>-8053.837</td> <td> -420.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>           <td> 3789.2411</td> <td> 2756.174</td> <td>    1.375</td> <td> 0.169</td> <td>-1613.357</td> <td> 9191.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>           <td> 4392.3223</td> <td> 1969.602</td> <td>    2.230</td> <td> 0.026</td> <td>  531.547</td> <td> 8253.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>           <td> 5565.8740</td> <td> 1875.178</td> <td>    2.968</td> <td> 0.003</td> <td> 1890.187</td> <td> 9241.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>           <td> 6012.9124</td> <td> 4125.935</td> <td>    1.457</td> <td> 0.145</td> <td>-2074.664</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>           <td> -106.6962</td> <td> 1865.740</td> <td>   -0.057</td> <td> 0.954</td> <td>-3763.884</td> <td> 3550.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>           <td> 5404.8441</td> <td> 1968.058</td> <td>    2.746</td> <td> 0.006</td> <td> 1547.095</td> <td> 9262.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>           <td> 3432.0264</td> <td> 1958.903</td> <td>    1.752</td> <td> 0.080</td> <td> -407.777</td> <td> 7271.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>           <td> 5707.4282</td> <td> 2035.863</td> <td>    2.803</td> <td> 0.005</td> <td> 1716.769</td> <td> 9698.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>           <td>-1.188e+04</td> <td> 2402.966</td> <td>   -4.945</td> <td> 0.000</td> <td>-1.66e+04</td> <td>-7172.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>           <td>-1277.6393</td> <td> 1876.659</td> <td>   -0.681</td> <td> 0.496</td> <td>-4956.230</td> <td> 2400.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>           <td> 5191.3064</td> <td> 1963.537</td> <td>    2.644</td> <td> 0.008</td> <td> 1342.420</td> <td> 9040.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>           <td>-1.673e+04</td> <td> 2578.502</td> <td>   -6.487</td> <td> 0.000</td> <td>-2.18e+04</td> <td>-1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>           <td> 4888.1541</td> <td> 2405.370</td> <td>    2.032</td> <td> 0.042</td> <td>  173.194</td> <td> 9603.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>           <td>  -65.0558</td> <td> 1868.881</td> <td>   -0.035</td> <td> 0.972</td> <td>-3728.399</td> <td> 3598.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>           <td>-2.308e+04</td> <td> 3592.736</td> <td>   -6.423</td> <td> 0.000</td> <td>-3.01e+04</td> <td> -1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>           <td>-1.182e+04</td> <td> 2859.325</td> <td>   -4.135</td> <td> 0.000</td> <td>-1.74e+04</td> <td>-6219.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>           <td>-1.559e+04</td> <td> 2704.001</td> <td>   -5.767</td> <td> 0.000</td> <td>-2.09e+04</td> <td>-1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>           <td> 1603.7968</td> <td> 1876.118</td> <td>    0.855</td> <td> 0.393</td> <td>-2073.734</td> <td> 5281.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>           <td>-2.347e+04</td> <td> 3564.311</td> <td>   -6.585</td> <td> 0.000</td> <td>-3.05e+04</td> <td>-1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>           <td> 4772.8156</td> <td> 1988.997</td> <td>    2.400</td> <td> 0.016</td> <td>  874.022</td> <td> 8671.609</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>           <td> 1317.3590</td> <td> 1941.398</td> <td>    0.679</td> <td> 0.497</td> <td>-2488.132</td> <td> 5122.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>           <td> 2633.9853</td> <td> 2736.286</td> <td>    0.963</td> <td> 0.336</td> <td>-2729.628</td> <td> 7997.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>           <td>-1.104e+04</td> <td> 2332.661</td> <td>   -4.732</td> <td> 0.000</td> <td>-1.56e+04</td> <td>-6465.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>           <td> 5013.5276</td> <td> 1964.396</td> <td>    2.552</td> <td> 0.011</td> <td> 1162.958</td> <td> 8864.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>           <td>-5563.0210</td> <td> 3202.170</td> <td>   -1.737</td> <td> 0.082</td> <td>-1.18e+04</td> <td>  713.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>           <td>  171.9872</td> <td> 1867.380</td> <td>    0.092</td> <td> 0.927</td> <td>-3488.414</td> <td> 3832.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>           <td> 4625.0950</td> <td> 2046.362</td> <td>    2.260</td> <td> 0.024</td> <td>  613.856</td> <td> 8636.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>           <td> 5677.6978</td> <td> 1979.027</td> <td>    2.869</td> <td> 0.004</td> <td> 1798.448</td> <td> 9556.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>           <td> 4524.4845</td> <td> 2039.435</td> <td>    2.218</td> <td> 0.027</td> <td>  526.824</td> <td> 8522.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>           <td> 1.262e+04</td> <td> 1944.351</td> <td>    6.490</td> <td> 0.000</td> <td> 8806.770</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>           <td> 2604.7223</td> <td> 1919.380</td> <td>    1.357</td> <td> 0.175</td> <td>-1157.609</td> <td> 6367.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>           <td> 5998.2727</td> <td> 1991.432</td> <td>    3.012</td> <td> 0.003</td> <td> 2094.707</td> <td> 9901.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>           <td>-7036.6871</td> <td> 2084.929</td> <td>   -3.375</td> <td> 0.001</td> <td>-1.11e+04</td> <td>-2949.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>           <td> 3892.1315</td> <td> 1894.530</td> <td>    2.054</td> <td> 0.040</td> <td>  178.510</td> <td> 7605.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>           <td>-6065.5675</td> <td> 1995.358</td> <td>   -3.040</td> <td> 0.002</td> <td>-9976.830</td> <td>-2154.305</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>           <td> 7447.8593</td> <td> 1975.356</td> <td>    3.770</td> <td> 0.000</td> <td> 3575.805</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>           <td>  742.9194</td> <td> 3319.868</td> <td>    0.224</td> <td> 0.823</td> <td>-5764.621</td> <td> 7250.460</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>           <td>-1.351e+04</td> <td> 2493.437</td> <td>   -5.417</td> <td> 0.000</td> <td>-1.84e+04</td> <td>-8618.829</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>           <td> 1759.3905</td> <td> 2464.664</td> <td>    0.714</td> <td> 0.475</td> <td>-3071.795</td> <td> 6590.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>           <td>-4322.5466</td> <td> 1951.395</td> <td>   -2.215</td> <td> 0.027</td> <td>-8147.632</td> <td> -497.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>           <td> 5682.9745</td> <td> 2650.040</td> <td>    2.144</td> <td> 0.032</td> <td>  488.418</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>           <td> 3507.8926</td> <td> 2476.673</td> <td>    1.416</td> <td> 0.157</td> <td>-1346.834</td> <td> 8362.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>           <td> 5845.8717</td> <td> 1990.790</td> <td>    2.936</td> <td> 0.003</td> <td> 1943.565</td> <td> 9748.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>           <td> 5650.9262</td> <td> 2261.961</td> <td>    2.498</td> <td> 0.012</td> <td> 1217.075</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>           <td>-9816.5403</td> <td> 2308.153</td> <td>   -4.253</td> <td> 0.000</td> <td>-1.43e+04</td> <td>-5292.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>           <td> 5592.3913</td> <td> 1956.548</td> <td>    2.858</td> <td> 0.004</td> <td> 1757.203</td> <td> 9427.579</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>           <td>-1.659e+04</td> <td> 2824.967</td> <td>   -5.873</td> <td> 0.000</td> <td>-2.21e+04</td> <td>-1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>           <td> 4893.6615</td> <td> 1952.224</td> <td>    2.507</td> <td> 0.012</td> <td> 1066.951</td> <td> 8720.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>           <td> 5444.7655</td> <td> 1979.581</td> <td>    2.750</td> <td> 0.006</td> <td> 1564.429</td> <td> 9325.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>           <td> 1455.3156</td> <td> 2103.531</td> <td>    0.692</td> <td> 0.489</td> <td>-2667.984</td> <td> 5578.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>           <td> 4445.5496</td> <td> 2042.920</td> <td>    2.176</td> <td> 0.030</td> <td>  441.058</td> <td> 8450.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>           <td> 6899.4013</td> <td> 1935.835</td> <td>    3.564</td> <td> 0.000</td> <td> 3104.815</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>           <td> 2788.0453</td> <td> 3348.558</td> <td>    0.833</td> <td> 0.405</td> <td>-3775.732</td> <td> 9351.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>           <td>  322.0616</td> <td> 1919.799</td> <td>    0.168</td> <td> 0.867</td> <td>-3441.090</td> <td> 4085.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>           <td> 4044.4674</td> <td> 2006.643</td> <td>    2.016</td> <td> 0.044</td> <td>  111.085</td> <td> 7977.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>           <td> 3792.7307</td> <td> 2025.507</td> <td>    1.872</td> <td> 0.061</td> <td> -177.628</td> <td> 7763.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>           <td>  651.3093</td> <td> 1869.294</td> <td>    0.348</td> <td> 0.728</td> <td>-3012.844</td> <td> 4315.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>           <td>-1.046e+04</td> <td> 2474.144</td> <td>   -4.229</td> <td> 0.000</td> <td>-1.53e+04</td> <td>-5612.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>           <td> 1627.3410</td> <td> 1913.421</td> <td>    0.850</td> <td> 0.395</td> <td>-2123.309</td> <td> 5377.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>           <td> 5937.2607</td> <td> 1999.939</td> <td>    2.969</td> <td> 0.003</td> <td> 2017.020</td> <td> 9857.502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>           <td> 2011.0006</td> <td> 8126.310</td> <td>    0.247</td> <td> 0.805</td> <td>-1.39e+04</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>           <td> 5735.7827</td> <td> 2092.053</td> <td>    2.742</td> <td> 0.006</td> <td> 1634.982</td> <td> 9836.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>           <td> 5244.1572</td> <td> 1969.212</td> <td>    2.663</td> <td> 0.008</td> <td> 1384.147</td> <td> 9104.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>           <td> 5451.5411</td> <td> 1980.169</td> <td>    2.753</td> <td> 0.006</td> <td> 1570.052</td> <td> 9333.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>           <td>  825.6749</td> <td> 1869.696</td> <td>    0.442</td> <td> 0.659</td> <td>-2839.266</td> <td> 4490.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>           <td>-1.452e+04</td> <td> 2749.307</td> <td>   -5.283</td> <td> 0.000</td> <td>-1.99e+04</td> <td>-9134.802</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>           <td> 5303.1652</td> <td> 1973.784</td> <td>    2.687</td> <td> 0.007</td> <td> 1434.194</td> <td> 9172.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>           <td>-1.353e+04</td> <td> 2568.616</td> <td>   -5.266</td> <td> 0.000</td> <td>-1.86e+04</td> <td>-8491.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>           <td> 2677.0338</td> <td> 1897.125</td> <td>    1.411</td> <td> 0.158</td> <td>-1041.673</td> <td> 6395.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>           <td> 3391.8093</td> <td> 1957.474</td> <td>    1.733</td> <td> 0.083</td> <td> -445.193</td> <td> 7228.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>           <td> 4839.4631</td> <td> 1992.062</td> <td>    2.429</td> <td> 0.015</td> <td>  934.663</td> <td> 8744.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>           <td> 5621.5296</td> <td> 1986.692</td> <td>    2.830</td> <td> 0.005</td> <td> 1727.256</td> <td> 9515.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>           <td> 3659.2201</td> <td> 1983.621</td> <td>    1.845</td> <td> 0.065</td> <td> -229.034</td> <td> 7547.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>           <td> 5746.4121</td> <td> 1994.123</td> <td>    2.882</td> <td> 0.004</td> <td> 1837.572</td> <td> 9655.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>           <td> 5517.8740</td> <td> 2029.245</td> <td>    2.719</td> <td> 0.007</td> <td> 1540.188</td> <td> 9495.560</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>           <td>-1.031e+05</td> <td> 3500.927</td> <td>  -29.447</td> <td> 0.000</td> <td> -1.1e+05</td> <td>-9.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>           <td>-2.287e+04</td> <td> 3564.575</td> <td>   -6.415</td> <td> 0.000</td> <td>-2.99e+04</td> <td>-1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>           <td> 3706.2439</td> <td> 1917.009</td> <td>    1.933</td> <td> 0.053</td> <td>  -51.440</td> <td> 7463.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>           <td> 1000.4167</td> <td> 1870.814</td> <td>    0.535</td> <td> 0.593</td> <td>-2666.715</td> <td> 4667.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>           <td> 4140.5831</td> <td> 1941.422</td> <td>    2.133</td> <td> 0.033</td> <td>  335.046</td> <td> 7946.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>           <td> 4091.6995</td> <td> 1976.008</td> <td>    2.071</td> <td> 0.038</td> <td>  218.367</td> <td> 7965.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>           <td>-1.199e+04</td> <td> 3211.013</td> <td>   -3.733</td> <td> 0.000</td> <td>-1.83e+04</td> <td>-5692.774</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>           <td> 1.075e+04</td> <td> 1962.935</td> <td>    5.479</td> <td> 0.000</td> <td> 6906.404</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>           <td> 5886.6423</td> <td> 1995.984</td> <td>    2.949</td> <td> 0.003</td> <td> 1974.155</td> <td> 9799.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>           <td> 2417.8721</td> <td> 1918.350</td> <td>    1.260</td> <td> 0.208</td> <td>-1342.441</td> <td> 6178.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>           <td>-1.963e+04</td> <td> 3495.660</td> <td>   -5.615</td> <td> 0.000</td> <td>-2.65e+04</td> <td>-1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>           <td>-1337.9866</td> <td> 1877.638</td> <td>   -0.713</td> <td> 0.476</td> <td>-5018.496</td> <td> 2342.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>           <td> 5205.9697</td> <td> 1978.250</td> <td>    2.632</td> <td> 0.009</td> <td> 1328.244</td> <td> 9083.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>           <td> -567.0871</td> <td> 1868.788</td> <td>   -0.303</td> <td> 0.762</td> <td>-4230.248</td> <td> 3096.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>           <td>-7653.5292</td> <td> 1954.591</td> <td>   -3.916</td> <td> 0.000</td> <td>-1.15e+04</td> <td>-3822.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>           <td> 4.078e+04</td> <td> 2885.953</td> <td>   14.131</td> <td> 0.000</td> <td> 3.51e+04</td> <td> 4.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>           <td> 4941.7237</td> <td> 2322.907</td> <td>    2.127</td> <td> 0.033</td> <td>  388.407</td> <td> 9495.041</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>           <td> 6412.1070</td> <td> 2386.808</td> <td>    2.686</td> <td> 0.007</td> <td> 1733.533</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>           <td>-1.551e+05</td> <td> 5742.807</td> <td>  -27.011</td> <td> 0.000</td> <td>-1.66e+05</td> <td>-1.44e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>           <td> -957.8256</td> <td> 1866.085</td> <td>   -0.513</td> <td> 0.608</td> <td>-4615.689</td> <td> 2700.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>           <td> 5582.2600</td> <td> 1994.067</td> <td>    2.799</td> <td> 0.005</td> <td> 1673.530</td> <td> 9490.991</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>           <td>  -1.6e+04</td> <td> 2941.916</td> <td>   -5.438</td> <td> 0.000</td> <td>-2.18e+04</td> <td>-1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>           <td>-1072.8236</td> <td> 1874.063</td> <td>   -0.572</td> <td> 0.567</td> <td>-4746.326</td> <td> 2600.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>           <td>-1667.6530</td> <td> 2041.682</td> <td>   -0.817</td> <td> 0.414</td> <td>-5669.719</td> <td> 2334.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>           <td> 1099.1601</td> <td> 2718.240</td> <td>    0.404</td> <td> 0.686</td> <td>-4229.080</td> <td> 6427.400</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>           <td>-3068.5101</td> <td> 2287.462</td> <td>   -1.341</td> <td> 0.180</td> <td>-7552.348</td> <td> 1415.328</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>           <td> 1.418e+04</td> <td> 1905.089</td> <td>    7.441</td> <td> 0.000</td> <td> 1.04e+04</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>           <td> -338.4584</td> <td> 2257.668</td> <td>   -0.150</td> <td> 0.881</td> <td>-4763.894</td> <td> 4086.977</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>           <td> 4324.9441</td> <td> 1929.858</td> <td>    2.241</td> <td> 0.025</td> <td>  542.075</td> <td> 8107.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>           <td> 5908.2768</td> <td> 2044.014</td> <td>    2.891</td> <td> 0.004</td> <td> 1901.641</td> <td> 9914.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>           <td> 5103.3022</td> <td> 2413.305</td> <td>    2.115</td> <td> 0.034</td> <td>  372.790</td> <td> 9833.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>           <td>-2728.6144</td> <td> 1878.659</td> <td>   -1.452</td> <td> 0.146</td> <td>-6411.124</td> <td>  953.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>           <td>-2638.3547</td> <td> 2073.913</td> <td>   -1.272</td> <td> 0.203</td> <td>-6703.598</td> <td> 1426.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>           <td> 4042.4159</td> <td> 1926.390</td> <td>    2.098</td> <td> 0.036</td> <td>  266.343</td> <td> 7818.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>           <td> 4353.2552</td> <td> 1935.470</td> <td>    2.249</td> <td> 0.025</td> <td>  559.384</td> <td> 8147.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>           <td> 4947.5640</td> <td> 1959.680</td> <td>    2.525</td> <td> 0.012</td> <td> 1106.238</td> <td> 8788.890</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>           <td> 5223.3645</td> <td> 1926.246</td> <td>    2.712</td> <td> 0.007</td> <td> 1447.576</td> <td> 8999.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>           <td> 5419.2139</td> <td> 1972.267</td> <td>    2.748</td> <td> 0.006</td> <td> 1553.215</td> <td> 9285.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>           <td> 5667.0751</td> <td> 1999.675</td> <td>    2.834</td> <td> 0.005</td> <td> 1747.352</td> <td> 9586.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>           <td> 3642.6006</td> <td> 1943.054</td> <td>    1.875</td> <td> 0.061</td> <td> -166.137</td> <td> 7451.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>           <td> 5815.5450</td> <td> 1997.849</td> <td>    2.911</td> <td> 0.004</td> <td> 1899.401</td> <td> 9731.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>           <td> 4177.8920</td> <td> 2529.093</td> <td>    1.652</td> <td> 0.099</td> <td> -779.587</td> <td> 9135.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>           <td> 4116.7352</td> <td> 1916.475</td> <td>    2.148</td> <td> 0.032</td> <td>  360.099</td> <td> 7873.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>           <td>-2.361e+04</td> <td> 3487.541</td> <td>   -6.770</td> <td> 0.000</td> <td>-3.04e+04</td> <td>-1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>           <td> 4462.4952</td> <td> 1946.019</td> <td>    2.293</td> <td> 0.022</td> <td>  647.947</td> <td> 8277.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>           <td> 5452.9185</td> <td> 1991.805</td> <td>    2.738</td> <td> 0.006</td> <td> 1548.621</td> <td> 9357.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>           <td> 4339.3471</td> <td> 2255.519</td> <td>    1.924</td> <td> 0.054</td> <td>  -81.877</td> <td> 8760.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>           <td>  925.9037</td> <td> 1870.957</td> <td>    0.495</td> <td> 0.621</td> <td>-2741.509</td> <td> 4593.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>           <td> 5919.0229</td> <td> 2093.288</td> <td>    2.828</td> <td> 0.005</td> <td> 1815.801</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>           <td> 8396.0701</td> <td> 1986.866</td> <td>    4.226</td> <td> 0.000</td> <td> 4501.454</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>           <td> 5771.3735</td> <td> 2040.638</td> <td>    2.828</td> <td> 0.005</td> <td> 1771.356</td> <td> 9771.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>           <td> 4779.1578</td> <td> 1944.786</td> <td>    2.457</td> <td> 0.014</td> <td>  967.026</td> <td> 8591.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>           <td> 4469.9235</td> <td> 1926.308</td> <td>    2.320</td> <td> 0.020</td> <td>  694.012</td> <td> 8245.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>           <td>-1927.8189</td> <td> 1891.860</td> <td>   -1.019</td> <td> 0.308</td> <td>-5636.205</td> <td> 1780.567</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>           <td> 7286.7965</td> <td> 2664.850</td> <td>    2.734</td> <td> 0.006</td> <td> 2063.210</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>           <td>-2.267e+04</td> <td> 2493.658</td> <td>   -9.091</td> <td> 0.000</td> <td>-2.76e+04</td> <td>-1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>           <td> 6295.8147</td> <td> 1979.720</td> <td>    3.180</td> <td> 0.001</td> <td> 2415.206</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>           <td>-3292.9448</td> <td> 1911.719</td> <td>   -1.723</td> <td> 0.085</td> <td>-7040.259</td> <td>  454.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>           <td> 5906.3927</td> <td> 1992.994</td> <td>    2.964</td> <td> 0.003</td> <td> 1999.765</td> <td> 9813.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>           <td> 4522.8440</td> <td> 1985.573</td> <td>    2.278</td> <td> 0.023</td> <td>  630.762</td> <td> 8414.926</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>           <td> 3228.0209</td> <td> 1900.051</td> <td>    1.699</td> <td> 0.089</td> <td> -496.422</td> <td> 6952.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>           <td> 4431.4327</td> <td> 1906.041</td> <td>    2.325</td> <td> 0.020</td> <td>  695.249</td> <td> 8167.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>           <td> 5377.1085</td> <td> 1982.015</td> <td>    2.713</td> <td> 0.007</td> <td> 1492.002</td> <td> 9262.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>           <td>-2.679e+04</td> <td> 2826.641</td> <td>   -9.476</td> <td> 0.000</td> <td>-3.23e+04</td> <td>-2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>           <td> 7425.3294</td> <td> 1919.565</td> <td>    3.868</td> <td> 0.000</td> <td> 3662.635</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>           <td>-4768.1203</td> <td> 1959.194</td> <td>   -2.434</td> <td> 0.015</td> <td>-8608.495</td> <td> -927.746</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>           <td> 4697.2879</td> <td> 1997.567</td> <td>    2.352</td> <td> 0.019</td> <td>  781.697</td> <td> 8612.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>           <td> 2939.7289</td> <td> 1981.699</td> <td>    1.483</td> <td> 0.138</td> <td> -944.758</td> <td> 6824.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>           <td> 2.481e+04</td> <td> 2046.226</td> <td>   12.126</td> <td> 0.000</td> <td> 2.08e+04</td> <td> 2.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>           <td> 5442.7672</td> <td> 2146.456</td> <td>    2.536</td> <td> 0.011</td> <td> 1235.326</td> <td> 9650.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>           <td>-8409.7028</td> <td> 2697.021</td> <td>   -3.118</td> <td> 0.002</td> <td>-1.37e+04</td> <td>-3123.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>           <td>-7472.8788</td> <td> 2066.166</td> <td>   -3.617</td> <td> 0.000</td> <td>-1.15e+04</td> <td>-3422.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>           <td> 4438.9665</td> <td> 1939.722</td> <td>    2.288</td> <td> 0.022</td> <td>  636.761</td> <td> 8241.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>           <td> 5696.2156</td> <td> 2218.601</td> <td>    2.567</td> <td> 0.010</td> <td> 1347.357</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>           <td> 4522.6985</td> <td> 3708.109</td> <td>    1.220</td> <td> 0.223</td> <td>-2745.864</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>           <td> 4583.2113</td> <td> 1876.629</td> <td>    2.442</td> <td> 0.015</td> <td>  904.681</td> <td> 8261.742</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>           <td>-2.271e+04</td> <td> 2967.263</td> <td>   -7.655</td> <td> 0.000</td> <td>-2.85e+04</td> <td>-1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>          <td>-2.908e+04</td> <td> 4955.987</td> <td>   -5.867</td> <td> 0.000</td> <td>-3.88e+04</td> <td>-1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>           <td> 3760.7454</td> <td> 1897.673</td> <td>    1.982</td> <td> 0.048</td> <td>   40.963</td> <td> 7480.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>           <td> 4171.7942</td> <td> 1983.047</td> <td>    2.104</td> <td> 0.035</td> <td>  284.665</td> <td> 8058.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>           <td>-4584.6433</td> <td> 2028.696</td> <td>   -2.260</td> <td> 0.024</td> <td>-8561.253</td> <td> -608.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>           <td>-8077.6585</td> <td> 2138.562</td> <td>   -3.777</td> <td> 0.000</td> <td>-1.23e+04</td> <td>-3885.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>           <td>-1505.8472</td> <td> 2298.475</td> <td>   -0.655</td> <td> 0.512</td> <td>-6011.272</td> <td> 2999.578</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>          <td>-1.362e+04</td> <td> 3834.535</td> <td>   -3.552</td> <td> 0.000</td> <td>-2.11e+04</td> <td>-6103.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>           <td>  298.7446</td> <td> 2034.574</td> <td>    0.147</td> <td> 0.883</td> <td>-3689.387</td> <td> 4286.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>           <td> 4291.1173</td> <td> 1929.504</td> <td>    2.224</td> <td> 0.026</td> <td>  508.941</td> <td> 8073.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>          <td> 4798.9194</td> <td> 4126.512</td> <td>    1.163</td> <td> 0.245</td> <td>-3289.788</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>           <td> 3546.3191</td> <td> 1912.662</td> <td>    1.854</td> <td> 0.064</td> <td> -202.843</td> <td> 7295.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>           <td> 5904.7161</td> <td> 1998.878</td> <td>    2.954</td> <td> 0.003</td> <td> 1986.555</td> <td> 9822.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>           <td> 4469.4616</td> <td> 1954.080</td> <td>    2.287</td> <td> 0.022</td> <td>  639.113</td> <td> 8299.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>          <td> 4902.3632</td> <td> 4128.231</td> <td>    1.188</td> <td> 0.235</td> <td>-3189.715</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>           <td>-8574.6540</td> <td> 2515.585</td> <td>   -3.409</td> <td> 0.001</td> <td>-1.35e+04</td> <td>-3643.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>          <td> 1.506e+04</td> <td> 4752.723</td> <td>    3.168</td> <td> 0.002</td> <td> 5740.036</td> <td> 2.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>           <td> -325.6656</td> <td> 2944.721</td> <td>   -0.111</td> <td> 0.912</td> <td>-6097.849</td> <td> 5446.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>           <td> 2348.9509</td> <td> 1999.256</td> <td>    1.175</td> <td> 0.240</td> <td>-1569.951</td> <td> 6267.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>           <td> 4423.1470</td> <td> 1927.638</td> <td>    2.295</td> <td> 0.022</td> <td>  644.630</td> <td> 8201.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>           <td> 6017.0597</td> <td> 2097.504</td> <td>    2.869</td> <td> 0.004</td> <td> 1905.574</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>           <td>-1.416e+04</td> <td> 2842.308</td> <td>   -4.984</td> <td> 0.000</td> <td>-1.97e+04</td> <td>-8593.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>           <td> 3846.2387</td> <td> 3365.323</td> <td>    1.143</td> <td> 0.253</td> <td>-2750.402</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>           <td> 4175.3038</td> <td> 1933.465</td> <td>    2.159</td> <td> 0.031</td> <td>  385.363</td> <td> 7965.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>           <td> -418.4821</td> <td> 1867.144</td> <td>   -0.224</td> <td> 0.823</td> <td>-4078.422</td> <td> 3241.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>           <td> 4087.5453</td> <td> 1931.523</td> <td>    2.116</td> <td> 0.034</td> <td>  301.411</td> <td> 7873.679</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>           <td> 5379.8847</td> <td> 2123.978</td> <td>    2.533</td> <td> 0.011</td> <td> 1216.504</td> <td> 9543.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>           <td> 1.029e+04</td> <td> 1969.737</td> <td>    5.226</td> <td> 0.000</td> <td> 6432.411</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>           <td> 5217.1985</td> <td> 1984.313</td> <td>    2.629</td> <td> 0.009</td> <td> 1327.586</td> <td> 9106.811</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>           <td> 1.094e+04</td> <td> 5644.764</td> <td>    1.938</td> <td> 0.053</td> <td> -127.842</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>           <td> 6123.9357</td> <td> 1995.295</td> <td>    3.069</td> <td> 0.002</td> <td> 2212.798</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>           <td>-2065.0828</td> <td> 1925.200</td> <td>   -1.073</td> <td> 0.283</td> <td>-5838.822</td> <td> 1708.656</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>           <td> 5872.2443</td> <td> 1997.358</td> <td>    2.940</td> <td> 0.003</td> <td> 1957.062</td> <td> 9787.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>           <td>-5951.1656</td> <td> 2108.075</td> <td>   -2.823</td> <td> 0.005</td> <td>-1.01e+04</td> <td>-1818.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>           <td> 4710.5828</td> <td> 1957.869</td> <td>    2.406</td> <td> 0.016</td> <td>  872.807</td> <td> 8548.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>           <td> 3397.3386</td> <td> 1908.228</td> <td>    1.780</td> <td> 0.075</td> <td> -343.133</td> <td> 7137.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>           <td> 6910.7687</td> <td> 1921.540</td> <td>    3.596</td> <td> 0.000</td> <td> 3144.204</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>           <td> 3766.6941</td> <td> 1921.662</td> <td>    1.960</td> <td> 0.050</td> <td>   -0.110</td> <td> 7533.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>           <td>-1.443e+04</td> <td> 2629.279</td> <td>   -5.487</td> <td> 0.000</td> <td>-1.96e+04</td> <td>-9271.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>           <td> -996.0686</td> <td> 8129.378</td> <td>   -0.123</td> <td> 0.902</td> <td>-1.69e+04</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>           <td> 4907.2634</td> <td> 1957.900</td> <td>    2.506</td> <td> 0.012</td> <td> 1069.426</td> <td> 8745.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>           <td> 5893.6325</td> <td> 2368.628</td> <td>    2.488</td> <td> 0.013</td> <td> 1250.694</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>           <td> 2253.1849</td> <td> 2111.822</td> <td>    1.067</td> <td> 0.286</td> <td>-1886.367</td> <td> 6392.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>           <td>-1258.6150</td> <td> 1957.951</td> <td>   -0.643</td> <td> 0.520</td> <td>-5096.552</td> <td> 2579.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>           <td> -325.0480</td> <td> 1919.895</td> <td>   -0.169</td> <td> 0.866</td> <td>-4088.389</td> <td> 3438.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>           <td> 5694.8539</td> <td> 1988.134</td> <td>    2.864</td> <td> 0.004</td> <td> 1797.753</td> <td> 9591.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>           <td> 4672.1866</td> <td> 1956.259</td> <td>    2.388</td> <td> 0.017</td> <td>  837.566</td> <td> 8506.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>           <td> 5613.9605</td> <td> 2208.789</td> <td>    2.542</td> <td> 0.011</td> <td> 1284.335</td> <td> 9943.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>           <td> 4852.5307</td> <td> 2141.141</td> <td>    2.266</td> <td> 0.023</td> <td>  655.508</td> <td> 9049.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>           <td> 4161.7340</td> <td> 2167.386</td> <td>    1.920</td> <td> 0.055</td> <td>  -86.733</td> <td> 8410.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>           <td> 5142.2645</td> <td> 1973.100</td> <td>    2.606</td> <td> 0.009</td> <td> 1274.632</td> <td> 9009.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>           <td> 4804.7906</td> <td> 1948.741</td> <td>    2.466</td> <td> 0.014</td> <td>  984.906</td> <td> 8624.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>           <td> -399.9826</td> <td> 4068.185</td> <td>   -0.098</td> <td> 0.922</td> <td>-8374.359</td> <td> 7574.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>           <td> 6153.3134</td> <td> 1992.015</td> <td>    3.089</td> <td> 0.002</td> <td> 2248.604</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>           <td>-4085.8431</td> <td> 3204.549</td> <td>   -1.275</td> <td> 0.202</td> <td>-1.04e+04</td> <td> 2195.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>           <td> 4182.8424</td> <td> 1932.897</td> <td>    2.164</td> <td> 0.030</td> <td>  394.016</td> <td> 7971.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>           <td> 2591.0039</td> <td> 4098.503</td> <td>    0.632</td> <td> 0.527</td> <td>-5442.802</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>           <td> 5954.5814</td> <td> 2001.155</td> <td>    2.976</td> <td> 0.003</td> <td> 2031.957</td> <td> 9877.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>           <td> 3836.5715</td> <td> 1919.730</td> <td>    1.998</td> <td> 0.046</td> <td>   73.554</td> <td> 7599.589</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>           <td>-1.681e+04</td> <td> 2683.691</td> <td>   -6.264</td> <td> 0.000</td> <td>-2.21e+04</td> <td>-1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>           <td> 6256.1243</td> <td> 2034.818</td> <td>    3.075</td> <td> 0.002</td> <td> 2267.515</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>           <td>-2.614e+04</td> <td> 3202.997</td> <td>   -8.162</td> <td> 0.000</td> <td>-3.24e+04</td> <td>-1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>           <td> 5646.2077</td> <td> 2455.892</td> <td>    2.299</td> <td> 0.022</td> <td>  832.217</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>           <td> 4701.3774</td> <td> 1956.186</td> <td>    2.403</td> <td> 0.016</td> <td>  866.900</td> <td> 8535.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>           <td> 4844.1439</td> <td> 1967.004</td> <td>    2.463</td> <td> 0.014</td> <td>  988.461</td> <td> 8699.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>           <td> 2159.2106</td> <td> 1877.370</td> <td>    1.150</td> <td> 0.250</td> <td>-1520.773</td> <td> 5839.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>           <td> 4949.2354</td> <td> 1959.344</td> <td>    2.526</td> <td> 0.012</td> <td> 1108.568</td> <td> 8789.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>           <td> 2164.0008</td> <td> 2110.021</td> <td>    1.026</td> <td> 0.305</td> <td>-1972.021</td> <td> 6300.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>           <td> 1893.3294</td> <td> 2772.788</td> <td>    0.683</td> <td> 0.495</td> <td>-3541.835</td> <td> 7328.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>           <td> 5171.9492</td> <td> 1965.983</td> <td>    2.631</td> <td> 0.009</td> <td> 1318.268</td> <td> 9025.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>           <td> 4728.0975</td> <td> 1945.157</td> <td>    2.431</td> <td> 0.015</td> <td>  915.239</td> <td> 8540.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>           <td>  731.2960</td> <td> 1868.224</td> <td>    0.391</td> <td> 0.695</td> <td>-2930.759</td> <td> 4393.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>           <td>-1.134e+04</td> <td> 2736.825</td> <td>   -4.145</td> <td> 0.000</td> <td>-1.67e+04</td> <td>-5978.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>           <td> 1.014e+04</td> <td> 1996.162</td> <td>    5.081</td> <td> 0.000</td> <td> 6230.132</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>           <td> 8188.2669</td> <td> 1985.465</td> <td>    4.124</td> <td> 0.000</td> <td> 4296.397</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>           <td> 5537.7463</td> <td> 2145.524</td> <td>    2.581</td> <td> 0.010</td> <td> 1332.133</td> <td> 9743.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>           <td> 5979.0806</td> <td> 1967.201</td> <td>    3.039</td> <td> 0.002</td> <td> 2123.013</td> <td> 9835.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>           <td> 5277.3971</td> <td> 3150.678</td> <td>    1.675</td> <td> 0.094</td> <td> -898.499</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>           <td> 3337.7779</td> <td> 1909.289</td> <td>    1.748</td> <td> 0.080</td> <td> -404.773</td> <td> 7080.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>           <td>  891.4098</td> <td> 2003.758</td> <td>    0.445</td> <td> 0.656</td> <td>-3036.318</td> <td> 4819.137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>           <td> 5258.5464</td> <td> 1978.876</td> <td>    2.657</td> <td> 0.008</td> <td> 1379.592</td> <td> 9137.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>           <td> 5334.3482</td> <td> 1973.436</td> <td>    2.703</td> <td> 0.007</td> <td> 1466.059</td> <td> 9202.638</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>           <td> 7833.1917</td> <td> 1972.917</td> <td>    3.970</td> <td> 0.000</td> <td> 3965.918</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>           <td> 2429.4741</td> <td> 1932.129</td> <td>    1.257</td> <td> 0.209</td> <td>-1357.847</td> <td> 6216.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>           <td> 3618.8978</td> <td> 1911.469</td> <td>    1.893</td> <td> 0.058</td> <td> -127.926</td> <td> 7365.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>           <td> 1.164e+04</td> <td> 1895.440</td> <td>    6.142</td> <td> 0.000</td> <td> 7927.017</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>           <td> -481.5654</td> <td> 3080.004</td> <td>   -0.156</td> <td> 0.876</td> <td>-6518.928</td> <td> 5555.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>           <td> -921.2075</td> <td> 2179.821</td> <td>   -0.423</td> <td> 0.673</td> <td>-5194.050</td> <td> 3351.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>           <td> 1.726e+04</td> <td> 3178.570</td> <td>    5.429</td> <td> 0.000</td> <td>  1.1e+04</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>           <td> 4843.1503</td> <td> 1946.744</td> <td>    2.488</td> <td> 0.013</td> <td> 1027.180</td> <td> 8659.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>           <td>   41.7571</td> <td> 2048.866</td> <td>    0.020</td> <td> 0.984</td> <td>-3974.390</td> <td> 4057.905</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>           <td>-1.684e+04</td> <td> 2894.155</td> <td>   -5.819</td> <td> 0.000</td> <td>-2.25e+04</td> <td>-1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>           <td> 5210.4769</td> <td> 2932.058</td> <td>    1.777</td> <td> 0.076</td> <td> -536.886</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>           <td> 4834.4826</td> <td> 1946.833</td> <td>    2.483</td> <td> 0.013</td> <td> 1018.340</td> <td> 8650.625</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>           <td>-1.104e+04</td> <td> 3484.277</td> <td>   -3.168</td> <td> 0.002</td> <td>-1.79e+04</td> <td>-4207.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>           <td>-8749.8624</td> <td> 2173.692</td> <td>   -4.025</td> <td> 0.000</td> <td> -1.3e+04</td> <td>-4489.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>           <td> 5949.0922</td> <td> 1998.542</td> <td>    2.977</td> <td> 0.003</td> <td> 2031.589</td> <td> 9866.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>           <td> 5543.7513</td> <td> 1967.066</td> <td>    2.818</td> <td> 0.005</td> <td> 1687.948</td> <td> 9399.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>           <td> 1119.1778</td> <td> 1871.032</td> <td>    0.598</td> <td> 0.550</td> <td>-2548.382</td> <td> 4786.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>           <td>  1.08e+04</td> <td> 2056.810</td> <td>    5.249</td> <td> 0.000</td> <td> 6765.135</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>           <td> 2379.6810</td> <td> 2005.849</td> <td>    1.186</td> <td> 0.236</td> <td>-1552.144</td> <td> 6311.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>           <td>-1.506e+04</td> <td> 2681.366</td> <td>   -5.617</td> <td> 0.000</td> <td>-2.03e+04</td> <td>-9804.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>           <td> 4986.3882</td> <td> 3674.490</td> <td>    1.357</td> <td> 0.175</td> <td>-2216.274</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>           <td> 6434.6407</td> <td> 1941.012</td> <td>    3.315</td> <td> 0.001</td> <td> 2629.907</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>           <td> 3786.8460</td> <td> 1919.409</td> <td>    1.973</td> <td> 0.049</td> <td>   24.457</td> <td> 7549.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>           <td> 4718.8137</td> <td> 1958.983</td> <td>    2.409</td> <td> 0.016</td> <td>  878.854</td> <td> 8558.774</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>           <td> 4578.5389</td> <td> 1957.554</td> <td>    2.339</td> <td> 0.019</td> <td>  741.380</td> <td> 8415.698</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>           <td> 4507.4862</td> <td> 1991.371</td> <td>    2.264</td> <td> 0.024</td> <td>  604.041</td> <td> 8410.932</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>           <td>  -1.1e+04</td> <td> 3197.194</td> <td>   -3.439</td> <td> 0.001</td> <td>-1.73e+04</td> <td>-4728.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>           <td> 3499.7827</td> <td> 1912.556</td> <td>    1.830</td> <td> 0.067</td> <td> -249.172</td> <td> 7248.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>           <td>-2958.4318</td> <td> 2133.959</td> <td>   -1.386</td> <td> 0.166</td> <td>-7141.376</td> <td> 1224.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>           <td> 3958.3364</td> <td> 1925.327</td> <td>    2.056</td> <td> 0.040</td> <td>  184.349</td> <td> 7732.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>           <td> 4582.9805</td> <td> 1943.067</td> <td>    2.359</td> <td> 0.018</td> <td>  774.218</td> <td> 8391.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>           <td> 3684.4239</td> <td> 1924.022</td> <td>    1.915</td> <td> 0.056</td> <td>  -87.005</td> <td> 7455.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>           <td> 6082.7650</td> <td> 2154.264</td> <td>    2.824</td> <td> 0.005</td> <td> 1860.020</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>           <td> 4896.4405</td> <td> 2183.537</td> <td>    2.242</td> <td> 0.025</td> <td>  616.315</td> <td> 9176.566</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>           <td>-1913.9549</td> <td> 1879.822</td> <td>   -1.018</td> <td> 0.309</td> <td>-5598.744</td> <td> 1770.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>           <td> 5606.3133</td> <td> 1975.729</td> <td>    2.838</td> <td> 0.005</td> <td> 1733.527</td> <td> 9479.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>           <td>-6987.3601</td> <td> 2071.660</td> <td>   -3.373</td> <td> 0.001</td> <td> -1.1e+04</td> <td>-2926.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>           <td> 4826.5255</td> <td> 1952.344</td> <td>    2.472</td> <td> 0.013</td> <td>  999.579</td> <td> 8653.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>           <td> -743.5441</td> <td> 1870.869</td> <td>   -0.397</td> <td> 0.691</td> <td>-4410.785</td> <td> 2923.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>           <td> 5910.7386</td> <td> 1990.271</td> <td>    2.970</td> <td> 0.003</td> <td> 2009.449</td> <td> 9812.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>           <td>-1.392e+04</td> <td> 2577.000</td> <td>   -5.402</td> <td> 0.000</td> <td> -1.9e+04</td> <td>-8869.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>           <td> 4965.3718</td> <td> 1956.885</td> <td>    2.537</td> <td> 0.011</td> <td> 1129.525</td> <td> 8801.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>           <td> 4206.8758</td> <td> 1987.093</td> <td>    2.117</td> <td> 0.034</td> <td>  311.816</td> <td> 8101.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>           <td> 7648.2490</td> <td> 1991.172</td> <td>    3.841</td> <td> 0.000</td> <td> 3745.192</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>           <td> 6255.4850</td> <td> 1976.128</td> <td>    3.166</td> <td> 0.002</td> <td> 2381.917</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>           <td> 4897.9116</td> <td> 2083.404</td> <td>    2.351</td> <td> 0.019</td> <td>  814.065</td> <td> 8981.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>           <td> 1663.1797</td> <td> 1882.307</td> <td>    0.884</td> <td> 0.377</td> <td>-2026.482</td> <td> 5352.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>           <td> 4331.9445</td> <td> 1973.788</td> <td>    2.195</td> <td> 0.028</td> <td>  462.965</td> <td> 8200.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>           <td>-2.095e+04</td> <td> 2840.387</td> <td>   -7.374</td> <td> 0.000</td> <td>-2.65e+04</td> <td>-1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>           <td> 1818.9112</td> <td> 1985.535</td> <td>    0.916</td> <td> 0.360</td> <td>-2073.095</td> <td> 5710.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>           <td> 5271.2842</td> <td> 1953.316</td> <td>    2.699</td> <td> 0.007</td> <td> 1442.433</td> <td> 9100.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>           <td> -1.41e+04</td> <td> 2617.753</td> <td>   -5.385</td> <td> 0.000</td> <td>-1.92e+04</td> <td>-8964.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>           <td> 4897.3794</td> <td> 2783.776</td> <td>    1.759</td> <td> 0.079</td> <td> -559.323</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>           <td> 2869.1377</td> <td> 1944.285</td> <td>    1.476</td> <td> 0.140</td> <td> -942.012</td> <td> 6680.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>           <td>-1.295e+04</td> <td> 2223.415</td> <td>   -5.825</td> <td> 0.000</td> <td>-1.73e+04</td> <td>-8593.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>           <td>    -2e+04</td> <td> 3143.524</td> <td>   -6.361</td> <td> 0.000</td> <td>-2.62e+04</td> <td>-1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>           <td> 5818.9744</td> <td> 1999.918</td> <td>    2.910</td> <td> 0.004</td> <td> 1898.774</td> <td> 9739.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>           <td> 4312.5818</td> <td> 1936.679</td> <td>    2.227</td> <td> 0.026</td> <td>  516.341</td> <td> 8108.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>           <td> 5654.5465</td> <td> 1988.576</td> <td>    2.844</td> <td> 0.004</td> <td> 1756.580</td> <td> 9552.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>           <td> 5053.8975</td> <td> 1967.357</td> <td>    2.569</td> <td> 0.010</td> <td> 1197.522</td> <td> 8910.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>           <td> 5768.0364</td> <td> 1996.610</td> <td>    2.889</td> <td> 0.004</td> <td> 1854.320</td> <td> 9681.753</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>           <td> 5755.4269</td> <td> 1988.504</td> <td>    2.894</td> <td> 0.004</td> <td> 1857.600</td> <td> 9653.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>           <td>-8448.5963</td> <td> 2569.276</td> <td>   -3.288</td> <td> 0.001</td> <td>-1.35e+04</td> <td>-3412.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>           <td> 3871.5628</td> <td> 2017.547</td> <td>    1.919</td> <td> 0.055</td> <td>  -83.192</td> <td> 7826.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>           <td> 3792.6248</td> <td> 1974.945</td> <td>    1.920</td> <td> 0.055</td> <td>  -78.623</td> <td> 7663.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>           <td> 2763.3451</td> <td> 2113.139</td> <td>    1.308</td> <td> 0.191</td> <td>-1378.788</td> <td> 6905.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>           <td> 5537.6888</td> <td> 1990.441</td> <td>    2.782</td> <td> 0.005</td> <td> 1636.065</td> <td> 9439.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>           <td> 4708.5506</td> <td> 1944.332</td> <td>    2.422</td> <td> 0.015</td> <td>  897.310</td> <td> 8519.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>           <td>-1.809e+04</td> <td> 2823.040</td> <td>   -6.406</td> <td> 0.000</td> <td>-2.36e+04</td> <td>-1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>           <td> 2764.7807</td> <td> 2079.920</td> <td>    1.329</td> <td> 0.184</td> <td>-1312.238</td> <td> 6841.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>           <td> 5899.3765</td> <td> 1996.449</td> <td>    2.955</td> <td> 0.003</td> <td> 1985.977</td> <td> 9812.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>           <td> 5848.3913</td> <td> 1938.951</td> <td>    3.016</td> <td> 0.003</td> <td> 2047.698</td> <td> 9649.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>           <td> 3024.1407</td> <td> 2008.596</td> <td>    1.506</td> <td> 0.132</td> <td> -913.070</td> <td> 6961.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>           <td> 5556.4878</td> <td> 1985.328</td> <td>    2.799</td> <td> 0.005</td> <td> 1664.886</td> <td> 9448.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>           <td> 5079.4213</td> <td> 1960.665</td> <td>    2.591</td> <td> 0.010</td> <td> 1236.165</td> <td> 8922.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>           <td> 4801.4051</td> <td> 1955.371</td> <td>    2.455</td> <td> 0.014</td> <td>  968.526</td> <td> 8634.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>           <td>-4093.8049</td> <td> 2374.182</td> <td>   -1.724</td> <td> 0.085</td> <td>-8747.629</td> <td>  560.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>           <td> 6216.4871</td> <td> 2366.258</td> <td>    2.627</td> <td> 0.009</td> <td> 1578.194</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>           <td> 4407.1963</td> <td> 1951.703</td> <td>    2.258</td> <td> 0.024</td> <td>  581.507</td> <td> 8232.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>           <td> 9641.6705</td> <td> 2783.822</td> <td>    3.463</td> <td> 0.001</td> <td> 4184.877</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>           <td> -675.0843</td> <td> 3366.838</td> <td>   -0.201</td> <td> 0.841</td> <td>-7274.693</td> <td> 5924.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>           <td> 2752.2508</td> <td> 1915.052</td> <td>    1.437</td> <td> 0.151</td> <td>-1001.596</td> <td> 6506.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>           <td> 2.363e+04</td> <td> 2252.988</td> <td>   10.488</td> <td> 0.000</td> <td> 1.92e+04</td> <td>  2.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>           <td>-9215.4301</td> <td> 2086.704</td> <td>   -4.416</td> <td> 0.000</td> <td>-1.33e+04</td> <td>-5125.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>           <td> 5270.8432</td> <td> 1986.267</td> <td>    2.654</td> <td> 0.008</td> <td> 1377.401</td> <td> 9164.285</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>           <td>-3645.5456</td> <td> 2115.744</td> <td>   -1.723</td> <td> 0.085</td> <td>-7792.785</td> <td>  501.694</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>           <td>-1.627e+04</td> <td> 2904.839</td> <td>   -5.601</td> <td> 0.000</td> <td> -2.2e+04</td> <td>-1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>           <td> 7964.2030</td> <td> 1983.187</td> <td>    4.016</td> <td> 0.000</td> <td> 4076.799</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>           <td> 4875.7978</td> <td> 1965.120</td> <td>    2.481</td> <td> 0.013</td> <td> 1023.809</td> <td> 8727.787</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>           <td> 4640.0866</td> <td> 1948.982</td> <td>    2.381</td> <td> 0.017</td> <td>  819.730</td> <td> 8460.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>           <td>-3330.0016</td> <td> 1917.841</td> <td>   -1.736</td> <td> 0.083</td> <td>-7089.315</td> <td>  429.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>           <td> 3299.0514</td> <td> 3674.268</td> <td>    0.898</td> <td> 0.369</td> <td>-3903.176</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>           <td> -917.2164</td> <td> 1872.610</td> <td>   -0.490</td> <td> 0.624</td> <td>-4587.870</td> <td> 2753.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>           <td> 1048.2952</td> <td> 2175.405</td> <td>    0.482</td> <td> 0.630</td> <td>-3215.890</td> <td> 5312.480</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>           <td> 3104.2442</td> <td> 1921.700</td> <td>    1.615</td> <td> 0.106</td> <td> -662.635</td> <td> 6871.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>           <td> 5391.0396</td> <td> 1978.249</td> <td>    2.725</td> <td> 0.006</td> <td> 1513.316</td> <td> 9268.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>           <td> 5756.4000</td> <td> 1983.942</td> <td>    2.901</td> <td> 0.004</td> <td> 1867.517</td> <td> 9645.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>           <td>-4660.8382</td> <td> 1959.097</td> <td>   -2.379</td> <td> 0.017</td> <td>-8501.021</td> <td> -820.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>           <td> 8546.7408</td> <td> 2018.068</td> <td>    4.235</td> <td> 0.000</td> <td> 4590.963</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>           <td> 5782.6825</td> <td> 1994.102</td> <td>    2.900</td> <td> 0.004</td> <td> 1873.883</td> <td> 9691.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>           <td> 5389.3481</td> <td> 1979.050</td> <td>    2.723</td> <td> 0.006</td> <td> 1510.054</td> <td> 9268.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>           <td> 5529.1546</td> <td> 1983.027</td> <td>    2.788</td> <td> 0.005</td> <td> 1642.064</td> <td> 9416.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>           <td> 4530.6367</td> <td> 1955.107</td> <td>    2.317</td> <td> 0.021</td> <td>  698.275</td> <td> 8362.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>           <td>-3956.3934</td> <td> 2044.298</td> <td>   -1.935</td> <td> 0.053</td> <td>-7963.586</td> <td>   50.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>           <td> 3397.4168</td> <td> 1909.748</td> <td>    1.779</td> <td> 0.075</td> <td> -346.034</td> <td> 7140.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>           <td> -333.9629</td> <td> 1869.765</td> <td>   -0.179</td> <td> 0.858</td> <td>-3999.040</td> <td> 3331.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>           <td>-2.272e+04</td> <td> 3204.562</td> <td>   -7.089</td> <td> 0.000</td> <td> -2.9e+04</td> <td>-1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>           <td>-1.458e+04</td> <td> 2732.112</td> <td>   -5.335</td> <td> 0.000</td> <td>-1.99e+04</td> <td>-9219.887</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>           <td> 5370.0245</td> <td> 2283.150</td> <td>    2.352</td> <td> 0.019</td> <td>  894.639</td> <td> 9845.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>           <td> 2046.2952</td> <td> 1880.221</td> <td>    1.088</td> <td> 0.276</td> <td>-1639.278</td> <td> 5731.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>           <td> 2368.2290</td> <td> 1887.367</td> <td>    1.255</td> <td> 0.210</td> <td>-1331.350</td> <td> 6067.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>           <td>-1.024e+04</td> <td> 2578.366</td> <td>   -3.973</td> <td> 0.000</td> <td>-1.53e+04</td> <td>-5190.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>           <td> 5656.2012</td> <td> 1976.330</td> <td>    2.862</td> <td> 0.004</td> <td> 1782.237</td> <td> 9530.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>           <td>-1417.9474</td> <td> 1875.489</td> <td>   -0.756</td> <td> 0.450</td> <td>-5094.244</td> <td> 2258.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>           <td> 1647.2302</td> <td> 1954.192</td> <td>    0.843</td> <td> 0.399</td> <td>-2183.339</td> <td> 5477.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>           <td> 2094.4429</td> <td> 3339.339</td> <td>    0.627</td> <td> 0.531</td> <td>-4451.263</td> <td> 8640.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>           <td> 4739.7071</td> <td> 2316.199</td> <td>    2.046</td> <td> 0.041</td> <td>  199.540</td> <td> 9279.874</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>           <td> 5631.9645</td> <td> 1997.191</td> <td>    2.820</td> <td> 0.005</td> <td> 1717.110</td> <td> 9546.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>           <td> 4493.3047</td> <td> 1941.754</td> <td>    2.314</td> <td> 0.021</td> <td>  687.116</td> <td> 8299.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>           <td> 2470.6002</td> <td> 3342.460</td> <td>    0.739</td> <td> 0.460</td> <td>-4081.223</td> <td> 9022.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>           <td>-1536.8001</td> <td> 1882.692</td> <td>   -0.816</td> <td> 0.414</td> <td>-5227.216</td> <td> 2153.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>           <td>-4464.1877</td> <td> 1954.492</td> <td>   -2.284</td> <td> 0.022</td> <td>-8295.344</td> <td> -633.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>           <td> 6238.3665</td> <td> 1997.849</td> <td>    3.123</td> <td> 0.002</td> <td> 2322.222</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>           <td> 4381.5157</td> <td> 2512.608</td> <td>    1.744</td> <td> 0.081</td> <td> -543.650</td> <td> 9306.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>           <td> 5834.6453</td> <td> 1995.708</td> <td>    2.924</td> <td> 0.003</td> <td> 1922.697</td> <td> 9746.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>           <td> 5936.9469</td> <td> 1988.853</td> <td>    2.985</td> <td> 0.003</td> <td> 2038.437</td> <td> 9835.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>           <td> 2477.2047</td> <td> 2000.816</td> <td>    1.238</td> <td> 0.216</td> <td>-1444.755</td> <td> 6399.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>           <td>-7746.5538</td> <td> 2112.622</td> <td>   -3.667</td> <td> 0.000</td> <td>-1.19e+04</td> <td>-3605.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>           <td>-2566.7239</td> <td> 3099.965</td> <td>   -0.828</td> <td> 0.408</td> <td>-8643.214</td> <td> 3509.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>           <td>-2848.8965</td> <td> 1878.359</td> <td>   -1.517</td> <td> 0.129</td> <td>-6530.819</td> <td>  833.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>           <td> 4546.5205</td> <td> 1994.091</td> <td>    2.280</td> <td> 0.023</td> <td>  637.742</td> <td> 8455.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>           <td> 5360.0205</td> <td> 1977.740</td> <td>    2.710</td> <td> 0.007</td> <td> 1483.294</td> <td> 9236.747</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>           <td>-4.099e+04</td> <td> 5543.358</td> <td>   -7.395</td> <td> 0.000</td> <td>-5.19e+04</td> <td>-3.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>           <td> 3247.9219</td> <td> 2267.303</td> <td>    1.433</td> <td> 0.152</td> <td>-1196.400</td> <td> 7692.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>           <td> 3231.7183</td> <td> 2071.422</td> <td>    1.560</td> <td> 0.119</td> <td> -828.643</td> <td> 7292.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>           <td> -1.17e+04</td> <td> 3358.202</td> <td>   -3.484</td> <td> 0.000</td> <td>-1.83e+04</td> <td>-5118.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>           <td> 1.242e+04</td> <td> 1955.181</td> <td>    6.351</td> <td> 0.000</td> <td> 8585.002</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>           <td>-3879.1823</td> <td> 1935.402</td> <td>   -2.004</td> <td> 0.045</td> <td>-7672.918</td> <td>  -85.446</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>           <td>-1.629e+04</td> <td> 2806.835</td> <td>   -5.803</td> <td> 0.000</td> <td>-2.18e+04</td> <td>-1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>           <td>-2.153e+04</td> <td> 3454.348</td> <td>   -6.233</td> <td> 0.000</td> <td>-2.83e+04</td> <td>-1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>           <td>-2660.5248</td> <td> 1904.053</td> <td>   -1.397</td> <td> 0.162</td> <td>-6392.812</td> <td> 1071.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>           <td>-6031.7426</td> <td> 2877.301</td> <td>   -2.096</td> <td> 0.036</td> <td>-1.17e+04</td> <td> -391.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>           <td>-7906.4917</td> <td> 2130.319</td> <td>   -3.711</td> <td> 0.000</td> <td>-1.21e+04</td> <td>-3730.682</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>           <td>-5952.7429</td> <td> 3376.881</td> <td>   -1.763</td> <td> 0.078</td> <td>-1.26e+04</td> <td>  666.552</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>           <td> 3481.6826</td> <td> 1911.796</td> <td>    1.821</td> <td> 0.069</td> <td> -265.783</td> <td> 7229.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>           <td> 5433.9198</td> <td> 1992.138</td> <td>    2.728</td> <td> 0.006</td> <td> 1528.970</td> <td> 9338.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>           <td> 5427.2120</td> <td> 1978.219</td> <td>    2.743</td> <td> 0.006</td> <td> 1549.546</td> <td> 9304.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>           <td>-1.512e+04</td> <td> 4541.405</td> <td>   -3.329</td> <td> 0.001</td> <td> -2.4e+04</td> <td>-6214.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>           <td> 4520.3865</td> <td> 1946.279</td> <td>    2.323</td> <td> 0.020</td> <td>  705.329</td> <td> 8335.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>           <td> 1367.7058</td> <td> 1873.827</td> <td>    0.730</td> <td> 0.465</td> <td>-2305.333</td> <td> 5040.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>           <td> 4281.7503</td> <td> 1896.184</td> <td>    2.258</td> <td> 0.024</td> <td>  564.888</td> <td> 7998.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>           <td>-8344.7727</td> <td> 2157.998</td> <td>   -3.867</td> <td> 0.000</td> <td>-1.26e+04</td> <td>-4114.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>           <td>-1.428e+04</td> <td> 2604.062</td> <td>   -5.484</td> <td> 0.000</td> <td>-1.94e+04</td> <td>-9176.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>           <td> 5812.7072</td> <td> 1998.234</td> <td>    2.909</td> <td> 0.004</td> <td> 1895.809</td> <td> 9729.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>           <td>-4544.7951</td> <td> 1939.992</td> <td>   -2.343</td> <td> 0.019</td> <td>-8347.529</td> <td> -742.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>           <td> 4591.0572</td> <td> 1934.805</td> <td>    2.373</td> <td> 0.018</td> <td>  798.490</td> <td> 8383.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>           <td> 3626.0662</td> <td> 1898.512</td> <td>    1.910</td> <td> 0.056</td> <td>  -95.359</td> <td> 7347.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>           <td>-7061.2889</td> <td> 2203.249</td> <td>   -3.205</td> <td> 0.001</td> <td>-1.14e+04</td> <td>-2742.525</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>           <td> 5697.1487</td> <td> 2072.046</td> <td>    2.750</td> <td> 0.006</td> <td> 1635.565</td> <td> 9758.733</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>           <td>-3.302e+04</td> <td> 2308.852</td> <td>  -14.300</td> <td> 0.000</td> <td>-3.75e+04</td> <td>-2.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>           <td> 5631.7113</td> <td> 1992.359</td> <td>    2.827</td> <td> 0.005</td> <td> 1726.329</td> <td> 9537.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>           <td>-8117.4344</td> <td> 2143.677</td> <td>   -3.787</td> <td> 0.000</td> <td>-1.23e+04</td> <td>-3915.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>           <td> 4399.1214</td> <td> 2611.570</td> <td>    1.684</td> <td> 0.092</td> <td> -720.027</td> <td> 9518.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>           <td> 4786.1265</td> <td> 1984.341</td> <td>    2.412</td> <td> 0.016</td> <td>  896.461</td> <td> 8675.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>           <td> 5320.7734</td> <td> 1998.236</td> <td>    2.663</td> <td> 0.008</td> <td> 1403.871</td> <td> 9237.676</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9999.0</th>           <td>-9357.5801</td> <td> 2209.351</td> <td>   -4.235</td> <td> 0.000</td> <td>-1.37e+04</td> <td>-5026.853</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1982</th> <td>   -0.0100</td> <td>    0.098</td> <td>   -0.102</td> <td> 0.919</td> <td>   -0.201</td> <td>    0.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1983</th> <td>   -0.0551</td> <td>    0.096</td> <td>   -0.575</td> <td> 0.565</td> <td>   -0.243</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1984</th> <td>   -0.1174</td> <td>    0.094</td> <td>   -1.244</td> <td> 0.213</td> <td>   -0.302</td> <td>    0.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1985</th> <td>   -0.1671</td> <td>    0.094</td> <td>   -1.774</td> <td> 0.076</td> <td>   -0.352</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1986</th> <td>   -0.2239</td> <td>    0.095</td> <td>   -2.367</td> <td> 0.018</td> <td>   -0.409</td> <td>   -0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1987</th> <td>   -0.2562</td> <td>    0.095</td> <td>   -2.694</td> <td> 0.007</td> <td>   -0.443</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1988</th> <td>   -0.2953</td> <td>    0.096</td> <td>   -3.087</td> <td> 0.002</td> <td>   -0.483</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1989</th> <td>   -0.2942</td> <td>    0.097</td> <td>   -3.049</td> <td> 0.002</td> <td>   -0.483</td> <td>   -0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1990</th> <td>   -0.3209</td> <td>    0.098</td> <td>   -3.287</td> <td> 0.001</td> <td>   -0.512</td> <td>   -0.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1991</th> <td>   -0.3112</td> <td>    0.099</td> <td>   -3.150</td> <td> 0.002</td> <td>   -0.505</td> <td>   -0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1992</th> <td>   -0.3633</td> <td>    0.100</td> <td>   -3.636</td> <td> 0.000</td> <td>   -0.559</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1993</th> <td>   -0.3810</td> <td>    0.101</td> <td>   -3.758</td> <td> 0.000</td> <td>   -0.580</td> <td>   -0.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1994</th> <td>   -0.3920</td> <td>    0.103</td> <td>   -3.820</td> <td> 0.000</td> <td>   -0.593</td> <td>   -0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1995</th> <td>   -0.3683</td> <td>    0.105</td> <td>   -3.518</td> <td> 0.000</td> <td>   -0.574</td> <td>   -0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1996</th> <td>   -0.4071</td> <td>    0.107</td> <td>   -3.788</td> <td> 0.000</td> <td>   -0.618</td> <td>   -0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1997</th> <td>   -0.3834</td> <td>    0.111</td> <td>   -3.467</td> <td> 0.001</td> <td>   -0.600</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1998</th> <td>   -0.3315</td> <td>    0.114</td> <td>   -2.913</td> <td> 0.004</td> <td>   -0.554</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1999</th> <td>   -0.2970</td> <td>    0.117</td> <td>   -2.546</td> <td> 0.011</td> <td>   -0.526</td> <td>   -0.068</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>22689.763</td> <th>  Durbin-Watson:     </th>   <td>   0.751</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>140364584.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>14.706</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>537.957</td>  <th>  Cond. No.          </th>   <td>1.66e+18</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 5.75e-25. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &       0.665    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &       0.641    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &       28.37    \\\\\n",
       "\\textbf{Date:}             & Mon, 14 Oct 2024 & \\textbf{  Prob (F-statistic):} &       0.00     \\\\\n",
       "\\textbf{Time:}             &     16:58:05     & \\textbf{  Log-Likelihood:    } &  -1.2191e+05   \\\\\n",
       "\\textbf{No. Observations:} &       11736      & \\textbf{  AIC:               } &   2.454e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       10968      & \\textbf{  BIC:               } &   2.510e+05    \\\\\n",
       "\\textbf{Df Model:}         &         767      & \\textbf{                     } &                \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &                \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &   -6154.3893  &      840.819     &    -7.320  &         0.000        &    -7802.546    &    -4506.232     \\\\\n",
       "\\textbf{gspillsicIV}      &       1.2284  &        0.186     &     6.618  &         0.000        &        0.865    &        1.592     \\\\\n",
       "\\textbf{pat\\_count}       &     -28.2387  &        1.740     &   -16.228  &         0.000        &      -31.650    &      -24.828     \\\\\n",
       "\\textbf{rsales}           &       1.0403  &        0.041     &    25.468  &         0.000        &        0.960    &        1.120     \\\\\n",
       "\\textbf{rppent}           &       0.5611  &        0.087     &     6.461  &         0.000        &        0.391    &        0.731     \\\\\n",
       "\\textbf{emp}              &     -11.1408  &        6.898     &    -1.615  &         0.106        &      -24.663    &        2.381     \\\\\n",
       "\\textbf{rxrd}             &       8.6120  &        0.670     &    12.852  &         0.000        &        7.299    &        9.925     \\\\\n",
       "\\textbf{1982}             &     115.9480  &      620.546     &     0.187  &         0.852        &    -1100.434    &     1332.330     \\\\\n",
       "\\textbf{1983}             &     214.6565  &      615.322     &     0.349  &         0.727        &     -991.485    &     1420.798     \\\\\n",
       "\\textbf{1984}             &     -58.6229  &      611.072     &    -0.096  &         0.924        &    -1256.434    &     1139.188     \\\\\n",
       "\\textbf{1985}             &     -38.8501  &      609.748     &    -0.064  &         0.949        &    -1234.065    &     1156.365     \\\\\n",
       "\\textbf{1986}             &     -33.9845  &      606.213     &    -0.056  &         0.955        &    -1222.270    &     1154.301     \\\\\n",
       "\\textbf{1987}             &    -146.6154  &      603.157     &    -0.243  &         0.808        &    -1328.911    &     1035.681     \\\\\n",
       "\\textbf{1988}             &    -282.7993  &      602.981     &    -0.469  &         0.639        &    -1464.750    &      899.151     \\\\\n",
       "\\textbf{1989}             &    -153.5184  &      600.503     &    -0.256  &         0.798        &    -1330.613    &     1023.577     \\\\\n",
       "\\textbf{1990}             &    -482.0750  &      598.185     &    -0.806  &         0.420        &    -1654.626    &      690.476     \\\\\n",
       "\\textbf{1991}             &    -175.7135  &      596.118     &    -0.295  &         0.768        &    -1344.212    &      992.785     \\\\\n",
       "\\textbf{1992}             &     -21.7112  &      595.147     &    -0.036  &         0.971        &    -1188.307    &     1144.884     \\\\\n",
       "\\textbf{1993}             &     103.0065  &      593.189     &     0.174  &         0.862        &    -1059.752    &     1265.765     \\\\\n",
       "\\textbf{1994}             &    -115.0479  &      594.848     &    -0.193  &         0.847        &    -1281.058    &     1050.962     \\\\\n",
       "\\textbf{1995}             &     272.6655  &      595.318     &     0.458  &         0.647        &     -894.265    &     1439.596     \\\\\n",
       "\\textbf{1996}             &     710.9281  &      597.093     &     1.191  &         0.234        &     -459.482    &     1881.338     \\\\\n",
       "\\textbf{1997}             &    1004.6717  &      599.536     &     1.676  &         0.094        &     -170.528    &     2179.871     \\\\\n",
       "\\textbf{1998}             &     779.1160  &      603.511     &     1.291  &         0.197        &     -403.874    &     1962.106     \\\\\n",
       "\\textbf{1999}             &    1039.8341  &      609.204     &     1.707  &         0.088        &     -154.315    &     2233.983     \\\\\n",
       "\\textbf{10005.0}          &    4813.4949  &     1951.882     &     2.466  &         0.014        &      987.455    &     8639.535     \\\\\n",
       "\\textbf{10006.0}          &    4797.8946  &     2330.607     &     2.059  &         0.040        &      229.485    &     9366.304     \\\\\n",
       "\\textbf{10008.0}          &    3837.7388  &     1921.166     &     1.998  &         0.046        &       71.907    &     7603.570     \\\\\n",
       "\\textbf{10016.0}          &    5633.0729  &     1974.935     &     2.852  &         0.004        &     1761.845    &     9504.301     \\\\\n",
       "\\textbf{10030.0}          &    5704.5362  &     1987.517     &     2.870  &         0.004        &     1808.645    &     9600.428     \\\\\n",
       "\\textbf{1004.0}           &    5673.8775  &     1989.429     &     2.852  &         0.004        &     1774.237    &     9573.518     \\\\\n",
       "\\textbf{10056.0}          &    2609.4076  &     1897.755     &     1.375  &         0.169        &    -1110.534    &     6329.349     \\\\\n",
       "\\textbf{10085.0}          &   -5372.5156  &     1996.211     &    -2.691  &         0.007        &    -9285.449    &    -1459.582     \\\\\n",
       "\\textbf{10092.0}          &    5091.1028  &     4720.797     &     1.078  &         0.281        &    -4162.511    &     1.43e+04     \\\\\n",
       "\\textbf{10097.0}          &   -2417.5921  &     1871.431     &    -1.292  &         0.196        &    -6085.934    &     1250.750     \\\\\n",
       "\\textbf{1010.0}           &    5474.0561  &     4741.603     &     1.154  &         0.248        &    -3820.341    &     1.48e+04     \\\\\n",
       "\\textbf{10109.0}          &    6035.9979  &     1994.555     &     3.026  &         0.002        &     2126.311    &     9945.685     \\\\\n",
       "\\textbf{10115.0}          &     179.9615  &     1873.196     &     0.096  &         0.923        &    -3491.840    &     3851.763     \\\\\n",
       "\\textbf{10124.0}          &    6155.4676  &     2001.421     &     3.076  &         0.002        &     2232.322    &     1.01e+04     \\\\\n",
       "\\textbf{1013.0}           &    -779.2988  &     1875.376     &    -0.416  &         0.678        &    -4455.374    &     2896.777     \\\\\n",
       "\\textbf{10150.0}          &   -4203.7456  &     2471.997     &    -1.701  &         0.089        &    -9049.305    &      641.814     \\\\\n",
       "\\textbf{10159.0}          &   -1.589e+04  &     4053.978     &    -3.921  &         0.000        &    -2.38e+04    &    -7947.204     \\\\\n",
       "\\textbf{10174.0}          &    6019.0041  &     2219.889     &     2.711  &         0.007        &     1667.621    &     1.04e+04     \\\\\n",
       "\\textbf{10185.0}          &     741.4345  &     2103.257     &     0.353  &         0.724        &    -3381.328    &     4864.197     \\\\\n",
       "\\textbf{10195.0}          &    -1.34e+04  &     2547.329     &    -5.260  &         0.000        &    -1.84e+04    &    -8404.482     \\\\\n",
       "\\textbf{10198.0}          &    5775.5626  &     1991.524     &     2.900  &         0.004        &     1871.817    &     9679.308     \\\\\n",
       "\\textbf{10215.0}          &    5892.5705  &     1999.611     &     2.947  &         0.003        &     1972.972    &     9812.169     \\\\\n",
       "\\textbf{10232.0}          &   -4786.0836  &     2217.613     &    -2.158  &         0.031        &    -9133.006    &     -439.161     \\\\\n",
       "\\textbf{10236.0}          &    4748.5453  &     1952.213     &     2.432  &         0.015        &      921.856    &     8575.235     \\\\\n",
       "\\textbf{10286.0}          &    4941.3511  &     1955.985     &     2.526  &         0.012        &     1107.267    &     8775.435     \\\\\n",
       "\\textbf{10301.0}          &   -2.169e+04  &     2733.118     &    -7.936  &         0.000        &     -2.7e+04    &    -1.63e+04     \\\\\n",
       "\\textbf{10312.0}          &    5687.1815  &     1989.389     &     2.859  &         0.004        &     1787.620    &     9586.743     \\\\\n",
       "\\textbf{10332.0}          &    5449.6824  &     3161.561     &     1.724  &         0.085        &     -747.547    &     1.16e+04     \\\\\n",
       "\\textbf{1036.0}           &     136.4403  &     1974.368     &     0.069  &         0.945        &    -3733.677    &     4006.558     \\\\\n",
       "\\textbf{10374.0}          &    3180.1289  &     1901.625     &     1.672  &         0.094        &     -547.398    &     6907.656     \\\\\n",
       "\\textbf{10386.0}          &     159.6047  &     1864.898     &     0.086  &         0.932        &    -3495.932    &     3815.142     \\\\\n",
       "\\textbf{10391.0}          &   -1304.6837  &     1874.653     &    -0.696  &         0.486        &    -4979.341    &     2369.974     \\\\\n",
       "\\textbf{10407.0}          &   -4510.2417  &     1971.110     &    -2.288  &         0.022        &    -8373.973    &     -646.510     \\\\\n",
       "\\textbf{10420.0}          &    2147.9748  &     1868.417     &     1.150  &         0.250        &    -1514.459    &     5810.409     \\\\\n",
       "\\textbf{10422.0}          &     827.9390  &     1978.150     &     0.419  &         0.676        &    -3049.591    &     4705.469     \\\\\n",
       "\\textbf{10426.0}          &    4101.8945  &     2116.924     &     1.938  &         0.053        &      -47.657    &     8251.446     \\\\\n",
       "\\textbf{10441.0}          &    5431.0637  &     1976.162     &     2.748  &         0.006        &     1557.431    &     9304.697     \\\\\n",
       "\\textbf{1045.0}           &   -4396.3123  &     2140.993     &    -2.053  &         0.040        &    -8593.045    &     -199.580     \\\\\n",
       "\\textbf{10453.0}          &     890.5197  &     1868.292     &     0.477  &         0.634        &    -2771.669    &     4552.709     \\\\\n",
       "\\textbf{10482.0}          &   -2.681e+04  &     2430.093     &   -11.033  &         0.000        &    -3.16e+04    &     -2.2e+04     \\\\\n",
       "\\textbf{10498.0}          &    4526.9593  &     1955.572     &     2.315  &         0.021        &      693.685    &     8360.234     \\\\\n",
       "\\textbf{10499.0}          &   -2289.7921  &     2219.374     &    -1.032  &         0.302        &    -6640.164    &     2060.580     \\\\\n",
       "\\textbf{10511.0}          &    5658.7336  &     2037.868     &     2.777  &         0.005        &     1664.146    &     9653.322     \\\\\n",
       "\\textbf{10519.0}          &   -1.549e+04  &     2455.731     &    -6.308  &         0.000        &    -2.03e+04    &    -1.07e+04     \\\\\n",
       "\\textbf{10530.0}          &    2896.2869  &     1891.579     &     1.531  &         0.126        &     -811.549    &     6604.123     \\\\\n",
       "\\textbf{10537.0}          &    1897.1505  &     2266.096     &     0.837  &         0.403        &    -2544.806    &     6339.107     \\\\\n",
       "\\textbf{10540.0}          &    3762.2070  &     1901.412     &     1.979  &         0.048        &       35.097    &     7489.317     \\\\\n",
       "\\textbf{10541.0}          &    5467.4940  &     2079.565     &     2.629  &         0.009        &     1391.172    &     9543.815     \\\\\n",
       "\\textbf{10550.0}          &     893.7277  &     5745.727     &     0.156  &         0.876        &    -1.04e+04    &     1.22e+04     \\\\\n",
       "\\textbf{10553.0}          &     548.1139  &     2037.324     &     0.269  &         0.788        &    -3445.408    &     4541.636     \\\\\n",
       "\\textbf{10565.0}          &    6318.3297  &     1997.411     &     3.163  &         0.002        &     2403.044    &     1.02e+04     \\\\\n",
       "\\textbf{10580.0}          &    5871.5698  &     1951.311     &     3.009  &         0.003        &     2046.647    &     9696.492     \\\\\n",
       "\\textbf{10581.0}          &    4012.5149  &     1958.587     &     2.049  &         0.041        &      173.331    &     7851.699     \\\\\n",
       "\\textbf{10588.0}          &   -9773.7159  &     2249.288     &    -4.345  &         0.000        &    -1.42e+04    &    -5364.705     \\\\\n",
       "\\textbf{10597.0}          &    4744.7244  &     1958.252     &     2.423  &         0.015        &      906.196    &     8583.252     \\\\\n",
       "\\textbf{10599.0}          &    5741.7786  &     1997.394     &     2.875  &         0.004        &     1826.527    &     9657.030     \\\\\n",
       "\\textbf{10618.0}          &    4077.2832  &     1936.830     &     2.105  &         0.035        &      280.747    &     7873.820     \\\\\n",
       "\\textbf{10656.0}          &    5841.9447  &     1995.542     &     2.927  &         0.003        &     1930.324    &     9753.566     \\\\\n",
       "\\textbf{10658.0}          &    5848.1400  &     1996.184     &     2.930  &         0.003        &     1935.260    &     9761.020     \\\\\n",
       "\\textbf{10726.0}          &    6950.3536  &     2107.448     &     3.298  &         0.001        &     2819.375    &     1.11e+04     \\\\\n",
       "\\textbf{10734.0}          &    1463.9948  &     2575.943     &     0.568  &         0.570        &    -3585.318    &     6513.308     \\\\\n",
       "\\textbf{10735.0}          &    4998.1236  &     1972.215     &     2.534  &         0.011        &     1132.227    &     8864.020     \\\\\n",
       "\\textbf{10764.0}          &    5669.5216  &     2036.622     &     2.784  &         0.005        &     1677.375    &     9661.668     \\\\\n",
       "\\textbf{10777.0}          &    5816.7134  &     1996.922     &     2.913  &         0.004        &     1902.387    &     9731.040     \\\\\n",
       "\\textbf{1078.0}           &   -7730.8735  &     3358.565     &    -2.302  &         0.021        &    -1.43e+04    &    -1147.480     \\\\\n",
       "\\textbf{10793.0}          &    4647.0994  &     1992.190     &     2.333  &         0.020        &      742.049    &     8552.150     \\\\\n",
       "\\textbf{10816.0}          &    4439.0498  &     1989.563     &     2.231  &         0.026        &      539.147    &     8338.952     \\\\\n",
       "\\textbf{10839.0}          &    5976.4480  &     1992.750     &     2.999  &         0.003        &     2070.298    &     9882.598     \\\\\n",
       "\\textbf{10857.0}          &   -4676.9149  &     1903.763     &    -2.457  &         0.014        &    -8408.633    &     -945.197     \\\\\n",
       "\\textbf{10867.0}          &     999.9244  &     2248.159     &     0.445  &         0.656        &    -3406.874    &     5406.722     \\\\\n",
       "\\textbf{10906.0}          &    4250.9368  &     1935.088     &     2.197  &         0.028        &      457.816    &     8044.057     \\\\\n",
       "\\textbf{10950.0}          &    5458.2452  &     3152.203     &     1.732  &         0.083        &     -720.641    &     1.16e+04     \\\\\n",
       "\\textbf{10983.0}          &   -1.863e+04  &     2228.504     &    -8.358  &         0.000        &     -2.3e+04    &    -1.43e+04     \\\\\n",
       "\\textbf{1099.0}           &    4642.8669  &     1949.594     &     2.381  &         0.017        &      821.311    &     8464.422     \\\\\n",
       "\\textbf{10991.0}          &   -3086.1868  &     2474.978     &    -1.247  &         0.212        &    -7937.590    &     1765.216     \\\\\n",
       "\\textbf{11012.0}          &     871.2078  &     1920.437     &     0.454  &         0.650        &    -2893.195    &     4635.611     \\\\\n",
       "\\textbf{11038.0}          &   -5938.0545  &     1948.190     &    -3.048  &         0.002        &    -9756.858    &    -2119.251     \\\\\n",
       "\\textbf{1104.0}           &    3785.6276  &     1920.141     &     1.972  &         0.049        &       21.806    &     7549.450     \\\\\n",
       "\\textbf{11060.0}          &    5844.4640  &     1996.187     &     2.928  &         0.003        &     1931.577    &     9757.351     \\\\\n",
       "\\textbf{11094.0}          &    4253.5271  &     1939.561     &     2.193  &         0.028        &      451.637    &     8055.417     \\\\\n",
       "\\textbf{11096.0}          &    2662.2524  &     1894.882     &     1.405  &         0.160        &    -1052.058    &     6376.563     \\\\\n",
       "\\textbf{11113.0}          &    4155.5135  &     2093.983     &     1.985  &         0.047        &       50.928    &     8260.099     \\\\\n",
       "\\textbf{1115.0}           &    2710.8281  &     1893.928     &     1.431  &         0.152        &    -1001.612    &     6423.268     \\\\\n",
       "\\textbf{11161.0}          &    -730.9916  &     1872.544     &    -0.390  &         0.696        &    -4401.516    &     2939.533     \\\\\n",
       "\\textbf{11225.0}          &    5313.3116  &     1972.168     &     2.694  &         0.007        &     1447.507    &     9179.116     \\\\\n",
       "\\textbf{11228.0}          &    5737.7176  &     1966.824     &     2.917  &         0.004        &     1882.388    &     9593.047     \\\\\n",
       "\\textbf{11236.0}          &    3217.8327  &     3357.319     &     0.958  &         0.338        &    -3363.118    &     9798.783     \\\\\n",
       "\\textbf{11288.0}          &   -1.427e+04  &     3389.850     &    -4.208  &         0.000        &    -2.09e+04    &    -7621.433     \\\\\n",
       "\\textbf{11312.0}          &   -1.228e+04  &     2504.240     &    -4.904  &         0.000        &    -1.72e+04    &    -7372.243     \\\\\n",
       "\\textbf{11361.0}          &    5916.8535  &     1999.738     &     2.959  &         0.003        &     1997.007    &     9836.700     \\\\\n",
       "\\textbf{11399.0}          &   -4233.5510  &     1889.819     &    -2.240  &         0.025        &    -7937.936    &     -529.166     \\\\\n",
       "\\textbf{114303.0}         &   -2.405e+04  &     5975.944     &    -4.024  &         0.000        &    -3.58e+04    &    -1.23e+04     \\\\\n",
       "\\textbf{11456.0}          &     957.2408  &     1997.016     &     0.479  &         0.632        &    -2957.270    &     4871.751     \\\\\n",
       "\\textbf{11465.0}          &     517.2102  &     1922.049     &     0.269  &         0.788        &    -3250.352    &     4284.772     \\\\\n",
       "\\textbf{11502.0}          &    3431.2725  &     1932.551     &     1.776  &         0.076        &     -356.876    &     7219.421     \\\\\n",
       "\\textbf{11506.0}          &     947.6186  &     1932.201     &     0.490  &         0.624        &    -2839.843    &     4735.081     \\\\\n",
       "\\textbf{11537.0}          &    5523.6159  &     1984.032     &     2.784  &         0.005        &     1634.555    &     9412.677     \\\\\n",
       "\\textbf{11566.0}          &    5765.0706  &     1990.728     &     2.896  &         0.004        &     1862.885    &     9667.256     \\\\\n",
       "\\textbf{11573.0}          &    4834.3260  &     1954.235     &     2.474  &         0.013        &     1003.672    &     8664.980     \\\\\n",
       "\\textbf{11580.0}          &    2486.7400  &     2370.999     &     1.049  &         0.294        &    -2160.846    &     7134.326     \\\\\n",
       "\\textbf{11600.0}          &    4550.4275  &     1936.905     &     2.349  &         0.019        &      753.745    &     8347.111     \\\\\n",
       "\\textbf{11609.0}          &    8343.8116  &     1974.738     &     4.225  &         0.000        &     4472.969    &     1.22e+04     \\\\\n",
       "\\textbf{1161.0}           &   -5580.3198  &     2088.450     &    -2.672  &         0.008        &    -9674.059    &    -1486.581     \\\\\n",
       "\\textbf{11636.0}          &   -1.131e+04  &     2113.276     &    -5.353  &         0.000        &    -1.55e+04    &    -7169.760     \\\\\n",
       "\\textbf{11670.0}          &    4993.7166  &     1966.121     &     2.540  &         0.011        &     1139.765    &     8847.668     \\\\\n",
       "\\textbf{11678.0}          &   -1.354e+04  &     2600.128     &    -5.209  &         0.000        &    -1.86e+04    &    -8447.179     \\\\\n",
       "\\textbf{11682.0}          &    5831.6292  &     2083.335     &     2.799  &         0.005        &     1747.917    &     9915.342     \\\\\n",
       "\\textbf{11694.0}          &    5737.1473  &     2145.189     &     2.674  &         0.007        &     1532.190    &     9942.104     \\\\\n",
       "\\textbf{11720.0}          &   -1.091e+04  &     3498.580     &    -3.118  &         0.002        &    -1.78e+04    &    -4051.663     \\\\\n",
       "\\textbf{11721.0}          &   -2.458e+04  &     3700.651     &    -6.642  &         0.000        &    -3.18e+04    &    -1.73e+04     \\\\\n",
       "\\textbf{11722.0}          &    4070.8052  &     2226.642     &     1.828  &         0.068        &     -293.815    &     8435.425     \\\\\n",
       "\\textbf{11793.0}          &    5046.0590  &     5772.818     &     0.874  &         0.382        &    -6269.706    &     1.64e+04     \\\\\n",
       "\\textbf{11797.0}          &    5953.8702  &     2368.831     &     2.513  &         0.012        &     1310.535    &     1.06e+04     \\\\\n",
       "\\textbf{11914.0}          &    4550.5342  &     2774.512     &     1.640  &         0.101        &     -888.010    &     9989.078     \\\\\n",
       "\\textbf{1209.0}           &    3185.2717  &     1883.937     &     1.691  &         0.091        &     -507.585    &     6878.128     \\\\\n",
       "\\textbf{12136.0}          &   -2.301e+04  &     3462.213     &    -6.645  &         0.000        &    -2.98e+04    &    -1.62e+04     \\\\\n",
       "\\textbf{12141.0}          &    5.769e+04  &     2498.058     &    23.094  &         0.000        &     5.28e+04    &     6.26e+04     \\\\\n",
       "\\textbf{12181.0}          &    1660.9963  &     3321.432     &     0.500  &         0.617        &    -4849.610    &     8171.602     \\\\\n",
       "\\textbf{12215.0}          &   -1.083e+04  &     2576.714     &    -4.203  &         0.000        &    -1.59e+04    &    -5778.209     \\\\\n",
       "\\textbf{12216.0}          &   -9691.8845  &     2653.284     &    -3.653  &         0.000        &    -1.49e+04    &    -4490.970     \\\\\n",
       "\\textbf{12256.0}          &   -5456.4670  &     2285.410     &    -2.388  &         0.017        &    -9936.282    &     -976.652     \\\\\n",
       "\\textbf{12262.0}          &    2818.3197  &     2200.443     &     1.281  &         0.200        &    -1494.946    &     7131.585     \\\\\n",
       "\\textbf{12389.0}          &    6567.9309  &     2276.230     &     2.885  &         0.004        &     2106.110    &      1.1e+04     \\\\\n",
       "\\textbf{1239.0}           &     536.9656  &     1867.287     &     0.288  &         0.774        &    -3123.254    &     4197.185     \\\\\n",
       "\\textbf{12390.0}          &    2560.0940  &     2595.506     &     0.986  &         0.324        &    -2527.565    &     7647.753     \\\\\n",
       "\\textbf{12397.0}          &     674.1276  &     4691.398     &     0.144  &         0.886        &    -8521.858    &     9870.113     \\\\\n",
       "\\textbf{1243.0}           &    1018.4517  &     2012.040     &     0.506  &         0.613        &    -2925.510    &     4962.414     \\\\\n",
       "\\textbf{12548.0}          &    4523.6422  &     2516.921     &     1.797  &         0.072        &     -409.977    &     9457.261     \\\\\n",
       "\\textbf{12570.0}          &    3242.1694  &     2287.571     &     1.417  &         0.156        &    -1241.883    &     7726.222     \\\\\n",
       "\\textbf{12581.0}          &    2985.4678  &     2491.175     &     1.198  &         0.231        &    -1897.685    &     7868.621     \\\\\n",
       "\\textbf{12592.0}          &     756.2421  &     2175.712     &     0.348  &         0.728        &    -3508.545    &     5021.030     \\\\\n",
       "\\textbf{12604.0}          &    -444.7947  &     4695.812     &    -0.095  &         0.925        &    -9649.432    &     8759.843     \\\\\n",
       "\\textbf{12656.0}          &    4983.3418  &     2258.664     &     2.206  &         0.027        &      555.953    &     9410.731     \\\\\n",
       "\\textbf{12679.0}          &    -2.38e+04  &     3491.395     &    -6.816  &         0.000        &    -3.06e+04    &     -1.7e+04     \\\\\n",
       "\\textbf{1278.0}           &    4713.1060  &     2015.046     &     2.339  &         0.019        &      763.253    &     8662.959     \\\\\n",
       "\\textbf{12788.0}          &   -2.537e+04  &     3749.737     &    -6.767  &         0.000        &    -3.27e+04    &     -1.8e+04     \\\\\n",
       "\\textbf{1283.0}           &    3009.1830  &     1901.024     &     1.583  &         0.113        &     -717.166    &     6735.532     \\\\\n",
       "\\textbf{1297.0}           &    4155.4428  &     1934.994     &     2.148  &         0.032        &      362.505    &     7948.381     \\\\\n",
       "\\textbf{12992.0}          &    5565.0936  &     2359.113     &     2.359  &         0.018        &      940.807    &     1.02e+04     \\\\\n",
       "\\textbf{13135.0}          &    -562.5675  &     2346.922     &    -0.240  &         0.811        &    -5162.958    &     4037.823     \\\\\n",
       "\\textbf{1327.0}           &   -1.224e+04  &     2470.053     &    -4.956  &         0.000        &    -1.71e+04    &    -7399.413     \\\\\n",
       "\\textbf{13282.0}          &   -2721.2310  &     4692.654     &    -0.580  &         0.562        &    -1.19e+04    &     6477.217     \\\\\n",
       "\\textbf{1334.0}           &   -1.811e+04  &     2965.443     &    -6.107  &         0.000        &    -2.39e+04    &    -1.23e+04     \\\\\n",
       "\\textbf{13351.0}          &    1502.3731  &     3123.311     &     0.481  &         0.631        &    -4619.879    &     7624.625     \\\\\n",
       "\\textbf{13365.0}          &   -3.106e+04  &     4404.453     &    -7.052  &         0.000        &    -3.97e+04    &    -2.24e+04     \\\\\n",
       "\\textbf{13369.0}          &    4498.0546  &     2323.950     &     1.936  &         0.053        &      -57.307    &     9053.416     \\\\\n",
       "\\textbf{13406.0}          &    5145.0388  &     2332.358     &     2.206  &         0.027        &      573.197    &     9716.880     \\\\\n",
       "\\textbf{13407.0}          &   -5843.5719  &     2354.762     &    -2.482  &         0.013        &    -1.05e+04    &    -1227.814     \\\\\n",
       "\\textbf{13417.0}          &    4967.9392  &     2428.751     &     2.045  &         0.041        &      207.149    &     9728.729     \\\\\n",
       "\\textbf{13525.0}          &   -1.318e+04  &     2754.299     &    -4.784  &         0.000        &    -1.86e+04    &    -7778.331     \\\\\n",
       "\\textbf{13554.0}          &    5611.4352  &     2447.232     &     2.293  &         0.022        &      814.420    &     1.04e+04     \\\\\n",
       "\\textbf{1359.0}           &   -2.247e+04  &     3632.149     &    -6.185  &         0.000        &    -2.96e+04    &    -1.53e+04     \\\\\n",
       "\\textbf{13623.0}          &    1210.0618  &     2353.506     &     0.514  &         0.607        &    -3403.234    &     5823.358     \\\\\n",
       "\\textbf{1372.0}           &   -1.121e+04  &     2350.270     &    -4.769  &         0.000        &    -1.58e+04    &    -6602.611     \\\\\n",
       "\\textbf{1380.0}           &   -7498.9924  &     1945.758     &    -3.854  &         0.000        &    -1.13e+04    &    -3684.956     \\\\\n",
       "\\textbf{13923.0}          &    4515.7166  &     2631.671     &     1.716  &         0.086        &     -642.833    &     9674.266     \\\\\n",
       "\\textbf{13932.0}          &    5289.0955  &     3397.307     &     1.557  &         0.120        &    -1370.239    &     1.19e+04     \\\\\n",
       "\\textbf{13941.0}          &   -1.168e+04  &     2772.846     &    -4.213  &         0.000        &    -1.71e+04    &    -6246.106     \\\\\n",
       "\\textbf{1397.0}           &    3250.9457  &     2199.079     &     1.478  &         0.139        &    -1059.646    &     7561.538     \\\\\n",
       "\\textbf{14064.0}          &      50.2213  &     2256.777     &     0.022  &         0.982        &    -4373.468    &     4473.911     \\\\\n",
       "\\textbf{14084.0}          &    5613.9947  &     2360.812     &     2.378  &         0.017        &      986.377    &     1.02e+04     \\\\\n",
       "\\textbf{14324.0}          &   -1.166e+04  &     2867.437     &    -4.067  &         0.000        &    -1.73e+04    &    -6039.865     \\\\\n",
       "\\textbf{14462.0}          &    4648.7560  &     2418.044     &     1.923  &         0.055        &      -91.046    &     9388.558     \\\\\n",
       "\\textbf{1447.0}           &    5310.8958  &     4144.973     &     1.281  &         0.200        &    -2813.998    &     1.34e+04     \\\\\n",
       "\\textbf{14593.0}          &    4720.7038  &     2542.214     &     1.857  &         0.063        &     -262.495    &     9703.902     \\\\\n",
       "\\textbf{14622.0}          &    5168.6326  &     8141.182     &     0.635  &         0.526        &    -1.08e+04    &     2.11e+04     \\\\\n",
       "\\textbf{1465.0}           &    1920.7060  &     2586.558     &     0.743  &         0.458        &    -3149.413    &     6990.825     \\\\\n",
       "\\textbf{1468.0}           &    6453.7636  &     2553.611     &     2.527  &         0.012        &     1448.225    &     1.15e+04     \\\\\n",
       "\\textbf{14897.0}          &    2124.2078  &     4695.488     &     0.452  &         0.651        &    -7079.795    &     1.13e+04     \\\\\n",
       "\\textbf{14954.0}          &    3534.6047  &     2492.565     &     1.418  &         0.156        &    -1351.272    &     8420.481     \\\\\n",
       "\\textbf{1496.0}           &    5981.1779  &     2001.187     &     2.989  &         0.003        &     2058.491    &     9903.865     \\\\\n",
       "\\textbf{15267.0}          &    3324.8172  &     2484.970     &     1.338  &         0.181        &    -1546.172    &     8195.807     \\\\\n",
       "\\textbf{15354.0}          &   -1237.4126  &     2579.186     &    -0.480  &         0.631        &    -6293.083    &     3818.258     \\\\\n",
       "\\textbf{1542.0}           &    3916.4533  &     1923.050     &     2.037  &         0.042        &      146.928    &     7685.978     \\\\\n",
       "\\textbf{15459.0}          &    3000.6691  &     2604.382     &     1.152  &         0.249        &    -2104.389    &     8105.728     \\\\\n",
       "\\textbf{1554.0}           &    5694.0640  &     1986.883     &     2.866  &         0.004        &     1799.414    &     9588.714     \\\\\n",
       "\\textbf{15708.0}          &   -3.218e+04  &     4715.431     &    -6.824  &         0.000        &    -4.14e+04    &    -2.29e+04     \\\\\n",
       "\\textbf{15711.0}          &    3230.4275  &     2509.289     &     1.287  &         0.198        &    -1688.231    &     8149.086     \\\\\n",
       "\\textbf{15761.0}          &    5213.4306  &     2949.338     &     1.768  &         0.077        &     -567.803    &      1.1e+04     \\\\\n",
       "\\textbf{1581.0}           &   -4841.1692  &     3034.031     &    -1.596  &         0.111        &    -1.08e+04    &     1106.079     \\\\\n",
       "\\textbf{1593.0}           &    4321.2884  &     1943.873     &     2.223  &         0.026        &      510.946    &     8131.630     \\\\\n",
       "\\textbf{1602.0}           &    1.022e+04  &     2061.778     &     4.959  &         0.000        &     6183.141    &     1.43e+04     \\\\\n",
       "\\textbf{1613.0}           &    4387.9538  &     1942.787     &     2.259  &         0.024        &      579.741    &     8196.166     \\\\\n",
       "\\textbf{16188.0}          &   -2185.7033  &     2725.376     &    -0.802  &         0.423        &    -7527.931    &     3156.524     \\\\\n",
       "\\textbf{1632.0}           &   -7123.0045  &     2129.102     &    -3.346  &         0.001        &    -1.13e+04    &    -2949.580     \\\\\n",
       "\\textbf{1633.0}           &    1749.7199  &     1877.093     &     0.932  &         0.351        &    -1929.720    &     5429.160     \\\\\n",
       "\\textbf{1635.0}           &   -2.416e+04  &     3591.521     &    -6.726  &         0.000        &    -3.12e+04    &    -1.71e+04     \\\\\n",
       "\\textbf{16401.0}          &   -7854.3757  &     2707.195     &    -2.901  &         0.004        &    -1.32e+04    &    -2547.785     \\\\\n",
       "\\textbf{16437.0}          &   -5618.1399  &     2978.211     &    -1.886  &         0.059        &    -1.15e+04    &      219.690     \\\\\n",
       "\\textbf{1651.0}           &   -2359.9296  &     1906.348     &    -1.238  &         0.216        &    -6096.715    &     1376.856     \\\\\n",
       "\\textbf{1655.0}           &    5918.5046  &     1996.516     &     2.964  &         0.003        &     2004.973    &     9832.036     \\\\\n",
       "\\textbf{1663.0}           &    7869.2121  &     2017.388     &     3.901  &         0.000        &     3914.768    &     1.18e+04     \\\\\n",
       "\\textbf{16710.0}          &   -2160.6819  &     2715.903     &    -0.796  &         0.426        &    -7484.341    &     3162.978     \\\\\n",
       "\\textbf{16729.0}          &   -4426.9439  &     2610.724     &    -1.696  &         0.090        &    -9544.433    &      690.545     \\\\\n",
       "\\textbf{1690.0}           &   -2.176e+04  &     2799.319     &    -7.773  &         0.000        &    -2.72e+04    &    -1.63e+04     \\\\\n",
       "\\textbf{1703.0}           &    3466.1648  &     1961.927     &     1.767  &         0.077        &     -379.566    &     7311.895     \\\\\n",
       "\\textbf{17202.0}          &    2006.6420  &     2588.229     &     0.775  &         0.438        &    -3066.754    &     7080.038     \\\\\n",
       "\\textbf{1722.0}           &    3116.2628  &     1993.580     &     1.563  &         0.118        &     -791.513    &     7024.039     \\\\\n",
       "\\textbf{1728.0}           &    5633.9310  &     1996.403     &     2.822  &         0.005        &     1720.621    &     9547.241     \\\\\n",
       "\\textbf{1743.0}           &    4381.9496  &     3153.520     &     1.390  &         0.165        &    -1799.518    &     1.06e+04     \\\\\n",
       "\\textbf{1754.0}           &    4698.8460  &     2045.572     &     2.297  &         0.022        &      689.156    &     8708.536     \\\\\n",
       "\\textbf{1762.0}           &    2996.5292  &     1967.402     &     1.523  &         0.128        &     -859.933    &     6852.991     \\\\\n",
       "\\textbf{1773.0}           &    3990.7601  &     1983.464     &     2.012  &         0.044        &      102.813    &     7878.707     \\\\\n",
       "\\textbf{1786.0}           &    -1.51e+04  &     2600.786     &    -5.807  &         0.000        &    -2.02e+04    &       -1e+04     \\\\\n",
       "\\textbf{18100.0}          &     684.5639  &     2575.351     &     0.266  &         0.790        &    -4363.588    &     5732.716     \\\\\n",
       "\\textbf{1820.0}           &   -1537.7552  &     1881.683     &    -0.817  &         0.414        &    -5226.194    &     2150.684     \\\\\n",
       "\\textbf{1848.0}           &   -1.003e+04  &     2495.059     &    -4.022  &         0.000        &    -1.49e+04    &    -5143.253     \\\\\n",
       "\\textbf{18654.0}          &    5071.6705  &     3705.654     &     1.369  &         0.171        &    -2192.080    &     1.23e+04     \\\\\n",
       "\\textbf{1875.0}           &   -3179.7872  &     4102.927     &    -0.775  &         0.438        &    -1.12e+04    &     4862.690     \\\\\n",
       "\\textbf{1884.0}           &    3490.0985  &     2017.841     &     1.730  &         0.084        &     -465.233    &     7445.430     \\\\\n",
       "\\textbf{1913.0}           &    2787.5754  &     1896.846     &     1.470  &         0.142        &     -930.584    &     6505.735     \\\\\n",
       "\\textbf{1919.0}           &    4405.2188  &     2080.640     &     2.117  &         0.034        &      326.790    &     8483.648     \\\\\n",
       "\\textbf{1920.0}           &     630.2583  &     1868.894     &     0.337  &         0.736        &    -3033.110    &     4293.627     \\\\\n",
       "\\textbf{1968.0}           &    4876.8374  &     1956.488     &     2.493  &         0.013        &     1041.769    &     8711.906     \\\\\n",
       "\\textbf{1976.0}           &    5531.6252  &     1926.111     &     2.872  &         0.004        &     1756.100    &     9307.150     \\\\\n",
       "\\textbf{1981.0}           &    4132.7220  &     1930.521     &     2.141  &         0.032        &      348.553    &     7916.891     \\\\\n",
       "\\textbf{1988.0}           &   -2144.8429  &     3071.256     &    -0.698  &         0.485        &    -8165.059    &     3875.373     \\\\\n",
       "\\textbf{1992.0}           &    5729.4776  &     1989.753     &     2.879  &         0.004        &     1829.202    &     9629.753     \\\\\n",
       "\\textbf{2008.0}           &    4293.8972  &     1920.835     &     2.235  &         0.025        &      528.714    &     8059.080     \\\\\n",
       "\\textbf{2033.0}           &    4877.5272  &     2537.714     &     1.922  &         0.055        &      -96.850    &     9851.904     \\\\\n",
       "\\textbf{2044.0}           &    1056.4120  &     1866.200     &     0.566  &         0.571        &    -2601.676    &     4714.500     \\\\\n",
       "\\textbf{2049.0}           &    4119.6311  &     1935.321     &     2.129  &         0.033        &      326.052    &     7913.210     \\\\\n",
       "\\textbf{2061.0}           &    5957.2341  &     2001.011     &     2.977  &         0.003        &     2034.892    &     9879.576     \\\\\n",
       "\\textbf{20779.0}          &    2.845e+04  &     2755.046     &    10.327  &         0.000        &     2.31e+04    &     3.39e+04     \\\\\n",
       "\\textbf{2085.0}           &   -2.111e+04  &     3328.065     &    -6.342  &         0.000        &    -2.76e+04    &    -1.46e+04     \\\\\n",
       "\\textbf{2086.0}           &    3094.3038  &     1884.339     &     1.642  &         0.101        &     -599.341    &     6787.949     \\\\\n",
       "\\textbf{2111.0}           &    2882.4529  &     1869.712     &     1.542  &         0.123        &     -782.519    &     6547.425     \\\\\n",
       "\\textbf{21204.0}          &   -7461.7244  &     2690.133     &    -2.774  &         0.006        &    -1.27e+04    &    -2188.579     \\\\\n",
       "\\textbf{21238.0}          &    2419.0385  &     2586.317     &     0.935  &         0.350        &    -2650.609    &     7488.686     \\\\\n",
       "\\textbf{2124.0}           &    5101.5536  &     2065.963     &     2.469  &         0.014        &     1051.894    &     9151.213     \\\\\n",
       "\\textbf{2146.0}           &    4601.4461  &     3060.832     &     1.503  &         0.133        &    -1398.337    &     1.06e+04     \\\\\n",
       "\\textbf{21496.0}          &   -3.386e+04  &     4726.783     &    -7.164  &         0.000        &    -4.31e+04    &    -2.46e+04     \\\\\n",
       "\\textbf{2154.0}           &    4044.6641  &     1933.674     &     2.092  &         0.036        &      254.314    &     7835.014     \\\\\n",
       "\\textbf{2176.0}           &    3.574e+04  &     2599.419     &    13.749  &         0.000        &     3.06e+04    &     4.08e+04     \\\\\n",
       "\\textbf{2188.0}           &    5672.6793  &     2033.203     &     2.790  &         0.005        &     1687.234    &     9658.124     \\\\\n",
       "\\textbf{2189.0}           &     148.4369  &     1959.794     &     0.076  &         0.940        &    -3693.113    &     3989.987     \\\\\n",
       "\\textbf{2220.0}           &    5015.6190  &     1965.133     &     2.552  &         0.011        &     1163.605    &     8867.633     \\\\\n",
       "\\textbf{22205.0}          &    5646.3189  &     2667.620     &     2.117  &         0.034        &      417.303    &     1.09e+04     \\\\\n",
       "\\textbf{2226.0}           &     814.4474  &     5745.805     &     0.142  &         0.887        &    -1.04e+04    &     1.21e+04     \\\\\n",
       "\\textbf{2230.0}           &    1301.6681  &     2174.922     &     0.598  &         0.550        &    -2961.572    &     5564.908     \\\\\n",
       "\\textbf{22325.0}          &   -1.485e+04  &     3411.237     &    -4.355  &         0.000        &    -2.15e+04    &    -8167.865     \\\\\n",
       "\\textbf{2255.0}           &    4579.6258  &     1972.396     &     2.322  &         0.020        &      713.374    &     8445.878     \\\\\n",
       "\\textbf{22619.0}          &    5145.3920  &     2652.925     &     1.940  &         0.052        &      -54.819    &     1.03e+04     \\\\\n",
       "\\textbf{2267.0}           &   -1.155e+04  &     2384.995     &    -4.844  &         0.000        &    -1.62e+04    &    -6878.030     \\\\\n",
       "\\textbf{22815.0}          &   -3121.7529  &     2602.541     &    -1.200  &         0.230        &    -8223.202    &     1979.696     \\\\\n",
       "\\textbf{2285.0}           &   -1.429e+04  &     2085.478     &    -6.854  &         0.000        &    -1.84e+04    &    -1.02e+04     \\\\\n",
       "\\textbf{2290.0}           &     468.8118  &     1942.062     &     0.241  &         0.809        &    -3337.979    &     4275.603     \\\\\n",
       "\\textbf{2295.0}           &    4937.7627  &     3362.044     &     1.469  &         0.142        &    -1652.450    &     1.15e+04     \\\\\n",
       "\\textbf{2316.0}           &   -1179.7652  &     2105.972     &    -0.560  &         0.575        &    -5307.850    &     2948.320     \\\\\n",
       "\\textbf{23220.0}          &    3943.3174  &     2769.146     &     1.424  &         0.154        &    -1484.709    &     9371.344     \\\\\n",
       "\\textbf{23224.0}          &   -2.318e+04  &     3768.793     &    -6.151  &         0.000        &    -3.06e+04    &    -1.58e+04     \\\\\n",
       "\\textbf{2343.0}           &   -1.672e+04  &     4434.085     &    -3.771  &         0.000        &    -2.54e+04    &    -8030.944     \\\\\n",
       "\\textbf{2352.0}           &    5331.8550  &     2194.582     &     2.430  &         0.015        &     1030.078    &     9633.632     \\\\\n",
       "\\textbf{23700.0}          &    -1.45e+04  &     3833.444     &    -3.784  &         0.000        &     -2.2e+04    &    -6990.455     \\\\\n",
       "\\textbf{2390.0}           &    5837.7262  &     1994.917     &     2.926  &         0.003        &     1927.330    &     9748.123     \\\\\n",
       "\\textbf{2393.0}           &    3446.0993  &     1910.184     &     1.804  &         0.071        &     -298.206    &     7190.405     \\\\\n",
       "\\textbf{2403.0}           &    2761.3152  &     3389.110     &     0.815  &         0.415        &    -3881.951    &     9404.582     \\\\\n",
       "\\textbf{2435.0}           &    7046.0132  &     1992.542     &     3.536  &         0.000        &     3140.271    &      1.1e+04     \\\\\n",
       "\\textbf{2444.0}           &    4135.4846  &     1943.015     &     2.128  &         0.033        &      326.825    &     7944.144     \\\\\n",
       "\\textbf{2448.0}           &    2405.0247  &     1886.047     &     1.275  &         0.202        &    -1291.967    &     6102.017     \\\\\n",
       "\\textbf{2469.0}           &    4508.6059  &     3694.083     &     1.220  &         0.222        &    -2732.462    &     1.17e+04     \\\\\n",
       "\\textbf{24720.0}          &    5465.7697  &     2965.238     &     1.843  &         0.065        &     -346.632    &     1.13e+04     \\\\\n",
       "\\textbf{24800.0}          &   -2.171e+04  &     4068.270     &    -5.338  &         0.000        &    -2.97e+04    &    -1.37e+04     \\\\\n",
       "\\textbf{2482.0}           &    5953.1001  &     2000.598     &     2.976  &         0.003        &     2031.568    &     9874.633     \\\\\n",
       "\\textbf{24969.0}          &    4786.0951  &     3134.337     &     1.527  &         0.127        &    -1357.770    &     1.09e+04     \\\\\n",
       "\\textbf{2498.0}           &   -8638.2868  &     2263.003     &    -3.817  &         0.000        &    -1.31e+04    &    -4202.394     \\\\\n",
       "\\textbf{2504.0}           &   -2210.6970  &     1925.030     &    -1.148  &         0.251        &    -5984.103    &     1562.709     \\\\\n",
       "\\textbf{2508.0}           &    5573.8713  &     2207.837     &     2.525  &         0.012        &     1246.113    &     9901.629     \\\\\n",
       "\\textbf{25124.0}          &    5829.1780  &     2947.556     &     1.978  &         0.048        &       51.438    &     1.16e+04     \\\\\n",
       "\\textbf{2518.0}           &    5596.9129  &     1996.457     &     2.803  &         0.005        &     1683.498    &     9510.328     \\\\\n",
       "\\textbf{25224.0}          &    6375.0727  &     8147.603     &     0.782  &         0.434        &    -9595.697    &     2.23e+04     \\\\\n",
       "\\textbf{25279.0}          &    2037.8755  &     2884.776     &     0.706  &         0.480        &    -3616.805    &     7692.556     \\\\\n",
       "\\textbf{2537.0}           &   -1.968e+04  &     3137.493     &    -6.274  &         0.000        &    -2.58e+04    &    -1.35e+04     \\\\\n",
       "\\textbf{2538.0}           &    4746.4598  &     2942.806     &     1.613  &         0.107        &    -1021.970    &     1.05e+04     \\\\\n",
       "\\textbf{25389.0}          &    4692.3411  &     4720.530     &     0.994  &         0.320        &    -4560.748    &     1.39e+04     \\\\\n",
       "\\textbf{2547.0}           &   -5269.2420  &     2189.996     &    -2.406  &         0.016        &    -9562.029    &     -976.455     \\\\\n",
       "\\textbf{2553.0}           &    3522.0549  &     1913.312     &     1.841  &         0.066        &     -228.381    &     7272.491     \\\\\n",
       "\\textbf{2574.0}           &   -1609.3684  &     2578.575     &    -0.624  &         0.533        &    -6663.840    &     3445.104     \\\\\n",
       "\\textbf{25747.0}          &    4900.2402  &     3135.869     &     1.563  &         0.118        &    -1246.628    &      1.1e+04     \\\\\n",
       "\\textbf{2577.0}           &    3835.8963  &     1915.962     &     2.002  &         0.045        &       80.266    &     7591.527     \\\\\n",
       "\\textbf{2593.0}           &    4531.3516  &     1964.152     &     2.307  &         0.021        &      681.259    &     8381.444     \\\\\n",
       "\\textbf{2596.0}           &   -3239.4189  &     1914.573     &    -1.692  &         0.091        &    -6992.327    &      513.489     \\\\\n",
       "\\textbf{2663.0}           &    9161.1645  &     1997.545     &     4.586  &         0.000        &     5245.617    &     1.31e+04     \\\\\n",
       "\\textbf{2771.0}           &   -2662.9682  &     1897.529     &    -1.403  &         0.161        &    -6382.467    &     1056.530     \\\\\n",
       "\\textbf{2787.0}           &    4574.8892  &     1953.949     &     2.341  &         0.019        &      744.797    &     8404.981     \\\\\n",
       "\\textbf{2797.0}           &   -1.562e+04  &     2718.260     &    -5.746  &         0.000        &    -2.09e+04    &    -1.03e+04     \\\\\n",
       "\\textbf{2802.0}           &    5276.4913  &     1972.838     &     2.675  &         0.007        &     1409.374    &     9143.609     \\\\\n",
       "\\textbf{2817.0}           &     129.1254  &     1954.573     &     0.066  &         0.947        &    -3702.190    &     3960.441     \\\\\n",
       "\\textbf{28678.0}          &   -2.352e+04  &     4034.940     &    -5.828  &         0.000        &    -3.14e+04    &    -1.56e+04     \\\\\n",
       "\\textbf{28701.0}          &    2830.2390  &     1909.723     &     1.482  &         0.138        &     -913.163    &     6573.641     \\\\\n",
       "\\textbf{28742.0}          &   -2.231e+04  &     3907.568     &    -5.710  &         0.000        &       -3e+04    &    -1.47e+04     \\\\\n",
       "\\textbf{2888.0}           &    5250.5375  &     2194.333     &     2.393  &         0.017        &      949.249    &     9551.826     \\\\\n",
       "\\textbf{2897.0}           &    5865.7959  &     2791.469     &     2.101  &         0.036        &      394.014    &     1.13e+04     \\\\\n",
       "\\textbf{2917.0}           &    -488.6654  &     1970.596     &    -0.248  &         0.804        &    -4351.390    &     3374.059     \\\\\n",
       "\\textbf{29392.0}          &   -9938.5533  &     3427.430     &    -2.900  &         0.004        &    -1.67e+04    &    -3220.172     \\\\\n",
       "\\textbf{2950.0}           &   -3.926e+04  &     5294.553     &    -7.415  &         0.000        &    -4.96e+04    &    -2.89e+04     \\\\\n",
       "\\textbf{2951.0}           &    5535.6905  &     2445.824     &     2.263  &         0.024        &      741.434    &     1.03e+04     \\\\\n",
       "\\textbf{2953.0}           &    4124.3201  &     1926.336     &     2.141  &         0.032        &      348.353    &     7900.287     \\\\\n",
       "\\textbf{2960.0}           &    3711.7939  &     2919.533     &     1.271  &         0.204        &    -2011.017    &     9434.605     \\\\\n",
       "\\textbf{2975.0}           &   -5691.6089  &     2000.969     &    -2.844  &         0.004        &    -9613.868    &    -1769.350     \\\\\n",
       "\\textbf{2982.0}           &    2898.8181  &     1904.850     &     1.522  &         0.128        &     -835.031    &     6632.667     \\\\\n",
       "\\textbf{2991.0}           &   -1.469e+04  &     2700.464     &    -5.440  &         0.000        &       -2e+04    &    -9396.901     \\\\\n",
       "\\textbf{3011.0}           &    -1.54e+04  &     2892.609     &    -5.325  &         0.000        &    -2.11e+04    &    -9732.571     \\\\\n",
       "\\textbf{3015.0}           &    5202.8797  &     1946.551     &     2.673  &         0.008        &     1387.288    &     9018.471     \\\\\n",
       "\\textbf{3026.0}           &    2363.1203  &     1933.109     &     1.222  &         0.222        &    -1426.122    &     6152.363     \\\\\n",
       "\\textbf{3031.0}           &   -2.915e+04  &     4370.172     &    -6.670  &         0.000        &    -3.77e+04    &    -2.06e+04     \\\\\n",
       "\\textbf{3062.0}           &    7463.7255  &     2096.541     &     3.560  &         0.000        &     3354.127    &     1.16e+04     \\\\\n",
       "\\textbf{3093.0}           &   -8806.6966  &     2541.664     &    -3.465  &         0.001        &    -1.38e+04    &    -3824.576     \\\\\n",
       "\\textbf{3107.0}           &    5301.6018  &     3708.550     &     1.430  &         0.153        &    -1967.826    &     1.26e+04     \\\\\n",
       "\\textbf{3121.0}           &    6374.5636  &     1949.454     &     3.270  &         0.001        &     2553.282    &     1.02e+04     \\\\\n",
       "\\textbf{3126.0}           &    5718.2200  &     1997.710     &     2.862  &         0.004        &     1802.348    &     9634.092     \\\\\n",
       "\\textbf{3144.0}           &    5.252e+04  &     2001.423     &    26.242  &         0.000        &     4.86e+04    &     5.64e+04     \\\\\n",
       "\\textbf{3156.0}           &    2456.8442  &     2357.562     &     1.042  &         0.297        &    -2164.401    &     7078.090     \\\\\n",
       "\\textbf{3157.0}           &    3843.2398  &     1921.376     &     2.000  &         0.045        &       76.997    &     7609.482     \\\\\n",
       "\\textbf{3170.0}           &    3315.0700  &     1871.969     &     1.771  &         0.077        &     -354.326    &     6984.466     \\\\\n",
       "\\textbf{3178.0}           &   -9202.2841  &     2296.026     &    -4.008  &         0.000        &    -1.37e+04    &    -4701.660     \\\\\n",
       "\\textbf{3206.0}           &    1499.4128  &     2197.761     &     0.682  &         0.495        &    -2808.596    &     5807.421     \\\\\n",
       "\\textbf{3229.0}           &   -1595.4902  &     1981.822     &    -0.805  &         0.421        &    -5480.219    &     2289.239     \\\\\n",
       "\\textbf{3235.0}           &    5792.5791  &     2148.702     &     2.696  &         0.007        &     1580.736    &        1e+04     \\\\\n",
       "\\textbf{3246.0}           &    3978.7541  &     1960.246     &     2.030  &         0.042        &      136.318    &     7821.190     \\\\\n",
       "\\textbf{3248.0}           &    5437.7381  &     1981.647     &     2.744  &         0.006        &     1553.353    &     9322.123     \\\\\n",
       "\\textbf{3282.0}           &   -2.539e+04  &     2993.946     &    -8.480  &         0.000        &    -3.13e+04    &    -1.95e+04     \\\\\n",
       "\\textbf{3362.0}           &   -4424.8740  &     2290.165     &    -1.932  &         0.053        &    -8914.011    &       64.263     \\\\\n",
       "\\textbf{3372.0}           &    4318.9152  &     2496.575     &     1.730  &         0.084        &     -574.822    &     9212.652     \\\\\n",
       "\\textbf{3422.0}           &    4207.4331  &     1922.122     &     2.189  &         0.029        &      439.727    &     7975.139     \\\\\n",
       "\\textbf{3497.0}           &   -1240.7329  &     1938.259     &    -0.640  &         0.522        &    -5040.069    &     2558.603     \\\\\n",
       "\\textbf{3502.0}           &   -4236.9220  &     1947.227     &    -2.176  &         0.030        &    -8053.837    &     -420.007     \\\\\n",
       "\\textbf{3504.0}           &    3789.2411  &     2756.174     &     1.375  &         0.169        &    -1613.357    &     9191.839     \\\\\n",
       "\\textbf{3505.0}           &    4392.3223  &     1969.602     &     2.230  &         0.026        &      531.547    &     8253.097     \\\\\n",
       "\\textbf{3532.0}           &    5565.8740  &     1875.178     &     2.968  &         0.003        &     1890.187    &     9241.561     \\\\\n",
       "\\textbf{3574.0}           &    6012.9124  &     4125.935     &     1.457  &         0.145        &    -2074.664    &     1.41e+04     \\\\\n",
       "\\textbf{3580.0}           &    -106.6962  &     1865.740     &    -0.057  &         0.954        &    -3763.884    &     3550.491     \\\\\n",
       "\\textbf{3612.0}           &    5404.8441  &     1968.058     &     2.746  &         0.006        &     1547.095    &     9262.593     \\\\\n",
       "\\textbf{3619.0}           &    3432.0264  &     1958.903     &     1.752  &         0.080        &     -407.777    &     7271.829     \\\\\n",
       "\\textbf{3622.0}           &    5707.4282  &     2035.863     &     2.803  &         0.005        &     1716.769    &     9698.088     \\\\\n",
       "\\textbf{3639.0}           &   -1.188e+04  &     2402.966     &    -4.945  &         0.000        &    -1.66e+04    &    -7172.555     \\\\\n",
       "\\textbf{3650.0}           &   -1277.6393  &     1876.659     &    -0.681  &         0.496        &    -4956.230    &     2400.951     \\\\\n",
       "\\textbf{3662.0}           &    5191.3064  &     1963.537     &     2.644  &         0.008        &     1342.420    &     9040.192     \\\\\n",
       "\\textbf{3734.0}           &   -1.673e+04  &     2578.502     &    -6.487  &         0.000        &    -2.18e+04    &    -1.17e+04     \\\\\n",
       "\\textbf{3735.0}           &    4888.1541  &     2405.370     &     2.032  &         0.042        &      173.194    &     9603.114     \\\\\n",
       "\\textbf{3761.0}           &     -65.0558  &     1868.881     &    -0.035  &         0.972        &    -3728.399    &     3598.287     \\\\\n",
       "\\textbf{3779.0}           &   -2.308e+04  &     3592.736     &    -6.423  &         0.000        &    -3.01e+04    &     -1.6e+04     \\\\\n",
       "\\textbf{3781.0}           &   -1.182e+04  &     2859.325     &    -4.135  &         0.000        &    -1.74e+04    &    -6219.574     \\\\\n",
       "\\textbf{3782.0}           &   -1.559e+04  &     2704.001     &    -5.767  &         0.000        &    -2.09e+04    &    -1.03e+04     \\\\\n",
       "\\textbf{3786.0}           &    1603.7968  &     1876.118     &     0.855  &         0.393        &    -2073.734    &     5281.327     \\\\\n",
       "\\textbf{3796.0}           &   -2.347e+04  &     3564.311     &    -6.585  &         0.000        &    -3.05e+04    &    -1.65e+04     \\\\\n",
       "\\textbf{3821.0}           &    4772.8156  &     1988.997     &     2.400  &         0.016        &      874.022    &     8671.609     \\\\\n",
       "\\textbf{3835.0}           &    1317.3590  &     1941.398     &     0.679  &         0.497        &    -2488.132    &     5122.850     \\\\\n",
       "\\textbf{3839.0}           &    2633.9853  &     2736.286     &     0.963  &         0.336        &    -2729.628    &     7997.598     \\\\\n",
       "\\textbf{3840.0}           &   -1.104e+04  &     2332.661     &    -4.732  &         0.000        &    -1.56e+04    &    -6465.851     \\\\\n",
       "\\textbf{3895.0}           &    5013.5276  &     1964.396     &     2.552  &         0.011        &     1162.958    &     8864.098     \\\\\n",
       "\\textbf{3908.0}           &   -5563.0210  &     3202.170     &    -1.737  &         0.082        &    -1.18e+04    &      713.810     \\\\\n",
       "\\textbf{3911.0}           &     171.9872  &     1867.380     &     0.092  &         0.927        &    -3488.414    &     3832.389     \\\\\n",
       "\\textbf{3917.0}           &    4625.0950  &     2046.362     &     2.260  &         0.024        &      613.856    &     8636.334     \\\\\n",
       "\\textbf{3946.0}           &    5677.6978  &     1979.027     &     2.869  &         0.004        &     1798.448    &     9556.948     \\\\\n",
       "\\textbf{3971.0}           &    4524.4845  &     2039.435     &     2.218  &         0.027        &      526.824    &     8522.146     \\\\\n",
       "\\textbf{3980.0}           &    1.262e+04  &     1944.351     &     6.490  &         0.000        &     8806.770    &     1.64e+04     \\\\\n",
       "\\textbf{4034.0}           &    2604.7223  &     1919.380     &     1.357  &         0.175        &    -1157.609    &     6367.053     \\\\\n",
       "\\textbf{4036.0}           &    5998.2727  &     1991.432     &     3.012  &         0.003        &     2094.707    &     9901.838     \\\\\n",
       "\\textbf{4040.0}           &   -7036.6871  &     2084.929     &    -3.375  &         0.001        &    -1.11e+04    &    -2949.851     \\\\\n",
       "\\textbf{4058.0}           &    3892.1315  &     1894.530     &     2.054  &         0.040        &      178.510    &     7605.753     \\\\\n",
       "\\textbf{4060.0}           &   -6065.5675  &     1995.358     &    -3.040  &         0.002        &    -9976.830    &    -2154.305     \\\\\n",
       "\\textbf{4062.0}           &    7447.8593  &     1975.356     &     3.770  &         0.000        &     3575.805    &     1.13e+04     \\\\\n",
       "\\textbf{4077.0}           &     742.9194  &     3319.868     &     0.224  &         0.823        &    -5764.621    &     7250.460     \\\\\n",
       "\\textbf{4087.0}           &   -1.351e+04  &     2493.437     &    -5.417  &         0.000        &    -1.84e+04    &    -8618.829     \\\\\n",
       "\\textbf{4091.0}           &    1759.3905  &     2464.664     &     0.714  &         0.475        &    -3071.795    &     6590.576     \\\\\n",
       "\\textbf{4127.0}           &   -4322.5466  &     1951.395     &    -2.215  &         0.027        &    -8147.632    &     -497.461     \\\\\n",
       "\\textbf{4138.0}           &    5682.9745  &     2650.040     &     2.144  &         0.032        &      488.418    &     1.09e+04     \\\\\n",
       "\\textbf{4162.0}           &    3507.8926  &     2476.673     &     1.416  &         0.157        &    -1346.834    &     8362.619     \\\\\n",
       "\\textbf{4186.0}           &    5845.8717  &     1990.790     &     2.936  &         0.003        &     1943.565    &     9748.179     \\\\\n",
       "\\textbf{4194.0}           &    5650.9262  &     2261.961     &     2.498  &         0.012        &     1217.075    &     1.01e+04     \\\\\n",
       "\\textbf{4199.0}           &   -9816.5403  &     2308.153     &    -4.253  &         0.000        &    -1.43e+04    &    -5292.144     \\\\\n",
       "\\textbf{4213.0}           &    5592.3913  &     1956.548     &     2.858  &         0.004        &     1757.203    &     9427.579     \\\\\n",
       "\\textbf{4222.0}           &   -1.659e+04  &     2824.967     &    -5.873  &         0.000        &    -2.21e+04    &    -1.11e+04     \\\\\n",
       "\\textbf{4223.0}           &    4893.6615  &     1952.224     &     2.507  &         0.012        &     1066.951    &     8720.372     \\\\\n",
       "\\textbf{4251.0}           &    5444.7655  &     1979.581     &     2.750  &         0.006        &     1564.429    &     9325.102     \\\\\n",
       "\\textbf{4265.0}           &    1455.3156  &     2103.531     &     0.692  &         0.489        &    -2667.984    &     5578.616     \\\\\n",
       "\\textbf{4274.0}           &    4445.5496  &     2042.920     &     2.176  &         0.030        &      441.058    &     8450.041     \\\\\n",
       "\\textbf{4321.0}           &    6899.4013  &     1935.835     &     3.564  &         0.000        &     3104.815    &     1.07e+04     \\\\\n",
       "\\textbf{4335.0}           &    2788.0453  &     3348.558     &     0.833  &         0.405        &    -3775.732    &     9351.823     \\\\\n",
       "\\textbf{4340.0}           &     322.0616  &     1919.799     &     0.168  &         0.867        &    -3441.090    &     4085.214     \\\\\n",
       "\\textbf{4371.0}           &    4044.4674  &     2006.643     &     2.016  &         0.044        &      111.085    &     7977.850     \\\\\n",
       "\\textbf{4415.0}           &    3792.7307  &     2025.507     &     1.872  &         0.061        &     -177.628    &     7763.090     \\\\\n",
       "\\textbf{4450.0}           &     651.3093  &     1869.294     &     0.348  &         0.728        &    -3012.844    &     4315.462     \\\\\n",
       "\\textbf{4476.0}           &   -1.046e+04  &     2474.144     &    -4.229  &         0.000        &    -1.53e+04    &    -5612.746     \\\\\n",
       "\\textbf{4510.0}           &    1627.3410  &     1913.421     &     0.850  &         0.395        &    -2123.309    &     5377.991     \\\\\n",
       "\\textbf{4520.0}           &    5937.2607  &     1999.939     &     2.969  &         0.003        &     2017.020    &     9857.502     \\\\\n",
       "\\textbf{4551.0}           &    2011.0006  &     8126.310     &     0.247  &         0.805        &    -1.39e+04    &     1.79e+04     \\\\\n",
       "\\textbf{4568.0}           &    5735.7827  &     2092.053     &     2.742  &         0.006        &     1634.982    &     9836.583     \\\\\n",
       "\\textbf{4579.0}           &    5244.1572  &     1969.212     &     2.663  &         0.008        &     1384.147    &     9104.167     \\\\\n",
       "\\textbf{4585.0}           &    5451.5411  &     1980.169     &     2.753  &         0.006        &     1570.052    &     9333.030     \\\\\n",
       "\\textbf{4595.0}           &     825.6749  &     1869.696     &     0.442  &         0.659        &    -2839.266    &     4490.616     \\\\\n",
       "\\textbf{4600.0}           &   -1.452e+04  &     2749.307     &    -5.283  &         0.000        &    -1.99e+04    &    -9134.802     \\\\\n",
       "\\textbf{4607.0}           &    5303.1652  &     1973.784     &     2.687  &         0.007        &     1434.194    &     9172.137     \\\\\n",
       "\\textbf{4608.0}           &   -1.353e+04  &     2568.616     &    -5.266  &         0.000        &    -1.86e+04    &    -8491.257     \\\\\n",
       "\\textbf{4622.0}           &    2677.0338  &     1897.125     &     1.411  &         0.158        &    -1041.673    &     6395.740     \\\\\n",
       "\\textbf{4623.0}           &    3391.8093  &     1957.474     &     1.733  &         0.083        &     -445.193    &     7228.812     \\\\\n",
       "\\textbf{4768.0}           &    4839.4631  &     1992.062     &     2.429  &         0.015        &      934.663    &     8744.263     \\\\\n",
       "\\textbf{4771.0}           &    5621.5296  &     1986.692     &     2.830  &         0.005        &     1727.256    &     9515.803     \\\\\n",
       "\\textbf{4800.0}           &    3659.2201  &     1983.621     &     1.845  &         0.065        &     -229.034    &     7547.474     \\\\\n",
       "\\textbf{4802.0}           &    5746.4121  &     1994.123     &     2.882  &         0.004        &     1837.572    &     9655.253     \\\\\n",
       "\\textbf{4807.0}           &    5517.8740  &     2029.245     &     2.719  &         0.007        &     1540.188    &     9495.560     \\\\\n",
       "\\textbf{4839.0}           &   -1.031e+05  &     3500.927     &   -29.447  &         0.000        &     -1.1e+05    &    -9.62e+04     \\\\\n",
       "\\textbf{4843.0}           &   -2.287e+04  &     3564.575     &    -6.415  &         0.000        &    -2.99e+04    &    -1.59e+04     \\\\\n",
       "\\textbf{4881.0}           &    3706.2439  &     1917.009     &     1.933  &         0.053        &      -51.440    &     7463.927     \\\\\n",
       "\\textbf{4900.0}           &    1000.4167  &     1870.814     &     0.535  &         0.593        &    -2666.715    &     4667.549     \\\\\n",
       "\\textbf{4926.0}           &    4140.5831  &     1941.422     &     2.133  &         0.033        &      335.046    &     7946.120     \\\\\n",
       "\\textbf{4941.0}           &    4091.6995  &     1976.008     &     2.071  &         0.038        &      218.367    &     7965.032     \\\\\n",
       "\\textbf{4961.0}           &   -1.199e+04  &     3211.013     &    -3.733  &         0.000        &    -1.83e+04    &    -5692.774     \\\\\n",
       "\\textbf{4988.0}           &    1.075e+04  &     1962.935     &     5.479  &         0.000        &     6906.404    &     1.46e+04     \\\\\n",
       "\\textbf{4993.0}           &    5886.6423  &     1995.984     &     2.949  &         0.003        &     1974.155    &     9799.130     \\\\\n",
       "\\textbf{5018.0}           &    2417.8721  &     1918.350     &     1.260  &         0.208        &    -1342.441    &     6178.185     \\\\\n",
       "\\textbf{5020.0}           &   -1.963e+04  &     3495.660     &    -5.615  &         0.000        &    -2.65e+04    &    -1.28e+04     \\\\\n",
       "\\textbf{5027.0}           &   -1337.9866  &     1877.638     &    -0.713  &         0.476        &    -5018.496    &     2342.523     \\\\\n",
       "\\textbf{5032.0}           &    5205.9697  &     1978.250     &     2.632  &         0.009        &     1328.244    &     9083.696     \\\\\n",
       "\\textbf{5043.0}           &    -567.0871  &     1868.788     &    -0.303  &         0.762        &    -4230.248    &     3096.074     \\\\\n",
       "\\textbf{5046.0}           &   -7653.5292  &     1954.591     &    -3.916  &         0.000        &    -1.15e+04    &    -3822.179     \\\\\n",
       "\\textbf{5047.0}           &    4.078e+04  &     2885.953     &    14.131  &         0.000        &     3.51e+04    &     4.64e+04     \\\\\n",
       "\\textbf{5065.0}           &    4941.7237  &     2322.907     &     2.127  &         0.033        &      388.407    &     9495.041     \\\\\n",
       "\\textbf{5071.0}           &    6412.1070  &     2386.808     &     2.686  &         0.007        &     1733.533    &     1.11e+04     \\\\\n",
       "\\textbf{5073.0}           &   -1.551e+05  &     5742.807     &   -27.011  &         0.000        &    -1.66e+05    &    -1.44e+05     \\\\\n",
       "\\textbf{5087.0}           &    -957.8256  &     1866.085     &    -0.513  &         0.608        &    -4615.689    &     2700.037     \\\\\n",
       "\\textbf{5109.0}           &    5582.2600  &     1994.067     &     2.799  &         0.005        &     1673.530    &     9490.991     \\\\\n",
       "\\textbf{5116.0}           &     -1.6e+04  &     2941.916     &    -5.438  &         0.000        &    -2.18e+04    &    -1.02e+04     \\\\\n",
       "\\textbf{5122.0}           &   -1072.8236  &     1874.063     &    -0.572  &         0.567        &    -4746.326    &     2600.678     \\\\\n",
       "\\textbf{5134.0}           &   -1667.6530  &     2041.682     &    -0.817  &         0.414        &    -5669.719    &     2334.412     \\\\\n",
       "\\textbf{5142.0}           &    1099.1601  &     2718.240     &     0.404  &         0.686        &    -4229.080    &     6427.400     \\\\\n",
       "\\textbf{5165.0}           &   -3068.5101  &     2287.462     &    -1.341  &         0.180        &    -7552.348    &     1415.328     \\\\\n",
       "\\textbf{5169.0}           &    1.418e+04  &     1905.089     &     7.441  &         0.000        &     1.04e+04    &     1.79e+04     \\\\\n",
       "\\textbf{5174.0}           &    -338.4584  &     2257.668     &    -0.150  &         0.881        &    -4763.894    &     4086.977     \\\\\n",
       "\\textbf{5179.0}           &    4324.9441  &     1929.858     &     2.241  &         0.025        &      542.075    &     8107.814     \\\\\n",
       "\\textbf{5181.0}           &    5908.2768  &     2044.014     &     2.891  &         0.004        &     1901.641    &     9914.913     \\\\\n",
       "\\textbf{5187.0}           &    5103.3022  &     2413.305     &     2.115  &         0.034        &      372.790    &     9833.814     \\\\\n",
       "\\textbf{5229.0}           &   -2728.6144  &     1878.659     &    -1.452  &         0.146        &    -6411.124    &      953.895     \\\\\n",
       "\\textbf{5234.0}           &   -2638.3547  &     2073.913     &    -1.272  &         0.203        &    -6703.598    &     1426.889     \\\\\n",
       "\\textbf{5237.0}           &    4042.4159  &     1926.390     &     2.098  &         0.036        &      266.343    &     7818.489     \\\\\n",
       "\\textbf{5252.0}           &    4353.2552  &     1935.470     &     2.249  &         0.025        &      559.384    &     8147.126     \\\\\n",
       "\\textbf{5254.0}           &    4947.5640  &     1959.680     &     2.525  &         0.012        &     1106.238    &     8788.890     \\\\\n",
       "\\textbf{5306.0}           &    5223.3645  &     1926.246     &     2.712  &         0.007        &     1447.576    &     8999.153     \\\\\n",
       "\\textbf{5338.0}           &    5419.2139  &     1972.267     &     2.748  &         0.006        &     1553.215    &     9285.213     \\\\\n",
       "\\textbf{5377.0}           &    5667.0751  &     1999.675     &     2.834  &         0.005        &     1747.352    &     9586.799     \\\\\n",
       "\\textbf{5439.0}           &    3642.6006  &     1943.054     &     1.875  &         0.061        &     -166.137    &     7451.338     \\\\\n",
       "\\textbf{5456.0}           &    5815.5450  &     1997.849     &     2.911  &         0.004        &     1899.401    &     9731.689     \\\\\n",
       "\\textbf{5464.0}           &    4177.8920  &     2529.093     &     1.652  &         0.099        &     -779.587    &     9135.371     \\\\\n",
       "\\textbf{5476.0}           &    4116.7352  &     1916.475     &     2.148  &         0.032        &      360.099    &     7873.372     \\\\\n",
       "\\textbf{5492.0}           &   -2.361e+04  &     3487.541     &    -6.770  &         0.000        &    -3.04e+04    &    -1.68e+04     \\\\\n",
       "\\textbf{5496.0}           &    4462.4952  &     1946.019     &     2.293  &         0.022        &      647.947    &     8277.043     \\\\\n",
       "\\textbf{5505.0}           &    5452.9185  &     1991.805     &     2.738  &         0.006        &     1548.621    &     9357.216     \\\\\n",
       "\\textbf{5518.0}           &    4339.3471  &     2255.519     &     1.924  &         0.054        &      -81.877    &     8760.571     \\\\\n",
       "\\textbf{5520.0}           &     925.9037  &     1870.957     &     0.495  &         0.621        &    -2741.509    &     4593.317     \\\\\n",
       "\\textbf{5545.0}           &    5919.0229  &     2093.288     &     2.828  &         0.005        &     1815.801    &        1e+04     \\\\\n",
       "\\textbf{5568.0}           &    8396.0701  &     1986.866     &     4.226  &         0.000        &     4501.454    &     1.23e+04     \\\\\n",
       "\\textbf{5569.0}           &    5771.3735  &     2040.638     &     2.828  &         0.005        &     1771.356    &     9771.391     \\\\\n",
       "\\textbf{5578.0}           &    4779.1578  &     1944.786     &     2.457  &         0.014        &      967.026    &     8591.289     \\\\\n",
       "\\textbf{5581.0}           &    4469.9235  &     1926.308     &     2.320  &         0.020        &      694.012    &     8245.835     \\\\\n",
       "\\textbf{5589.0}           &   -1927.8189  &     1891.860     &    -1.019  &         0.308        &    -5636.205    &     1780.567     \\\\\n",
       "\\textbf{5597.0}           &    7286.7965  &     2664.850     &     2.734  &         0.006        &     2063.210    &     1.25e+04     \\\\\n",
       "\\textbf{5606.0}           &   -2.267e+04  &     2493.658     &    -9.091  &         0.000        &    -2.76e+04    &    -1.78e+04     \\\\\n",
       "\\textbf{5639.0}           &    6295.8147  &     1979.720     &     3.180  &         0.001        &     2415.206    &     1.02e+04     \\\\\n",
       "\\textbf{5667.0}           &   -3292.9448  &     1911.719     &    -1.723  &         0.085        &    -7040.259    &      454.369     \\\\\n",
       "\\textbf{5690.0}           &    5906.3927  &     1992.994     &     2.964  &         0.003        &     1999.765    &     9813.020     \\\\\n",
       "\\textbf{5709.0}           &    4522.8440  &     1985.573     &     2.278  &         0.023        &      630.762    &     8414.926     \\\\\n",
       "\\textbf{5726.0}           &    3228.0209  &     1900.051     &     1.699  &         0.089        &     -496.422    &     6952.464     \\\\\n",
       "\\textbf{5764.0}           &    4431.4327  &     1906.041     &     2.325  &         0.020        &      695.249    &     8167.616     \\\\\n",
       "\\textbf{5772.0}           &    5377.1085  &     1982.015     &     2.713  &         0.007        &     1492.002    &     9262.215     \\\\\n",
       "\\textbf{5860.0}           &   -2.679e+04  &     2826.641     &    -9.476  &         0.000        &    -3.23e+04    &    -2.12e+04     \\\\\n",
       "\\textbf{5878.0}           &    7425.3294  &     1919.565     &     3.868  &         0.000        &     3662.635    &     1.12e+04     \\\\\n",
       "\\textbf{5903.0}           &   -4768.1203  &     1959.194     &    -2.434  &         0.015        &    -8608.495    &     -927.746     \\\\\n",
       "\\textbf{5905.0}           &    4697.2879  &     1997.567     &     2.352  &         0.019        &      781.697    &     8612.879     \\\\\n",
       "\\textbf{5959.0}           &    2939.7289  &     1981.699     &     1.483  &         0.138        &     -944.758    &     6824.215     \\\\\n",
       "\\textbf{6008.0}           &    2.481e+04  &     2046.226     &    12.126  &         0.000        &     2.08e+04    &     2.88e+04     \\\\\n",
       "\\textbf{6034.0}           &    5442.7672  &     2146.456     &     2.536  &         0.011        &     1235.326    &     9650.208     \\\\\n",
       "\\textbf{6035.0}           &   -8409.7028  &     2697.021     &    -3.118  &         0.002        &    -1.37e+04    &    -3123.055     \\\\\n",
       "\\textbf{6036.0}           &   -7472.8788  &     2066.166     &    -3.617  &         0.000        &    -1.15e+04    &    -3422.821     \\\\\n",
       "\\textbf{6039.0}           &    4438.9665  &     1939.722     &     2.288  &         0.022        &      636.761    &     8241.172     \\\\\n",
       "\\textbf{6044.0}           &    5696.2156  &     2218.601     &     2.567  &         0.010        &     1347.357    &        1e+04     \\\\\n",
       "\\textbf{6066.0}           &    4522.6985  &     3708.109     &     1.220  &         0.223        &    -2745.864    &     1.18e+04     \\\\\n",
       "\\textbf{6078.0}           &    4583.2113  &     1876.629     &     2.442  &         0.015        &      904.681    &     8261.742     \\\\\n",
       "\\textbf{6081.0}           &   -2.271e+04  &     2967.263     &    -7.655  &         0.000        &    -2.85e+04    &    -1.69e+04     \\\\\n",
       "\\textbf{60893.0}          &   -2.908e+04  &     4955.987     &    -5.867  &         0.000        &    -3.88e+04    &    -1.94e+04     \\\\\n",
       "\\textbf{6097.0}           &    3760.7454  &     1897.673     &     1.982  &         0.048        &       40.963    &     7480.528     \\\\\n",
       "\\textbf{6102.0}           &    4171.7942  &     1983.047     &     2.104  &         0.035        &      284.665    &     8058.924     \\\\\n",
       "\\textbf{6104.0}           &   -4584.6433  &     2028.696     &    -2.260  &         0.024        &    -8561.253    &     -608.034     \\\\\n",
       "\\textbf{6109.0}           &   -8077.6585  &     2138.562     &    -3.777  &         0.000        &    -1.23e+04    &    -3885.691     \\\\\n",
       "\\textbf{6127.0}           &   -1505.8472  &     2298.475     &    -0.655  &         0.512        &    -6011.272    &     2999.578     \\\\\n",
       "\\textbf{61552.0}          &   -1.362e+04  &     3834.535     &    -3.552  &         0.000        &    -2.11e+04    &    -6103.565     \\\\\n",
       "\\textbf{6158.0}           &     298.7446  &     2034.574     &     0.147  &         0.883        &    -3689.387    &     4286.876     \\\\\n",
       "\\textbf{6171.0}           &    4291.1173  &     1929.504     &     2.224  &         0.026        &      508.941    &     8073.293     \\\\\n",
       "\\textbf{61780.0}          &    4798.9194  &     4126.512     &     1.163  &         0.245        &    -3289.788    &     1.29e+04     \\\\\n",
       "\\textbf{6207.0}           &    3546.3191  &     1912.662     &     1.854  &         0.064        &     -202.843    &     7295.481     \\\\\n",
       "\\textbf{6214.0}           &    5904.7161  &     1998.878     &     2.954  &         0.003        &     1986.555    &     9822.878     \\\\\n",
       "\\textbf{6216.0}           &    4469.4616  &     1954.080     &     2.287  &         0.022        &      639.113    &     8299.810     \\\\\n",
       "\\textbf{62221.0}          &    4902.3632  &     4128.231     &     1.188  &         0.235        &    -3189.715    &      1.3e+04     \\\\\n",
       "\\textbf{6259.0}           &   -8574.6540  &     2515.585     &    -3.409  &         0.001        &    -1.35e+04    &    -3643.654     \\\\\n",
       "\\textbf{62599.0}          &    1.506e+04  &     4752.723     &     3.168  &         0.002        &     5740.036    &     2.44e+04     \\\\\n",
       "\\textbf{6266.0}           &    -325.6656  &     2944.721     &    -0.111  &         0.912        &    -6097.849    &     5446.518     \\\\\n",
       "\\textbf{6268.0}           &    2348.9509  &     1999.256     &     1.175  &         0.240        &    -1569.951    &     6267.853     \\\\\n",
       "\\textbf{6288.0}           &    4423.1470  &     1927.638     &     2.295  &         0.022        &      644.630    &     8201.664     \\\\\n",
       "\\textbf{6297.0}           &    6017.0597  &     2097.504     &     2.869  &         0.004        &     1905.574    &     1.01e+04     \\\\\n",
       "\\textbf{6307.0}           &   -1.416e+04  &     2842.308     &    -4.984  &         0.000        &    -1.97e+04    &    -8593.247     \\\\\n",
       "\\textbf{6313.0}           &    3846.2387  &     3365.323     &     1.143  &         0.253        &    -2750.402    &     1.04e+04     \\\\\n",
       "\\textbf{6314.0}           &    4175.3038  &     1933.465     &     2.159  &         0.031        &      385.363    &     7965.244     \\\\\n",
       "\\textbf{6326.0}           &    -418.4821  &     1867.144     &    -0.224  &         0.823        &    -4078.422    &     3241.458     \\\\\n",
       "\\textbf{6349.0}           &    4087.5453  &     1931.523     &     2.116  &         0.034        &      301.411    &     7873.679     \\\\\n",
       "\\textbf{6357.0}           &    5379.8847  &     2123.978     &     2.533  &         0.011        &     1216.504    &     9543.265     \\\\\n",
       "\\textbf{6375.0}           &    1.029e+04  &     1969.737     &     5.226  &         0.000        &     6432.411    &     1.42e+04     \\\\\n",
       "\\textbf{6376.0}           &    5217.1985  &     1984.313     &     2.629  &         0.009        &     1327.586    &     9106.811     \\\\\n",
       "\\textbf{6379.0}           &    1.094e+04  &     5644.764     &     1.938  &         0.053        &     -127.842    &      2.2e+04     \\\\\n",
       "\\textbf{6386.0}           &    6123.9357  &     1995.295     &     3.069  &         0.002        &     2212.798    &        1e+04     \\\\\n",
       "\\textbf{6403.0}           &   -2065.0828  &     1925.200     &    -1.073  &         0.283        &    -5838.822    &     1708.656     \\\\\n",
       "\\textbf{6410.0}           &    5872.2443  &     1997.358     &     2.940  &         0.003        &     1957.062    &     9787.427     \\\\\n",
       "\\textbf{6416.0}           &   -5951.1656  &     2108.075     &    -2.823  &         0.005        &    -1.01e+04    &    -1818.959     \\\\\n",
       "\\textbf{6424.0}           &    4710.5828  &     1957.869     &     2.406  &         0.016        &      872.807    &     8548.359     \\\\\n",
       "\\textbf{6433.0}           &    3397.3386  &     1908.228     &     1.780  &         0.075        &     -343.133    &     7137.810     \\\\\n",
       "\\textbf{6435.0}           &    6910.7687  &     1921.540     &     3.596  &         0.000        &     3144.204    &     1.07e+04     \\\\\n",
       "\\textbf{6492.0}           &    3766.6941  &     1921.662     &     1.960  &         0.050        &       -0.110    &     7533.499     \\\\\n",
       "\\textbf{6497.0}           &   -1.443e+04  &     2629.279     &    -5.487  &         0.000        &    -1.96e+04    &    -9271.690     \\\\\n",
       "\\textbf{6500.0}           &    -996.0686  &     8129.378     &    -0.123  &         0.902        &    -1.69e+04    &     1.49e+04     \\\\\n",
       "\\textbf{6509.0}           &    4907.2634  &     1957.900     &     2.506  &         0.012        &     1069.426    &     8745.101     \\\\\n",
       "\\textbf{6527.0}           &    5893.6325  &     2368.628     &     2.488  &         0.013        &     1250.694    &     1.05e+04     \\\\\n",
       "\\textbf{6528.0}           &    2253.1849  &     2111.822     &     1.067  &         0.286        &    -1886.367    &     6392.737     \\\\\n",
       "\\textbf{6531.0}           &   -1258.6150  &     1957.951     &    -0.643  &         0.520        &    -5096.552    &     2579.322     \\\\\n",
       "\\textbf{6532.0}           &    -325.0480  &     1919.895     &    -0.169  &         0.866        &    -4088.389    &     3438.293     \\\\\n",
       "\\textbf{6543.0}           &    5694.8539  &     1988.134     &     2.864  &         0.004        &     1797.753    &     9591.955     \\\\\n",
       "\\textbf{6548.0}           &    4672.1866  &     1956.259     &     2.388  &         0.017        &      837.566    &     8506.807     \\\\\n",
       "\\textbf{6550.0}           &    5613.9605  &     2208.789     &     2.542  &         0.011        &     1284.335    &     9943.586     \\\\\n",
       "\\textbf{6552.0}           &    4852.5307  &     2141.141     &     2.266  &         0.023        &      655.508    &     9049.553     \\\\\n",
       "\\textbf{6565.0}           &    4161.7340  &     2167.386     &     1.920  &         0.055        &      -86.733    &     8410.201     \\\\\n",
       "\\textbf{6571.0}           &    5142.2645  &     1973.100     &     2.606  &         0.009        &     1274.632    &     9009.897     \\\\\n",
       "\\textbf{6573.0}           &    4804.7906  &     1948.741     &     2.466  &         0.014        &      984.906    &     8624.675     \\\\\n",
       "\\textbf{6641.0}           &    -399.9826  &     4068.185     &    -0.098  &         0.922        &    -8374.359    &     7574.394     \\\\\n",
       "\\textbf{6649.0}           &    6153.3134  &     1992.015     &     3.089  &         0.002        &     2248.604    &     1.01e+04     \\\\\n",
       "\\textbf{6730.0}           &   -4085.8431  &     3204.549     &    -1.275  &         0.202        &    -1.04e+04    &     2195.650     \\\\\n",
       "\\textbf{6731.0}           &    4182.8424  &     1932.897     &     2.164  &         0.030        &      394.016    &     7971.669     \\\\\n",
       "\\textbf{6742.0}           &    2591.0039  &     4098.503     &     0.632  &         0.527        &    -5442.802    &     1.06e+04     \\\\\n",
       "\\textbf{6745.0}           &    5954.5814  &     2001.155     &     2.976  &         0.003        &     2031.957    &     9877.205     \\\\\n",
       "\\textbf{6756.0}           &    3836.5715  &     1919.730     &     1.998  &         0.046        &       73.554    &     7599.589     \\\\\n",
       "\\textbf{6765.0}           &   -1.681e+04  &     2683.691     &    -6.264  &         0.000        &    -2.21e+04    &    -1.16e+04     \\\\\n",
       "\\textbf{6768.0}           &    6256.1243  &     2034.818     &     3.075  &         0.002        &     2267.515    &     1.02e+04     \\\\\n",
       "\\textbf{6774.0}           &   -2.614e+04  &     3202.997     &    -8.162  &         0.000        &    -3.24e+04    &    -1.99e+04     \\\\\n",
       "\\textbf{6797.0}           &    5646.2077  &     2455.892     &     2.299  &         0.022        &      832.217    &     1.05e+04     \\\\\n",
       "\\textbf{6803.0}           &    4701.3774  &     1956.186     &     2.403  &         0.016        &      866.900    &     8535.855     \\\\\n",
       "\\textbf{6821.0}           &    4844.1439  &     1967.004     &     2.463  &         0.014        &      988.461    &     8699.827     \\\\\n",
       "\\textbf{6830.0}           &    2159.2106  &     1877.370     &     1.150  &         0.250        &    -1520.773    &     5839.195     \\\\\n",
       "\\textbf{6845.0}           &    4949.2354  &     1959.344     &     2.526  &         0.012        &     1108.568    &     8789.903     \\\\\n",
       "\\textbf{6848.0}           &    2164.0008  &     2110.021     &     1.026  &         0.305        &    -1972.021    &     6300.022     \\\\\n",
       "\\textbf{6873.0}           &    1893.3294  &     2772.788     &     0.683  &         0.495        &    -3541.835    &     7328.494     \\\\\n",
       "\\textbf{6900.0}           &    5171.9492  &     1965.983     &     2.631  &         0.009        &     1318.268    &     9025.630     \\\\\n",
       "\\textbf{6908.0}           &    4728.0975  &     1945.157     &     2.431  &         0.015        &      915.239    &     8540.956     \\\\\n",
       "\\textbf{6994.0}           &     731.2960  &     1868.224     &     0.391  &         0.695        &    -2930.759    &     4393.351     \\\\\n",
       "\\textbf{7045.0}           &   -1.134e+04  &     2736.825     &    -4.145  &         0.000        &    -1.67e+04    &    -5978.329     \\\\\n",
       "\\textbf{7065.0}           &    1.014e+04  &     1996.162     &     5.081  &         0.000        &     6230.132    &     1.41e+04     \\\\\n",
       "\\textbf{7085.0}           &    8188.2669  &     1985.465     &     4.124  &         0.000        &     4296.397    &     1.21e+04     \\\\\n",
       "\\textbf{7107.0}           &    5537.7463  &     2145.524     &     2.581  &         0.010        &     1332.133    &     9743.359     \\\\\n",
       "\\textbf{7116.0}           &    5979.0806  &     1967.201     &     3.039  &         0.002        &     2123.013    &     9835.148     \\\\\n",
       "\\textbf{7117.0}           &    5277.3971  &     3150.678     &     1.675  &         0.094        &     -898.499    &     1.15e+04     \\\\\n",
       "\\textbf{7121.0}           &    3337.7779  &     1909.289     &     1.748  &         0.080        &     -404.773    &     7080.329     \\\\\n",
       "\\textbf{7127.0}           &     891.4098  &     2003.758     &     0.445  &         0.656        &    -3036.318    &     4819.137     \\\\\n",
       "\\textbf{7139.0}           &    5258.5464  &     1978.876     &     2.657  &         0.008        &     1379.592    &     9137.501     \\\\\n",
       "\\textbf{7146.0}           &    5334.3482  &     1973.436     &     2.703  &         0.007        &     1466.059    &     9202.638     \\\\\n",
       "\\textbf{7163.0}           &    7833.1917  &     1972.917     &     3.970  &         0.000        &     3965.918    &     1.17e+04     \\\\\n",
       "\\textbf{7180.0}           &    2429.4741  &     1932.129     &     1.257  &         0.209        &    -1357.847    &     6216.795     \\\\\n",
       "\\textbf{7183.0}           &    3618.8978  &     1911.469     &     1.893  &         0.058        &     -127.926    &     7365.721     \\\\\n",
       "\\textbf{7228.0}           &    1.164e+04  &     1895.440     &     6.142  &         0.000        &     7927.017    &     1.54e+04     \\\\\n",
       "\\textbf{7232.0}           &    -481.5654  &     3080.004     &    -0.156  &         0.876        &    -6518.928    &     5555.797     \\\\\n",
       "\\textbf{7250.0}           &    -921.2075  &     2179.821     &    -0.423  &         0.673        &    -5194.050    &     3351.634     \\\\\n",
       "\\textbf{7257.0}           &    1.726e+04  &     3178.570     &     5.429  &         0.000        &      1.1e+04    &     2.35e+04     \\\\\n",
       "\\textbf{7260.0}           &    4843.1503  &     1946.744     &     2.488  &         0.013        &     1027.180    &     8659.120     \\\\\n",
       "\\textbf{7267.0}           &      41.7571  &     2048.866     &     0.020  &         0.984        &    -3974.390    &     4057.905     \\\\\n",
       "\\textbf{7268.0}           &   -1.684e+04  &     2894.155     &    -5.819  &         0.000        &    -2.25e+04    &    -1.12e+04     \\\\\n",
       "\\textbf{7281.0}           &    5210.4769  &     2932.058     &     1.777  &         0.076        &     -536.886    &      1.1e+04     \\\\\n",
       "\\textbf{7291.0}           &    4834.4826  &     1946.833     &     2.483  &         0.013        &     1018.340    &     8650.625     \\\\\n",
       "\\textbf{7343.0}           &   -1.104e+04  &     3484.277     &    -3.168  &         0.002        &    -1.79e+04    &    -4207.797     \\\\\n",
       "\\textbf{7346.0}           &   -8749.8624  &     2173.692     &    -4.025  &         0.000        &     -1.3e+04    &    -4489.034     \\\\\n",
       "\\textbf{7401.0}           &    5949.0922  &     1998.542     &     2.977  &         0.003        &     2031.589    &     9866.596     \\\\\n",
       "\\textbf{7409.0}           &    5543.7513  &     1967.066     &     2.818  &         0.005        &     1687.948    &     9399.555     \\\\\n",
       "\\textbf{7420.0}           &    1119.1778  &     1871.032     &     0.598  &         0.550        &    -2548.382    &     4786.737     \\\\\n",
       "\\textbf{7435.0}           &     1.08e+04  &     2056.810     &     5.249  &         0.000        &     6765.135    &     1.48e+04     \\\\\n",
       "\\textbf{7466.0}           &    2379.6810  &     2005.849     &     1.186  &         0.236        &    -1552.144    &     6311.506     \\\\\n",
       "\\textbf{7486.0}           &   -1.506e+04  &     2681.366     &    -5.617  &         0.000        &    -2.03e+04    &    -9804.392     \\\\\n",
       "\\textbf{7503.0}           &    4986.3882  &     3674.490     &     1.357  &         0.175        &    -2216.274    &     1.22e+04     \\\\\n",
       "\\textbf{7506.0}           &    6434.6407  &     1941.012     &     3.315  &         0.001        &     2629.907    &     1.02e+04     \\\\\n",
       "\\textbf{7537.0}           &    3786.8460  &     1919.409     &     1.973  &         0.049        &       24.457    &     7549.235     \\\\\n",
       "\\textbf{7549.0}           &    4718.8137  &     1958.983     &     2.409  &         0.016        &      878.854    &     8558.774     \\\\\n",
       "\\textbf{7554.0}           &    4578.5389  &     1957.554     &     2.339  &         0.019        &      741.380    &     8415.698     \\\\\n",
       "\\textbf{7557.0}           &    4507.4862  &     1991.371     &     2.264  &         0.024        &      604.041    &     8410.932     \\\\\n",
       "\\textbf{7585.0}           &     -1.1e+04  &     3197.194     &    -3.439  &         0.001        &    -1.73e+04    &    -4728.826     \\\\\n",
       "\\textbf{7602.0}           &    3499.7827  &     1912.556     &     1.830  &         0.067        &     -249.172    &     7248.737     \\\\\n",
       "\\textbf{7620.0}           &   -2958.4318  &     2133.959     &    -1.386  &         0.166        &    -7141.376    &     1224.513     \\\\\n",
       "\\textbf{7636.0}           &    3958.3364  &     1925.327     &     2.056  &         0.040        &      184.349    &     7732.324     \\\\\n",
       "\\textbf{7646.0}           &    4582.9805  &     1943.067     &     2.359  &         0.018        &      774.218    &     8391.743     \\\\\n",
       "\\textbf{7658.0}           &    3684.4239  &     1924.022     &     1.915  &         0.056        &      -87.005    &     7455.853     \\\\\n",
       "\\textbf{7683.0}           &    6082.7650  &     2154.264     &     2.824  &         0.005        &     1860.020    &     1.03e+04     \\\\\n",
       "\\textbf{7685.0}           &    4896.4405  &     2183.537     &     2.242  &         0.025        &      616.315    &     9176.566     \\\\\n",
       "\\textbf{7692.0}           &   -1913.9549  &     1879.822     &    -1.018  &         0.309        &    -5598.744    &     1770.834     \\\\\n",
       "\\textbf{7762.0}           &    5606.3133  &     1975.729     &     2.838  &         0.005        &     1733.527    &     9479.099     \\\\\n",
       "\\textbf{7772.0}           &   -6987.3601  &     2071.660     &    -3.373  &         0.001        &     -1.1e+04    &    -2926.532     \\\\\n",
       "\\textbf{7773.0}           &    4826.5255  &     1952.344     &     2.472  &         0.013        &      999.579    &     8653.472     \\\\\n",
       "\\textbf{7777.0}           &    -743.5441  &     1870.869     &    -0.397  &         0.691        &    -4410.785    &     2923.696     \\\\\n",
       "\\textbf{7835.0}           &    5910.7386  &     1990.271     &     2.970  &         0.003        &     2009.449    &     9812.028     \\\\\n",
       "\\textbf{7873.0}           &   -1.392e+04  &     2577.000     &    -5.402  &         0.000        &     -1.9e+04    &    -8869.376     \\\\\n",
       "\\textbf{7883.0}           &    4965.3718  &     1956.885     &     2.537  &         0.011        &     1129.525    &     8801.219     \\\\\n",
       "\\textbf{7904.0}           &    4206.8758  &     1987.093     &     2.117  &         0.034        &      311.816    &     8101.936     \\\\\n",
       "\\textbf{7906.0}           &    7648.2490  &     1991.172     &     3.841  &         0.000        &     3745.192    &     1.16e+04     \\\\\n",
       "\\textbf{7921.0}           &    6255.4850  &     1976.128     &     3.166  &         0.002        &     2381.917    &     1.01e+04     \\\\\n",
       "\\textbf{7923.0}           &    4897.9116  &     2083.404     &     2.351  &         0.019        &      814.065    &     8981.758     \\\\\n",
       "\\textbf{7935.0}           &    1663.1797  &     1882.307     &     0.884  &         0.377        &    -2026.482    &     5352.841     \\\\\n",
       "\\textbf{7938.0}           &    4331.9445  &     1973.788     &     2.195  &         0.028        &      462.965    &     8200.925     \\\\\n",
       "\\textbf{7985.0}           &   -2.095e+04  &     2840.387     &    -7.374  &         0.000        &    -2.65e+04    &    -1.54e+04     \\\\\n",
       "\\textbf{8014.0}           &    1818.9112  &     1985.535     &     0.916  &         0.360        &    -2073.095    &     5710.917     \\\\\n",
       "\\textbf{8030.0}           &    5271.2842  &     1953.316     &     2.699  &         0.007        &     1442.433    &     9100.136     \\\\\n",
       "\\textbf{8046.0}           &    -1.41e+04  &     2617.753     &    -5.385  &         0.000        &    -1.92e+04    &    -8964.658     \\\\\n",
       "\\textbf{8047.0}           &    4897.3794  &     2783.776     &     1.759  &         0.079        &     -559.323    &     1.04e+04     \\\\\n",
       "\\textbf{8062.0}           &    2869.1377  &     1944.285     &     1.476  &         0.140        &     -942.012    &     6680.287     \\\\\n",
       "\\textbf{8068.0}           &   -1.295e+04  &     2223.415     &    -5.825  &         0.000        &    -1.73e+04    &    -8593.051     \\\\\n",
       "\\textbf{8087.0}           &       -2e+04  &     3143.524     &    -6.361  &         0.000        &    -2.62e+04    &    -1.38e+04     \\\\\n",
       "\\textbf{8095.0}           &    5818.9744  &     1999.918     &     2.910  &         0.004        &     1898.774    &     9739.174     \\\\\n",
       "\\textbf{8096.0}           &    4312.5818  &     1936.679     &     2.227  &         0.026        &      516.341    &     8108.822     \\\\\n",
       "\\textbf{8109.0}           &    5654.5465  &     1988.576     &     2.844  &         0.004        &     1756.580    &     9552.513     \\\\\n",
       "\\textbf{8123.0}           &    5053.8975  &     1967.357     &     2.569  &         0.010        &     1197.522    &     8910.273     \\\\\n",
       "\\textbf{8150.0}           &    5768.0364  &     1996.610     &     2.889  &         0.004        &     1854.320    &     9681.753     \\\\\n",
       "\\textbf{8163.0}           &    5755.4269  &     1988.504     &     2.894  &         0.004        &     1857.600    &     9653.254     \\\\\n",
       "\\textbf{8176.0}           &   -8448.5963  &     2569.276     &    -3.288  &         0.001        &    -1.35e+04    &    -3412.353     \\\\\n",
       "\\textbf{8202.0}           &    3871.5628  &     2017.547     &     1.919  &         0.055        &      -83.192    &     7826.318     \\\\\n",
       "\\textbf{8214.0}           &    3792.6248  &     1974.945     &     1.920  &         0.055        &      -78.623    &     7663.873     \\\\\n",
       "\\textbf{8215.0}           &    2763.3451  &     2113.139     &     1.308  &         0.191        &    -1378.788    &     6905.478     \\\\\n",
       "\\textbf{8219.0}           &    5537.6888  &     1990.441     &     2.782  &         0.005        &     1636.065    &     9439.313     \\\\\n",
       "\\textbf{8247.0}           &    4708.5506  &     1944.332     &     2.422  &         0.015        &      897.310    &     8519.792     \\\\\n",
       "\\textbf{8253.0}           &   -1.809e+04  &     2823.040     &    -6.406  &         0.000        &    -2.36e+04    &    -1.26e+04     \\\\\n",
       "\\textbf{8290.0}           &    2764.7807  &     2079.920     &     1.329  &         0.184        &    -1312.238    &     6841.799     \\\\\n",
       "\\textbf{8293.0}           &    5899.3765  &     1996.449     &     2.955  &         0.003        &     1985.977    &     9812.776     \\\\\n",
       "\\textbf{8304.0}           &    5848.3913  &     1938.951     &     3.016  &         0.003        &     2047.698    &     9649.085     \\\\\n",
       "\\textbf{8334.0}           &    3024.1407  &     2008.596     &     1.506  &         0.132        &     -913.070    &     6961.352     \\\\\n",
       "\\textbf{8348.0}           &    5556.4878  &     1985.328     &     2.799  &         0.005        &     1664.886    &     9448.089     \\\\\n",
       "\\textbf{8357.0}           &    5079.4213  &     1960.665     &     2.591  &         0.010        &     1236.165    &     8922.678     \\\\\n",
       "\\textbf{8358.0}           &    4801.4051  &     1955.371     &     2.455  &         0.014        &      968.526    &     8634.284     \\\\\n",
       "\\textbf{8446.0}           &   -4093.8049  &     2374.182     &    -1.724  &         0.085        &    -8747.629    &      560.020     \\\\\n",
       "\\textbf{8460.0}           &    6216.4871  &     2366.258     &     2.627  &         0.009        &     1578.194    &     1.09e+04     \\\\\n",
       "\\textbf{8463.0}           &    4407.1963  &     1951.703     &     2.258  &         0.024        &      581.507    &     8232.885     \\\\\n",
       "\\textbf{8479.0}           &    9641.6705  &     2783.822     &     3.463  &         0.001        &     4184.877    &     1.51e+04     \\\\\n",
       "\\textbf{8530.0}           &    -675.0843  &     3366.838     &    -0.201  &         0.841        &    -7274.693    &     5924.524     \\\\\n",
       "\\textbf{8536.0}           &    2752.2508  &     1915.052     &     1.437  &         0.151        &    -1001.596    &     6506.098     \\\\\n",
       "\\textbf{8543.0}           &    2.363e+04  &     2252.988     &    10.488  &         0.000        &     1.92e+04    &      2.8e+04     \\\\\n",
       "\\textbf{8549.0}           &   -9215.4301  &     2086.704     &    -4.416  &         0.000        &    -1.33e+04    &    -5125.114     \\\\\n",
       "\\textbf{8551.0}           &    5270.8432  &     1986.267     &     2.654  &         0.008        &     1377.401    &     9164.285     \\\\\n",
       "\\textbf{8559.0}           &   -3645.5456  &     2115.744     &    -1.723  &         0.085        &    -7792.785    &      501.694     \\\\\n",
       "\\textbf{8573.0}           &   -1.627e+04  &     2904.839     &    -5.601  &         0.000        &     -2.2e+04    &    -1.06e+04     \\\\\n",
       "\\textbf{8606.0}           &    7964.2030  &     1983.187     &     4.016  &         0.000        &     4076.799    &     1.19e+04     \\\\\n",
       "\\textbf{8607.0}           &    4875.7978  &     1965.120     &     2.481  &         0.013        &     1023.809    &     8727.787     \\\\\n",
       "\\textbf{8648.0}           &    4640.0866  &     1948.982     &     2.381  &         0.017        &      819.730    &     8460.443     \\\\\n",
       "\\textbf{8657.0}           &   -3330.0016  &     1917.841     &    -1.736  &         0.083        &    -7089.315    &      429.312     \\\\\n",
       "\\textbf{8675.0}           &    3299.0514  &     3674.268     &     0.898  &         0.369        &    -3903.176    &     1.05e+04     \\\\\n",
       "\\textbf{8681.0}           &    -917.2164  &     1872.610     &    -0.490  &         0.624        &    -4587.870    &     2753.437     \\\\\n",
       "\\textbf{8687.0}           &    1048.2952  &     2175.405     &     0.482  &         0.630        &    -3215.890    &     5312.480     \\\\\n",
       "\\textbf{8692.0}           &    3104.2442  &     1921.700     &     1.615  &         0.106        &     -662.635    &     6871.123     \\\\\n",
       "\\textbf{8699.0}           &    5391.0396  &     1978.249     &     2.725  &         0.006        &     1513.316    &     9268.763     \\\\\n",
       "\\textbf{8717.0}           &    5756.4000  &     1983.942     &     2.901  &         0.004        &     1867.517    &     9645.283     \\\\\n",
       "\\textbf{8759.0}           &   -4660.8382  &     1959.097     &    -2.379  &         0.017        &    -8501.021    &     -820.655     \\\\\n",
       "\\textbf{8762.0}           &    8546.7408  &     2018.068     &     4.235  &         0.000        &     4590.963    &     1.25e+04     \\\\\n",
       "\\textbf{8819.0}           &    5782.6825  &     1994.102     &     2.900  &         0.004        &     1873.883    &     9691.482     \\\\\n",
       "\\textbf{8850.0}           &    5389.3481  &     1979.050     &     2.723  &         0.006        &     1510.054    &     9268.642     \\\\\n",
       "\\textbf{8852.0}           &    5529.1546  &     1983.027     &     2.788  &         0.005        &     1642.064    &     9416.246     \\\\\n",
       "\\textbf{8859.0}           &    4530.6367  &     1955.107     &     2.317  &         0.021        &      698.275    &     8362.999     \\\\\n",
       "\\textbf{8867.0}           &   -3956.3934  &     2044.298     &    -1.935  &         0.053        &    -7963.586    &       50.799     \\\\\n",
       "\\textbf{8881.0}           &    3397.4168  &     1909.748     &     1.779  &         0.075        &     -346.034    &     7140.868     \\\\\n",
       "\\textbf{8958.0}           &    -333.9629  &     1869.765     &    -0.179  &         0.858        &    -3999.040    &     3331.114     \\\\\n",
       "\\textbf{8972.0}           &   -2.272e+04  &     3204.562     &    -7.089  &         0.000        &     -2.9e+04    &    -1.64e+04     \\\\\n",
       "\\textbf{8990.0}           &   -1.458e+04  &     2732.112     &    -5.335  &         0.000        &    -1.99e+04    &    -9219.887     \\\\\n",
       "\\textbf{9004.0}           &    5370.0245  &     2283.150     &     2.352  &         0.019        &      894.639    &     9845.410     \\\\\n",
       "\\textbf{9016.0}           &    2046.2952  &     1880.221     &     1.088  &         0.276        &    -1639.278    &     5731.868     \\\\\n",
       "\\textbf{9048.0}           &    2368.2290  &     1887.367     &     1.255  &         0.210        &    -1331.350    &     6067.808     \\\\\n",
       "\\textbf{9051.0}           &   -1.024e+04  &     2578.366     &    -3.973  &         0.000        &    -1.53e+04    &    -5190.757     \\\\\n",
       "\\textbf{9071.0}           &    5656.2012  &     1976.330     &     2.862  &         0.004        &     1782.237    &     9530.165     \\\\\n",
       "\\textbf{9112.0}           &   -1417.9474  &     1875.489     &    -0.756  &         0.450        &    -5094.244    &     2258.349     \\\\\n",
       "\\textbf{9114.0}           &    1647.2302  &     1954.192     &     0.843  &         0.399        &    -2183.339    &     5477.799     \\\\\n",
       "\\textbf{9132.0}           &    2094.4429  &     3339.339     &     0.627  &         0.531        &    -4451.263    &     8640.149     \\\\\n",
       "\\textbf{9173.0}           &    4739.7071  &     2316.199     &     2.046  &         0.041        &      199.540    &     9279.874     \\\\\n",
       "\\textbf{9180.0}           &    5631.9645  &     1997.191     &     2.820  &         0.005        &     1717.110    &     9546.819     \\\\\n",
       "\\textbf{9186.0}           &    4493.3047  &     1941.754     &     2.314  &         0.021        &      687.116    &     8299.493     \\\\\n",
       "\\textbf{9191.0}           &    2470.6002  &     3342.460     &     0.739  &         0.460        &    -4081.223    &     9022.424     \\\\\n",
       "\\textbf{9216.0}           &   -1536.8001  &     1882.692     &    -0.816  &         0.414        &    -5227.216    &     2153.615     \\\\\n",
       "\\textbf{9217.0}           &   -4464.1877  &     1954.492     &    -2.284  &         0.022        &    -8295.344    &     -633.031     \\\\\n",
       "\\textbf{9225.0}           &    6238.3665  &     1997.849     &     3.123  &         0.002        &     2322.222    &     1.02e+04     \\\\\n",
       "\\textbf{9230.0}           &    4381.5157  &     2512.608     &     1.744  &         0.081        &     -543.650    &     9306.681     \\\\\n",
       "\\textbf{9259.0}           &    5834.6453  &     1995.708     &     2.924  &         0.003        &     1922.697    &     9746.594     \\\\\n",
       "\\textbf{9293.0}           &    5936.9469  &     1988.853     &     2.985  &         0.003        &     2038.437    &     9835.457     \\\\\n",
       "\\textbf{9299.0}           &    2477.2047  &     2000.816     &     1.238  &         0.216        &    -1444.755    &     6399.164     \\\\\n",
       "\\textbf{9308.0}           &   -7746.5538  &     2112.622     &    -3.667  &         0.000        &    -1.19e+04    &    -3605.434     \\\\\n",
       "\\textbf{9311.0}           &   -2566.7239  &     3099.965     &    -0.828  &         0.408        &    -8643.214    &     3509.766     \\\\\n",
       "\\textbf{9313.0}           &   -2848.8965  &     1878.359     &    -1.517  &         0.129        &    -6530.819    &      833.026     \\\\\n",
       "\\textbf{9325.0}           &    4546.5205  &     1994.091     &     2.280  &         0.023        &      637.742    &     8455.299     \\\\\n",
       "\\textbf{9332.0}           &    5360.0205  &     1977.740     &     2.710  &         0.007        &     1483.294    &     9236.747     \\\\\n",
       "\\textbf{9340.0}           &   -4.099e+04  &     5543.358     &    -7.395  &         0.000        &    -5.19e+04    &    -3.01e+04     \\\\\n",
       "\\textbf{9372.0}           &    3247.9219  &     2267.303     &     1.433  &         0.152        &    -1196.400    &     7692.244     \\\\\n",
       "\\textbf{9411.0}           &    3231.7183  &     2071.422     &     1.560  &         0.119        &     -828.643    &     7292.080     \\\\\n",
       "\\textbf{9459.0}           &    -1.17e+04  &     3358.202     &    -3.484  &         0.000        &    -1.83e+04    &    -5118.586     \\\\\n",
       "\\textbf{9465.0}           &    1.242e+04  &     1955.181     &     6.351  &         0.000        &     8585.002    &     1.63e+04     \\\\\n",
       "\\textbf{9472.0}           &   -3879.1823  &     1935.402     &    -2.004  &         0.045        &    -7672.918    &      -85.446     \\\\\n",
       "\\textbf{9483.0}           &   -1.629e+04  &     2806.835     &    -5.803  &         0.000        &    -2.18e+04    &    -1.08e+04     \\\\\n",
       "\\textbf{9563.0}           &   -2.153e+04  &     3454.348     &    -6.233  &         0.000        &    -2.83e+04    &    -1.48e+04     \\\\\n",
       "\\textbf{9590.0}           &   -2660.5248  &     1904.053     &    -1.397  &         0.162        &    -6392.812    &     1071.763     \\\\\n",
       "\\textbf{9598.0}           &   -6031.7426  &     2877.301     &    -2.096  &         0.036        &    -1.17e+04    &     -391.713     \\\\\n",
       "\\textbf{9599.0}           &   -7906.4917  &     2130.319     &    -3.711  &         0.000        &    -1.21e+04    &    -3730.682     \\\\\n",
       "\\textbf{9602.0}           &   -5952.7429  &     3376.881     &    -1.763  &         0.078        &    -1.26e+04    &      666.552     \\\\\n",
       "\\textbf{9619.0}           &    3481.6826  &     1911.796     &     1.821  &         0.069        &     -265.783    &     7229.148     \\\\\n",
       "\\textbf{9643.0}           &    5433.9198  &     1992.138     &     2.728  &         0.006        &     1528.970    &     9338.869     \\\\\n",
       "\\textbf{9650.0}           &    5427.2120  &     1978.219     &     2.743  &         0.006        &     1549.546    &     9304.878     \\\\\n",
       "\\textbf{9653.0}           &   -1.512e+04  &     4541.405     &    -3.329  &         0.001        &     -2.4e+04    &    -6214.896     \\\\\n",
       "\\textbf{9667.0}           &    4520.3865  &     1946.279     &     2.323  &         0.020        &      705.329    &     8335.444     \\\\\n",
       "\\textbf{9698.0}           &    1367.7058  &     1873.827     &     0.730  &         0.465        &    -2305.333    &     5040.745     \\\\\n",
       "\\textbf{9699.0}           &    4281.7503  &     1896.184     &     2.258  &         0.024        &      564.888    &     7998.612     \\\\\n",
       "\\textbf{9719.0}           &   -8344.7727  &     2157.998     &    -3.867  &         0.000        &    -1.26e+04    &    -4114.707     \\\\\n",
       "\\textbf{9742.0}           &   -1.428e+04  &     2604.062     &    -5.484  &         0.000        &    -1.94e+04    &    -9176.000     \\\\\n",
       "\\textbf{9761.0}           &    5812.7072  &     1998.234     &     2.909  &         0.004        &     1895.809    &     9729.606     \\\\\n",
       "\\textbf{9771.0}           &   -4544.7951  &     1939.992     &    -2.343  &         0.019        &    -8347.529    &     -742.061     \\\\\n",
       "\\textbf{9772.0}           &    4591.0572  &     1934.805     &     2.373  &         0.018        &      798.490    &     8383.624     \\\\\n",
       "\\textbf{9778.0}           &    3626.0662  &     1898.512     &     1.910  &         0.056        &      -95.359    &     7347.491     \\\\\n",
       "\\textbf{9799.0}           &   -7061.2889  &     2203.249     &    -3.205  &         0.001        &    -1.14e+04    &    -2742.525     \\\\\n",
       "\\textbf{9815.0}           &    5697.1487  &     2072.046     &     2.750  &         0.006        &     1635.565    &     9758.733     \\\\\n",
       "\\textbf{9818.0}           &   -3.302e+04  &     2308.852     &   -14.300  &         0.000        &    -3.75e+04    &    -2.85e+04     \\\\\n",
       "\\textbf{9837.0}           &    5631.7113  &     1992.359     &     2.827  &         0.005        &     1726.329    &     9537.094     \\\\\n",
       "\\textbf{9922.0}           &   -8117.4344  &     2143.677     &    -3.787  &         0.000        &    -1.23e+04    &    -3915.440     \\\\\n",
       "\\textbf{9954.0}           &    4399.1214  &     2611.570     &     1.684  &         0.092        &     -720.027    &     9518.270     \\\\\n",
       "\\textbf{9963.0}           &    4786.1265  &     1984.341     &     2.412  &         0.016        &      896.461    &     8675.792     \\\\\n",
       "\\textbf{9988.0}           &    5320.7734  &     1998.236     &     2.663  &         0.008        &     1403.871    &     9237.676     \\\\\n",
       "\\textbf{9999.0}           &   -9357.5801  &     2209.351     &    -4.235  &         0.000        &    -1.37e+04    &    -5026.853     \\\\\n",
       "\\textbf{gspillsicIVX1982} &      -0.0100  &        0.098     &    -0.102  &         0.919        &       -0.201    &        0.181     \\\\\n",
       "\\textbf{gspillsicIVX1983} &      -0.0551  &        0.096     &    -0.575  &         0.565        &       -0.243    &        0.133     \\\\\n",
       "\\textbf{gspillsicIVX1984} &      -0.1174  &        0.094     &    -1.244  &         0.213        &       -0.302    &        0.068     \\\\\n",
       "\\textbf{gspillsicIVX1985} &      -0.1671  &        0.094     &    -1.774  &         0.076        &       -0.352    &        0.018     \\\\\n",
       "\\textbf{gspillsicIVX1986} &      -0.2239  &        0.095     &    -2.367  &         0.018        &       -0.409    &       -0.038     \\\\\n",
       "\\textbf{gspillsicIVX1987} &      -0.2562  &        0.095     &    -2.694  &         0.007        &       -0.443    &       -0.070     \\\\\n",
       "\\textbf{gspillsicIVX1988} &      -0.2953  &        0.096     &    -3.087  &         0.002        &       -0.483    &       -0.108     \\\\\n",
       "\\textbf{gspillsicIVX1989} &      -0.2942  &        0.097     &    -3.049  &         0.002        &       -0.483    &       -0.105     \\\\\n",
       "\\textbf{gspillsicIVX1990} &      -0.3209  &        0.098     &    -3.287  &         0.001        &       -0.512    &       -0.130     \\\\\n",
       "\\textbf{gspillsicIVX1991} &      -0.3112  &        0.099     &    -3.150  &         0.002        &       -0.505    &       -0.118     \\\\\n",
       "\\textbf{gspillsicIVX1992} &      -0.3633  &        0.100     &    -3.636  &         0.000        &       -0.559    &       -0.167     \\\\\n",
       "\\textbf{gspillsicIVX1993} &      -0.3810  &        0.101     &    -3.758  &         0.000        &       -0.580    &       -0.182     \\\\\n",
       "\\textbf{gspillsicIVX1994} &      -0.3920  &        0.103     &    -3.820  &         0.000        &       -0.593    &       -0.191     \\\\\n",
       "\\textbf{gspillsicIVX1995} &      -0.3683  &        0.105     &    -3.518  &         0.000        &       -0.574    &       -0.163     \\\\\n",
       "\\textbf{gspillsicIVX1996} &      -0.4071  &        0.107     &    -3.788  &         0.000        &       -0.618    &       -0.196     \\\\\n",
       "\\textbf{gspillsicIVX1997} &      -0.3834  &        0.111     &    -3.467  &         0.001        &       -0.600    &       -0.167     \\\\\n",
       "\\textbf{gspillsicIVX1998} &      -0.3315  &        0.114     &    -2.913  &         0.004        &       -0.554    &       -0.108     \\\\\n",
       "\\textbf{gspillsicIVX1999} &      -0.2970  &        0.117     &    -2.546  &         0.011        &       -0.526    &       -0.068     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 22689.763 & \\textbf{  Durbin-Watson:     } &       0.751    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 140364584.441  \\\\\n",
       "\\textbf{Skew:}          &   14.706  & \\textbf{  Prob(JB):          } &        0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  537.957  & \\textbf{  Cond. No.          } &    1.66e+18    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 5.75e-25. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.665\n",
       "Model:                            OLS   Adj. R-squared:                  0.641\n",
       "Method:                 Least Squares   F-statistic:                     28.37\n",
       "Date:                Mon, 14 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        16:58:05   Log-Likelihood:            -1.2191e+05\n",
       "No. Observations:               11736   AIC:                         2.454e+05\n",
       "Df Residuals:                   10968   BIC:                         2.510e+05\n",
       "Df Model:                         767                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -6154.3893    840.819     -7.320      0.000   -7802.546   -4506.232\n",
       "gspillsicIV          1.2284      0.186      6.618      0.000       0.865       1.592\n",
       "pat_count          -28.2387      1.740    -16.228      0.000     -31.650     -24.828\n",
       "rsales               1.0403      0.041     25.468      0.000       0.960       1.120\n",
       "rppent               0.5611      0.087      6.461      0.000       0.391       0.731\n",
       "emp                -11.1408      6.898     -1.615      0.106     -24.663       2.381\n",
       "rxrd                 8.6120      0.670     12.852      0.000       7.299       9.925\n",
       "1982               115.9480    620.546      0.187      0.852   -1100.434    1332.330\n",
       "1983               214.6565    615.322      0.349      0.727    -991.485    1420.798\n",
       "1984               -58.6229    611.072     -0.096      0.924   -1256.434    1139.188\n",
       "1985               -38.8501    609.748     -0.064      0.949   -1234.065    1156.365\n",
       "1986               -33.9845    606.213     -0.056      0.955   -1222.270    1154.301\n",
       "1987              -146.6154    603.157     -0.243      0.808   -1328.911    1035.681\n",
       "1988              -282.7993    602.981     -0.469      0.639   -1464.750     899.151\n",
       "1989              -153.5184    600.503     -0.256      0.798   -1330.613    1023.577\n",
       "1990              -482.0750    598.185     -0.806      0.420   -1654.626     690.476\n",
       "1991              -175.7135    596.118     -0.295      0.768   -1344.212     992.785\n",
       "1992               -21.7112    595.147     -0.036      0.971   -1188.307    1144.884\n",
       "1993               103.0065    593.189      0.174      0.862   -1059.752    1265.765\n",
       "1994              -115.0479    594.848     -0.193      0.847   -1281.058    1050.962\n",
       "1995               272.6655    595.318      0.458      0.647    -894.265    1439.596\n",
       "1996               710.9281    597.093      1.191      0.234    -459.482    1881.338\n",
       "1997              1004.6717    599.536      1.676      0.094    -170.528    2179.871\n",
       "1998               779.1160    603.511      1.291      0.197    -403.874    1962.106\n",
       "1999              1039.8341    609.204      1.707      0.088    -154.315    2233.983\n",
       "10005.0           4813.4949   1951.882      2.466      0.014     987.455    8639.535\n",
       "10006.0           4797.8946   2330.607      2.059      0.040     229.485    9366.304\n",
       "10008.0           3837.7388   1921.166      1.998      0.046      71.907    7603.570\n",
       "10016.0           5633.0729   1974.935      2.852      0.004    1761.845    9504.301\n",
       "10030.0           5704.5362   1987.517      2.870      0.004    1808.645    9600.428\n",
       "1004.0            5673.8775   1989.429      2.852      0.004    1774.237    9573.518\n",
       "10056.0           2609.4076   1897.755      1.375      0.169   -1110.534    6329.349\n",
       "10085.0          -5372.5156   1996.211     -2.691      0.007   -9285.449   -1459.582\n",
       "10092.0           5091.1028   4720.797      1.078      0.281   -4162.511    1.43e+04\n",
       "10097.0          -2417.5921   1871.431     -1.292      0.196   -6085.934    1250.750\n",
       "1010.0            5474.0561   4741.603      1.154      0.248   -3820.341    1.48e+04\n",
       "10109.0           6035.9979   1994.555      3.026      0.002    2126.311    9945.685\n",
       "10115.0            179.9615   1873.196      0.096      0.923   -3491.840    3851.763\n",
       "10124.0           6155.4676   2001.421      3.076      0.002    2232.322    1.01e+04\n",
       "1013.0            -779.2988   1875.376     -0.416      0.678   -4455.374    2896.777\n",
       "10150.0          -4203.7456   2471.997     -1.701      0.089   -9049.305     641.814\n",
       "10159.0          -1.589e+04   4053.978     -3.921      0.000   -2.38e+04   -7947.204\n",
       "10174.0           6019.0041   2219.889      2.711      0.007    1667.621    1.04e+04\n",
       "10185.0            741.4345   2103.257      0.353      0.724   -3381.328    4864.197\n",
       "10195.0           -1.34e+04   2547.329     -5.260      0.000   -1.84e+04   -8404.482\n",
       "10198.0           5775.5626   1991.524      2.900      0.004    1871.817    9679.308\n",
       "10215.0           5892.5705   1999.611      2.947      0.003    1972.972    9812.169\n",
       "10232.0          -4786.0836   2217.613     -2.158      0.031   -9133.006    -439.161\n",
       "10236.0           4748.5453   1952.213      2.432      0.015     921.856    8575.235\n",
       "10286.0           4941.3511   1955.985      2.526      0.012    1107.267    8775.435\n",
       "10301.0          -2.169e+04   2733.118     -7.936      0.000    -2.7e+04   -1.63e+04\n",
       "10312.0           5687.1815   1989.389      2.859      0.004    1787.620    9586.743\n",
       "10332.0           5449.6824   3161.561      1.724      0.085    -747.547    1.16e+04\n",
       "1036.0             136.4403   1974.368      0.069      0.945   -3733.677    4006.558\n",
       "10374.0           3180.1289   1901.625      1.672      0.094    -547.398    6907.656\n",
       "10386.0            159.6047   1864.898      0.086      0.932   -3495.932    3815.142\n",
       "10391.0          -1304.6837   1874.653     -0.696      0.486   -4979.341    2369.974\n",
       "10407.0          -4510.2417   1971.110     -2.288      0.022   -8373.973    -646.510\n",
       "10420.0           2147.9748   1868.417      1.150      0.250   -1514.459    5810.409\n",
       "10422.0            827.9390   1978.150      0.419      0.676   -3049.591    4705.469\n",
       "10426.0           4101.8945   2116.924      1.938      0.053     -47.657    8251.446\n",
       "10441.0           5431.0637   1976.162      2.748      0.006    1557.431    9304.697\n",
       "1045.0           -4396.3123   2140.993     -2.053      0.040   -8593.045    -199.580\n",
       "10453.0            890.5197   1868.292      0.477      0.634   -2771.669    4552.709\n",
       "10482.0          -2.681e+04   2430.093    -11.033      0.000   -3.16e+04    -2.2e+04\n",
       "10498.0           4526.9593   1955.572      2.315      0.021     693.685    8360.234\n",
       "10499.0          -2289.7921   2219.374     -1.032      0.302   -6640.164    2060.580\n",
       "10511.0           5658.7336   2037.868      2.777      0.005    1664.146    9653.322\n",
       "10519.0          -1.549e+04   2455.731     -6.308      0.000   -2.03e+04   -1.07e+04\n",
       "10530.0           2896.2869   1891.579      1.531      0.126    -811.549    6604.123\n",
       "10537.0           1897.1505   2266.096      0.837      0.403   -2544.806    6339.107\n",
       "10540.0           3762.2070   1901.412      1.979      0.048      35.097    7489.317\n",
       "10541.0           5467.4940   2079.565      2.629      0.009    1391.172    9543.815\n",
       "10550.0            893.7277   5745.727      0.156      0.876   -1.04e+04    1.22e+04\n",
       "10553.0            548.1139   2037.324      0.269      0.788   -3445.408    4541.636\n",
       "10565.0           6318.3297   1997.411      3.163      0.002    2403.044    1.02e+04\n",
       "10580.0           5871.5698   1951.311      3.009      0.003    2046.647    9696.492\n",
       "10581.0           4012.5149   1958.587      2.049      0.041     173.331    7851.699\n",
       "10588.0          -9773.7159   2249.288     -4.345      0.000   -1.42e+04   -5364.705\n",
       "10597.0           4744.7244   1958.252      2.423      0.015     906.196    8583.252\n",
       "10599.0           5741.7786   1997.394      2.875      0.004    1826.527    9657.030\n",
       "10618.0           4077.2832   1936.830      2.105      0.035     280.747    7873.820\n",
       "10656.0           5841.9447   1995.542      2.927      0.003    1930.324    9753.566\n",
       "10658.0           5848.1400   1996.184      2.930      0.003    1935.260    9761.020\n",
       "10726.0           6950.3536   2107.448      3.298      0.001    2819.375    1.11e+04\n",
       "10734.0           1463.9948   2575.943      0.568      0.570   -3585.318    6513.308\n",
       "10735.0           4998.1236   1972.215      2.534      0.011    1132.227    8864.020\n",
       "10764.0           5669.5216   2036.622      2.784      0.005    1677.375    9661.668\n",
       "10777.0           5816.7134   1996.922      2.913      0.004    1902.387    9731.040\n",
       "1078.0           -7730.8735   3358.565     -2.302      0.021   -1.43e+04   -1147.480\n",
       "10793.0           4647.0994   1992.190      2.333      0.020     742.049    8552.150\n",
       "10816.0           4439.0498   1989.563      2.231      0.026     539.147    8338.952\n",
       "10839.0           5976.4480   1992.750      2.999      0.003    2070.298    9882.598\n",
       "10857.0          -4676.9149   1903.763     -2.457      0.014   -8408.633    -945.197\n",
       "10867.0            999.9244   2248.159      0.445      0.656   -3406.874    5406.722\n",
       "10906.0           4250.9368   1935.088      2.197      0.028     457.816    8044.057\n",
       "10950.0           5458.2452   3152.203      1.732      0.083    -720.641    1.16e+04\n",
       "10983.0          -1.863e+04   2228.504     -8.358      0.000    -2.3e+04   -1.43e+04\n",
       "1099.0            4642.8669   1949.594      2.381      0.017     821.311    8464.422\n",
       "10991.0          -3086.1868   2474.978     -1.247      0.212   -7937.590    1765.216\n",
       "11012.0            871.2078   1920.437      0.454      0.650   -2893.195    4635.611\n",
       "11038.0          -5938.0545   1948.190     -3.048      0.002   -9756.858   -2119.251\n",
       "1104.0            3785.6276   1920.141      1.972      0.049      21.806    7549.450\n",
       "11060.0           5844.4640   1996.187      2.928      0.003    1931.577    9757.351\n",
       "11094.0           4253.5271   1939.561      2.193      0.028     451.637    8055.417\n",
       "11096.0           2662.2524   1894.882      1.405      0.160   -1052.058    6376.563\n",
       "11113.0           4155.5135   2093.983      1.985      0.047      50.928    8260.099\n",
       "1115.0            2710.8281   1893.928      1.431      0.152   -1001.612    6423.268\n",
       "11161.0           -730.9916   1872.544     -0.390      0.696   -4401.516    2939.533\n",
       "11225.0           5313.3116   1972.168      2.694      0.007    1447.507    9179.116\n",
       "11228.0           5737.7176   1966.824      2.917      0.004    1882.388    9593.047\n",
       "11236.0           3217.8327   3357.319      0.958      0.338   -3363.118    9798.783\n",
       "11288.0          -1.427e+04   3389.850     -4.208      0.000   -2.09e+04   -7621.433\n",
       "11312.0          -1.228e+04   2504.240     -4.904      0.000   -1.72e+04   -7372.243\n",
       "11361.0           5916.8535   1999.738      2.959      0.003    1997.007    9836.700\n",
       "11399.0          -4233.5510   1889.819     -2.240      0.025   -7937.936    -529.166\n",
       "114303.0         -2.405e+04   5975.944     -4.024      0.000   -3.58e+04   -1.23e+04\n",
       "11456.0            957.2408   1997.016      0.479      0.632   -2957.270    4871.751\n",
       "11465.0            517.2102   1922.049      0.269      0.788   -3250.352    4284.772\n",
       "11502.0           3431.2725   1932.551      1.776      0.076    -356.876    7219.421\n",
       "11506.0            947.6186   1932.201      0.490      0.624   -2839.843    4735.081\n",
       "11537.0           5523.6159   1984.032      2.784      0.005    1634.555    9412.677\n",
       "11566.0           5765.0706   1990.728      2.896      0.004    1862.885    9667.256\n",
       "11573.0           4834.3260   1954.235      2.474      0.013    1003.672    8664.980\n",
       "11580.0           2486.7400   2370.999      1.049      0.294   -2160.846    7134.326\n",
       "11600.0           4550.4275   1936.905      2.349      0.019     753.745    8347.111\n",
       "11609.0           8343.8116   1974.738      4.225      0.000    4472.969    1.22e+04\n",
       "1161.0           -5580.3198   2088.450     -2.672      0.008   -9674.059   -1486.581\n",
       "11636.0          -1.131e+04   2113.276     -5.353      0.000   -1.55e+04   -7169.760\n",
       "11670.0           4993.7166   1966.121      2.540      0.011    1139.765    8847.668\n",
       "11678.0          -1.354e+04   2600.128     -5.209      0.000   -1.86e+04   -8447.179\n",
       "11682.0           5831.6292   2083.335      2.799      0.005    1747.917    9915.342\n",
       "11694.0           5737.1473   2145.189      2.674      0.007    1532.190    9942.104\n",
       "11720.0          -1.091e+04   3498.580     -3.118      0.002   -1.78e+04   -4051.663\n",
       "11721.0          -2.458e+04   3700.651     -6.642      0.000   -3.18e+04   -1.73e+04\n",
       "11722.0           4070.8052   2226.642      1.828      0.068    -293.815    8435.425\n",
       "11793.0           5046.0590   5772.818      0.874      0.382   -6269.706    1.64e+04\n",
       "11797.0           5953.8702   2368.831      2.513      0.012    1310.535    1.06e+04\n",
       "11914.0           4550.5342   2774.512      1.640      0.101    -888.010    9989.078\n",
       "1209.0            3185.2717   1883.937      1.691      0.091    -507.585    6878.128\n",
       "12136.0          -2.301e+04   3462.213     -6.645      0.000   -2.98e+04   -1.62e+04\n",
       "12141.0           5.769e+04   2498.058     23.094      0.000    5.28e+04    6.26e+04\n",
       "12181.0           1660.9963   3321.432      0.500      0.617   -4849.610    8171.602\n",
       "12215.0          -1.083e+04   2576.714     -4.203      0.000   -1.59e+04   -5778.209\n",
       "12216.0          -9691.8845   2653.284     -3.653      0.000   -1.49e+04   -4490.970\n",
       "12256.0          -5456.4670   2285.410     -2.388      0.017   -9936.282    -976.652\n",
       "12262.0           2818.3197   2200.443      1.281      0.200   -1494.946    7131.585\n",
       "12389.0           6567.9309   2276.230      2.885      0.004    2106.110     1.1e+04\n",
       "1239.0             536.9656   1867.287      0.288      0.774   -3123.254    4197.185\n",
       "12390.0           2560.0940   2595.506      0.986      0.324   -2527.565    7647.753\n",
       "12397.0            674.1276   4691.398      0.144      0.886   -8521.858    9870.113\n",
       "1243.0            1018.4517   2012.040      0.506      0.613   -2925.510    4962.414\n",
       "12548.0           4523.6422   2516.921      1.797      0.072    -409.977    9457.261\n",
       "12570.0           3242.1694   2287.571      1.417      0.156   -1241.883    7726.222\n",
       "12581.0           2985.4678   2491.175      1.198      0.231   -1897.685    7868.621\n",
       "12592.0            756.2421   2175.712      0.348      0.728   -3508.545    5021.030\n",
       "12604.0           -444.7947   4695.812     -0.095      0.925   -9649.432    8759.843\n",
       "12656.0           4983.3418   2258.664      2.206      0.027     555.953    9410.731\n",
       "12679.0           -2.38e+04   3491.395     -6.816      0.000   -3.06e+04    -1.7e+04\n",
       "1278.0            4713.1060   2015.046      2.339      0.019     763.253    8662.959\n",
       "12788.0          -2.537e+04   3749.737     -6.767      0.000   -3.27e+04    -1.8e+04\n",
       "1283.0            3009.1830   1901.024      1.583      0.113    -717.166    6735.532\n",
       "1297.0            4155.4428   1934.994      2.148      0.032     362.505    7948.381\n",
       "12992.0           5565.0936   2359.113      2.359      0.018     940.807    1.02e+04\n",
       "13135.0           -562.5675   2346.922     -0.240      0.811   -5162.958    4037.823\n",
       "1327.0           -1.224e+04   2470.053     -4.956      0.000   -1.71e+04   -7399.413\n",
       "13282.0          -2721.2310   4692.654     -0.580      0.562   -1.19e+04    6477.217\n",
       "1334.0           -1.811e+04   2965.443     -6.107      0.000   -2.39e+04   -1.23e+04\n",
       "13351.0           1502.3731   3123.311      0.481      0.631   -4619.879    7624.625\n",
       "13365.0          -3.106e+04   4404.453     -7.052      0.000   -3.97e+04   -2.24e+04\n",
       "13369.0           4498.0546   2323.950      1.936      0.053     -57.307    9053.416\n",
       "13406.0           5145.0388   2332.358      2.206      0.027     573.197    9716.880\n",
       "13407.0          -5843.5719   2354.762     -2.482      0.013   -1.05e+04   -1227.814\n",
       "13417.0           4967.9392   2428.751      2.045      0.041     207.149    9728.729\n",
       "13525.0          -1.318e+04   2754.299     -4.784      0.000   -1.86e+04   -7778.331\n",
       "13554.0           5611.4352   2447.232      2.293      0.022     814.420    1.04e+04\n",
       "1359.0           -2.247e+04   3632.149     -6.185      0.000   -2.96e+04   -1.53e+04\n",
       "13623.0           1210.0618   2353.506      0.514      0.607   -3403.234    5823.358\n",
       "1372.0           -1.121e+04   2350.270     -4.769      0.000   -1.58e+04   -6602.611\n",
       "1380.0           -7498.9924   1945.758     -3.854      0.000   -1.13e+04   -3684.956\n",
       "13923.0           4515.7166   2631.671      1.716      0.086    -642.833    9674.266\n",
       "13932.0           5289.0955   3397.307      1.557      0.120   -1370.239    1.19e+04\n",
       "13941.0          -1.168e+04   2772.846     -4.213      0.000   -1.71e+04   -6246.106\n",
       "1397.0            3250.9457   2199.079      1.478      0.139   -1059.646    7561.538\n",
       "14064.0             50.2213   2256.777      0.022      0.982   -4373.468    4473.911\n",
       "14084.0           5613.9947   2360.812      2.378      0.017     986.377    1.02e+04\n",
       "14324.0          -1.166e+04   2867.437     -4.067      0.000   -1.73e+04   -6039.865\n",
       "14462.0           4648.7560   2418.044      1.923      0.055     -91.046    9388.558\n",
       "1447.0            5310.8958   4144.973      1.281      0.200   -2813.998    1.34e+04\n",
       "14593.0           4720.7038   2542.214      1.857      0.063    -262.495    9703.902\n",
       "14622.0           5168.6326   8141.182      0.635      0.526   -1.08e+04    2.11e+04\n",
       "1465.0            1920.7060   2586.558      0.743      0.458   -3149.413    6990.825\n",
       "1468.0            6453.7636   2553.611      2.527      0.012    1448.225    1.15e+04\n",
       "14897.0           2124.2078   4695.488      0.452      0.651   -7079.795    1.13e+04\n",
       "14954.0           3534.6047   2492.565      1.418      0.156   -1351.272    8420.481\n",
       "1496.0            5981.1779   2001.187      2.989      0.003    2058.491    9903.865\n",
       "15267.0           3324.8172   2484.970      1.338      0.181   -1546.172    8195.807\n",
       "15354.0          -1237.4126   2579.186     -0.480      0.631   -6293.083    3818.258\n",
       "1542.0            3916.4533   1923.050      2.037      0.042     146.928    7685.978\n",
       "15459.0           3000.6691   2604.382      1.152      0.249   -2104.389    8105.728\n",
       "1554.0            5694.0640   1986.883      2.866      0.004    1799.414    9588.714\n",
       "15708.0          -3.218e+04   4715.431     -6.824      0.000   -4.14e+04   -2.29e+04\n",
       "15711.0           3230.4275   2509.289      1.287      0.198   -1688.231    8149.086\n",
       "15761.0           5213.4306   2949.338      1.768      0.077    -567.803     1.1e+04\n",
       "1581.0           -4841.1692   3034.031     -1.596      0.111   -1.08e+04    1106.079\n",
       "1593.0            4321.2884   1943.873      2.223      0.026     510.946    8131.630\n",
       "1602.0            1.022e+04   2061.778      4.959      0.000    6183.141    1.43e+04\n",
       "1613.0            4387.9538   1942.787      2.259      0.024     579.741    8196.166\n",
       "16188.0          -2185.7033   2725.376     -0.802      0.423   -7527.931    3156.524\n",
       "1632.0           -7123.0045   2129.102     -3.346      0.001   -1.13e+04   -2949.580\n",
       "1633.0            1749.7199   1877.093      0.932      0.351   -1929.720    5429.160\n",
       "1635.0           -2.416e+04   3591.521     -6.726      0.000   -3.12e+04   -1.71e+04\n",
       "16401.0          -7854.3757   2707.195     -2.901      0.004   -1.32e+04   -2547.785\n",
       "16437.0          -5618.1399   2978.211     -1.886      0.059   -1.15e+04     219.690\n",
       "1651.0           -2359.9296   1906.348     -1.238      0.216   -6096.715    1376.856\n",
       "1655.0            5918.5046   1996.516      2.964      0.003    2004.973    9832.036\n",
       "1663.0            7869.2121   2017.388      3.901      0.000    3914.768    1.18e+04\n",
       "16710.0          -2160.6819   2715.903     -0.796      0.426   -7484.341    3162.978\n",
       "16729.0          -4426.9439   2610.724     -1.696      0.090   -9544.433     690.545\n",
       "1690.0           -2.176e+04   2799.319     -7.773      0.000   -2.72e+04   -1.63e+04\n",
       "1703.0            3466.1648   1961.927      1.767      0.077    -379.566    7311.895\n",
       "17202.0           2006.6420   2588.229      0.775      0.438   -3066.754    7080.038\n",
       "1722.0            3116.2628   1993.580      1.563      0.118    -791.513    7024.039\n",
       "1728.0            5633.9310   1996.403      2.822      0.005    1720.621    9547.241\n",
       "1743.0            4381.9496   3153.520      1.390      0.165   -1799.518    1.06e+04\n",
       "1754.0            4698.8460   2045.572      2.297      0.022     689.156    8708.536\n",
       "1762.0            2996.5292   1967.402      1.523      0.128    -859.933    6852.991\n",
       "1773.0            3990.7601   1983.464      2.012      0.044     102.813    7878.707\n",
       "1786.0            -1.51e+04   2600.786     -5.807      0.000   -2.02e+04      -1e+04\n",
       "18100.0            684.5639   2575.351      0.266      0.790   -4363.588    5732.716\n",
       "1820.0           -1537.7552   1881.683     -0.817      0.414   -5226.194    2150.684\n",
       "1848.0           -1.003e+04   2495.059     -4.022      0.000   -1.49e+04   -5143.253\n",
       "18654.0           5071.6705   3705.654      1.369      0.171   -2192.080    1.23e+04\n",
       "1875.0           -3179.7872   4102.927     -0.775      0.438   -1.12e+04    4862.690\n",
       "1884.0            3490.0985   2017.841      1.730      0.084    -465.233    7445.430\n",
       "1913.0            2787.5754   1896.846      1.470      0.142    -930.584    6505.735\n",
       "1919.0            4405.2188   2080.640      2.117      0.034     326.790    8483.648\n",
       "1920.0             630.2583   1868.894      0.337      0.736   -3033.110    4293.627\n",
       "1968.0            4876.8374   1956.488      2.493      0.013    1041.769    8711.906\n",
       "1976.0            5531.6252   1926.111      2.872      0.004    1756.100    9307.150\n",
       "1981.0            4132.7220   1930.521      2.141      0.032     348.553    7916.891\n",
       "1988.0           -2144.8429   3071.256     -0.698      0.485   -8165.059    3875.373\n",
       "1992.0            5729.4776   1989.753      2.879      0.004    1829.202    9629.753\n",
       "2008.0            4293.8972   1920.835      2.235      0.025     528.714    8059.080\n",
       "2033.0            4877.5272   2537.714      1.922      0.055     -96.850    9851.904\n",
       "2044.0            1056.4120   1866.200      0.566      0.571   -2601.676    4714.500\n",
       "2049.0            4119.6311   1935.321      2.129      0.033     326.052    7913.210\n",
       "2061.0            5957.2341   2001.011      2.977      0.003    2034.892    9879.576\n",
       "20779.0           2.845e+04   2755.046     10.327      0.000    2.31e+04    3.39e+04\n",
       "2085.0           -2.111e+04   3328.065     -6.342      0.000   -2.76e+04   -1.46e+04\n",
       "2086.0            3094.3038   1884.339      1.642      0.101    -599.341    6787.949\n",
       "2111.0            2882.4529   1869.712      1.542      0.123    -782.519    6547.425\n",
       "21204.0          -7461.7244   2690.133     -2.774      0.006   -1.27e+04   -2188.579\n",
       "21238.0           2419.0385   2586.317      0.935      0.350   -2650.609    7488.686\n",
       "2124.0            5101.5536   2065.963      2.469      0.014    1051.894    9151.213\n",
       "2146.0            4601.4461   3060.832      1.503      0.133   -1398.337    1.06e+04\n",
       "21496.0          -3.386e+04   4726.783     -7.164      0.000   -4.31e+04   -2.46e+04\n",
       "2154.0            4044.6641   1933.674      2.092      0.036     254.314    7835.014\n",
       "2176.0            3.574e+04   2599.419     13.749      0.000    3.06e+04    4.08e+04\n",
       "2188.0            5672.6793   2033.203      2.790      0.005    1687.234    9658.124\n",
       "2189.0             148.4369   1959.794      0.076      0.940   -3693.113    3989.987\n",
       "2220.0            5015.6190   1965.133      2.552      0.011    1163.605    8867.633\n",
       "22205.0           5646.3189   2667.620      2.117      0.034     417.303    1.09e+04\n",
       "2226.0             814.4474   5745.805      0.142      0.887   -1.04e+04    1.21e+04\n",
       "2230.0            1301.6681   2174.922      0.598      0.550   -2961.572    5564.908\n",
       "22325.0          -1.485e+04   3411.237     -4.355      0.000   -2.15e+04   -8167.865\n",
       "2255.0            4579.6258   1972.396      2.322      0.020     713.374    8445.878\n",
       "22619.0           5145.3920   2652.925      1.940      0.052     -54.819    1.03e+04\n",
       "2267.0           -1.155e+04   2384.995     -4.844      0.000   -1.62e+04   -6878.030\n",
       "22815.0          -3121.7529   2602.541     -1.200      0.230   -8223.202    1979.696\n",
       "2285.0           -1.429e+04   2085.478     -6.854      0.000   -1.84e+04   -1.02e+04\n",
       "2290.0             468.8118   1942.062      0.241      0.809   -3337.979    4275.603\n",
       "2295.0            4937.7627   3362.044      1.469      0.142   -1652.450    1.15e+04\n",
       "2316.0           -1179.7652   2105.972     -0.560      0.575   -5307.850    2948.320\n",
       "23220.0           3943.3174   2769.146      1.424      0.154   -1484.709    9371.344\n",
       "23224.0          -2.318e+04   3768.793     -6.151      0.000   -3.06e+04   -1.58e+04\n",
       "2343.0           -1.672e+04   4434.085     -3.771      0.000   -2.54e+04   -8030.944\n",
       "2352.0            5331.8550   2194.582      2.430      0.015    1030.078    9633.632\n",
       "23700.0           -1.45e+04   3833.444     -3.784      0.000    -2.2e+04   -6990.455\n",
       "2390.0            5837.7262   1994.917      2.926      0.003    1927.330    9748.123\n",
       "2393.0            3446.0993   1910.184      1.804      0.071    -298.206    7190.405\n",
       "2403.0            2761.3152   3389.110      0.815      0.415   -3881.951    9404.582\n",
       "2435.0            7046.0132   1992.542      3.536      0.000    3140.271     1.1e+04\n",
       "2444.0            4135.4846   1943.015      2.128      0.033     326.825    7944.144\n",
       "2448.0            2405.0247   1886.047      1.275      0.202   -1291.967    6102.017\n",
       "2469.0            4508.6059   3694.083      1.220      0.222   -2732.462    1.17e+04\n",
       "24720.0           5465.7697   2965.238      1.843      0.065    -346.632    1.13e+04\n",
       "24800.0          -2.171e+04   4068.270     -5.338      0.000   -2.97e+04   -1.37e+04\n",
       "2482.0            5953.1001   2000.598      2.976      0.003    2031.568    9874.633\n",
       "24969.0           4786.0951   3134.337      1.527      0.127   -1357.770    1.09e+04\n",
       "2498.0           -8638.2868   2263.003     -3.817      0.000   -1.31e+04   -4202.394\n",
       "2504.0           -2210.6970   1925.030     -1.148      0.251   -5984.103    1562.709\n",
       "2508.0            5573.8713   2207.837      2.525      0.012    1246.113    9901.629\n",
       "25124.0           5829.1780   2947.556      1.978      0.048      51.438    1.16e+04\n",
       "2518.0            5596.9129   1996.457      2.803      0.005    1683.498    9510.328\n",
       "25224.0           6375.0727   8147.603      0.782      0.434   -9595.697    2.23e+04\n",
       "25279.0           2037.8755   2884.776      0.706      0.480   -3616.805    7692.556\n",
       "2537.0           -1.968e+04   3137.493     -6.274      0.000   -2.58e+04   -1.35e+04\n",
       "2538.0            4746.4598   2942.806      1.613      0.107   -1021.970    1.05e+04\n",
       "25389.0           4692.3411   4720.530      0.994      0.320   -4560.748    1.39e+04\n",
       "2547.0           -5269.2420   2189.996     -2.406      0.016   -9562.029    -976.455\n",
       "2553.0            3522.0549   1913.312      1.841      0.066    -228.381    7272.491\n",
       "2574.0           -1609.3684   2578.575     -0.624      0.533   -6663.840    3445.104\n",
       "25747.0           4900.2402   3135.869      1.563      0.118   -1246.628     1.1e+04\n",
       "2577.0            3835.8963   1915.962      2.002      0.045      80.266    7591.527\n",
       "2593.0            4531.3516   1964.152      2.307      0.021     681.259    8381.444\n",
       "2596.0           -3239.4189   1914.573     -1.692      0.091   -6992.327     513.489\n",
       "2663.0            9161.1645   1997.545      4.586      0.000    5245.617    1.31e+04\n",
       "2771.0           -2662.9682   1897.529     -1.403      0.161   -6382.467    1056.530\n",
       "2787.0            4574.8892   1953.949      2.341      0.019     744.797    8404.981\n",
       "2797.0           -1.562e+04   2718.260     -5.746      0.000   -2.09e+04   -1.03e+04\n",
       "2802.0            5276.4913   1972.838      2.675      0.007    1409.374    9143.609\n",
       "2817.0             129.1254   1954.573      0.066      0.947   -3702.190    3960.441\n",
       "28678.0          -2.352e+04   4034.940     -5.828      0.000   -3.14e+04   -1.56e+04\n",
       "28701.0           2830.2390   1909.723      1.482      0.138    -913.163    6573.641\n",
       "28742.0          -2.231e+04   3907.568     -5.710      0.000      -3e+04   -1.47e+04\n",
       "2888.0            5250.5375   2194.333      2.393      0.017     949.249    9551.826\n",
       "2897.0            5865.7959   2791.469      2.101      0.036     394.014    1.13e+04\n",
       "2917.0            -488.6654   1970.596     -0.248      0.804   -4351.390    3374.059\n",
       "29392.0          -9938.5533   3427.430     -2.900      0.004   -1.67e+04   -3220.172\n",
       "2950.0           -3.926e+04   5294.553     -7.415      0.000   -4.96e+04   -2.89e+04\n",
       "2951.0            5535.6905   2445.824      2.263      0.024     741.434    1.03e+04\n",
       "2953.0            4124.3201   1926.336      2.141      0.032     348.353    7900.287\n",
       "2960.0            3711.7939   2919.533      1.271      0.204   -2011.017    9434.605\n",
       "2975.0           -5691.6089   2000.969     -2.844      0.004   -9613.868   -1769.350\n",
       "2982.0            2898.8181   1904.850      1.522      0.128    -835.031    6632.667\n",
       "2991.0           -1.469e+04   2700.464     -5.440      0.000      -2e+04   -9396.901\n",
       "3011.0            -1.54e+04   2892.609     -5.325      0.000   -2.11e+04   -9732.571\n",
       "3015.0            5202.8797   1946.551      2.673      0.008    1387.288    9018.471\n",
       "3026.0            2363.1203   1933.109      1.222      0.222   -1426.122    6152.363\n",
       "3031.0           -2.915e+04   4370.172     -6.670      0.000   -3.77e+04   -2.06e+04\n",
       "3062.0            7463.7255   2096.541      3.560      0.000    3354.127    1.16e+04\n",
       "3093.0           -8806.6966   2541.664     -3.465      0.001   -1.38e+04   -3824.576\n",
       "3107.0            5301.6018   3708.550      1.430      0.153   -1967.826    1.26e+04\n",
       "3121.0            6374.5636   1949.454      3.270      0.001    2553.282    1.02e+04\n",
       "3126.0            5718.2200   1997.710      2.862      0.004    1802.348    9634.092\n",
       "3144.0            5.252e+04   2001.423     26.242      0.000    4.86e+04    5.64e+04\n",
       "3156.0            2456.8442   2357.562      1.042      0.297   -2164.401    7078.090\n",
       "3157.0            3843.2398   1921.376      2.000      0.045      76.997    7609.482\n",
       "3170.0            3315.0700   1871.969      1.771      0.077    -354.326    6984.466\n",
       "3178.0           -9202.2841   2296.026     -4.008      0.000   -1.37e+04   -4701.660\n",
       "3206.0            1499.4128   2197.761      0.682      0.495   -2808.596    5807.421\n",
       "3229.0           -1595.4902   1981.822     -0.805      0.421   -5480.219    2289.239\n",
       "3235.0            5792.5791   2148.702      2.696      0.007    1580.736       1e+04\n",
       "3246.0            3978.7541   1960.246      2.030      0.042     136.318    7821.190\n",
       "3248.0            5437.7381   1981.647      2.744      0.006    1553.353    9322.123\n",
       "3282.0           -2.539e+04   2993.946     -8.480      0.000   -3.13e+04   -1.95e+04\n",
       "3362.0           -4424.8740   2290.165     -1.932      0.053   -8914.011      64.263\n",
       "3372.0            4318.9152   2496.575      1.730      0.084    -574.822    9212.652\n",
       "3422.0            4207.4331   1922.122      2.189      0.029     439.727    7975.139\n",
       "3497.0           -1240.7329   1938.259     -0.640      0.522   -5040.069    2558.603\n",
       "3502.0           -4236.9220   1947.227     -2.176      0.030   -8053.837    -420.007\n",
       "3504.0            3789.2411   2756.174      1.375      0.169   -1613.357    9191.839\n",
       "3505.0            4392.3223   1969.602      2.230      0.026     531.547    8253.097\n",
       "3532.0            5565.8740   1875.178      2.968      0.003    1890.187    9241.561\n",
       "3574.0            6012.9124   4125.935      1.457      0.145   -2074.664    1.41e+04\n",
       "3580.0            -106.6962   1865.740     -0.057      0.954   -3763.884    3550.491\n",
       "3612.0            5404.8441   1968.058      2.746      0.006    1547.095    9262.593\n",
       "3619.0            3432.0264   1958.903      1.752      0.080    -407.777    7271.829\n",
       "3622.0            5707.4282   2035.863      2.803      0.005    1716.769    9698.088\n",
       "3639.0           -1.188e+04   2402.966     -4.945      0.000   -1.66e+04   -7172.555\n",
       "3650.0           -1277.6393   1876.659     -0.681      0.496   -4956.230    2400.951\n",
       "3662.0            5191.3064   1963.537      2.644      0.008    1342.420    9040.192\n",
       "3734.0           -1.673e+04   2578.502     -6.487      0.000   -2.18e+04   -1.17e+04\n",
       "3735.0            4888.1541   2405.370      2.032      0.042     173.194    9603.114\n",
       "3761.0             -65.0558   1868.881     -0.035      0.972   -3728.399    3598.287\n",
       "3779.0           -2.308e+04   3592.736     -6.423      0.000   -3.01e+04    -1.6e+04\n",
       "3781.0           -1.182e+04   2859.325     -4.135      0.000   -1.74e+04   -6219.574\n",
       "3782.0           -1.559e+04   2704.001     -5.767      0.000   -2.09e+04   -1.03e+04\n",
       "3786.0            1603.7968   1876.118      0.855      0.393   -2073.734    5281.327\n",
       "3796.0           -2.347e+04   3564.311     -6.585      0.000   -3.05e+04   -1.65e+04\n",
       "3821.0            4772.8156   1988.997      2.400      0.016     874.022    8671.609\n",
       "3835.0            1317.3590   1941.398      0.679      0.497   -2488.132    5122.850\n",
       "3839.0            2633.9853   2736.286      0.963      0.336   -2729.628    7997.598\n",
       "3840.0           -1.104e+04   2332.661     -4.732      0.000   -1.56e+04   -6465.851\n",
       "3895.0            5013.5276   1964.396      2.552      0.011    1162.958    8864.098\n",
       "3908.0           -5563.0210   3202.170     -1.737      0.082   -1.18e+04     713.810\n",
       "3911.0             171.9872   1867.380      0.092      0.927   -3488.414    3832.389\n",
       "3917.0            4625.0950   2046.362      2.260      0.024     613.856    8636.334\n",
       "3946.0            5677.6978   1979.027      2.869      0.004    1798.448    9556.948\n",
       "3971.0            4524.4845   2039.435      2.218      0.027     526.824    8522.146\n",
       "3980.0            1.262e+04   1944.351      6.490      0.000    8806.770    1.64e+04\n",
       "4034.0            2604.7223   1919.380      1.357      0.175   -1157.609    6367.053\n",
       "4036.0            5998.2727   1991.432      3.012      0.003    2094.707    9901.838\n",
       "4040.0           -7036.6871   2084.929     -3.375      0.001   -1.11e+04   -2949.851\n",
       "4058.0            3892.1315   1894.530      2.054      0.040     178.510    7605.753\n",
       "4060.0           -6065.5675   1995.358     -3.040      0.002   -9976.830   -2154.305\n",
       "4062.0            7447.8593   1975.356      3.770      0.000    3575.805    1.13e+04\n",
       "4077.0             742.9194   3319.868      0.224      0.823   -5764.621    7250.460\n",
       "4087.0           -1.351e+04   2493.437     -5.417      0.000   -1.84e+04   -8618.829\n",
       "4091.0            1759.3905   2464.664      0.714      0.475   -3071.795    6590.576\n",
       "4127.0           -4322.5466   1951.395     -2.215      0.027   -8147.632    -497.461\n",
       "4138.0            5682.9745   2650.040      2.144      0.032     488.418    1.09e+04\n",
       "4162.0            3507.8926   2476.673      1.416      0.157   -1346.834    8362.619\n",
       "4186.0            5845.8717   1990.790      2.936      0.003    1943.565    9748.179\n",
       "4194.0            5650.9262   2261.961      2.498      0.012    1217.075    1.01e+04\n",
       "4199.0           -9816.5403   2308.153     -4.253      0.000   -1.43e+04   -5292.144\n",
       "4213.0            5592.3913   1956.548      2.858      0.004    1757.203    9427.579\n",
       "4222.0           -1.659e+04   2824.967     -5.873      0.000   -2.21e+04   -1.11e+04\n",
       "4223.0            4893.6615   1952.224      2.507      0.012    1066.951    8720.372\n",
       "4251.0            5444.7655   1979.581      2.750      0.006    1564.429    9325.102\n",
       "4265.0            1455.3156   2103.531      0.692      0.489   -2667.984    5578.616\n",
       "4274.0            4445.5496   2042.920      2.176      0.030     441.058    8450.041\n",
       "4321.0            6899.4013   1935.835      3.564      0.000    3104.815    1.07e+04\n",
       "4335.0            2788.0453   3348.558      0.833      0.405   -3775.732    9351.823\n",
       "4340.0             322.0616   1919.799      0.168      0.867   -3441.090    4085.214\n",
       "4371.0            4044.4674   2006.643      2.016      0.044     111.085    7977.850\n",
       "4415.0            3792.7307   2025.507      1.872      0.061    -177.628    7763.090\n",
       "4450.0             651.3093   1869.294      0.348      0.728   -3012.844    4315.462\n",
       "4476.0           -1.046e+04   2474.144     -4.229      0.000   -1.53e+04   -5612.746\n",
       "4510.0            1627.3410   1913.421      0.850      0.395   -2123.309    5377.991\n",
       "4520.0            5937.2607   1999.939      2.969      0.003    2017.020    9857.502\n",
       "4551.0            2011.0006   8126.310      0.247      0.805   -1.39e+04    1.79e+04\n",
       "4568.0            5735.7827   2092.053      2.742      0.006    1634.982    9836.583\n",
       "4579.0            5244.1572   1969.212      2.663      0.008    1384.147    9104.167\n",
       "4585.0            5451.5411   1980.169      2.753      0.006    1570.052    9333.030\n",
       "4595.0             825.6749   1869.696      0.442      0.659   -2839.266    4490.616\n",
       "4600.0           -1.452e+04   2749.307     -5.283      0.000   -1.99e+04   -9134.802\n",
       "4607.0            5303.1652   1973.784      2.687      0.007    1434.194    9172.137\n",
       "4608.0           -1.353e+04   2568.616     -5.266      0.000   -1.86e+04   -8491.257\n",
       "4622.0            2677.0338   1897.125      1.411      0.158   -1041.673    6395.740\n",
       "4623.0            3391.8093   1957.474      1.733      0.083    -445.193    7228.812\n",
       "4768.0            4839.4631   1992.062      2.429      0.015     934.663    8744.263\n",
       "4771.0            5621.5296   1986.692      2.830      0.005    1727.256    9515.803\n",
       "4800.0            3659.2201   1983.621      1.845      0.065    -229.034    7547.474\n",
       "4802.0            5746.4121   1994.123      2.882      0.004    1837.572    9655.253\n",
       "4807.0            5517.8740   2029.245      2.719      0.007    1540.188    9495.560\n",
       "4839.0           -1.031e+05   3500.927    -29.447      0.000    -1.1e+05   -9.62e+04\n",
       "4843.0           -2.287e+04   3564.575     -6.415      0.000   -2.99e+04   -1.59e+04\n",
       "4881.0            3706.2439   1917.009      1.933      0.053     -51.440    7463.927\n",
       "4900.0            1000.4167   1870.814      0.535      0.593   -2666.715    4667.549\n",
       "4926.0            4140.5831   1941.422      2.133      0.033     335.046    7946.120\n",
       "4941.0            4091.6995   1976.008      2.071      0.038     218.367    7965.032\n",
       "4961.0           -1.199e+04   3211.013     -3.733      0.000   -1.83e+04   -5692.774\n",
       "4988.0            1.075e+04   1962.935      5.479      0.000    6906.404    1.46e+04\n",
       "4993.0            5886.6423   1995.984      2.949      0.003    1974.155    9799.130\n",
       "5018.0            2417.8721   1918.350      1.260      0.208   -1342.441    6178.185\n",
       "5020.0           -1.963e+04   3495.660     -5.615      0.000   -2.65e+04   -1.28e+04\n",
       "5027.0           -1337.9866   1877.638     -0.713      0.476   -5018.496    2342.523\n",
       "5032.0            5205.9697   1978.250      2.632      0.009    1328.244    9083.696\n",
       "5043.0            -567.0871   1868.788     -0.303      0.762   -4230.248    3096.074\n",
       "5046.0           -7653.5292   1954.591     -3.916      0.000   -1.15e+04   -3822.179\n",
       "5047.0            4.078e+04   2885.953     14.131      0.000    3.51e+04    4.64e+04\n",
       "5065.0            4941.7237   2322.907      2.127      0.033     388.407    9495.041\n",
       "5071.0            6412.1070   2386.808      2.686      0.007    1733.533    1.11e+04\n",
       "5073.0           -1.551e+05   5742.807    -27.011      0.000   -1.66e+05   -1.44e+05\n",
       "5087.0            -957.8256   1866.085     -0.513      0.608   -4615.689    2700.037\n",
       "5109.0            5582.2600   1994.067      2.799      0.005    1673.530    9490.991\n",
       "5116.0             -1.6e+04   2941.916     -5.438      0.000   -2.18e+04   -1.02e+04\n",
       "5122.0           -1072.8236   1874.063     -0.572      0.567   -4746.326    2600.678\n",
       "5134.0           -1667.6530   2041.682     -0.817      0.414   -5669.719    2334.412\n",
       "5142.0            1099.1601   2718.240      0.404      0.686   -4229.080    6427.400\n",
       "5165.0           -3068.5101   2287.462     -1.341      0.180   -7552.348    1415.328\n",
       "5169.0            1.418e+04   1905.089      7.441      0.000    1.04e+04    1.79e+04\n",
       "5174.0            -338.4584   2257.668     -0.150      0.881   -4763.894    4086.977\n",
       "5179.0            4324.9441   1929.858      2.241      0.025     542.075    8107.814\n",
       "5181.0            5908.2768   2044.014      2.891      0.004    1901.641    9914.913\n",
       "5187.0            5103.3022   2413.305      2.115      0.034     372.790    9833.814\n",
       "5229.0           -2728.6144   1878.659     -1.452      0.146   -6411.124     953.895\n",
       "5234.0           -2638.3547   2073.913     -1.272      0.203   -6703.598    1426.889\n",
       "5237.0            4042.4159   1926.390      2.098      0.036     266.343    7818.489\n",
       "5252.0            4353.2552   1935.470      2.249      0.025     559.384    8147.126\n",
       "5254.0            4947.5640   1959.680      2.525      0.012    1106.238    8788.890\n",
       "5306.0            5223.3645   1926.246      2.712      0.007    1447.576    8999.153\n",
       "5338.0            5419.2139   1972.267      2.748      0.006    1553.215    9285.213\n",
       "5377.0            5667.0751   1999.675      2.834      0.005    1747.352    9586.799\n",
       "5439.0            3642.6006   1943.054      1.875      0.061    -166.137    7451.338\n",
       "5456.0            5815.5450   1997.849      2.911      0.004    1899.401    9731.689\n",
       "5464.0            4177.8920   2529.093      1.652      0.099    -779.587    9135.371\n",
       "5476.0            4116.7352   1916.475      2.148      0.032     360.099    7873.372\n",
       "5492.0           -2.361e+04   3487.541     -6.770      0.000   -3.04e+04   -1.68e+04\n",
       "5496.0            4462.4952   1946.019      2.293      0.022     647.947    8277.043\n",
       "5505.0            5452.9185   1991.805      2.738      0.006    1548.621    9357.216\n",
       "5518.0            4339.3471   2255.519      1.924      0.054     -81.877    8760.571\n",
       "5520.0             925.9037   1870.957      0.495      0.621   -2741.509    4593.317\n",
       "5545.0            5919.0229   2093.288      2.828      0.005    1815.801       1e+04\n",
       "5568.0            8396.0701   1986.866      4.226      0.000    4501.454    1.23e+04\n",
       "5569.0            5771.3735   2040.638      2.828      0.005    1771.356    9771.391\n",
       "5578.0            4779.1578   1944.786      2.457      0.014     967.026    8591.289\n",
       "5581.0            4469.9235   1926.308      2.320      0.020     694.012    8245.835\n",
       "5589.0           -1927.8189   1891.860     -1.019      0.308   -5636.205    1780.567\n",
       "5597.0            7286.7965   2664.850      2.734      0.006    2063.210    1.25e+04\n",
       "5606.0           -2.267e+04   2493.658     -9.091      0.000   -2.76e+04   -1.78e+04\n",
       "5639.0            6295.8147   1979.720      3.180      0.001    2415.206    1.02e+04\n",
       "5667.0           -3292.9448   1911.719     -1.723      0.085   -7040.259     454.369\n",
       "5690.0            5906.3927   1992.994      2.964      0.003    1999.765    9813.020\n",
       "5709.0            4522.8440   1985.573      2.278      0.023     630.762    8414.926\n",
       "5726.0            3228.0209   1900.051      1.699      0.089    -496.422    6952.464\n",
       "5764.0            4431.4327   1906.041      2.325      0.020     695.249    8167.616\n",
       "5772.0            5377.1085   1982.015      2.713      0.007    1492.002    9262.215\n",
       "5860.0           -2.679e+04   2826.641     -9.476      0.000   -3.23e+04   -2.12e+04\n",
       "5878.0            7425.3294   1919.565      3.868      0.000    3662.635    1.12e+04\n",
       "5903.0           -4768.1203   1959.194     -2.434      0.015   -8608.495    -927.746\n",
       "5905.0            4697.2879   1997.567      2.352      0.019     781.697    8612.879\n",
       "5959.0            2939.7289   1981.699      1.483      0.138    -944.758    6824.215\n",
       "6008.0            2.481e+04   2046.226     12.126      0.000    2.08e+04    2.88e+04\n",
       "6034.0            5442.7672   2146.456      2.536      0.011    1235.326    9650.208\n",
       "6035.0           -8409.7028   2697.021     -3.118      0.002   -1.37e+04   -3123.055\n",
       "6036.0           -7472.8788   2066.166     -3.617      0.000   -1.15e+04   -3422.821\n",
       "6039.0            4438.9665   1939.722      2.288      0.022     636.761    8241.172\n",
       "6044.0            5696.2156   2218.601      2.567      0.010    1347.357       1e+04\n",
       "6066.0            4522.6985   3708.109      1.220      0.223   -2745.864    1.18e+04\n",
       "6078.0            4583.2113   1876.629      2.442      0.015     904.681    8261.742\n",
       "6081.0           -2.271e+04   2967.263     -7.655      0.000   -2.85e+04   -1.69e+04\n",
       "60893.0          -2.908e+04   4955.987     -5.867      0.000   -3.88e+04   -1.94e+04\n",
       "6097.0            3760.7454   1897.673      1.982      0.048      40.963    7480.528\n",
       "6102.0            4171.7942   1983.047      2.104      0.035     284.665    8058.924\n",
       "6104.0           -4584.6433   2028.696     -2.260      0.024   -8561.253    -608.034\n",
       "6109.0           -8077.6585   2138.562     -3.777      0.000   -1.23e+04   -3885.691\n",
       "6127.0           -1505.8472   2298.475     -0.655      0.512   -6011.272    2999.578\n",
       "61552.0          -1.362e+04   3834.535     -3.552      0.000   -2.11e+04   -6103.565\n",
       "6158.0             298.7446   2034.574      0.147      0.883   -3689.387    4286.876\n",
       "6171.0            4291.1173   1929.504      2.224      0.026     508.941    8073.293\n",
       "61780.0           4798.9194   4126.512      1.163      0.245   -3289.788    1.29e+04\n",
       "6207.0            3546.3191   1912.662      1.854      0.064    -202.843    7295.481\n",
       "6214.0            5904.7161   1998.878      2.954      0.003    1986.555    9822.878\n",
       "6216.0            4469.4616   1954.080      2.287      0.022     639.113    8299.810\n",
       "62221.0           4902.3632   4128.231      1.188      0.235   -3189.715     1.3e+04\n",
       "6259.0           -8574.6540   2515.585     -3.409      0.001   -1.35e+04   -3643.654\n",
       "62599.0           1.506e+04   4752.723      3.168      0.002    5740.036    2.44e+04\n",
       "6266.0            -325.6656   2944.721     -0.111      0.912   -6097.849    5446.518\n",
       "6268.0            2348.9509   1999.256      1.175      0.240   -1569.951    6267.853\n",
       "6288.0            4423.1470   1927.638      2.295      0.022     644.630    8201.664\n",
       "6297.0            6017.0597   2097.504      2.869      0.004    1905.574    1.01e+04\n",
       "6307.0           -1.416e+04   2842.308     -4.984      0.000   -1.97e+04   -8593.247\n",
       "6313.0            3846.2387   3365.323      1.143      0.253   -2750.402    1.04e+04\n",
       "6314.0            4175.3038   1933.465      2.159      0.031     385.363    7965.244\n",
       "6326.0            -418.4821   1867.144     -0.224      0.823   -4078.422    3241.458\n",
       "6349.0            4087.5453   1931.523      2.116      0.034     301.411    7873.679\n",
       "6357.0            5379.8847   2123.978      2.533      0.011    1216.504    9543.265\n",
       "6375.0            1.029e+04   1969.737      5.226      0.000    6432.411    1.42e+04\n",
       "6376.0            5217.1985   1984.313      2.629      0.009    1327.586    9106.811\n",
       "6379.0            1.094e+04   5644.764      1.938      0.053    -127.842     2.2e+04\n",
       "6386.0            6123.9357   1995.295      3.069      0.002    2212.798       1e+04\n",
       "6403.0           -2065.0828   1925.200     -1.073      0.283   -5838.822    1708.656\n",
       "6410.0            5872.2443   1997.358      2.940      0.003    1957.062    9787.427\n",
       "6416.0           -5951.1656   2108.075     -2.823      0.005   -1.01e+04   -1818.959\n",
       "6424.0            4710.5828   1957.869      2.406      0.016     872.807    8548.359\n",
       "6433.0            3397.3386   1908.228      1.780      0.075    -343.133    7137.810\n",
       "6435.0            6910.7687   1921.540      3.596      0.000    3144.204    1.07e+04\n",
       "6492.0            3766.6941   1921.662      1.960      0.050      -0.110    7533.499\n",
       "6497.0           -1.443e+04   2629.279     -5.487      0.000   -1.96e+04   -9271.690\n",
       "6500.0            -996.0686   8129.378     -0.123      0.902   -1.69e+04    1.49e+04\n",
       "6509.0            4907.2634   1957.900      2.506      0.012    1069.426    8745.101\n",
       "6527.0            5893.6325   2368.628      2.488      0.013    1250.694    1.05e+04\n",
       "6528.0            2253.1849   2111.822      1.067      0.286   -1886.367    6392.737\n",
       "6531.0           -1258.6150   1957.951     -0.643      0.520   -5096.552    2579.322\n",
       "6532.0            -325.0480   1919.895     -0.169      0.866   -4088.389    3438.293\n",
       "6543.0            5694.8539   1988.134      2.864      0.004    1797.753    9591.955\n",
       "6548.0            4672.1866   1956.259      2.388      0.017     837.566    8506.807\n",
       "6550.0            5613.9605   2208.789      2.542      0.011    1284.335    9943.586\n",
       "6552.0            4852.5307   2141.141      2.266      0.023     655.508    9049.553\n",
       "6565.0            4161.7340   2167.386      1.920      0.055     -86.733    8410.201\n",
       "6571.0            5142.2645   1973.100      2.606      0.009    1274.632    9009.897\n",
       "6573.0            4804.7906   1948.741      2.466      0.014     984.906    8624.675\n",
       "6641.0            -399.9826   4068.185     -0.098      0.922   -8374.359    7574.394\n",
       "6649.0            6153.3134   1992.015      3.089      0.002    2248.604    1.01e+04\n",
       "6730.0           -4085.8431   3204.549     -1.275      0.202   -1.04e+04    2195.650\n",
       "6731.0            4182.8424   1932.897      2.164      0.030     394.016    7971.669\n",
       "6742.0            2591.0039   4098.503      0.632      0.527   -5442.802    1.06e+04\n",
       "6745.0            5954.5814   2001.155      2.976      0.003    2031.957    9877.205\n",
       "6756.0            3836.5715   1919.730      1.998      0.046      73.554    7599.589\n",
       "6765.0           -1.681e+04   2683.691     -6.264      0.000   -2.21e+04   -1.16e+04\n",
       "6768.0            6256.1243   2034.818      3.075      0.002    2267.515    1.02e+04\n",
       "6774.0           -2.614e+04   3202.997     -8.162      0.000   -3.24e+04   -1.99e+04\n",
       "6797.0            5646.2077   2455.892      2.299      0.022     832.217    1.05e+04\n",
       "6803.0            4701.3774   1956.186      2.403      0.016     866.900    8535.855\n",
       "6821.0            4844.1439   1967.004      2.463      0.014     988.461    8699.827\n",
       "6830.0            2159.2106   1877.370      1.150      0.250   -1520.773    5839.195\n",
       "6845.0            4949.2354   1959.344      2.526      0.012    1108.568    8789.903\n",
       "6848.0            2164.0008   2110.021      1.026      0.305   -1972.021    6300.022\n",
       "6873.0            1893.3294   2772.788      0.683      0.495   -3541.835    7328.494\n",
       "6900.0            5171.9492   1965.983      2.631      0.009    1318.268    9025.630\n",
       "6908.0            4728.0975   1945.157      2.431      0.015     915.239    8540.956\n",
       "6994.0             731.2960   1868.224      0.391      0.695   -2930.759    4393.351\n",
       "7045.0           -1.134e+04   2736.825     -4.145      0.000   -1.67e+04   -5978.329\n",
       "7065.0            1.014e+04   1996.162      5.081      0.000    6230.132    1.41e+04\n",
       "7085.0            8188.2669   1985.465      4.124      0.000    4296.397    1.21e+04\n",
       "7107.0            5537.7463   2145.524      2.581      0.010    1332.133    9743.359\n",
       "7116.0            5979.0806   1967.201      3.039      0.002    2123.013    9835.148\n",
       "7117.0            5277.3971   3150.678      1.675      0.094    -898.499    1.15e+04\n",
       "7121.0            3337.7779   1909.289      1.748      0.080    -404.773    7080.329\n",
       "7127.0             891.4098   2003.758      0.445      0.656   -3036.318    4819.137\n",
       "7139.0            5258.5464   1978.876      2.657      0.008    1379.592    9137.501\n",
       "7146.0            5334.3482   1973.436      2.703      0.007    1466.059    9202.638\n",
       "7163.0            7833.1917   1972.917      3.970      0.000    3965.918    1.17e+04\n",
       "7180.0            2429.4741   1932.129      1.257      0.209   -1357.847    6216.795\n",
       "7183.0            3618.8978   1911.469      1.893      0.058    -127.926    7365.721\n",
       "7228.0            1.164e+04   1895.440      6.142      0.000    7927.017    1.54e+04\n",
       "7232.0            -481.5654   3080.004     -0.156      0.876   -6518.928    5555.797\n",
       "7250.0            -921.2075   2179.821     -0.423      0.673   -5194.050    3351.634\n",
       "7257.0            1.726e+04   3178.570      5.429      0.000     1.1e+04    2.35e+04\n",
       "7260.0            4843.1503   1946.744      2.488      0.013    1027.180    8659.120\n",
       "7267.0              41.7571   2048.866      0.020      0.984   -3974.390    4057.905\n",
       "7268.0           -1.684e+04   2894.155     -5.819      0.000   -2.25e+04   -1.12e+04\n",
       "7281.0            5210.4769   2932.058      1.777      0.076    -536.886     1.1e+04\n",
       "7291.0            4834.4826   1946.833      2.483      0.013    1018.340    8650.625\n",
       "7343.0           -1.104e+04   3484.277     -3.168      0.002   -1.79e+04   -4207.797\n",
       "7346.0           -8749.8624   2173.692     -4.025      0.000    -1.3e+04   -4489.034\n",
       "7401.0            5949.0922   1998.542      2.977      0.003    2031.589    9866.596\n",
       "7409.0            5543.7513   1967.066      2.818      0.005    1687.948    9399.555\n",
       "7420.0            1119.1778   1871.032      0.598      0.550   -2548.382    4786.737\n",
       "7435.0             1.08e+04   2056.810      5.249      0.000    6765.135    1.48e+04\n",
       "7466.0            2379.6810   2005.849      1.186      0.236   -1552.144    6311.506\n",
       "7486.0           -1.506e+04   2681.366     -5.617      0.000   -2.03e+04   -9804.392\n",
       "7503.0            4986.3882   3674.490      1.357      0.175   -2216.274    1.22e+04\n",
       "7506.0            6434.6407   1941.012      3.315      0.001    2629.907    1.02e+04\n",
       "7537.0            3786.8460   1919.409      1.973      0.049      24.457    7549.235\n",
       "7549.0            4718.8137   1958.983      2.409      0.016     878.854    8558.774\n",
       "7554.0            4578.5389   1957.554      2.339      0.019     741.380    8415.698\n",
       "7557.0            4507.4862   1991.371      2.264      0.024     604.041    8410.932\n",
       "7585.0             -1.1e+04   3197.194     -3.439      0.001   -1.73e+04   -4728.826\n",
       "7602.0            3499.7827   1912.556      1.830      0.067    -249.172    7248.737\n",
       "7620.0           -2958.4318   2133.959     -1.386      0.166   -7141.376    1224.513\n",
       "7636.0            3958.3364   1925.327      2.056      0.040     184.349    7732.324\n",
       "7646.0            4582.9805   1943.067      2.359      0.018     774.218    8391.743\n",
       "7658.0            3684.4239   1924.022      1.915      0.056     -87.005    7455.853\n",
       "7683.0            6082.7650   2154.264      2.824      0.005    1860.020    1.03e+04\n",
       "7685.0            4896.4405   2183.537      2.242      0.025     616.315    9176.566\n",
       "7692.0           -1913.9549   1879.822     -1.018      0.309   -5598.744    1770.834\n",
       "7762.0            5606.3133   1975.729      2.838      0.005    1733.527    9479.099\n",
       "7772.0           -6987.3601   2071.660     -3.373      0.001    -1.1e+04   -2926.532\n",
       "7773.0            4826.5255   1952.344      2.472      0.013     999.579    8653.472\n",
       "7777.0            -743.5441   1870.869     -0.397      0.691   -4410.785    2923.696\n",
       "7835.0            5910.7386   1990.271      2.970      0.003    2009.449    9812.028\n",
       "7873.0           -1.392e+04   2577.000     -5.402      0.000    -1.9e+04   -8869.376\n",
       "7883.0            4965.3718   1956.885      2.537      0.011    1129.525    8801.219\n",
       "7904.0            4206.8758   1987.093      2.117      0.034     311.816    8101.936\n",
       "7906.0            7648.2490   1991.172      3.841      0.000    3745.192    1.16e+04\n",
       "7921.0            6255.4850   1976.128      3.166      0.002    2381.917    1.01e+04\n",
       "7923.0            4897.9116   2083.404      2.351      0.019     814.065    8981.758\n",
       "7935.0            1663.1797   1882.307      0.884      0.377   -2026.482    5352.841\n",
       "7938.0            4331.9445   1973.788      2.195      0.028     462.965    8200.925\n",
       "7985.0           -2.095e+04   2840.387     -7.374      0.000   -2.65e+04   -1.54e+04\n",
       "8014.0            1818.9112   1985.535      0.916      0.360   -2073.095    5710.917\n",
       "8030.0            5271.2842   1953.316      2.699      0.007    1442.433    9100.136\n",
       "8046.0            -1.41e+04   2617.753     -5.385      0.000   -1.92e+04   -8964.658\n",
       "8047.0            4897.3794   2783.776      1.759      0.079    -559.323    1.04e+04\n",
       "8062.0            2869.1377   1944.285      1.476      0.140    -942.012    6680.287\n",
       "8068.0           -1.295e+04   2223.415     -5.825      0.000   -1.73e+04   -8593.051\n",
       "8087.0               -2e+04   3143.524     -6.361      0.000   -2.62e+04   -1.38e+04\n",
       "8095.0            5818.9744   1999.918      2.910      0.004    1898.774    9739.174\n",
       "8096.0            4312.5818   1936.679      2.227      0.026     516.341    8108.822\n",
       "8109.0            5654.5465   1988.576      2.844      0.004    1756.580    9552.513\n",
       "8123.0            5053.8975   1967.357      2.569      0.010    1197.522    8910.273\n",
       "8150.0            5768.0364   1996.610      2.889      0.004    1854.320    9681.753\n",
       "8163.0            5755.4269   1988.504      2.894      0.004    1857.600    9653.254\n",
       "8176.0           -8448.5963   2569.276     -3.288      0.001   -1.35e+04   -3412.353\n",
       "8202.0            3871.5628   2017.547      1.919      0.055     -83.192    7826.318\n",
       "8214.0            3792.6248   1974.945      1.920      0.055     -78.623    7663.873\n",
       "8215.0            2763.3451   2113.139      1.308      0.191   -1378.788    6905.478\n",
       "8219.0            5537.6888   1990.441      2.782      0.005    1636.065    9439.313\n",
       "8247.0            4708.5506   1944.332      2.422      0.015     897.310    8519.792\n",
       "8253.0           -1.809e+04   2823.040     -6.406      0.000   -2.36e+04   -1.26e+04\n",
       "8290.0            2764.7807   2079.920      1.329      0.184   -1312.238    6841.799\n",
       "8293.0            5899.3765   1996.449      2.955      0.003    1985.977    9812.776\n",
       "8304.0            5848.3913   1938.951      3.016      0.003    2047.698    9649.085\n",
       "8334.0            3024.1407   2008.596      1.506      0.132    -913.070    6961.352\n",
       "8348.0            5556.4878   1985.328      2.799      0.005    1664.886    9448.089\n",
       "8357.0            5079.4213   1960.665      2.591      0.010    1236.165    8922.678\n",
       "8358.0            4801.4051   1955.371      2.455      0.014     968.526    8634.284\n",
       "8446.0           -4093.8049   2374.182     -1.724      0.085   -8747.629     560.020\n",
       "8460.0            6216.4871   2366.258      2.627      0.009    1578.194    1.09e+04\n",
       "8463.0            4407.1963   1951.703      2.258      0.024     581.507    8232.885\n",
       "8479.0            9641.6705   2783.822      3.463      0.001    4184.877    1.51e+04\n",
       "8530.0            -675.0843   3366.838     -0.201      0.841   -7274.693    5924.524\n",
       "8536.0            2752.2508   1915.052      1.437      0.151   -1001.596    6506.098\n",
       "8543.0            2.363e+04   2252.988     10.488      0.000    1.92e+04     2.8e+04\n",
       "8549.0           -9215.4301   2086.704     -4.416      0.000   -1.33e+04   -5125.114\n",
       "8551.0            5270.8432   1986.267      2.654      0.008    1377.401    9164.285\n",
       "8559.0           -3645.5456   2115.744     -1.723      0.085   -7792.785     501.694\n",
       "8573.0           -1.627e+04   2904.839     -5.601      0.000    -2.2e+04   -1.06e+04\n",
       "8606.0            7964.2030   1983.187      4.016      0.000    4076.799    1.19e+04\n",
       "8607.0            4875.7978   1965.120      2.481      0.013    1023.809    8727.787\n",
       "8648.0            4640.0866   1948.982      2.381      0.017     819.730    8460.443\n",
       "8657.0           -3330.0016   1917.841     -1.736      0.083   -7089.315     429.312\n",
       "8675.0            3299.0514   3674.268      0.898      0.369   -3903.176    1.05e+04\n",
       "8681.0            -917.2164   1872.610     -0.490      0.624   -4587.870    2753.437\n",
       "8687.0            1048.2952   2175.405      0.482      0.630   -3215.890    5312.480\n",
       "8692.0            3104.2442   1921.700      1.615      0.106    -662.635    6871.123\n",
       "8699.0            5391.0396   1978.249      2.725      0.006    1513.316    9268.763\n",
       "8717.0            5756.4000   1983.942      2.901      0.004    1867.517    9645.283\n",
       "8759.0           -4660.8382   1959.097     -2.379      0.017   -8501.021    -820.655\n",
       "8762.0            8546.7408   2018.068      4.235      0.000    4590.963    1.25e+04\n",
       "8819.0            5782.6825   1994.102      2.900      0.004    1873.883    9691.482\n",
       "8850.0            5389.3481   1979.050      2.723      0.006    1510.054    9268.642\n",
       "8852.0            5529.1546   1983.027      2.788      0.005    1642.064    9416.246\n",
       "8859.0            4530.6367   1955.107      2.317      0.021     698.275    8362.999\n",
       "8867.0           -3956.3934   2044.298     -1.935      0.053   -7963.586      50.799\n",
       "8881.0            3397.4168   1909.748      1.779      0.075    -346.034    7140.868\n",
       "8958.0            -333.9629   1869.765     -0.179      0.858   -3999.040    3331.114\n",
       "8972.0           -2.272e+04   3204.562     -7.089      0.000    -2.9e+04   -1.64e+04\n",
       "8990.0           -1.458e+04   2732.112     -5.335      0.000   -1.99e+04   -9219.887\n",
       "9004.0            5370.0245   2283.150      2.352      0.019     894.639    9845.410\n",
       "9016.0            2046.2952   1880.221      1.088      0.276   -1639.278    5731.868\n",
       "9048.0            2368.2290   1887.367      1.255      0.210   -1331.350    6067.808\n",
       "9051.0           -1.024e+04   2578.366     -3.973      0.000   -1.53e+04   -5190.757\n",
       "9071.0            5656.2012   1976.330      2.862      0.004    1782.237    9530.165\n",
       "9112.0           -1417.9474   1875.489     -0.756      0.450   -5094.244    2258.349\n",
       "9114.0            1647.2302   1954.192      0.843      0.399   -2183.339    5477.799\n",
       "9132.0            2094.4429   3339.339      0.627      0.531   -4451.263    8640.149\n",
       "9173.0            4739.7071   2316.199      2.046      0.041     199.540    9279.874\n",
       "9180.0            5631.9645   1997.191      2.820      0.005    1717.110    9546.819\n",
       "9186.0            4493.3047   1941.754      2.314      0.021     687.116    8299.493\n",
       "9191.0            2470.6002   3342.460      0.739      0.460   -4081.223    9022.424\n",
       "9216.0           -1536.8001   1882.692     -0.816      0.414   -5227.216    2153.615\n",
       "9217.0           -4464.1877   1954.492     -2.284      0.022   -8295.344    -633.031\n",
       "9225.0            6238.3665   1997.849      3.123      0.002    2322.222    1.02e+04\n",
       "9230.0            4381.5157   2512.608      1.744      0.081    -543.650    9306.681\n",
       "9259.0            5834.6453   1995.708      2.924      0.003    1922.697    9746.594\n",
       "9293.0            5936.9469   1988.853      2.985      0.003    2038.437    9835.457\n",
       "9299.0            2477.2047   2000.816      1.238      0.216   -1444.755    6399.164\n",
       "9308.0           -7746.5538   2112.622     -3.667      0.000   -1.19e+04   -3605.434\n",
       "9311.0           -2566.7239   3099.965     -0.828      0.408   -8643.214    3509.766\n",
       "9313.0           -2848.8965   1878.359     -1.517      0.129   -6530.819     833.026\n",
       "9325.0            4546.5205   1994.091      2.280      0.023     637.742    8455.299\n",
       "9332.0            5360.0205   1977.740      2.710      0.007    1483.294    9236.747\n",
       "9340.0           -4.099e+04   5543.358     -7.395      0.000   -5.19e+04   -3.01e+04\n",
       "9372.0            3247.9219   2267.303      1.433      0.152   -1196.400    7692.244\n",
       "9411.0            3231.7183   2071.422      1.560      0.119    -828.643    7292.080\n",
       "9459.0            -1.17e+04   3358.202     -3.484      0.000   -1.83e+04   -5118.586\n",
       "9465.0            1.242e+04   1955.181      6.351      0.000    8585.002    1.63e+04\n",
       "9472.0           -3879.1823   1935.402     -2.004      0.045   -7672.918     -85.446\n",
       "9483.0           -1.629e+04   2806.835     -5.803      0.000   -2.18e+04   -1.08e+04\n",
       "9563.0           -2.153e+04   3454.348     -6.233      0.000   -2.83e+04   -1.48e+04\n",
       "9590.0           -2660.5248   1904.053     -1.397      0.162   -6392.812    1071.763\n",
       "9598.0           -6031.7426   2877.301     -2.096      0.036   -1.17e+04    -391.713\n",
       "9599.0           -7906.4917   2130.319     -3.711      0.000   -1.21e+04   -3730.682\n",
       "9602.0           -5952.7429   3376.881     -1.763      0.078   -1.26e+04     666.552\n",
       "9619.0            3481.6826   1911.796      1.821      0.069    -265.783    7229.148\n",
       "9643.0            5433.9198   1992.138      2.728      0.006    1528.970    9338.869\n",
       "9650.0            5427.2120   1978.219      2.743      0.006    1549.546    9304.878\n",
       "9653.0           -1.512e+04   4541.405     -3.329      0.001    -2.4e+04   -6214.896\n",
       "9667.0            4520.3865   1946.279      2.323      0.020     705.329    8335.444\n",
       "9698.0            1367.7058   1873.827      0.730      0.465   -2305.333    5040.745\n",
       "9699.0            4281.7503   1896.184      2.258      0.024     564.888    7998.612\n",
       "9719.0           -8344.7727   2157.998     -3.867      0.000   -1.26e+04   -4114.707\n",
       "9742.0           -1.428e+04   2604.062     -5.484      0.000   -1.94e+04   -9176.000\n",
       "9761.0            5812.7072   1998.234      2.909      0.004    1895.809    9729.606\n",
       "9771.0           -4544.7951   1939.992     -2.343      0.019   -8347.529    -742.061\n",
       "9772.0            4591.0572   1934.805      2.373      0.018     798.490    8383.624\n",
       "9778.0            3626.0662   1898.512      1.910      0.056     -95.359    7347.491\n",
       "9799.0           -7061.2889   2203.249     -3.205      0.001   -1.14e+04   -2742.525\n",
       "9815.0            5697.1487   2072.046      2.750      0.006    1635.565    9758.733\n",
       "9818.0           -3.302e+04   2308.852    -14.300      0.000   -3.75e+04   -2.85e+04\n",
       "9837.0            5631.7113   1992.359      2.827      0.005    1726.329    9537.094\n",
       "9922.0           -8117.4344   2143.677     -3.787      0.000   -1.23e+04   -3915.440\n",
       "9954.0            4399.1214   2611.570      1.684      0.092    -720.027    9518.270\n",
       "9963.0            4786.1265   1984.341      2.412      0.016     896.461    8675.792\n",
       "9988.0            5320.7734   1998.236      2.663      0.008    1403.871    9237.676\n",
       "9999.0           -9357.5801   2209.351     -4.235      0.000   -1.37e+04   -5026.853\n",
       "gspillsicIVX1982    -0.0100      0.098     -0.102      0.919      -0.201       0.181\n",
       "gspillsicIVX1983    -0.0551      0.096     -0.575      0.565      -0.243       0.133\n",
       "gspillsicIVX1984    -0.1174      0.094     -1.244      0.213      -0.302       0.068\n",
       "gspillsicIVX1985    -0.1671      0.094     -1.774      0.076      -0.352       0.018\n",
       "gspillsicIVX1986    -0.2239      0.095     -2.367      0.018      -0.409      -0.038\n",
       "gspillsicIVX1987    -0.2562      0.095     -2.694      0.007      -0.443      -0.070\n",
       "gspillsicIVX1988    -0.2953      0.096     -3.087      0.002      -0.483      -0.108\n",
       "gspillsicIVX1989    -0.2942      0.097     -3.049      0.002      -0.483      -0.105\n",
       "gspillsicIVX1990    -0.3209      0.098     -3.287      0.001      -0.512      -0.130\n",
       "gspillsicIVX1991    -0.3112      0.099     -3.150      0.002      -0.505      -0.118\n",
       "gspillsicIVX1992    -0.3633      0.100     -3.636      0.000      -0.559      -0.167\n",
       "gspillsicIVX1993    -0.3810      0.101     -3.758      0.000      -0.580      -0.182\n",
       "gspillsicIVX1994    -0.3920      0.103     -3.820      0.000      -0.593      -0.191\n",
       "gspillsicIVX1995    -0.3683      0.105     -3.518      0.000      -0.574      -0.163\n",
       "gspillsicIVX1996    -0.4071      0.107     -3.788      0.000      -0.618      -0.196\n",
       "gspillsicIVX1997    -0.3834      0.111     -3.467      0.001      -0.600      -0.167\n",
       "gspillsicIVX1998    -0.3315      0.114     -2.913      0.004      -0.554      -0.108\n",
       "gspillsicIVX1999    -0.2970      0.117     -2.546      0.011      -0.526      -0.068\n",
       "==============================================================================\n",
       "Omnibus:                    22689.763   Durbin-Watson:                   0.751\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):        140364584.441\n",
       "Skew:                          14.706   Prob(JB):                         0.00\n",
       "Kurtosis:                     537.957   Cond. No.                     1.66e+18\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.75e-25. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product spillovers model, firm FE's\n",
    "x_vars_fe = x_vars.drop(columns=drop_columns)\n",
    "\n",
    "year_model5 = sm.OLS(y_var,x_vars_fe).fit()\n",
    "year_model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d70dcb93-017d-4292-ac78-1e52c59abadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab estimates. Add non-interactive term to each year coefficient\n",
    "# Build Dataframe of year, coefficient, conf intervals\n",
    "\n",
    "# use years to interate through\n",
    "# years = np.append(years, '1981')\n",
    "# years = years.sort_values()\n",
    "\n",
    "time_coefs_sic = {\n",
    "    'year': np.append(years,'1981'),\n",
    "    'coef': [np.nan for i in range(0,len(years)+1)],\n",
    "    'CI_l': [np.nan for i in range(0,len(years)+1)],\n",
    "    'CI_h': [np.nan for i in range(0,len(years)+1)],\n",
    "    'se': [np.nan for i in range(0,len(years)+1)]\n",
    "}\n",
    "\n",
    "time_coefs_sic = pd.DataFrame(time_coefs_sic)\n",
    "time_coefs_sic = time_coefs_sic.sort_values(by='year').reset_index(drop=True)\n",
    "\n",
    "conf_intervals = year_model5.conf_int(alpha=0.05, cols=None)\n",
    "s_errors = year_model5.HC0_se\n",
    "\n",
    "# grab coefficients and confidence intervals\n",
    "coef_81 = year_model5.params['gspillsicIV'] # coef for 1981\n",
    "\n",
    "for year in time_coefs_sic['year'].unique():\n",
    "    if year == '1981':\n",
    "        coef = coef_81\n",
    "        ci_l = 0 \n",
    "        ci_h = 0\n",
    "        \n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'coef'] = coef\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_l'] = ci_l\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_h'] = ci_h\n",
    "        \n",
    "    else:\n",
    "        col_name = f\"gspillsicIVX{year}\"\n",
    "        coef = year_model5.params[col_name] + coef_81\n",
    "        ci_l = conf_intervals.loc[conf_intervals.index == col_name,0].values[0] + coef_81\n",
    "        ci_h = conf_intervals.loc[conf_intervals.index == col_name,1].values[0] + coef_81\n",
    "        se = \n",
    "        \n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'coef'] = coef\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_l'] = ci_l\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_h'] = ci_h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9a13d487-f1f8-42ea-af72-6aa9a3b84519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_coefs_sic['upper'] = time_coefs_sic['CI_h'] - time_coefs_sic['coef']\n",
    "time_coefs_sic['lower'] = time_coefs_sic['coef'] - time_coefs_sic['CI_l']\n",
    "time_coefs_sic.loc[time_coefs_sic['year'] == '1981', ['upper','lower']] = 0\n",
    "\n",
    "cis = time_coefs_sic[['lower','upper']].T\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(x=time_coefs_sic['year'],y=time_coefs_sic['coef'],yerr=cis)\n",
    "plt.axhline(y=coef_81, linestyle='dashed', color='r')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Values by Year with Confidence Intervals')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "620daffe-ce4c-4423-8e2b-08b236e23f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>coef</th>\n",
       "      <th>CI_l</th>\n",
       "      <th>CI_h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981</td>\n",
       "      <td>1.228403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982</td>\n",
       "      <td>1.218424</td>\n",
       "      <td>1.027256</td>\n",
       "      <td>1.409592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983</td>\n",
       "      <td>1.173297</td>\n",
       "      <td>0.985423</td>\n",
       "      <td>1.361171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>1.111012</td>\n",
       "      <td>0.926054</td>\n",
       "      <td>1.295970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985</td>\n",
       "      <td>1.061254</td>\n",
       "      <td>0.876589</td>\n",
       "      <td>1.245919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year      coef      CI_l      CI_h\n",
       "0  1981  1.228403  0.000000  0.000000\n",
       "1  1982  1.218424  1.027256  1.409592\n",
       "2  1983  1.173297  0.985423  1.361171\n",
       "3  1984  1.111012  0.926054  1.295970\n",
       "4  1985  1.061254  0.876589  1.245919"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb59ca5-5fca-4adf-bdd6-f013ae965e66",
   "metadata": {},
   "source": [
    "# DAG model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b1200-939b-4733-b44d-d4fd27e5ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG3 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 0\n",
    "    label \"rmkvaf\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"rxrd\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"rsales\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 3\n",
    "    label \"gspilltecIV\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"gspillsicIV\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 5\n",
    "    label \"pat_count\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"rppent\"\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 0\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 7\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 3\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 4\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 4\n",
    "    target 2\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 7\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 5\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 2\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 2\n",
    "    target 0\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 0\n",
    "    ]\n",
    "\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4fa94-6a7f-44b4-8c2d-1c2592fa852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalModel(\n",
    "    data=df,\n",
    "    treatment='gspilltecIV',\n",
    "    outcome='rmkvaf',\n",
    "    graph=DAG3)\n",
    "\n",
    "model.view_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750d61e-8f45-4335-a771-e7279042f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify estimand\n",
    "estimand = model.identify_effect()\n",
    "print(estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f5b87-4a1f-471e-b67a-12b1b9ffc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain estimates\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand=estimand,\n",
    "    method_name='backdoor.linear_regression')\n",
    "\n",
    "print(estimate.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e0e80-ebcb-4543-b1b5-8eedafa7ac0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# refutation test\n",
    "refute_subset = model.refute_estimate(\n",
    "    estimand=estimand,\n",
    "    estimate=estimate,\n",
    "    method_name=\"data_subset_refuter\",\n",
    "    subset_fraction=0.4)\n",
    "\n",
    "print(refute_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e48a0-25a6-4faf-8a5a-2db6ba5e8635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39600fbc-c968-438a-b2d4-1758b578a563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0c591-95c7-42e6-b42f-8d3ba9b0f447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c3ef64-ca2f-4b0e-ae15-74b57b958650",
   "metadata": {},
   "source": [
    "# Scrap Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce3ff2-b571-4e3c-9b57-c7c6210b0ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56cc964a-8741-4ebf-8c7c-ea8c06631143",
   "metadata": {},
   "source": [
    "X_spills = df[['gspilltecIV', 'gspillsicIV']]\n",
    "\n",
    "(X_trainsp,\n",
    " X_testsp,\n",
    " y_trainsp,\n",
    " y_testsp) = skm.train_test_split(X_spills,\n",
    "                                df['rmkvaf'],\n",
    "                                test_size=0.3,\n",
    "                                random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704c099-cfef-4cc6-8905-38615eb18d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try a neural network\n",
    "# one_layers = [\n",
    "#     (1),\n",
    "#     (2),\n",
    "#     (3),\n",
    "#     (4),\n",
    "#     (5)\n",
    "# ]\n",
    "\n",
    "# two_layers = [\n",
    "#     (1,1),(1,2),(1,3),\n",
    "#     (2,1),(2,2),(2,3),(2,4),\n",
    "#     (3,1),(3,2),(3,3),(3,4),(3,5),\n",
    "#     (4,1),(4,2),(4,3),(4,4),(4,5),\n",
    "#     (5,1),(5,2),(5,3),(5,4),(5,5)\n",
    "# ]\n",
    "\n",
    "# # Use DML with a PLR equation, keeping spillovers entering linearly\n",
    "# # Use Random Forest as the ML model\n",
    "\n",
    "# # Specify doubleML data model\n",
    "\n",
    "# x_vars = ['pat_count','rsales','rppent','emp','rxrd']\n",
    "# data_dml_tec = dml.DoubleMLData(df,\n",
    "#                                  y_col='rmkvaf',\n",
    "#                                  d_cols='gspilltec',\n",
    "#                                  x_cols=x_vars)\n",
    "\n",
    "# RF_DML = RF(max_features=3, random_state=0)\n",
    "\n",
    "# # Implement PLR DML estimation with gspilltec linear\n",
    "\n",
    "# dml_plr_tec = dml.DoubleMLPLR(data_dml_tec,\n",
    "#                                 ml_l = RF_DML,\n",
    "#                                 ml_m = RF_DML,\n",
    "#                                 n_folds = 3)\n",
    "\n",
    "# dml_plr_tec.fit(store_predictions=True)\n",
    "# print(dml_plr_tec.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
