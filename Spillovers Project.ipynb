{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d745b36-8f92-4c0e-a42b-21d8de06dc8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import statsmodels.api as sm\n",
    "    import sklearn.linear_model as lm\n",
    "    import sklearn.model_selection as skm\n",
    "    import statsmodels.formula.api as smf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from ISLP.models import ModelSpec as MS\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "    from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss,\n",
    "                                mean_squared_error)\n",
    "    from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)\n",
    "    from ISLP.bart import BART\n",
    "    import xgboost as xgb\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    #import doubleml as dml\n",
    "    import graphviz\n",
    "    import networkx as nx\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel\n",
    "    \n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install ISLP -q\n",
    "    !pip install stargazer -q\n",
    "    !pip install xgboost -q\n",
    "    !pip install doubleml -q\n",
    "    !pip install dowhy -q\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import statsmodels.api as sm\n",
    "    import sklearn.linear_model as lm\n",
    "    import sklearn.model_selection as skm\n",
    "    import statsmodels.formula.api as smf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from stargazer.stargazer import Stargazer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from ISLP.models import ModelSpec as MS\n",
    "    from sklearn.tree import (DecisionTreeClassifier as DTC,\n",
    "                          DecisionTreeRegressor as DTR,\n",
    "                          plot_tree,\n",
    "                          export_text)\n",
    "    from sklearn.metrics import (accuracy_score,\n",
    "                             log_loss,\n",
    "                                mean_squared_error)\n",
    "    from sklearn.ensemble import \\\n",
    "     (RandomForestRegressor as RF,\n",
    "      GradientBoostingRegressor as GBR)\n",
    "    from ISLP.bart import BART\n",
    "    import xgboost as xgb\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    #import doubleml as dml\n",
    "    import graphviz\n",
    "    import networkx as nx\n",
    "    import dowhy\n",
    "    from dowhy import CausalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3175a2c0-27be-498a-bae5-6078dd085bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"spillovers.dta\"\n",
    "\n",
    "data = pd.read_stata(data_loc, iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7398c00a-ae54-4b7b-aa65-a93e6eb2b121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cusip': 'CUSIP',\n",
       " 'year': 'year',\n",
       " 'xrd': 'Expenditure on R&D',\n",
       " 'ppent': 'Net book value of property, plant and equipment',\n",
       " 'spillsic': 'SIC correlation weighted R&D of other firms, 1996 values',\n",
       " 'spillcovsic': '',\n",
       " 'spillmalsic': '',\n",
       " 'spillmalcovsic': '',\n",
       " 'spilltec': 'Patent NClass correlation weighted R&D of other firms, 1996 values',\n",
       " 'spillcovtec': '',\n",
       " 'spillmaltec': '',\n",
       " 'spillmalcovtec': '',\n",
       " 'spilltloc': '',\n",
       " 'spillsloc': '',\n",
       " 'spilltectloc': '',\n",
       " 'spilltecsloc': '',\n",
       " 'spillsicsloc': '',\n",
       " 'spillsictloc': '',\n",
       " 'lstate': '',\n",
       " 'lfirm': '',\n",
       " 'firm_dum': '',\n",
       " 'hxrd': '',\n",
       " 'spillsicIV': '',\n",
       " 'spilltecIV': '',\n",
       " 'spillsicIV_mal': '',\n",
       " 'spilltecIV_mal': '',\n",
       " 'p005': '',\n",
       " 'sales_ind': 'Control variable - Total sales weighted by sic sales matrix',\n",
       " 'sales_ind_ns': 'Control variable - Total Value Shipments in Manufacturing from National Statisti',\n",
       " 'patents_ind': 'Control variable - Total number of patents weighted by tech matrix',\n",
       " 'pat_count': 'Patents per firm applied for that year',\n",
       " 'i': 'Unique firm level indentifier, like cusip but in numeric values',\n",
       " 'tic': 'company ticker',\n",
       " 'comn': 'company name',\n",
       " 'sic': 'primary SIC code (firm)',\n",
       " 'at': 'total assets (firm)',\n",
       " 'sales': 'total sales (firm)',\n",
       " 'oibdp': 'op. income before depr. (firm)',\n",
       " 'ib': 'income before extraord. items (firm)',\n",
       " 'emp': 'employment (firm)',\n",
       " 'dldte': 'data of deletion',\n",
       " 'dlrsn': 'reason of deletion',\n",
       " 'segnum': 'number of business segments',\n",
       " 'invt': 'Total inventories',\n",
       " 'intan': 'Total intangibles',\n",
       " 'ivaeq': 'Investments and advances - equity method',\n",
       " 'ivao': 'Investments and advances - other',\n",
       " 'act': 'Current assets - total',\n",
       " 'ao': 'Assets other',\n",
       " 'oiadp': 'Operating profits before amortization and depreciation',\n",
       " 'gics': '',\n",
       " 'poa': 'profits over assets',\n",
       " 'pos': 'profits over sales',\n",
       " 'mkvaf': 'Market value',\n",
       " 'dt': 'Total debt',\n",
       " 'pstk': 'Preferred Equity Total (par/stated value)',\n",
       " 'xad': 'Advertising expense',\n",
       " 'pi': 'Pre-tax income',\n",
       " 'aqi': '',\n",
       " 'aqs': '',\n",
       " 'ppegt': 'Plant, property and equipment, gross book value',\n",
       " 'capx': 'Capital expenditures',\n",
       " 'capxv': 'Capital expenditure through acquisition as well',\n",
       " 'xrent': '',\n",
       " 'xint': '',\n",
       " 'xintd': '',\n",
       " 'xlr': 'Labour expenses',\n",
       " 'xpr': '',\n",
       " 'xsga': '',\n",
       " 'cogs': '',\n",
       " 'invfg': '',\n",
       " 'invwip': '',\n",
       " 'invo': '',\n",
       " 'invrm': '',\n",
       " 'invval1': '',\n",
       " 'lifr': '',\n",
       " 'dp': '',\n",
       " 'la': 'log10(assets)',\n",
       " 'ls': 'log10(sales)',\n",
       " 'pindex': 'CPI price index used to deflate all variables',\n",
       " 'infl': '',\n",
       " 'inf': '',\n",
       " 'lsic': '',\n",
       " 'lcovsic': '',\n",
       " 'lmalsic': '',\n",
       " 'lmalcovsic': '',\n",
       " 'ltec': '',\n",
       " 'lcovtec': '',\n",
       " 'lmaltec': '',\n",
       " 'lmalcovtec': '',\n",
       " 'ltloc': '',\n",
       " 'lsloc': '',\n",
       " 'ltectloc': '',\n",
       " 'ltecsloc': '',\n",
       " 'lsicsloc': '',\n",
       " 'lsictloc': '',\n",
       " 'ltecIV': '',\n",
       " 'lsicIV': '',\n",
       " 'ltecIV_mal': '',\n",
       " 'lsicIV_mal': '',\n",
       " 'lxrd': 'Log R&D expenditure',\n",
       " 'lpatents_ind': 'Log Total patents in n-class weighted industries (by firm year) - tech shock con',\n",
       " 'lpatents_indt': '',\n",
       " 'lpatents_indm': '',\n",
       " 'ttt': '',\n",
       " 'lpatpat': '',\n",
       " 'lpat_count': 'Log of patent count by year - missing values -1 and missing indicator lpat_count',\n",
       " 'lpat_cite_norm': 'Log of cite weighted patent count by year',\n",
       " 'lsales': 'Log sales',\n",
       " 'lppent': 'Log of ppent - i.e. net tangible fixed assets',\n",
       " 'lemp': 'Log count of employees',\n",
       " 'pat_cite': 'Cites per firm',\n",
       " 'pat_cite_norm': 'Cites per firm, normalized to average 1 per year',\n",
       " 'dsales': '',\n",
       " 'demp': '',\n",
       " 'sales_emp': '',\n",
       " 'ppent_emp': '',\n",
       " 'dy': '',\n",
       " 'dym': '',\n",
       " 'jumpyear': '',\n",
       " 'oldnum': '',\n",
       " 'num': '',\n",
       " 'prob': '',\n",
       " 'dyear': '',\n",
       " 'lxrd1': 'Lag Log R&D expenditure',\n",
       " 'lsales1': 'Lag Log sales',\n",
       " 'sales1': '',\n",
       " 'lppent1': 'Lag Log of ppent - i.e. net tangible fixed assets',\n",
       " 'lemp1': 'Lag Log count of employees',\n",
       " 'lpatents_ind1': 'Lag Log Total patents in n-class weighted industries (by firm year) - tech shock',\n",
       " 'lpatents_ind2': 'Twice Lagged Log Total patents in n-class weighted industries (by firm year) - t',\n",
       " 'lpat_cite_norm1': 'Lag of Log of cite weighted patent count by year',\n",
       " 'lpat_count1': 'Lag of Log of patent count by year - missing values -1 and missing indicator lpa',\n",
       " 'pat_count1': '',\n",
       " 'gpatent': 'Cite weighted and year normalized stock of firm patents',\n",
       " 'gpatent_count': 'Stock of firm patent count',\n",
       " 'kpat_cite': '',\n",
       " 'rxrd': 'Expenditure on R&D, 1996 values',\n",
       " 'grd': 'Stock of R&D expenditures',\n",
       " 'rhxrd': '',\n",
       " 'ghxrd': 'Stock of instrument predicted R&D expenditures',\n",
       " 'rspillsic': '',\n",
       " 'gspillsic': '',\n",
       " 'lgspillsic': 'Log stock of sic weighted R&D (spillovers)',\n",
       " 'lgspillsic1': 'Lagged Log stock of sic weighted R&D (spillovers)',\n",
       " 'rspillcovsic': '',\n",
       " 'gspillcovsic': '',\n",
       " 'lgspillcovsic': '',\n",
       " 'lgspillcovsic1': '',\n",
       " 'rspillmalsic': '',\n",
       " 'gspillmalsic': '',\n",
       " 'lgspillmalsic': '',\n",
       " 'lgspillmalsic1': '',\n",
       " 'rspillmalcovsic': '',\n",
       " 'gspillmalcovsic': '',\n",
       " 'lgspillmalcovsic': '',\n",
       " 'lgspillmalcovsic1': '',\n",
       " 'rspilltec': '',\n",
       " 'gspilltec': '',\n",
       " 'lgspilltec': 'Log stock of tec weighted R&D (spillovers)',\n",
       " 'lgspilltec1': 'Lag Log stock of tec weighted R&D (spillovers)',\n",
       " 'rspillcovtec': '',\n",
       " 'gspillcovtec': '',\n",
       " 'lgspillcovtec': '',\n",
       " 'lgspillcovtec1': '',\n",
       " 'rspillmaltec': '',\n",
       " 'gspillmaltec': '',\n",
       " 'lgspillmaltec': '',\n",
       " 'lgspillmaltec1': '',\n",
       " 'rspillmalcovtec': '',\n",
       " 'gspillmalcovtec': '',\n",
       " 'lgspillmalcovtec': '',\n",
       " 'lgspillmalcovtec1': '',\n",
       " 'rspilltloc': '',\n",
       " 'gspilltloc': '',\n",
       " 'lgspilltloc': 'Log stock of location weighted R&D (spillovers)',\n",
       " 'lgspilltloc1': 'Lagged Log stock of location weighted R&D (spillovers)',\n",
       " 'rspillsloc': '',\n",
       " 'gspillsloc': '',\n",
       " 'lgspillsloc': 'Log stock of sales location weighted R&D (spillovers)',\n",
       " 'lgspillsloc1': 'Lagged Log stock of location weighted R&D (spillovers)',\n",
       " 'rspilltectloc': '',\n",
       " 'gspilltectloc': '',\n",
       " 'lgspilltectloc': '',\n",
       " 'lgspilltectloc1': '',\n",
       " 'rspilltecsloc': '',\n",
       " 'gspilltecsloc': '',\n",
       " 'lgspilltecsloc': '',\n",
       " 'lgspilltecsloc1': '',\n",
       " 'rspillsicsloc': '',\n",
       " 'gspillsicsloc': '',\n",
       " 'lgspillsicsloc': '',\n",
       " 'lgspillsicsloc1': '',\n",
       " 'rspillsictloc': '',\n",
       " 'gspillsictloc': '',\n",
       " 'lgspillsictloc': '',\n",
       " 'lgspillsictloc1': '',\n",
       " 'rspilltecIV': '',\n",
       " 'gspilltecIV': '',\n",
       " 'lgspilltecIV': '',\n",
       " 'lgspilltecIV1': '',\n",
       " 'rspillsicIV': '',\n",
       " 'gspillsicIV': '',\n",
       " 'lgspillsicIV': '',\n",
       " 'lgspillsicIV1': '',\n",
       " 'rspilltecIV_mal': '',\n",
       " 'gspilltecIV_mal': '',\n",
       " 'lgspilltecIV_mal': '',\n",
       " 'lgspilltecIV_mal1': '',\n",
       " 'rspillsicIV_mal': '',\n",
       " 'gspillsicIV_mal': '',\n",
       " 'lgspillsicIV_mal': '',\n",
       " 'lgspillsicIV_mal1': '',\n",
       " 'lkpat_cite': '',\n",
       " 'rppent': 'Net book value of property, plant and equipment in 1996 values',\n",
       " 'rcapx': '',\n",
       " 'kstock': 'Capital stock calculated by the perpetual inventory method from ppent initial va',\n",
       " 'lgrd': 'Log of stock of R&D expenditures',\n",
       " 'lghxrd': 'Log of stock of  instrument predicted R&D expenditures',\n",
       " 'lgpatent': 'Log of Cite weighted and year normalized stock of firm patents',\n",
       " 'lgpatent_count': 'Log Stock of firm patent count',\n",
       " 'lkstock': 'Log of Capital stock calculated by the perpetual inventory method from ppent ini',\n",
       " 'lcogs1': '',\n",
       " 'lgrd1': 'Lag Log of stock of R&D expenditures',\n",
       " 'lghxrd1': '',\n",
       " 'grd1': '',\n",
       " 'lgpatent1': 'Lag Log of Cite weighted and year normalized stock of firm patents',\n",
       " 'lgpatent_count1': 'Lag Log Stock of firm patent count',\n",
       " 'gpatent_count1': '',\n",
       " 'lkstock1': 'Lag Log of Capital stock calculated by the perpetual inventory method from ppent',\n",
       " 'dlsales': '',\n",
       " 'dlemp': '',\n",
       " 'dlppent': '',\n",
       " 'profit': '',\n",
       " 'lprofit': '',\n",
       " 'rmkvaf': '',\n",
       " 'rpstk': '',\n",
       " 'rdt': '',\n",
       " 'rinvt': '',\n",
       " 'rivaeq': '',\n",
       " 'rivao': '',\n",
       " 'rintan': '',\n",
       " 'ract': '',\n",
       " 'value': '',\n",
       " 'value_e': '',\n",
       " 'value_d': '',\n",
       " 'qkstock': '',\n",
       " 'dum_qkstock': \"Some minor numbers missing for kstock in Tobin's q - set to zero when missing\",\n",
       " 'tobinq': \"Tobin's Q calculated following Hall, Jaffe and Trajtenberg, 2000\",\n",
       " 'tobinq_e': \"Equity component of Tobin's Q calculated following Hall, Jaffe and Trajtenberg, \",\n",
       " 'tobinq_d': \"Debt component of Tobin's Q calculated following Hall, Jaffe and Trajtenberg, 20\",\n",
       " 'rawtobinq': '',\n",
       " 'lq': \"Log Tobin's Q\",\n",
       " 'lq_e': \"Log Tobin's Equity Q - tobinq_e\",\n",
       " 'lq_d': \"Log Tobin's Debt Q - tobinq_d\",\n",
       " 'lq1': \"Lag Log Tobin's Q\",\n",
       " 'grd_k': 'R&D stock divided by capital stock',\n",
       " 'gpat_k': 'Patent stock divided by capital stock',\n",
       " 'gpatcount_k': '',\n",
       " 'gtec_k': '',\n",
       " 'gsic_k': '',\n",
       " 'pat_k': '',\n",
       " 'grd_k_dum': 'R&D stock over capital missing dummy',\n",
       " 'grd_k1': 'lagged R&D stock over capital',\n",
       " 'gtec_k1': '',\n",
       " 'gsic_k1': '',\n",
       " 'pat_k1': '',\n",
       " 'grd_k1_dum': 'Lagged R&D stock over capital missing dummy',\n",
       " 'gpat_k1': 'Lagged Patent stock divided by capital stock',\n",
       " 'gpatcount_k1': '',\n",
       " 'gpat_k1_dum': 'Missing value indicator for lagged Patent stock divided by capital stock',\n",
       " 'gpatcount_k1_dum': '',\n",
       " 'gtecxrd_k': '',\n",
       " 'lkpat_cite1': '',\n",
       " 'pat_cite_norm1': '',\n",
       " 'yy12': 'year==  1981.0000',\n",
       " 'yy13': 'year==  1982.0000',\n",
       " 'yy14': 'year==  1983.0000',\n",
       " 'yy15': 'year==  1984.0000',\n",
       " 'yy16': 'year==  1985.0000',\n",
       " 'yy17': 'year==  1986.0000',\n",
       " 'yy18': 'year==  1987.0000',\n",
       " 'yy19': 'year==  1988.0000',\n",
       " 'yy20': 'year==  1989.0000',\n",
       " 'yy21': 'year==  1990.0000',\n",
       " 'yy22': 'year==  1991.0000',\n",
       " 'yy23': 'year==  1992.0000',\n",
       " 'yy24': 'year==  1993.0000',\n",
       " 'yy25': 'year==  1994.0000',\n",
       " 'yy26': 'year==  1995.0000',\n",
       " 'yy27': 'year==  1996.0000',\n",
       " 'yy28': 'year==  1997.0000',\n",
       " 'yy29': 'year==  1998.0000',\n",
       " 'yy30': 'year==  1999.0000',\n",
       " 'yy31': 'year==  2000.0000',\n",
       " 'yy32': 'year==  2001.0000',\n",
       " 'pat_all': 'Total granted patents per firm applied for over 1970-1999',\n",
       " 'ccog': '',\n",
       " 'sic3': 'Three digit SIC code',\n",
       " 'sic2': '',\n",
       " 'indwage': '',\n",
       " 'nwage': '',\n",
       " 'materials': '',\n",
       " 'rati': '',\n",
       " 'lmat1': '',\n",
       " 'grd_kt2': 'R&D stock over capital^2',\n",
       " 'grd_kt3': 'R&D stock over capital^3',\n",
       " 'grd_kt4': 'R&D stock over capital^4',\n",
       " 'grd_kt5': 'R&D stock over capital^5',\n",
       " 'grd_kt6': '',\n",
       " 'gpat_kt2': 'Patent stock over capital^2',\n",
       " 'gpat_kt3': 'Patent stock over capital^3',\n",
       " 'gpat_kt4': 'Patent stock over capital^4',\n",
       " 'gpat_kt5': 'Patent stock over capital^5',\n",
       " 'f_year': '',\n",
       " 'fyear': 'First year using actual data in patents stuff',\n",
       " 'myear': '',\n",
       " 'prior_years': 'Number of years used to make initial conditions',\n",
       " 'riorpat': '',\n",
       " 'riorpat_cite': '',\n",
       " 'riorlgpat': '',\n",
       " 'riorgrd': '',\n",
       " 'riorppent': '',\n",
       " 'riorsales': '',\n",
       " 'riorgspilltec': '',\n",
       " 'riorgspillsic': '',\n",
       " 'riorlq': '',\n",
       " 'riorgtec_k': '',\n",
       " 'riorgsic_k': '',\n",
       " 'riorgrd_k': '',\n",
       " 'riorlsales_ind': '',\n",
       " 'priorpat': '',\n",
       " 'priorpat_cite': '',\n",
       " 'priorlgpat': '',\n",
       " 'priorgrd': '',\n",
       " 'priorppent': '',\n",
       " 'priorsales': '',\n",
       " 'priorgspilltec': '',\n",
       " 'priorgspillsic': '',\n",
       " 'priorlq': '',\n",
       " 'priorgtec_k': '',\n",
       " 'priorgsic_k': '',\n",
       " 'priorgrd_k': '',\n",
       " 'priorlsales_ind': '',\n",
       " 'lsales_ind': 'Logged total sales weighted by SIC matrix',\n",
       " 'lsales_ind_ns': '',\n",
       " 'lsales_ind_ns_dum': '',\n",
       " 'lsales_ind1': 'Lagged Logged total sales weighted by SIC matrix',\n",
       " 'lsales_ind_ns1': '',\n",
       " 'lsales_ind_ns_dum1': '',\n",
       " 'lsales_ind2': 'Twice Lagged Logged total sales weighted by SIC matrix',\n",
       " 'priordum_grd_zero': '',\n",
       " 'lpriorgrd': 'Logged initial conditions stock of R&D (pre-sample values)',\n",
       " 'lpriorppent': 'Logged initial conditions ppent (pre-sample values)',\n",
       " 'lpriorsales': 'Logged initial conditions sales (pre-sample values)',\n",
       " 'lpriorgspilltec': 'Logged initial conditions lgspilltec (pre-sample values)',\n",
       " 'lpriorgspillsic': 'Logged initial conditions lgspillsic (pre-sample values)',\n",
       " 'lpriorpat': 'Logged initial conditions patent count (pre-sample values)',\n",
       " 'lpriorpat_cite': '',\n",
       " 'lpriorgrd_dum': 'Missing indicator for Logged initial conditions stock of R&D (pre-sample values)',\n",
       " 'lpriorgrd1': 'Lagged Logged initial conditions stock of R&D (pre-sample values)',\n",
       " 'lpriorgrd1_dum': 'Missing indicator for lagged Logged initial conditions stock of R&D (pre-sample ',\n",
       " 'lpriorpat_dum': 'Missing indicator for Logged initial conditions patent count (pre-sample values)',\n",
       " 'lpriorpat_cite_dum': '',\n",
       " 'lpriorpat1': 'Lagged Logged initial conditions patent count (pre-sample values)',\n",
       " 'lpriorpat1_dum': 'Missing indicator for Lagged Logged initial conditions patent count (pre-sample ',\n",
       " 'lpind_ind': 'Price index, yearly at 3-digit SIC level',\n",
       " 'pat_cite1': '',\n",
       " 'lpat_cite1': '',\n",
       " 'lpat_cite1_dum': '',\n",
       " 'lpat_cite': '',\n",
       " 'lgrd_dum': 'Missing indicator for Log of stock of R&D expenditures',\n",
       " 'lgrd1_dum': 'Missing indicator for Lag Log of stock of R&D expenditures',\n",
       " 'lpat_count_dum': 'Dummy for missing value of log of patent count by year',\n",
       " 'lpat_count1_dum': 'Dummy for missing value of lag of log of patent count by year',\n",
       " 'lgpatent_dum': 'Missing indicator for Log of Cite weighted and year normalized stock of firm pat',\n",
       " 'lgpatent1_dum': 'Missing indicator for lagged Log of Cite weighted and year normalized stock of f',\n",
       " 'lgpatent_count_dum': 'Missing indicator for Log Stock of firm patent count',\n",
       " 'lgpatent_count1_dum': 'Missing indicator for lagged Log Stock of firm patent count',\n",
       " 'code': 'CUSIP',\n",
       " 'pind_ind': 'Price index, yearly at 3-digit SIC level',\n",
       " 'psmooth': 'Fitted values',\n",
       " 'plin': '',\n",
       " 'pind_indm': '',\n",
       " 'rsales': 'Sales in 1996 values',\n",
       " 'lrsales': 'Log Sales in 1996 values',\n",
       " 'lrsales1': 'Lag Log Sales in 1996 values',\n",
       " 'lqq': '',\n",
       " 'lxrd_sales': '',\n",
       " 'lxrd_sales1': '',\n",
       " 'lfirm_dum': '',\n",
       " 'gfirm': '',\n",
       " 'lgfirm': '',\n",
       " 'lgfirm_dum': '',\n",
       " 'state': '',\n",
       " 'gstate': '',\n",
       " 'lgstate': '',\n",
       " 'lgfirm1': '',\n",
       " 'lgfirm_dum1': '',\n",
       " 'lgstate1': '',\n",
       " 'lfirm1': '',\n",
       " 'lfirm_dum1': '',\n",
       " 'lstate1': '',\n",
       " 'noj': 'Count observations per firm (includes pre-sample patent observations)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.variable_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caed78af-4e44-4cdf-ad1d-04e70a2b2cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['i', 'year', 'rmkvaf', 'grd', 'grd_k1', 'rxrd', 'gspillsic',\n",
       "       'gspilltec', 'pat_count', 'pat_cite', 'rsales', 'rppent', 'emp',\n",
       "       'gspilltecIV', 'gspillsicIV'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_of_int = ['i','year','rmkvaf','grd','grd_k1','rxrd','gspillsic','gspilltec','pat_count','pat_cite','rsales',\n",
    "              'rppent','emp','gspilltecIV','gspillsicIV']\n",
    "\n",
    "data = pd.read_stata(data_loc)\n",
    "df = data[vars_of_int]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf406bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_417/3340598184.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['year'] = df['year'].astype(str)\n",
      "/tmp/ipykernel_417/3340598184.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['i'] = df['i'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# convert categorical columns to strings\n",
    "df['year'] = df['year'].astype(str)\n",
    "df['i'] = df['i'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ada76bb-9a0a-461e-b92b-39cfb388358b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmkvaf</th>\n",
       "      <th>gspilltecIV</th>\n",
       "      <th>gspillsicIV</th>\n",
       "      <th>pat_count</th>\n",
       "      <th>rsales</th>\n",
       "      <th>rppent</th>\n",
       "      <th>emp</th>\n",
       "      <th>rxrd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "      <td>13385.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3863.04</td>\n",
       "      <td>21341.64</td>\n",
       "      <td>6164.57</td>\n",
       "      <td>16.85</td>\n",
       "      <td>2852.77</td>\n",
       "      <td>1309.39</td>\n",
       "      <td>18.68</td>\n",
       "      <td>106.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16402.81</td>\n",
       "      <td>16288.44</td>\n",
       "      <td>9019.92</td>\n",
       "      <td>75.96</td>\n",
       "      <td>8782.54</td>\n",
       "      <td>4070.72</td>\n",
       "      <td>53.65</td>\n",
       "      <td>473.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.43</td>\n",
       "      <td>230.28</td>\n",
       "      <td>4.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>88.34</td>\n",
       "      <td>8991.68</td>\n",
       "      <td>607.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>117.32</td>\n",
       "      <td>27.22</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>409.62</td>\n",
       "      <td>17508.95</td>\n",
       "      <td>2067.49</td>\n",
       "      <td>1.00</td>\n",
       "      <td>450.26</td>\n",
       "      <td>121.87</td>\n",
       "      <td>3.85</td>\n",
       "      <td>4.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1990.41</td>\n",
       "      <td>29810.46</td>\n",
       "      <td>7534.20</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1950.00</td>\n",
       "      <td>731.75</td>\n",
       "      <td>14.00</td>\n",
       "      <td>30.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>485566.69</td>\n",
       "      <td>92324.67</td>\n",
       "      <td>55576.60</td>\n",
       "      <td>2405.00</td>\n",
       "      <td>140609.58</td>\n",
       "      <td>72825.98</td>\n",
       "      <td>876.80</td>\n",
       "      <td>8900.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rmkvaf  gspilltecIV  gspillsicIV  pat_count     rsales    rppent  \\\n",
       "count   13385.00     13385.00     13385.00   13385.00   13385.00  13385.00   \n",
       "mean     3863.04     21341.64      6164.57      16.85    2852.77   1309.39   \n",
       "std     16402.81     16288.44      9019.92      75.96    8782.54   4070.72   \n",
       "min         0.43       230.28         4.31       0.00       1.08      0.91   \n",
       "25%        88.34      8991.68       607.00       0.00     117.32     27.22   \n",
       "50%       409.62     17508.95      2067.49       1.00     450.26    121.87   \n",
       "75%      1990.41     29810.46      7534.20       5.00    1950.00    731.75   \n",
       "max    485566.69     92324.67     55576.60    2405.00  140609.58  72825.98   \n",
       "\n",
       "            emp      rxrd  \n",
       "count  13385.00  13385.00  \n",
       "mean      18.68    106.59  \n",
       "std       53.65    473.90  \n",
       "min        0.10      0.00  \n",
       "25%        1.09      0.00  \n",
       "50%        3.85      4.68  \n",
       "75%       14.00     30.19  \n",
       "max      876.80   8900.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_of_int_noindex = ['rmkvaf','gspilltecIV','gspillsicIV','pat_count','rsales','rppent','emp','rxrd']\n",
    "\n",
    "# filter dataframe to variables of interest\n",
    "df = df.drop(columns=['grd','grd_k1','pat_cite','gspilltec','gspillsic'])\n",
    "\n",
    "# delete NaN values\n",
    "df = df.dropna(axis=0)\n",
    "\n",
    "#for i in range(0, len(var_of_int_noindex)):\n",
    "#    df = df[df[var_of_int_noindex[i]].isna() == False]\n",
    "\n",
    "#df = df.loc[df['rmkvaf'].isna() == False]\n",
    "\n",
    "df_sum_stats = df[var_of_int_noindex].describe()\n",
    "df_sum_stats = df_sum_stats.round(2)\n",
    "df_sum_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7521d293-0372-4623-b152-86df3a5fb4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      " & count & mean & std & min & 25% & 50% & 75% & max \\\\\n",
      "\\midrule\n",
      "rmkvaf & 13385.000000 & 3863.040000 & 16402.810000 & 0.430000 & 88.340000 & 409.620000 & 1990.410000 & 485566.690000 \\\\\n",
      "gspilltecIV & 13385.000000 & 21341.640000 & 16288.440000 & 230.280000 & 8991.680000 & 17508.950000 & 29810.460000 & 92324.670000 \\\\\n",
      "gspillsicIV & 13385.000000 & 6164.570000 & 9019.920000 & 4.310000 & 607.000000 & 2067.490000 & 7534.200000 & 55576.600000 \\\\\n",
      "pat_count & 13385.000000 & 16.850000 & 75.960000 & 0.000000 & 0.000000 & 1.000000 & 5.000000 & 2405.000000 \\\\\n",
      "rsales & 13385.000000 & 2852.770000 & 8782.540000 & 1.080000 & 117.320000 & 450.260000 & 1950.000000 & 140609.580000 \\\\\n",
      "rppent & 13385.000000 & 1309.390000 & 4070.720000 & 0.910000 & 27.220000 & 121.870000 & 731.750000 & 72825.980000 \\\\\n",
      "emp & 13385.000000 & 18.680000 & 53.650000 & 0.100000 & 1.090000 & 3.850000 & 14.000000 & 876.800000 \\\\\n",
      "rxrd & 13385.000000 & 106.590000 & 473.900000 & 0.000000 & 0.000000 & 4.680000 & 30.190000 & 8900.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df_sum_stats.T.to_excel(\"sum_stats.xlsx\") # export summary statistics\n",
    "print(df_sum_stats.T.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee5bf3",
   "metadata": {},
   "source": [
    "# OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2734b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear model\n",
    "# gen fixed effects\n",
    "time_effects = pd.get_dummies(df['year'])\n",
    "#time_effects.columns = time_effects.columns.astype(str)\n",
    "firm_effects = pd.get_dummies(df['i'])\n",
    "#time_effects.columns = time_effects.columns.astype(str)\n",
    "\n",
    "\n",
    "df = pd.merge(df, time_effects, left_on=df.index, right_on=time_effects.index, how='left')\n",
    "\n",
    "df = df.rename(columns={'key_0': 'old_key'})\n",
    "\n",
    "df = pd.merge(df, firm_effects, left_on=df['old_key'], right_on=firm_effects.index, how='left')\n",
    "\n",
    "fixed_effects = list(time_effects.columns.values)\n",
    "for col in firm_effects.columns.values:\n",
    "    fixed_effects.append(col)\n",
    "# remove ref categories\n",
    "fixed_effects.remove('1980')\n",
    "fixed_effects.remove('9999.0')\n",
    "\n",
    "y_var = df['rmkvaf']\n",
    "\n",
    "# get df for x vars + fixed effects\n",
    "x_vars = ['gspilltecIV','gspillsicIV','pat_count','rsales','rppent','emp','rxrd']\n",
    "for col in fixed_effects:\n",
    "    x_vars.append(col)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8119f9ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.665</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.645</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   33.22</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:58:42</td>     <th>  Log-Likelihood:    </th> <td>-1.4157e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13385</td>      <th>  AIC:               </th>  <td>2.847e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 12629</td>      <th>  BIC:               </th>  <td>2.903e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   755</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>-9940.5765</td> <td> 2385.347</td> <td>   -4.167</td> <td> 0.000</td> <td>-1.46e+04</td> <td>-5264.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th> <td>    0.1002</td> <td>    0.027</td> <td>    3.746</td> <td> 0.000</td> <td>    0.048</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th> <td>    0.3399</td> <td>    0.049</td> <td>    6.902</td> <td> 0.000</td> <td>    0.243</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>   <td>  -30.6018</td> <td>    1.838</td> <td>  -16.652</td> <td> 0.000</td> <td>  -34.204</td> <td>  -26.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>      <td>    0.7812</td> <td>    0.037</td> <td>   21.055</td> <td> 0.000</td> <td>    0.708</td> <td>    0.854</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>      <td>    0.6108</td> <td>    0.084</td> <td>    7.234</td> <td> 0.000</td> <td>    0.445</td> <td>    0.776</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>         <td>   18.0641</td> <td>    7.147</td> <td>    2.527</td> <td> 0.012</td> <td>    4.054</td> <td>   32.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>        <td>   18.5941</td> <td>    0.614</td> <td>   30.295</td> <td> 0.000</td> <td>   17.391</td> <td>   19.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981</th>        <td> -425.5119</td> <td>  618.180</td> <td>   -0.688</td> <td> 0.491</td> <td>-1637.239</td> <td>  786.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>        <td> -295.2372</td> <td>  616.239</td> <td>   -0.479</td> <td> 0.632</td> <td>-1503.159</td> <td>  912.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>        <td> -294.1870</td> <td>  609.557</td> <td>   -0.483</td> <td> 0.629</td> <td>-1489.011</td> <td>  900.637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>        <td> -812.2257</td> <td>  607.964</td> <td>   -1.336</td> <td> 0.182</td> <td>-2003.927</td> <td>  379.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>        <td> -929.1950</td> <td>  609.921</td> <td>   -1.523</td> <td> 0.128</td> <td>-2124.732</td> <td>  266.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>        <td>-1184.5891</td> <td>  607.667</td> <td>   -1.949</td> <td> 0.051</td> <td>-2375.708</td> <td>    6.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>        <td>-1372.3041</td> <td>  607.937</td> <td>   -2.257</td> <td> 0.024</td> <td>-2563.953</td> <td> -180.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>        <td>-1686.4878</td> <td>  609.953</td> <td>   -2.765</td> <td> 0.006</td> <td>-2882.088</td> <td> -490.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>        <td>-1550.4891</td> <td>  610.972</td> <td>   -2.538</td> <td> 0.011</td> <td>-2748.088</td> <td> -352.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>        <td>-2052.0735</td> <td>  612.118</td> <td>   -3.352</td> <td> 0.001</td> <td>-3251.918</td> <td> -852.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>        <td>-1629.8854</td> <td>  614.732</td> <td>   -2.651</td> <td> 0.008</td> <td>-2834.854</td> <td> -424.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>        <td>-1754.6866</td> <td>  617.583</td> <td>   -2.841</td> <td> 0.005</td> <td>-2965.243</td> <td> -544.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>        <td>-1685.0176</td> <td>  621.230</td> <td>   -2.712</td> <td> 0.007</td> <td>-2902.723</td> <td> -467.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>        <td>-1916.7490</td> <td>  627.407</td> <td>   -3.055</td> <td> 0.002</td> <td>-3146.563</td> <td> -686.935</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>        <td>-1380.6021</td> <td>  636.986</td> <td>   -2.167</td> <td> 0.030</td> <td>-2629.191</td> <td> -132.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>        <td>-1098.4502</td> <td>  651.099</td> <td>   -1.687</td> <td> 0.092</td> <td>-2374.702</td> <td>  177.802</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>        <td> -692.3174</td> <td>  667.994</td> <td>   -1.036</td> <td> 0.300</td> <td>-2001.686</td> <td>  617.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>        <td> -575.0406</td> <td>  687.698</td> <td>   -0.836</td> <td> 0.403</td> <td>-1923.033</td> <td>  772.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>        <td>  142.3167</td> <td>  710.226</td> <td>    0.200</td> <td> 0.841</td> <td>-1249.835</td> <td> 1534.468</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2000</th>        <td> -207.4580</td> <td>  738.322</td> <td>   -0.281</td> <td> 0.779</td> <td>-1654.682</td> <td> 1239.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2001</th>        <td>-2283.3834</td> <td>  768.945</td> <td>   -2.970</td> <td> 0.003</td> <td>-3790.633</td> <td> -776.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>     <td> 8596.1312</td> <td> 3076.471</td> <td>    2.794</td> <td> 0.005</td> <td> 2565.781</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>     <td> 8240.3993</td> <td> 3516.122</td> <td>    2.344</td> <td> 0.019</td> <td> 1348.266</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>     <td> 7558.3544</td> <td> 3040.531</td> <td>    2.486</td> <td> 0.013</td> <td> 1598.452</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>     <td> 8488.3062</td> <td> 3060.494</td> <td>    2.774</td> <td> 0.006</td> <td> 2489.273</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>     <td> 9963.3568</td> <td> 3135.789</td> <td>    3.177</td> <td> 0.001</td> <td> 3816.734</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>      <td> 9403.1634</td> <td> 3133.966</td> <td>    3.000</td> <td> 0.003</td> <td> 3260.115</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>     <td> 7589.4824</td> <td> 3061.708</td> <td>    2.479</td> <td> 0.013</td> <td> 1588.069</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>     <td> 4090.3185</td> <td> 2997.861</td> <td>    1.364</td> <td> 0.172</td> <td>-1785.945</td> <td> 9966.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>     <td> 9204.1913</td> <td> 5393.483</td> <td>    1.707</td> <td> 0.088</td> <td>-1367.854</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>     <td> 2206.4047</td> <td> 2993.452</td> <td>    0.737</td> <td> 0.461</td> <td>-3661.215</td> <td> 8074.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>      <td> 8207.1099</td> <td> 5379.135</td> <td>    1.526</td> <td> 0.127</td> <td>-2336.812</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>     <td> 1.097e+04</td> <td> 3193.217</td> <td>    3.436</td> <td> 0.001</td> <td> 4711.604</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>     <td> 7957.3563</td> <td> 3058.537</td> <td>    2.602</td> <td> 0.009</td> <td> 1962.159</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>     <td>  1.11e+04</td> <td> 3199.490</td> <td>    3.470</td> <td> 0.001</td> <td> 4829.168</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>      <td> 4769.1715</td> <td> 2983.003</td> <td>    1.599</td> <td> 0.110</td> <td>-1077.968</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>     <td> 3170.2099</td> <td> 3465.956</td> <td>    0.915</td> <td> 0.360</td> <td>-3623.590</td> <td> 9964.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>     <td> 3378.9006</td> <td> 4384.246</td> <td>    0.771</td> <td> 0.441</td> <td>-5214.887</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>     <td> 1.021e+04</td> <td> 3389.760</td> <td>    3.012</td> <td> 0.003</td> <td> 3565.375</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>     <td> 8313.1068</td> <td> 3338.064</td> <td>    2.490</td> <td> 0.013</td> <td> 1769.995</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>     <td> 2593.7222</td> <td> 3141.189</td> <td>    0.826</td> <td> 0.409</td> <td>-3563.486</td> <td> 8750.931</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>     <td> 9933.5369</td> <td> 3131.022</td> <td>    3.173</td> <td> 0.002</td> <td> 3796.259</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>     <td>  1.08e+04</td> <td> 3193.341</td> <td>    3.381</td> <td> 0.001</td> <td> 4537.838</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>     <td> 6925.4903</td> <td> 3326.104</td> <td>    2.082</td> <td> 0.037</td> <td>  405.821</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>     <td> 9750.4628</td> <td> 3137.458</td> <td>    3.108</td> <td> 0.002</td> <td> 3600.569</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>     <td> 8274.2845</td> <td> 3121.219</td> <td>    2.651</td> <td> 0.008</td> <td> 2156.221</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>     <td>-1.476e+04</td> <td> 3014.308</td> <td>   -4.895</td> <td> 0.000</td> <td>-2.07e+04</td> <td>-8846.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>     <td> 9198.1294</td> <td> 3124.523</td> <td>    2.944</td> <td> 0.003</td> <td> 3073.591</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>     <td> 3868.1997</td> <td> 4061.176</td> <td>    0.952</td> <td> 0.341</td> <td>-4092.322</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>      <td> 7245.0618</td> <td> 3261.623</td> <td>    2.221</td> <td> 0.026</td> <td>  851.786</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>     <td> 8066.1980</td> <td> 3065.104</td> <td>    2.632</td> <td> 0.009</td> <td> 2058.129</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>     <td> 4140.6626</td> <td> 2986.789</td> <td>    1.386</td> <td> 0.166</td> <td>-1713.897</td> <td> 9995.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>     <td> 1064.5542</td> <td> 3026.844</td> <td>    0.352</td> <td> 0.725</td> <td>-4868.521</td> <td> 6997.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>     <td> 4895.6611</td> <td> 3017.572</td> <td>    1.622</td> <td> 0.105</td> <td>-1019.237</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>     <td> 8765.7651</td> <td> 3082.897</td> <td>    2.843</td> <td> 0.004</td> <td> 2722.820</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>     <td> 6959.5604</td> <td> 3252.126</td> <td>    2.140</td> <td> 0.032</td> <td>  584.899</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>     <td> 9660.2255</td> <td> 3299.137</td> <td>    2.928</td> <td> 0.003</td> <td> 3193.415</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>     <td> 1.026e+04</td> <td> 3164.254</td> <td>    3.241</td> <td> 0.001</td> <td> 4053.796</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>      <td>-1654.6262</td> <td> 3243.965</td> <td>   -0.510</td> <td> 0.610</td> <td>-8013.291</td> <td> 4704.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>     <td> 4005.2538</td> <td> 2991.151</td> <td>    1.339</td> <td> 0.181</td> <td>-1857.856</td> <td> 9868.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>     <td> -1.46e+04</td> <td> 3475.563</td> <td>   -4.201</td> <td> 0.000</td> <td>-2.14e+04</td> <td>-7789.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>     <td> 9301.1887</td> <td> 3158.846</td> <td>    2.944</td> <td> 0.003</td> <td> 3109.370</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>     <td>  290.8155</td> <td> 3069.162</td> <td>    0.095</td> <td> 0.925</td> <td>-5725.207</td> <td> 6306.838</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>     <td> 1.071e+04</td> <td> 3287.724</td> <td>    3.258</td> <td> 0.001</td> <td> 4268.117</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>     <td>-5675.1567</td> <td> 2981.921</td> <td>   -1.903</td> <td> 0.057</td> <td>-1.15e+04</td> <td>  169.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>     <td> 5301.0836</td> <td> 3009.281</td> <td>    1.762</td> <td> 0.078</td> <td> -597.565</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>     <td> 6186.0975</td> <td> 3392.545</td> <td>    1.823</td> <td> 0.068</td> <td> -463.806</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>     <td> 7576.5476</td> <td> 3032.964</td> <td>    2.498</td> <td> 0.012</td> <td> 1631.477</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>     <td> 8964.6580</td> <td> 3229.126</td> <td>    2.776</td> <td> 0.006</td> <td> 2635.080</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>     <td> 6593.4131</td> <td> 6048.079</td> <td>    1.090</td> <td> 0.276</td> <td>-5261.741</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>     <td> 3851.8113</td> <td> 3153.072</td> <td>    1.222</td> <td> 0.222</td> <td>-2328.689</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>     <td> 1.053e+04</td> <td> 3138.295</td> <td>    3.355</td> <td> 0.001</td> <td> 4378.941</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>     <td>  1.08e+04</td> <td> 3213.498</td> <td>    3.360</td> <td> 0.001</td> <td> 4498.657</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>     <td> 7665.2631</td> <td> 3106.263</td> <td>    2.468</td> <td> 0.014</td> <td> 1576.515</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>     <td> 1176.5368</td> <td> 2963.147</td> <td>    0.397</td> <td> 0.691</td> <td>-4631.681</td> <td> 6984.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>     <td> 8900.5001</td> <td> 3128.719</td> <td>    2.845</td> <td> 0.004</td> <td> 2767.736</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>     <td> 9271.0496</td> <td> 3139.906</td> <td>    2.953</td> <td> 0.003</td> <td> 3116.357</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>     <td> 8556.4419</td> <td> 3093.780</td> <td>    2.766</td> <td> 0.006</td> <td> 2492.162</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>     <td> 8664.1385</td> <td> 3068.114</td> <td>    2.824</td> <td> 0.005</td> <td> 2650.168</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>     <td> 8540.1624</td> <td> 3064.938</td> <td>    2.786</td> <td> 0.005</td> <td> 2532.418</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>     <td> 1.264e+04</td> <td> 3268.775</td> <td>    3.866</td> <td> 0.000</td> <td> 6231.058</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>     <td> 8389.8503</td> <td> 3727.109</td> <td>    2.251</td> <td> 0.024</td> <td> 1084.150</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>     <td> 1.002e+04</td> <td> 3173.342</td> <td>    3.159</td> <td> 0.002</td> <td> 3803.931</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>     <td> 1.051e+04</td> <td> 3276.657</td> <td>    3.207</td> <td> 0.001</td> <td> 4084.421</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>     <td> 8530.3531</td> <td> 3067.334</td> <td>    2.781</td> <td> 0.005</td> <td> 2517.913</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>      <td> 5453.6061</td> <td> 3082.313</td> <td>    1.769</td> <td> 0.077</td> <td> -588.196</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>     <td> 8093.7684</td> <td> 3137.200</td> <td>    2.580</td> <td> 0.010</td> <td> 1944.380</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>     <td> 7496.2748</td> <td> 3087.546</td> <td>    2.428</td> <td> 0.015</td> <td> 1444.216</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>     <td> 9481.3260</td> <td> 3097.020</td> <td>    3.061</td> <td> 0.002</td> <td> 3410.697</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>     <td>-2182.7305</td> <td> 3058.140</td> <td>   -0.714</td> <td> 0.475</td> <td>-8177.148</td> <td> 3811.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>     <td> 4643.9565</td> <td> 3339.296</td> <td>    1.391</td> <td> 0.164</td> <td>-1901.571</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>     <td> 8932.4951</td> <td> 3098.783</td> <td>    2.883</td> <td> 0.004</td> <td> 2858.410</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>     <td> 9034.0200</td> <td> 4140.834</td> <td>    2.182</td> <td> 0.029</td> <td>  917.357</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>     <td>-2.383e+04</td> <td> 3227.575</td> <td>   -7.383</td> <td> 0.000</td> <td>-3.02e+04</td> <td>-1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>      <td> 8536.2893</td> <td> 3106.895</td> <td>    2.748</td> <td> 0.006</td> <td> 2446.304</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>     <td> 6926.2901</td> <td> 3614.928</td> <td>    1.916</td> <td> 0.055</td> <td> -159.519</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>     <td> 7880.6911</td> <td> 3137.824</td> <td>    2.512</td> <td> 0.012</td> <td> 1730.080</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>     <td> 3251.3206</td> <td> 3219.125</td> <td>    1.010</td> <td> 0.313</td> <td>-3058.654</td> <td> 9561.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>      <td> 9488.5996</td> <td> 3138.344</td> <td>    3.023</td> <td> 0.003</td> <td> 3336.969</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>     <td> 9235.2591</td> <td> 3132.146</td> <td>    2.949</td> <td> 0.003</td> <td> 3095.777</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>     <td> 8684.5592</td> <td> 3089.265</td> <td>    2.811</td> <td> 0.005</td> <td> 2629.130</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>     <td> 6740.1208</td> <td> 3028.318</td> <td>    2.226</td> <td> 0.026</td> <td>  804.158</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>     <td> 9316.9328</td> <td> 3367.569</td> <td>    2.767</td> <td> 0.006</td> <td> 2715.986</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>      <td> 7875.2019</td> <td> 3125.463</td> <td>    2.520</td> <td> 0.012</td> <td> 1748.819</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>     <td> 6631.5622</td> <td> 3048.603</td> <td>    2.175</td> <td> 0.030</td> <td>  655.838</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>     <td> 1.031e+04</td> <td> 3223.600</td> <td>    3.198</td> <td> 0.001</td> <td> 3991.623</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>     <td> 1.001e+04</td> <td> 3111.122</td> <td>    3.217</td> <td> 0.001</td> <td> 3909.102</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>     <td> 4820.4345</td> <td> 4564.449</td> <td>    1.056</td> <td> 0.291</td> <td>-4126.579</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>     <td>  726.5183</td> <td> 3139.333</td> <td>    0.231</td> <td> 0.817</td> <td>-5427.050</td> <td> 6880.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>     <td>  624.6433</td> <td> 3085.276</td> <td>    0.202</td> <td> 0.840</td> <td>-5422.965</td> <td> 6672.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>     <td> 8139.1393</td> <td> 3054.443</td> <td>    2.665</td> <td> 0.008</td> <td> 2151.966</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>     <td> 1883.4788</td> <td> 2984.861</td> <td>    0.631</td> <td> 0.528</td> <td>-3967.302</td> <td> 7734.260</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>    <td>-8981.4766</td> <td> 5402.255</td> <td>   -1.663</td> <td> 0.096</td> <td>-1.96e+04</td> <td> 1607.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>     <td> 3026.3818</td> <td> 3078.672</td> <td>    0.983</td> <td> 0.326</td> <td>-3008.282</td> <td> 9061.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>     <td> 4113.0085</td> <td> 3092.455</td> <td>    1.330</td> <td> 0.184</td> <td>-1948.673</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>     <td> 9343.9264</td> <td> 3174.590</td> <td>    2.943</td> <td> 0.003</td> <td> 3121.249</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>     <td> 4190.2588</td> <td> 3073.981</td> <td>    1.363</td> <td> 0.173</td> <td>-1835.210</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>     <td> 9104.8465</td> <td> 3090.809</td> <td>    2.946</td> <td> 0.003</td> <td> 3046.392</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>     <td> 1.069e+04</td> <td> 3188.150</td> <td>    3.352</td> <td> 0.001</td> <td> 4436.526</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>     <td> 8122.9750</td> <td> 3053.915</td> <td>    2.660</td> <td> 0.008</td> <td> 2136.837</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>     <td> 4660.7676</td> <td> 3401.528</td> <td>    1.370</td> <td> 0.171</td> <td>-2006.744</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>     <td> 9955.4797</td> <td> 3181.578</td> <td>    3.129</td> <td> 0.002</td> <td> 3719.104</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>     <td> 1.339e+04</td> <td> 3141.316</td> <td>    4.263</td> <td> 0.000</td> <td> 7234.942</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>      <td>-1142.7795</td> <td> 2963.636</td> <td>   -0.386</td> <td> 0.700</td> <td>-6951.956</td> <td> 4666.397</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>     <td>-1.033e+04</td> <td> 3195.420</td> <td>   -3.234</td> <td> 0.001</td> <td>-1.66e+04</td> <td>-4069.451</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>     <td> 1.045e+04</td> <td> 3195.585</td> <td>    3.270</td> <td> 0.001</td> <td> 4186.263</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>     <td> -172.3965</td> <td> 3111.159</td> <td>   -0.055</td> <td> 0.956</td> <td>-6270.740</td> <td> 5925.947</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>     <td> 8157.6451</td> <td> 3195.335</td> <td>    2.553</td> <td> 0.011</td> <td> 1894.303</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>     <td> 9735.1845</td> <td> 3265.189</td> <td>    2.982</td> <td> 0.003</td> <td> 3334.919</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>     <td> 2342.1807</td> <td> 3978.963</td> <td>    0.589</td> <td> 0.556</td> <td>-5457.192</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>     <td>-3708.6926</td> <td> 3442.247</td> <td>   -1.077</td> <td> 0.281</td> <td>-1.05e+04</td> <td> 3038.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>     <td> 7939.5465</td> <td> 3308.424</td> <td>    2.400</td> <td> 0.016</td> <td> 1454.532</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>     <td> 6392.4048</td> <td> 6063.166</td> <td>    1.054</td> <td> 0.292</td> <td>-5492.321</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>     <td> 1.098e+04</td> <td> 3552.518</td> <td>    3.091</td> <td> 0.002</td> <td> 4015.910</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>     <td> 9253.8306</td> <td> 3755.065</td> <td>    2.464</td> <td> 0.014</td> <td> 1893.333</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>      <td> 6916.3882</td> <td> 3021.090</td> <td>    2.289</td> <td> 0.022</td> <td>  994.593</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>     <td>-5842.3430</td> <td> 3290.133</td> <td>   -1.776</td> <td> 0.076</td> <td>-1.23e+04</td> <td>  606.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>     <td> 8.981e+04</td> <td> 3291.619</td> <td>   27.284</td> <td> 0.000</td> <td> 8.34e+04</td> <td> 9.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>     <td> 6468.7957</td> <td> 4281.785</td> <td>    1.511</td> <td> 0.131</td> <td>-1924.153</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>     <td> -753.1168</td> <td> 3219.104</td> <td>   -0.234</td> <td> 0.815</td> <td>-7063.050</td> <td> 5556.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>     <td> 3866.8554</td> <td> 3252.277</td> <td>    1.189</td> <td> 0.234</td> <td>-2508.101</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>     <td> 2682.4219</td> <td> 3240.915</td> <td>    0.828</td> <td> 0.408</td> <td>-3670.264</td> <td> 9035.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>     <td> 9564.5090</td> <td> 3423.360</td> <td>    2.794</td> <td> 0.005</td> <td> 2854.203</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>     <td> 8986.4050</td> <td> 3304.204</td> <td>    2.720</td> <td> 0.007</td> <td> 2509.663</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>      <td> 6577.3859</td> <td> 3030.471</td> <td>    2.170</td> <td> 0.030</td> <td>  637.203</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>     <td> 7461.3813</td> <td> 3602.948</td> <td>    2.071</td> <td> 0.038</td> <td>  399.057</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>     <td> 6292.8548</td> <td> 5345.675</td> <td>    1.177</td> <td> 0.239</td> <td>-4185.481</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>      <td> 4184.8534</td> <td> 3158.124</td> <td>    1.325</td> <td> 0.185</td> <td>-2005.548</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>     <td> 9038.3456</td> <td> 3632.299</td> <td>    2.488</td> <td> 0.013</td> <td> 1918.488</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>     <td> 8969.7124</td> <td> 3499.499</td> <td>    2.563</td> <td> 0.010</td> <td> 2110.162</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>     <td> 6220.9585</td> <td> 3679.891</td> <td>    1.691</td> <td> 0.091</td> <td> -992.187</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>     <td> 7907.3965</td> <td> 3465.117</td> <td>    2.282</td> <td> 0.023</td> <td> 1115.241</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>     <td> 7117.2488</td> <td> 6104.480</td> <td>    1.166</td> <td> 0.244</td> <td>-4848.459</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>     <td> 1.071e+04</td> <td> 3513.018</td> <td>    3.048</td> <td> 0.002</td> <td> 3822.505</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>     <td>-8243.1620</td> <td> 3377.006</td> <td>   -2.441</td> <td> 0.015</td> <td>-1.49e+04</td> <td>-1623.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>      <td> 9559.3089</td> <td> 3276.729</td> <td>    2.917</td> <td> 0.004</td> <td> 3136.423</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>     <td>-2571.9810</td> <td> 3720.888</td> <td>   -0.691</td> <td> 0.489</td> <td>-9865.486</td> <td> 4721.524</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>      <td> 9778.5694</td> <td> 3188.479</td> <td>    3.067</td> <td> 0.002</td> <td> 3528.666</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>      <td> 8851.0971</td> <td> 3160.094</td> <td>    2.801</td> <td> 0.005</td> <td> 2656.833</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>     <td> 1.022e+04</td> <td> 3469.470</td> <td>    2.945</td> <td> 0.003</td> <td> 3415.928</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>     <td> 5736.5602</td> <td> 3444.412</td> <td>    1.665</td> <td> 0.096</td> <td>-1015.010</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>      <td> 2830.7666</td> <td> 3184.260</td> <td>    0.889</td> <td> 0.374</td> <td>-3410.866</td> <td> 9072.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>     <td> 3707.5355</td> <td> 5340.406</td> <td>    0.694</td> <td> 0.488</td> <td>-6760.470</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>      <td> 1959.2153</td> <td> 3392.421</td> <td>    0.578</td> <td> 0.564</td> <td>-4690.446</td> <td> 8608.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>     <td> 3934.9513</td> <td> 3942.379</td> <td>    0.998</td> <td> 0.318</td> <td>-3792.710</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>     <td>-7664.6310</td> <td> 3538.787</td> <td>   -2.166</td> <td> 0.030</td> <td>-1.46e+04</td> <td> -728.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>     <td> 6192.2784</td> <td> 3353.245</td> <td>    1.847</td> <td> 0.065</td> <td> -380.592</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>     <td> 9511.9068</td> <td> 3422.019</td> <td>    2.780</td> <td> 0.005</td> <td> 2804.230</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>     <td> 3435.2039</td> <td> 3325.530</td> <td>    1.033</td> <td> 0.302</td> <td>-3083.341</td> <td> 9953.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>     <td> 1.054e+04</td> <td> 3627.974</td> <td>    2.906</td> <td> 0.004</td> <td> 3431.229</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>     <td> 2730.9588</td> <td> 3557.222</td> <td>    0.768</td> <td> 0.443</td> <td>-4241.736</td> <td> 9703.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>     <td>  1.11e+04</td> <td> 3515.274</td> <td>    3.158</td> <td> 0.002</td> <td> 4209.150</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>      <td>  231.6921</td> <td> 3271.221</td> <td>    0.071</td> <td> 0.944</td> <td>-6180.398</td> <td> 6643.783</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>     <td> 7757.8946</td> <td> 3398.861</td> <td>    2.282</td> <td> 0.022</td> <td> 1095.611</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>      <td> 2969.1064</td> <td> 3094.408</td> <td>    0.960</td> <td> 0.337</td> <td>-3096.404</td> <td> 9034.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>      <td> 2428.4690</td> <td> 3086.422</td> <td>    0.787</td> <td> 0.431</td> <td>-3621.386</td> <td> 8478.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>     <td> 9004.3585</td> <td> 3723.204</td> <td>    2.418</td> <td> 0.016</td> <td> 1706.314</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>     <td> 9906.5945</td> <td> 4225.421</td> <td>    2.345</td> <td> 0.019</td> <td> 1624.127</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>     <td> -775.7343</td> <td> 3345.291</td> <td>   -0.232</td> <td> 0.817</td> <td>-7333.012</td> <td> 5781.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>      <td> 7611.2849</td> <td> 3344.240</td> <td>    2.276</td> <td> 0.023</td> <td> 1056.066</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>     <td> 7822.3534</td> <td> 3374.212</td> <td>    2.318</td> <td> 0.020</td> <td> 1208.386</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>     <td> 7161.9191</td> <td> 3367.382</td> <td>    2.127</td> <td> 0.033</td> <td>  561.340</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>     <td> 2450.1067</td> <td> 3416.939</td> <td>    0.717</td> <td> 0.473</td> <td>-4247.612</td> <td> 9147.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>     <td> 5896.9695</td> <td> 3424.853</td> <td>    1.722</td> <td> 0.085</td> <td> -816.263</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>      <td> 1.291e+04</td> <td> 4617.436</td> <td>    2.796</td> <td> 0.005</td> <td> 3861.195</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14531.0</th>     <td> 6013.3098</td> <td>    1e+04</td> <td>    0.600</td> <td> 0.549</td> <td>-1.36e+04</td> <td> 2.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>     <td> 9717.6801</td> <td> 3546.928</td> <td>    2.740</td> <td> 0.006</td> <td> 2765.162</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>     <td> 8494.5972</td> <td> 7256.705</td> <td>    1.171</td> <td> 0.242</td> <td>-5729.646</td> <td> 2.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>      <td> 9293.7357</td> <td> 3759.137</td> <td>    2.472</td> <td> 0.013</td> <td> 1925.257</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>      <td> 9813.2527</td> <td> 3591.620</td> <td>    2.732</td> <td> 0.006</td> <td> 2773.133</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>     <td> 8912.7989</td> <td> 5404.388</td> <td>    1.649</td> <td> 0.099</td> <td>-1680.623</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>     <td> 9310.1232</td> <td> 3588.857</td> <td>    2.594</td> <td> 0.009</td> <td> 2275.418</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>      <td> 1.018e+04</td> <td> 3142.118</td> <td>    3.241</td> <td> 0.001</td> <td> 4023.346</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>     <td> 8561.1899</td> <td> 3545.299</td> <td>    2.415</td> <td> 0.016</td> <td> 1611.865</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>     <td> 3951.6292</td> <td> 3545.461</td> <td>    1.115</td> <td> 0.265</td> <td>-2998.013</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>      <td> 8274.0285</td> <td> 3128.411</td> <td>    2.645</td> <td> 0.008</td> <td> 2141.867</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>     <td> 4755.0666</td> <td> 3491.964</td> <td>    1.362</td> <td> 0.173</td> <td>-2089.713</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>      <td> 1.008e+04</td> <td> 3139.681</td> <td>    3.211</td> <td> 0.001</td> <td> 3928.295</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>     <td>-8685.5979</td> <td> 3678.105</td> <td>   -2.361</td> <td> 0.018</td> <td>-1.59e+04</td> <td>-1475.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>     <td> 7018.0335</td> <td> 3521.077</td> <td>    1.993</td> <td> 0.046</td> <td>  116.187</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>     <td> 9603.3202</td> <td> 4161.317</td> <td>    2.308</td> <td> 0.021</td> <td> 1446.507</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>      <td>-2.338e+04</td> <td> 4066.536</td> <td>   -5.750</td> <td> 0.000</td> <td>-3.14e+04</td> <td>-1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>      <td> 7360.1914</td> <td> 3039.605</td> <td>    2.421</td> <td> 0.015</td> <td> 1402.105</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>      <td> 1.541e+04</td> <td> 3144.958</td> <td>    4.899</td> <td> 0.000</td> <td> 9241.495</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>      <td> 9518.7977</td> <td> 3135.907</td> <td>    3.035</td> <td> 0.002</td> <td> 3371.944</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>     <td> 6141.7233</td> <td> 3600.100</td> <td>    1.706</td> <td> 0.088</td> <td> -915.020</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>      <td> 2730.6569</td> <td> 2967.596</td> <td>    0.920</td> <td> 0.358</td> <td>-3086.281</td> <td> 8547.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>      <td> 7645.1535</td> <td> 3073.458</td> <td>    2.487</td> <td> 0.013</td> <td> 1620.710</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>      <td>-2279.6622</td> <td> 3207.382</td> <td>   -0.711</td> <td> 0.477</td> <td>-8566.618</td> <td> 4007.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>     <td>-2106.7953</td> <td> 3535.255</td> <td>   -0.596</td> <td> 0.551</td> <td>-9036.432</td> <td> 4822.842</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>     <td> 2453.7358</td> <td> 4040.565</td> <td>    0.607</td> <td> 0.544</td> <td>-5466.386</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>      <td> 5550.2093</td> <td> 3015.031</td> <td>    1.841</td> <td> 0.066</td> <td> -359.709</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>      <td> 9615.4742</td> <td> 3116.290</td> <td>    3.086</td> <td> 0.002</td> <td> 3507.073</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>      <td> 1.375e+04</td> <td> 3117.223</td> <td>    4.412</td> <td> 0.000</td> <td> 7643.430</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>     <td> 4741.7051</td> <td> 3561.723</td> <td>    1.331</td> <td> 0.183</td> <td>-2239.813</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>     <td> 4221.0150</td> <td> 3488.991</td> <td>    1.210</td> <td> 0.226</td> <td>-2617.938</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>      <td>-9476.8601</td> <td> 3052.010</td> <td>   -3.105</td> <td> 0.002</td> <td>-1.55e+04</td> <td>-3494.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>      <td> 7131.1531</td> <td> 3145.834</td> <td>    2.267</td> <td> 0.023</td> <td>  964.841</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17101.0</th>     <td>-1.039e+04</td> <td> 1.02e+04</td> <td>   -1.021</td> <td> 0.307</td> <td>-3.04e+04</td> <td> 9567.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>     <td> 8565.2847</td> <td> 3580.793</td> <td>    2.392</td> <td> 0.017</td> <td> 1546.386</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>      <td> 7635.3384</td> <td> 3120.097</td> <td>    2.447</td> <td> 0.014</td> <td> 1519.474</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>      <td> 9786.0208</td> <td> 3133.400</td> <td>    3.123</td> <td> 0.002</td> <td> 3644.082</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>      <td> 9206.6002</td> <td> 4065.126</td> <td>    2.265</td> <td> 0.024</td> <td> 1238.335</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>      <td> 9356.9783</td> <td> 3245.442</td> <td>    2.883</td> <td> 0.004</td> <td> 2995.419</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>      <td> 3530.4584</td> <td> 3070.682</td> <td>    1.150</td> <td> 0.250</td> <td>-2488.544</td> <td> 9549.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>      <td> 9383.6572</td> <td> 3200.790</td> <td>    2.932</td> <td> 0.003</td> <td> 3109.623</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>      <td>-2446.1653</td> <td> 3027.221</td> <td>   -0.808</td> <td> 0.419</td> <td>-8379.979</td> <td> 3487.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>     <td> 7610.5863</td> <td> 3547.409</td> <td>    2.145</td> <td> 0.032</td> <td>  657.126</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>      <td> 7035.3148</td> <td> 3094.333</td> <td>    2.274</td> <td> 0.023</td> <td>  969.953</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>      <td>-1341.5977</td> <td> 3489.590</td> <td>   -0.384</td> <td> 0.701</td> <td>-8181.724</td> <td> 5498.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>     <td> 9308.6209</td> <td> 4398.704</td> <td>    2.116</td> <td> 0.034</td> <td>  686.493</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>      <td> 5422.0706</td> <td> 4559.415</td> <td>    1.189</td> <td> 0.234</td> <td>-3515.075</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>      <td> 9556.3065</td> <td> 3257.766</td> <td>    2.933</td> <td> 0.003</td> <td> 3170.590</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>      <td> 5067.5445</td> <td> 3013.058</td> <td>    1.682</td> <td> 0.093</td> <td> -838.507</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>      <td> 8460.0632</td> <td> 3225.767</td> <td>    2.623</td> <td> 0.009</td> <td> 2137.070</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>      <td> 5677.3684</td> <td> 2997.340</td> <td>    1.894</td> <td> 0.058</td> <td> -197.872</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>      <td> 7609.1915</td> <td> 3040.834</td> <td>    2.502</td> <td> 0.012</td> <td> 1648.695</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>      <td> 1.067e+04</td> <td> 3116.261</td> <td>    3.425</td> <td> 0.001</td> <td> 4565.380</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>      <td> 9170.2560</td> <td> 3119.840</td> <td>    2.939</td> <td> 0.003</td> <td> 3054.895</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>      <td>   67.7686</td> <td> 3946.416</td> <td>    0.017</td> <td> 0.986</td> <td>-7667.806</td> <td> 7803.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>      <td> 7764.2033</td> <td> 3046.140</td> <td>    2.549</td> <td> 0.011</td> <td> 1793.305</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>      <td> 8115.3533</td> <td> 3045.881</td> <td>    2.664</td> <td> 0.008</td> <td> 2144.965</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>      <td> 9191.5623</td> <td> 3578.712</td> <td>    2.568</td> <td> 0.010</td> <td> 2176.744</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>      <td> 7375.6500</td> <td> 3054.171</td> <td>    2.415</td> <td> 0.016</td> <td> 1389.011</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>      <td> 7958.9282</td> <td> 3057.521</td> <td>    2.603</td> <td> 0.009</td> <td> 1965.724</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>      <td> 1.067e+04</td> <td> 3184.425</td> <td>    3.350</td> <td> 0.001</td> <td> 4425.887</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>     <td> 5.298e+04</td> <td> 3626.010</td> <td>   14.611</td> <td> 0.000</td> <td> 4.59e+04</td> <td> 6.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>      <td>-2797.9037</td> <td> 3091.351</td> <td>   -0.905</td> <td> 0.365</td> <td>-8857.420</td> <td> 3261.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>      <td> 6298.7693</td> <td> 3040.273</td> <td>    2.072</td> <td> 0.038</td> <td>  339.373</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>      <td> 7111.0506</td> <td> 3027.347</td> <td>    2.349</td> <td> 0.019</td> <td> 1176.992</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>     <td> 5065.4342</td> <td> 3773.658</td> <td>    1.342</td> <td> 0.180</td> <td>-2331.508</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>     <td> 9630.2308</td> <td> 3715.731</td> <td>    2.592</td> <td> 0.010</td> <td> 2346.833</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>      <td> 7814.7292</td> <td> 3156.973</td> <td>    2.475</td> <td> 0.013</td> <td> 1626.582</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>      <td> 1.782e+04</td> <td> 3967.047</td> <td>    4.493</td> <td> 0.000</td> <td>    1e+04</td> <td> 2.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>     <td>-9707.4783</td> <td> 3795.661</td> <td>   -2.558</td> <td> 0.011</td> <td>-1.71e+04</td> <td>-2267.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>      <td> 8531.2191</td> <td> 3092.395</td> <td>    2.759</td> <td> 0.006</td> <td> 2469.654</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>      <td> 4.917e+04</td> <td> 3655.506</td> <td>   13.450</td> <td> 0.000</td> <td>  4.2e+04</td> <td> 5.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>      <td> 1.064e+04</td> <td> 3272.692</td> <td>    3.252</td> <td> 0.001</td> <td> 4229.171</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>      <td> 1987.7047</td> <td> 3044.824</td> <td>    0.653</td> <td> 0.514</td> <td>-3980.612</td> <td> 7956.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>      <td> 8121.7655</td> <td> 3066.803</td> <td>    2.648</td> <td> 0.008</td> <td> 2110.366</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>     <td> 1.097e+04</td> <td> 3742.158</td> <td>    2.932</td> <td> 0.003</td> <td> 3635.194</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>      <td> 6564.1694</td> <td> 6056.783</td> <td>    1.084</td> <td> 0.278</td> <td>-5308.045</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>      <td> 9663.4059</td> <td> 3448.942</td> <td>    2.802</td> <td> 0.005</td> <td> 2902.955</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>     <td> 2036.3731</td> <td> 3623.720</td> <td>    0.562</td> <td> 0.574</td> <td>-5066.669</td> <td> 9139.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>      <td> 7276.8607</td> <td> 3077.041</td> <td>    2.365</td> <td> 0.018</td> <td> 1245.393</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>     <td> 9459.5788</td> <td> 3864.126</td> <td>    2.448</td> <td> 0.014</td> <td> 1885.306</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>      <td> 1604.9439</td> <td> 3063.270</td> <td>    0.524</td> <td> 0.600</td> <td>-4399.530</td> <td> 7609.418</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>     <td> 4875.5667</td> <td> 3570.634</td> <td>    1.365</td> <td> 0.172</td> <td>-2123.419</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>      <td>-2.061e+04</td> <td> 3176.312</td> <td>   -6.490</td> <td> 0.000</td> <td>-2.68e+04</td> <td>-1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>      <td> 4219.0616</td> <td> 3064.579</td> <td>    1.377</td> <td> 0.169</td> <td>-1787.979</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>      <td>  1.02e+04</td> <td> 4631.605</td> <td>    2.202</td> <td> 0.028</td> <td> 1118.124</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>      <td> 3932.7481</td> <td> 3268.816</td> <td>    1.203</td> <td> 0.229</td> <td>-2474.628</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>     <td> 7708.2043</td> <td> 3613.811</td> <td>    2.133</td> <td> 0.033</td> <td>  624.587</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>     <td>-1416.5488</td> <td> 3999.474</td> <td>   -0.354</td> <td> 0.723</td> <td>-9256.124</td> <td> 6423.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>      <td>  371.2913</td> <td> 5445.150</td> <td>    0.068</td> <td> 0.946</td> <td>-1.03e+04</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>      <td> 7033.5174</td> <td> 3195.255</td> <td>    2.201</td> <td> 0.028</td> <td>  770.332</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>     <td>-4104.2067</td> <td> 4540.832</td> <td>   -0.904</td> <td> 0.366</td> <td> -1.3e+04</td> <td> 4796.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>      <td> 1.022e+04</td> <td> 3146.886</td> <td>    3.249</td> <td> 0.001</td> <td> 4054.843</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>      <td> 5256.7276</td> <td> 3021.333</td> <td>    1.740</td> <td> 0.082</td> <td> -665.543</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>      <td> 1.544e+04</td> <td> 3116.842</td> <td>    4.954</td> <td> 0.000</td> <td> 9331.361</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>      <td> 1.204e+04</td> <td> 3215.430</td> <td>    3.744</td> <td> 0.000</td> <td> 5734.523</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>      <td> 5543.3070</td> <td> 3031.522</td> <td>    1.829</td> <td> 0.067</td> <td> -398.936</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>      <td> 7626.0071</td> <td> 3053.285</td> <td>    2.498</td> <td> 0.013</td> <td> 1641.104</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>      <td> 9427.6259</td> <td> 4407.154</td> <td>    2.139</td> <td> 0.032</td> <td>  788.935</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>     <td> 8470.8001</td> <td> 3726.615</td> <td>    2.273</td> <td> 0.023</td> <td> 1166.069</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>     <td> 2679.9470</td> <td> 3896.093</td> <td>    0.688</td> <td> 0.492</td> <td>-4956.987</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>      <td> 1.036e+04</td> <td> 3154.774</td> <td>    3.283</td> <td> 0.001</td> <td> 4172.094</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>     <td>  1.01e+04</td> <td> 4399.409</td> <td>    2.296</td> <td> 0.022</td> <td> 1476.724</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>      <td> 2822.1067</td> <td> 3205.564</td> <td>    0.880</td> <td> 0.379</td> <td>-3461.286</td> <td> 9105.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>      <td>-5740.6155</td> <td> 3125.446</td> <td>   -1.837</td> <td> 0.066</td> <td>-1.19e+04</td> <td>  385.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>      <td> 9551.1213</td> <td> 3313.820</td> <td>    2.882</td> <td> 0.004</td> <td> 3055.530</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>     <td> 8839.2099</td> <td> 3850.901</td> <td>    2.295</td> <td> 0.022</td> <td> 1290.860</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>      <td> 9462.2606</td> <td> 3118.179</td> <td>    3.035</td> <td> 0.002</td> <td> 3350.156</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>     <td> 9181.2871</td> <td> 7262.529</td> <td>    1.264</td> <td> 0.206</td> <td>-5054.373</td> <td> 2.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>     <td> 8309.3827</td> <td> 3823.370</td> <td>    2.173</td> <td> 0.030</td> <td>  814.996</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>      <td>-1331.6614</td> <td> 3141.364</td> <td>   -0.424</td> <td> 0.672</td> <td>-7489.212</td> <td> 4825.889</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>      <td> 1.059e+04</td> <td> 4051.182</td> <td>    2.615</td> <td> 0.009</td> <td> 2653.302</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>     <td> 1.016e+04</td> <td> 6108.961</td> <td>    1.663</td> <td> 0.096</td> <td>-1815.943</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>      <td> 1402.5130</td> <td> 3210.761</td> <td>    0.437</td> <td> 0.662</td> <td>-4891.066</td> <td> 7696.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>      <td> 9104.4999</td> <td> 3154.705</td> <td>    2.886</td> <td> 0.004</td> <td> 2920.799</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>      <td> 2817.6652</td> <td> 3621.446</td> <td>    0.778</td> <td> 0.437</td> <td>-4280.919</td> <td> 9916.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>     <td> 8887.4249</td> <td> 3853.652</td> <td>    2.306</td> <td> 0.021</td> <td> 1333.682</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>      <td> 7333.8930</td> <td> 3033.785</td> <td>    2.417</td> <td> 0.016</td> <td> 1387.213</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>      <td> 7608.5207</td> <td> 3056.291</td> <td>    2.489</td> <td> 0.013</td> <td> 1617.727</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>      <td> 7104.2843</td> <td> 3165.531</td> <td>    2.244</td> <td> 0.025</td> <td>  899.363</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>      <td> 1.201e+04</td> <td> 3109.709</td> <td>    3.862</td> <td> 0.000</td> <td> 5914.315</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>      <td> 7170.4426</td> <td> 3157.640</td> <td>    2.271</td> <td> 0.023</td> <td>  980.989</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>      <td> 8843.2302</td> <td> 3108.248</td> <td>    2.845</td> <td> 0.004</td> <td> 2750.591</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>      <td>-1136.7600</td> <td> 3047.444</td> <td>   -0.373</td> <td> 0.709</td> <td>-7110.213</td> <td> 4836.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>      <td> 9981.2391</td> <td> 3145.382</td> <td>    3.173</td> <td> 0.002</td> <td> 3815.813</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>      <td>-2423.3733</td> <td> 3105.183</td> <td>   -0.780</td> <td> 0.435</td> <td>-8510.003</td> <td> 3663.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>     <td>-6800.8821</td> <td> 3938.721</td> <td>   -1.727</td> <td> 0.084</td> <td>-1.45e+04</td> <td>  919.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>     <td> 7289.6069</td> <td> 3122.789</td> <td>    2.334</td> <td> 0.020</td> <td> 1168.466</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>     <td>-4379.2282</td> <td> 4023.812</td> <td>   -1.088</td> <td> 0.276</td> <td>-1.23e+04</td> <td> 3508.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>      <td> 8333.3928</td> <td> 3213.797</td> <td>    2.593</td> <td> 0.010</td> <td> 2033.862</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>      <td> 9522.3975</td> <td> 3838.939</td> <td>    2.480</td> <td> 0.013</td> <td> 1997.494</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>      <td> 4179.7509</td> <td> 3174.337</td> <td>    1.317</td> <td> 0.188</td> <td>-2042.431</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>     <td>-4484.0845</td> <td> 3895.740</td> <td>   -1.151</td> <td> 0.250</td> <td>-1.21e+04</td> <td> 3152.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>      <td>-8939.5631</td> <td> 4197.529</td> <td>   -2.130</td> <td> 0.033</td> <td>-1.72e+04</td> <td> -711.769</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>      <td> 1.053e+04</td> <td> 3554.152</td> <td>    2.964</td> <td> 0.003</td> <td> 3567.561</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>      <td> 8670.7332</td> <td> 3081.692</td> <td>    2.814</td> <td> 0.005</td> <td> 2630.148</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>      <td> 6927.1661</td> <td> 3807.516</td> <td>    1.819</td> <td> 0.069</td> <td> -536.143</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>      <td> 4129.9550</td> <td> 3074.147</td> <td>    1.343</td> <td> 0.179</td> <td>-1895.841</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>      <td> 8607.4496</td> <td> 3114.061</td> <td>    2.764</td> <td> 0.006</td> <td> 2503.416</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>      <td>-2091.3592</td> <td> 3611.129</td> <td>   -0.579</td> <td> 0.563</td> <td>-9169.721</td> <td> 4987.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>      <td>  352.0291</td> <td> 3228.393</td> <td>    0.109</td> <td> 0.913</td> <td>-5976.112</td> <td> 6680.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>      <td> 1.066e+04</td> <td> 3189.868</td> <td>    3.341</td> <td> 0.001</td> <td> 4404.847</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>      <td> 8297.5216</td> <td> 3114.849</td> <td>    2.664</td> <td> 0.008</td> <td> 2191.944</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>      <td>-7156.8279</td> <td> 3963.813</td> <td>   -1.806</td> <td> 0.071</td> <td>-1.49e+04</td> <td>  612.847</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>      <td> 1.013e+04</td> <td> 3216.055</td> <td>    3.150</td> <td> 0.002</td> <td> 3827.484</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>      <td> 4614.1396</td> <td> 3380.925</td> <td>    1.365</td> <td> 0.172</td> <td>-2012.986</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>      <td> 8261.4341</td> <td> 4593.460</td> <td>    1.799</td> <td> 0.072</td> <td> -742.444</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>      <td> 1.075e+04</td> <td> 3100.529</td> <td>    3.467</td> <td> 0.001</td> <td> 4670.905</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>      <td> 8639.8575</td> <td> 3074.886</td> <td>    2.810</td> <td> 0.005</td> <td> 2612.613</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>      <td> 6.167e+04</td> <td> 3143.112</td> <td>   19.622</td> <td> 0.000</td> <td> 5.55e+04</td> <td> 6.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>      <td> 9057.0292</td> <td> 3556.046</td> <td>    2.547</td> <td> 0.011</td> <td> 2086.640</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>      <td> 8487.6305</td> <td> 3086.887</td> <td>    2.750</td> <td> 0.006</td> <td> 2436.864</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>      <td> 1.112e+04</td> <td> 3070.843</td> <td>    3.621</td> <td> 0.000</td> <td> 5098.953</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>      <td> 4719.2301</td> <td> 3279.399</td> <td>    1.439</td> <td> 0.150</td> <td>-1708.889</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>      <td> 5242.4391</td> <td> 3326.387</td> <td>    1.576</td> <td> 0.115</td> <td>-1277.785</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>      <td> 6940.2745</td> <td> 3201.204</td> <td>    2.168</td> <td> 0.030</td> <td>  665.428</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>      <td> 8557.6434</td> <td> 3259.016</td> <td>    2.626</td> <td> 0.009</td> <td> 2169.477</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>      <td> 8959.3648</td> <td> 3141.052</td> <td>    2.852</td> <td> 0.004</td> <td> 2802.426</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>      <td> 8795.1454</td> <td> 3144.903</td> <td>    2.797</td> <td> 0.005</td> <td> 2630.657</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>      <td>-1.882e+04</td> <td> 3179.239</td> <td>   -5.919</td> <td> 0.000</td> <td> -2.5e+04</td> <td>-1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>      <td> 1467.2473</td> <td> 3430.530</td> <td>    0.428</td> <td> 0.669</td> <td>-5257.112</td> <td> 8191.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>      <td> 8967.9006</td> <td> 3615.991</td> <td>    2.480</td> <td> 0.013</td> <td> 1880.009</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>      <td> 8150.3780</td> <td> 3116.503</td> <td>    2.615</td> <td> 0.009</td> <td> 2041.559</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>      <td> 3305.8909</td> <td> 3020.187</td> <td>    1.095</td> <td> 0.274</td> <td>-2614.134</td> <td> 9225.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>      <td> 4413.0364</td> <td> 3019.260</td> <td>    1.462</td> <td> 0.144</td> <td>-1505.172</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>      <td> 7330.3624</td> <td> 3693.007</td> <td>    1.985</td> <td> 0.047</td> <td>   91.508</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>      <td> 6509.4875</td> <td> 3039.592</td> <td>    2.142</td> <td> 0.032</td> <td>  551.425</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>      <td> 9367.0102</td> <td> 3036.064</td> <td>    3.085</td> <td> 0.002</td> <td> 3415.864</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>      <td> 9567.2283</td> <td> 4941.074</td> <td>    1.936</td> <td> 0.053</td> <td> -118.027</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>      <td> 5668.8773</td> <td> 3023.668</td> <td>    1.875</td> <td> 0.061</td> <td> -257.971</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>      <td> 1.077e+04</td> <td> 3190.707</td> <td>    3.376</td> <td> 0.001</td> <td> 4518.562</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>      <td> 7583.6331</td> <td> 3125.470</td> <td>    2.426</td> <td> 0.015</td> <td> 1457.238</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>      <td> 1.024e+04</td> <td> 3247.561</td> <td>    3.154</td> <td> 0.002</td> <td> 3877.236</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>      <td>  701.7362</td> <td> 2978.785</td> <td>    0.236</td> <td> 0.814</td> <td>-5137.134</td> <td> 6540.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>      <td>  -56.5141</td> <td> 3013.203</td> <td>   -0.019</td> <td> 0.985</td> <td>-5962.849</td> <td> 5849.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>      <td> 6515.3615</td> <td> 3036.473</td> <td>    2.146</td> <td> 0.032</td> <td>  563.413</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>      <td>-4540.5498</td> <td> 3009.478</td> <td>   -1.509</td> <td> 0.131</td> <td>-1.04e+04</td> <td> 1358.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>      <td> 5998.0243</td> <td> 3363.381</td> <td>    1.783</td> <td> 0.075</td> <td> -594.714</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>      <td> 4676.2483</td> <td> 3019.021</td> <td>    1.549</td> <td> 0.121</td> <td>-1241.491</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>      <td>-1115.3957</td> <td> 3351.604</td> <td>   -0.333</td> <td> 0.739</td> <td>-7685.049</td> <td> 5454.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>      <td> 3388.6729</td> <td> 3657.813</td> <td>    0.926</td> <td> 0.354</td> <td>-3781.196</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>      <td> -945.5519</td> <td> 3097.134</td> <td>   -0.305</td> <td> 0.760</td> <td>-7016.405</td> <td> 5125.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>      <td> 8059.2754</td> <td> 3095.380</td> <td>    2.604</td> <td> 0.009</td> <td> 1991.860</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>      <td>-1921.4741</td> <td> 3437.318</td> <td>   -0.559</td> <td> 0.576</td> <td>-8659.140</td> <td> 4816.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>      <td> 9374.5415</td> <td> 3182.442</td> <td>    2.946</td> <td> 0.003</td> <td> 3136.471</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>      <td> 1623.3259</td> <td> 3068.940</td> <td>    0.529</td> <td> 0.597</td> <td>-4392.263</td> <td> 7638.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>      <td> 5038.7123</td> <td> 3787.676</td> <td>    1.330</td> <td> 0.183</td> <td>-2385.707</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>      <td> 3096.0560</td> <td> 3106.065</td> <td>    0.997</td> <td> 0.319</td> <td>-2992.302</td> <td> 9184.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>      <td> 9436.3688</td> <td> 3115.660</td> <td>    3.029</td> <td> 0.002</td> <td> 3329.202</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>      <td> 6499.5003</td> <td> 4150.425</td> <td>    1.566</td> <td> 0.117</td> <td>-1635.964</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>      <td> 3978.4256</td> <td> 3062.447</td> <td>    1.299</td> <td> 0.194</td> <td>-2024.435</td> <td> 9981.286</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>      <td> 9644.8467</td> <td> 3201.184</td> <td>    3.013</td> <td> 0.003</td> <td> 3370.040</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>      <td>  1.02e+04</td> <td> 3163.781</td> <td>    3.224</td> <td> 0.001</td> <td> 3997.651</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>      <td> 8857.9912</td> <td> 3190.900</td> <td>    2.776</td> <td> 0.006</td> <td> 2603.342</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>      <td> 1.889e+04</td> <td> 3114.804</td> <td>    6.066</td> <td> 0.000</td> <td> 1.28e+04</td> <td>  2.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>      <td> 5433.4828</td> <td> 3023.927</td> <td>    1.797</td> <td> 0.072</td> <td> -493.873</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>      <td> 1.006e+04</td> <td> 3132.064</td> <td>    3.210</td> <td> 0.001</td> <td> 3915.798</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>      <td> 4402.5134</td> <td> 3085.061</td> <td>    1.427</td> <td> 0.154</td> <td>-1644.674</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>      <td> 8496.1476</td> <td> 3074.049</td> <td>    2.764</td> <td> 0.006</td> <td> 2470.545</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>      <td>-8502.7681</td> <td> 3085.688</td> <td>   -2.756</td> <td> 0.006</td> <td>-1.46e+04</td> <td>-2454.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>      <td> 1.245e+04</td> <td> 3186.197</td> <td>    3.906</td> <td> 0.000</td> <td> 6200.250</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>      <td> 8049.1131</td> <td> 4325.812</td> <td>    1.861</td> <td> 0.063</td> <td> -430.135</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>      <td>-1.922e+04</td> <td> 3417.870</td> <td>   -5.622</td> <td> 0.000</td> <td>-2.59e+04</td> <td>-1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>      <td> 7764.4934</td> <td> 3536.914</td> <td>    2.195</td> <td> 0.028</td> <td>  831.604</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>      <td> 3991.2384</td> <td> 2983.946</td> <td>    1.338</td> <td> 0.181</td> <td>-1857.749</td> <td> 9840.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>      <td> 1.012e+04</td> <td> 3751.511</td> <td>    2.696</td> <td> 0.007</td> <td> 2761.757</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>      <td> 8848.7229</td> <td> 3711.438</td> <td>    2.384</td> <td> 0.017</td> <td> 1573.741</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>      <td> 1.029e+04</td> <td> 3147.338</td> <td>    3.271</td> <td> 0.001</td> <td> 4124.177</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>      <td> -206.5396</td> <td> 3254.206</td> <td>   -0.063</td> <td> 0.949</td> <td>-6585.277</td> <td> 6172.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>      <td>-3080.7950</td> <td> 2986.731</td> <td>   -1.031</td> <td> 0.302</td> <td>-8935.242</td> <td> 2773.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>      <td> 9040.9619</td> <td> 3063.929</td> <td>    2.951</td> <td> 0.003</td> <td> 3035.197</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>      <td> -427.7763</td> <td> 3036.691</td> <td>   -0.141</td> <td> 0.888</td> <td>-6380.151</td> <td> 5524.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>      <td> 8795.2266</td> <td> 3075.179</td> <td>    2.860</td> <td> 0.004</td> <td> 2767.409</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>      <td> 9920.6121</td> <td> 3135.291</td> <td>    3.164</td> <td> 0.002</td> <td> 3774.966</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>      <td> 8311.0668</td> <td> 3331.549</td> <td>    2.495</td> <td> 0.013</td> <td> 1780.724</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>      <td> 7457.4585</td> <td> 3194.105</td> <td>    2.335</td> <td> 0.020</td> <td> 1196.528</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>      <td> 5939.8774</td> <td> 3058.043</td> <td>    1.942</td> <td> 0.052</td> <td>  -54.352</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>      <td> 7276.7703</td> <td> 4582.157</td> <td>    1.588</td> <td> 0.112</td> <td>-1704.953</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>      <td> 6243.3898</td> <td> 3049.262</td> <td>    2.048</td> <td> 0.041</td> <td>  266.372</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>      <td> 6746.0318</td> <td> 3081.412</td> <td>    2.189</td> <td> 0.029</td> <td>  705.997</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>      <td> 9237.0407</td> <td> 3230.539</td> <td>    2.859</td> <td> 0.004</td> <td> 2904.693</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>      <td> 7979.6767</td> <td> 3101.329</td> <td>    2.573</td> <td> 0.010</td> <td> 1900.602</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>      <td>  675.3081</td> <td> 3174.296</td> <td>    0.213</td> <td> 0.832</td> <td>-5546.793</td> <td> 6897.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>      <td> 2475.6995</td> <td> 3026.171</td> <td>    0.818</td> <td> 0.413</td> <td>-3456.054</td> <td> 8407.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>      <td> 7980.2863</td> <td> 3051.589</td> <td>    2.615</td> <td> 0.009</td> <td> 1998.708</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>      <td> 8011.4429</td> <td> 7270.601</td> <td>    1.102</td> <td> 0.271</td> <td>-6240.039</td> <td> 2.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>      <td> 8842.8576</td> <td> 3229.403</td> <td>    2.738</td> <td> 0.006</td> <td> 2512.738</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>      <td> 1.065e+04</td> <td> 3189.762</td> <td>    3.338</td> <td> 0.001</td> <td> 4393.423</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>      <td> 1.003e+04</td> <td> 3206.938</td> <td>    3.129</td> <td> 0.002</td> <td> 3747.041</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>      <td> 6865.7401</td> <td> 3039.451</td> <td>    2.259</td> <td> 0.024</td> <td>  907.954</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>      <td> 1012.0246</td> <td> 3200.037</td> <td>    0.316</td> <td> 0.752</td> <td>-5260.535</td> <td> 7284.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>      <td> 9896.4315</td> <td> 3136.909</td> <td>    3.155</td> <td> 0.002</td> <td> 3747.613</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>      <td> 1926.7708</td> <td> 3084.280</td> <td>    0.625</td> <td> 0.532</td> <td>-4118.886</td> <td> 7972.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>      <td> 5050.9157</td> <td> 3012.296</td> <td>    1.677</td> <td> 0.094</td> <td> -853.642</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>      <td> 9205.2343</td> <td> 3264.415</td> <td>    2.820</td> <td> 0.005</td> <td> 2806.486</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>      <td> 8149.0733</td> <td> 3123.950</td> <td>    2.609</td> <td> 0.009</td> <td> 2025.657</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>      <td>  1.02e+04</td> <td> 3154.399</td> <td>    3.233</td> <td> 0.001</td> <td> 4014.036</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>      <td> 8510.9355</td> <td> 3222.793</td> <td>    2.641</td> <td> 0.008</td> <td> 2193.771</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>      <td> 9756.0632</td> <td> 3122.659</td> <td>    3.124</td> <td> 0.002</td> <td> 3635.177</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>      <td> 9311.2841</td> <td> 3209.003</td> <td>    2.902</td> <td> 0.004</td> <td> 3021.150</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>      <td> -1.39e+05</td> <td> 4400.348</td> <td>  -31.598</td> <td> 0.000</td> <td>-1.48e+05</td> <td> -1.3e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>      <td> -155.0920</td> <td> 3300.774</td> <td>   -0.047</td> <td> 0.963</td> <td>-6625.111</td> <td> 6314.927</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>      <td> 7349.9526</td> <td> 3033.927</td> <td>    2.423</td> <td> 0.015</td> <td> 1402.995</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>      <td> 8056.7145</td> <td> 3130.004</td> <td>    2.574</td> <td> 0.010</td> <td> 1921.431</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>      <td> 6951.4250</td> <td> 3036.486</td> <td>    2.289</td> <td> 0.022</td> <td>  999.451</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>      <td> 7701.0234</td> <td> 3119.428</td> <td>    2.469</td> <td> 0.014</td> <td> 1586.472</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>      <td>-6713.3878</td> <td> 3845.668</td> <td>   -1.746</td> <td> 0.081</td> <td>-1.43e+04</td> <td>  824.706</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>      <td> 1.555e+04</td> <td> 3152.418</td> <td>    4.932</td> <td> 0.000</td> <td> 9368.460</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>      <td> 1.078e+04</td> <td> 3191.719</td> <td>    3.379</td> <td> 0.001</td> <td> 4527.925</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>      <td> 2756.6116</td> <td> 3051.145</td> <td>    0.903</td> <td> 0.366</td> <td>-3224.095</td> <td> 8737.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>      <td> 1703.6436</td> <td> 3206.794</td> <td>    0.531</td> <td> 0.595</td> <td>-4582.160</td> <td> 7989.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>      <td> 5583.6057</td> <td> 3073.165</td> <td>    1.817</td> <td> 0.069</td> <td> -440.265</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>      <td> 8848.7731</td> <td> 3093.419</td> <td>    2.861</td> <td> 0.004</td> <td> 2785.202</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>      <td> 5765.7251</td> <td> 3056.378</td> <td>    1.886</td> <td> 0.059</td> <td> -225.240</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>      <td>-3742.7011</td> <td> 3008.827</td> <td>   -1.244</td> <td> 0.214</td> <td>-9640.459</td> <td> 2155.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>      <td> 4.951e+04</td> <td> 3945.666</td> <td>   12.549</td> <td> 0.000</td> <td> 4.18e+04</td> <td> 5.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>      <td> 9943.6682</td> <td> 3565.116</td> <td>    2.789</td> <td> 0.005</td> <td> 2955.499</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>      <td> 8402.8199</td> <td> 3506.244</td> <td>    2.397</td> <td> 0.017</td> <td> 1530.049</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>      <td> -2.03e+05</td> <td> 6327.061</td> <td>  -32.087</td> <td> 0.000</td> <td>-2.15e+05</td> <td>-1.91e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>      <td> 4808.3060</td> <td> 3031.181</td> <td>    1.586</td> <td> 0.113</td> <td>-1133.268</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>      <td>     1e+04</td> <td> 3156.702</td> <td>    3.169</td> <td> 0.002</td> <td> 3815.966</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>      <td>  331.7479</td> <td> 3230.260</td> <td>    0.103</td> <td> 0.918</td> <td>-6000.052</td> <td> 6663.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>      <td> 5093.7234</td> <td> 3033.485</td> <td>    1.679</td> <td> 0.093</td> <td> -852.368</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>      <td> 1619.9886</td> <td> 3164.783</td> <td>    0.512</td> <td> 0.609</td> <td>-4583.467</td> <td> 7823.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>      <td> 6629.5952</td> <td> 3817.878</td> <td>    1.736</td> <td> 0.083</td> <td> -854.026</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>      <td> 7201.9002</td> <td> 3418.120</td> <td>    2.107</td> <td> 0.035</td> <td>  501.866</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>      <td> 1.921e+04</td> <td> 3111.749</td> <td>    6.173</td> <td> 0.000</td> <td> 1.31e+04</td> <td> 2.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>      <td> 6861.1596</td> <td> 3315.866</td> <td>    2.069</td> <td> 0.039</td> <td>  361.558</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>      <td> 9522.8653</td> <td> 3123.992</td> <td>    3.048</td> <td> 0.002</td> <td> 3399.366</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>      <td> 1.022e+04</td> <td> 3247.843</td> <td>    3.148</td> <td> 0.002</td> <td> 3857.092</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>      <td> 1.051e+04</td> <td> 3595.928</td> <td>    2.924</td> <td> 0.003</td> <td> 3466.116</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>      <td> 1832.7007</td> <td> 2976.839</td> <td>    0.616</td> <td> 0.538</td> <td>-4002.356</td> <td> 7667.757</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>      <td>-5223.8465</td> <td> 3133.458</td> <td>   -1.667</td> <td> 0.096</td> <td>-1.14e+04</td> <td>  918.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>      <td> 9043.0280</td> <td> 3101.586</td> <td>    2.916</td> <td> 0.004</td> <td> 2963.448</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>      <td> 8212.2020</td> <td> 3060.018</td> <td>    2.684</td> <td> 0.007</td> <td> 2214.102</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>      <td> 8446.7502</td> <td> 3097.320</td> <td>    2.727</td> <td> 0.006</td> <td> 2375.532</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>      <td> 6465.5650</td> <td> 3027.709</td> <td>    2.135</td> <td> 0.033</td> <td>  530.795</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>      <td> 9319.2221</td> <td> 3098.305</td> <td>    3.008</td> <td> 0.003</td> <td> 3246.074</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>      <td> 1.009e+04</td> <td> 3165.731</td> <td>    3.188</td> <td> 0.001</td> <td> 3885.677</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>      <td> 7334.3884</td> <td> 3132.147</td> <td>    2.342</td> <td> 0.019</td> <td> 1194.905</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>      <td> 1.067e+04</td> <td> 3219.404</td> <td>    3.313</td> <td> 0.001</td> <td> 4356.183</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>      <td> 8046.9137</td> <td> 3717.507</td> <td>    2.165</td> <td> 0.030</td> <td>  760.036</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>      <td> 1.038e+04</td> <td> 3188.810</td> <td>    3.254</td> <td> 0.001</td> <td> 4125.428</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>      <td>-7692.0384</td> <td> 3028.409</td> <td>   -2.540</td> <td> 0.011</td> <td>-1.36e+04</td> <td>-1755.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>      <td> 8328.8408</td> <td> 3078.327</td> <td>    2.706</td> <td> 0.007</td> <td> 2294.852</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>      <td> 9584.8474</td> <td> 3145.488</td> <td>    3.047</td> <td> 0.002</td> <td> 3419.213</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>      <td> 8040.5569</td> <td> 3394.234</td> <td>    2.369</td> <td> 0.018</td> <td> 1387.343</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>      <td> 7141.4059</td> <td> 3085.889</td> <td>    2.314</td> <td> 0.021</td> <td> 1092.595</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>      <td> 9850.2695</td> <td> 3257.198</td> <td>    3.024</td> <td> 0.002</td> <td> 3465.667</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>      <td>   1.2e+04</td> <td> 3131.140</td> <td>    3.834</td> <td> 0.000</td> <td> 5866.525</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>      <td> 1.006e+04</td> <td> 3173.656</td> <td>    3.171</td> <td> 0.002</td> <td> 3841.616</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>      <td> 9572.4372</td> <td> 3115.964</td> <td>    3.072</td> <td> 0.002</td> <td> 3464.675</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>      <td> 9414.8809</td> <td> 3097.341</td> <td>    3.040</td> <td> 0.002</td> <td> 3343.622</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>      <td> 2400.9953</td> <td> 2973.079</td> <td>    0.808</td> <td> 0.419</td> <td>-3426.690</td> <td> 8228.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>      <td>  1.13e+04</td> <td> 3648.090</td> <td>    3.097</td> <td> 0.002</td> <td> 4145.936</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>      <td>-2.745e+04</td> <td> 3152.384</td> <td>   -8.709</td> <td> 0.000</td> <td>-3.36e+04</td> <td>-2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>      <td> 1.082e+04</td> <td> 3159.665</td> <td>    3.425</td> <td> 0.001</td> <td> 4628.029</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>      <td> 7446.7098</td> <td> 3218.672</td> <td>    2.314</td> <td> 0.021</td> <td> 1137.624</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>      <td> 9706.2182</td> <td> 3117.215</td> <td>    3.114</td> <td> 0.002</td> <td> 3596.004</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>      <td> 9020.8914</td> <td> 3130.883</td> <td>    2.881</td> <td> 0.004</td> <td> 2883.886</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>      <td> 9123.8740</td> <td> 3144.647</td> <td>    2.901</td> <td> 0.004</td> <td> 2959.887</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>      <td> 7723.0628</td> <td> 3018.519</td> <td>    2.559</td> <td> 0.011</td> <td> 1806.306</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>      <td> 8901.7882</td> <td> 3086.811</td> <td>    2.884</td> <td> 0.004</td> <td> 2851.170</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>      <td>-1.948e+04</td> <td> 3110.857</td> <td>   -6.261</td> <td> 0.000</td> <td>-2.56e+04</td> <td>-1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>      <td> 1.119e+04</td> <td> 3037.648</td> <td>    3.682</td> <td> 0.000</td> <td> 5231.276</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>      <td> 4917.5431</td> <td> 3123.826</td> <td>    1.574</td> <td> 0.115</td> <td>-1205.631</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>      <td> 6472.3316</td> <td> 3072.948</td> <td>    2.106</td> <td> 0.035</td> <td>  448.886</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>      <td> 4032.3371</td> <td> 3074.491</td> <td>    1.312</td> <td> 0.190</td> <td>-1994.131</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>      <td> 2.842e+04</td> <td> 3030.015</td> <td>    9.380</td> <td> 0.000</td> <td> 2.25e+04</td> <td> 3.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>      <td> 5915.9539</td> <td> 3166.969</td> <td>    1.868</td> <td> 0.062</td> <td> -291.787</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>      <td> 4321.0088</td> <td> 3624.230</td> <td>    1.192</td> <td> 0.233</td> <td>-2783.033</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>      <td>  672.3068</td> <td> 2994.346</td> <td>    0.225</td> <td> 0.822</td> <td>-5197.066</td> <td> 6541.680</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>      <td> 9107.9457</td> <td> 3133.532</td> <td>    2.907</td> <td> 0.004</td> <td> 2965.747</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>      <td> 1.053e+04</td> <td> 3386.931</td> <td>    3.110</td> <td> 0.002</td> <td> 3894.343</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>      <td> -2.49e+04</td> <td> 4424.278</td> <td>   -5.629</td> <td> 0.000</td> <td>-3.36e+04</td> <td>-1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>      <td> 9970.6042</td> <td> 3077.931</td> <td>    3.239</td> <td> 0.001</td> <td> 3937.391</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>      <td>-8372.7134</td> <td> 2993.920</td> <td>   -2.797</td> <td> 0.005</td> <td>-1.42e+04</td> <td>-2504.175</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>     <td>-4392.3442</td> <td> 4603.659</td> <td>   -0.954</td> <td> 0.340</td> <td>-1.34e+04</td> <td> 4631.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>      <td> 1.012e+04</td> <td> 3186.388</td> <td>    3.177</td> <td> 0.001</td> <td> 3876.500</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>      <td> 8471.8648</td> <td> 3120.976</td> <td>    2.714</td> <td> 0.007</td> <td> 2354.277</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>      <td>-1015.6692</td> <td> 3148.374</td> <td>   -0.323</td> <td> 0.747</td> <td>-7186.960</td> <td> 5155.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>      <td> 1845.7093</td> <td> 2960.815</td> <td>    0.623</td> <td> 0.533</td> <td>-3957.937</td> <td> 7649.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>      <td> 3170.7530</td> <td> 3399.610</td> <td>    0.933</td> <td> 0.351</td> <td>-3492.999</td> <td> 9834.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>     <td>-3744.5941</td> <td> 4251.815</td> <td>   -0.881</td> <td> 0.378</td> <td>-1.21e+04</td> <td> 4589.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>      <td> 6412.2005</td> <td> 3180.992</td> <td>    2.016</td> <td> 0.044</td> <td>  176.973</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>      <td> 9057.6581</td> <td> 3095.055</td> <td>    2.926</td> <td> 0.003</td> <td> 2990.880</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>     <td> 7728.4892</td> <td> 4929.729</td> <td>    1.568</td> <td> 0.117</td> <td>-1934.529</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>      <td> 9047.4137</td> <td> 3117.371</td> <td>    2.902</td> <td> 0.004</td> <td> 2936.893</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>      <td> 9507.7433</td> <td> 3104.135</td> <td>    3.063</td> <td> 0.002</td> <td> 3423.167</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>      <td> 9878.3520</td> <td> 3184.210</td> <td>    3.102</td> <td> 0.002</td> <td> 3636.816</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>     <td> 9234.8611</td> <td> 4394.723</td> <td>    2.101</td> <td> 0.036</td> <td>  620.537</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>      <td> 5439.7175</td> <td> 3440.702</td> <td>    1.581</td> <td> 0.114</td> <td>-1304.581</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>     <td>-2.283e+04</td> <td> 4958.737</td> <td>   -4.603</td> <td> 0.000</td> <td>-3.25e+04</td> <td>-1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>      <td> 9238.7331</td> <td> 3091.436</td> <td>    2.988</td> <td> 0.003</td> <td> 3179.048</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>      <td> 1962.6459</td> <td> 3058.799</td> <td>    0.642</td> <td> 0.521</td> <td>-4033.064</td> <td> 7958.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>      <td> 8168.8157</td> <td> 3114.238</td> <td>    2.623</td> <td> 0.009</td> <td> 2064.437</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>      <td> 9274.3327</td> <td> 3198.409</td> <td>    2.900</td> <td> 0.004</td> <td> 3004.965</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>      <td>-1.469e+04</td> <td> 3698.147</td> <td>   -3.973</td> <td> 0.000</td> <td>-2.19e+04</td> <td>-7443.955</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>      <td> 8992.0347</td> <td> 4390.677</td> <td>    2.048</td> <td> 0.041</td> <td>  385.640</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>      <td> 9736.1987</td> <td> 3151.801</td> <td>    3.089</td> <td> 0.002</td> <td> 3558.191</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>      <td> 3826.7169</td> <td> 2984.925</td> <td>    1.282</td> <td> 0.200</td> <td>-2024.190</td> <td> 9677.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>      <td> 8643.9164</td> <td> 3087.391</td> <td>    2.800</td> <td> 0.005</td> <td> 2592.161</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>      <td> 1.032e+04</td> <td> 3297.883</td> <td>    3.130</td> <td> 0.002</td> <td> 3857.899</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>      <td> 1.413e+04</td> <td> 3134.685</td> <td>    4.507</td> <td> 0.000</td> <td> 7984.330</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>      <td> 9471.0692</td> <td> 3147.599</td> <td>    3.009</td> <td> 0.003</td> <td> 3301.297</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>      <td>-3775.7784</td> <td> 6131.764</td> <td>   -0.616</td> <td> 0.538</td> <td>-1.58e+04</td> <td> 8243.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>      <td> 9629.8217</td> <td> 3110.678</td> <td>    3.096</td> <td> 0.002</td> <td> 3532.421</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>      <td> 5780.6448</td> <td> 3183.328</td> <td>    1.816</td> <td> 0.069</td> <td> -459.161</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>      <td> 1.062e+04</td> <td> 3209.167</td> <td>    3.309</td> <td> 0.001</td> <td> 4327.639</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>      <td> 4480.9728</td> <td> 3166.668</td> <td>    1.415</td> <td> 0.157</td> <td>-1726.177</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>      <td> 9498.0653</td> <td> 3132.089</td> <td>    3.033</td> <td> 0.002</td> <td> 3358.695</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>      <td> 9589.1419</td> <td> 3170.423</td> <td>    3.025</td> <td> 0.002</td> <td> 3374.632</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>      <td> 9723.4153</td> <td> 3042.211</td> <td>    3.196</td> <td> 0.001</td> <td> 3760.220</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>      <td> 6260.6289</td> <td> 3090.244</td> <td>    2.026</td> <td> 0.043</td> <td>  203.281</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>      <td> 2903.0101</td> <td> 3215.604</td> <td>    0.903</td> <td> 0.367</td> <td>-3400.062</td> <td> 9206.082</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>      <td> 8402.1659</td> <td> 7302.670</td> <td>    1.151</td> <td> 0.250</td> <td>-5912.176</td> <td> 2.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>      <td> 8464.7557</td> <td> 3077.538</td> <td>    2.750</td> <td> 0.006</td> <td> 2432.313</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>      <td> 1.055e+04</td> <td> 3417.715</td> <td>    3.086</td> <td> 0.002</td> <td> 3848.357</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>      <td> 8661.1370</td> <td> 3343.926</td> <td>    2.590</td> <td> 0.010</td> <td> 2106.534</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>      <td>  -25.9974</td> <td> 3078.405</td> <td>   -0.008</td> <td> 0.993</td> <td>-6060.139</td> <td> 6008.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>      <td> 5885.9647</td> <td> 3095.765</td> <td>    1.901</td> <td> 0.057</td> <td> -182.205</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>      <td> 9929.9352</td> <td> 3144.078</td> <td>    3.158</td> <td> 0.002</td> <td> 3767.065</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>      <td> 9593.2367</td> <td> 3169.925</td> <td>    3.026</td> <td> 0.002</td> <td> 3379.702</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>      <td> 9616.0676</td> <td> 3359.843</td> <td>    2.862</td> <td> 0.004</td> <td> 3030.264</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>      <td> 9638.8646</td> <td> 3275.149</td> <td>    2.943</td> <td> 0.003</td> <td> 3219.075</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>      <td> 4782.1084</td> <td> 3198.665</td> <td>    1.495</td> <td> 0.135</td> <td>-1487.761</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>      <td> 9305.0305</td> <td> 3110.487</td> <td>    2.992</td> <td> 0.003</td> <td> 3208.004</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>      <td> 9082.6070</td> <td> 3092.638</td> <td>    2.937</td> <td> 0.003</td> <td> 3020.566</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>      <td> 7749.3873</td> <td> 5382.286</td> <td>    1.440</td> <td> 0.150</td> <td>-2800.711</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>      <td> 1.053e+04</td> <td> 3159.208</td> <td>    3.335</td> <td> 0.001</td> <td> 4342.190</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>      <td> 8500.3738</td> <td> 3089.775</td> <td>    2.751</td> <td> 0.006</td> <td> 2443.946</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>      <td> 6466.6645</td> <td> 3134.642</td> <td>    2.063</td> <td> 0.039</td> <td>  322.291</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>      <td> 9550.9622</td> <td> 4649.788</td> <td>    2.054</td> <td> 0.040</td> <td>  436.672</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>      <td> 1.009e+04</td> <td> 3200.253</td> <td>    3.154</td> <td> 0.002</td> <td> 3820.559</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>      <td> 9834.8380</td> <td> 3160.429</td> <td>    3.112</td> <td> 0.002</td> <td> 3639.917</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>      <td>-4414.1271</td> <td> 3022.513</td> <td>   -1.460</td> <td> 0.144</td> <td>-1.03e+04</td> <td> 1510.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>      <td> 1.156e+04</td> <td> 3234.772</td> <td>    3.575</td> <td> 0.000</td> <td> 5223.749</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>      <td> -1.45e+04</td> <td> 3423.679</td> <td>   -4.235</td> <td> 0.000</td> <td>-2.12e+04</td> <td>-7789.918</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>      <td> 1.062e+04</td> <td> 3560.156</td> <td>    2.982</td> <td> 0.003</td> <td> 3636.640</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>      <td> 9790.5158</td> <td> 3152.629</td> <td>    3.106</td> <td> 0.002</td> <td> 3610.883</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>      <td> 8916.7458</td> <td> 3111.850</td> <td>    2.865</td> <td> 0.004</td> <td> 2817.047</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>      <td> 8333.4226</td> <td> 3124.559</td> <td>    2.667</td> <td> 0.008</td> <td> 2208.812</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>      <td> 6582.6117</td> <td> 3033.170</td> <td>    2.170</td> <td> 0.030</td> <td>  637.138</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>      <td> 9265.8494</td> <td> 3380.966</td> <td>    2.741</td> <td> 0.006</td> <td> 2638.642</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>      <td> 5262.2493</td> <td> 3710.128</td> <td>    1.418</td> <td> 0.156</td> <td>-2010.164</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>      <td> 7470.5391</td> <td> 3040.133</td> <td>    2.457</td> <td> 0.014</td> <td> 1511.417</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>      <td> 7574.9222</td> <td> 3037.300</td> <td>    2.494</td> <td> 0.013</td> <td> 1621.353</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>      <td> 8255.8589</td> <td> 3119.283</td> <td>    2.647</td> <td> 0.008</td> <td> 2141.590</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>      <td> 1844.4045</td> <td> 3736.226</td> <td>    0.494</td> <td> 0.622</td> <td>-5479.167</td> <td> 9167.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>      <td> 1.408e+04</td> <td> 3067.167</td> <td>    4.591</td> <td> 0.000</td> <td> 8070.340</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>      <td>  1.13e+04</td> <td> 3081.095</td> <td>    3.669</td> <td> 0.000</td> <td> 5264.878</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>      <td> 8052.7198</td> <td> 3211.390</td> <td>    2.508</td> <td> 0.012</td> <td> 1757.908</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>      <td> 1.032e+04</td> <td> 3179.999</td> <td>    3.245</td> <td> 0.001</td> <td> 4086.849</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>      <td> 1.055e+04</td> <td> 3949.975</td> <td>    2.671</td> <td> 0.008</td> <td> 2809.099</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>      <td> 8775.8315</td> <td> 3133.720</td> <td>    2.800</td> <td> 0.005</td> <td> 2633.264</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>      <td> 6420.8313</td> <td> 3243.158</td> <td>    1.980</td> <td> 0.048</td> <td>   63.750</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>      <td> 8727.2476</td> <td> 3099.294</td> <td>    2.816</td> <td> 0.005</td> <td> 2652.161</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>      <td> 9720.1875</td> <td> 3126.664</td> <td>    3.109</td> <td> 0.002</td> <td> 3591.451</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>      <td> 1.279e+04</td> <td> 3134.484</td> <td>    4.081</td> <td> 0.000</td> <td> 6646.405</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>      <td> 6160.9922</td> <td> 3087.312</td> <td>    1.996</td> <td> 0.046</td> <td>  109.392</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>      <td> 6762.3251</td> <td> 3128.386</td> <td>    2.162</td> <td> 0.031</td> <td>  630.214</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>      <td> 1.764e+04</td> <td> 3168.523</td> <td>    5.567</td> <td> 0.000</td> <td> 1.14e+04</td> <td> 2.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>      <td> 8154.6539</td> <td> 4154.025</td> <td>    1.963</td> <td> 0.050</td> <td>   12.134</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>      <td> 6766.0899</td> <td> 3377.887</td> <td>    2.003</td> <td> 0.045</td> <td>  144.918</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>      <td> 2.971e+04</td> <td> 3115.326</td> <td>    9.536</td> <td> 0.000</td> <td> 2.36e+04</td> <td> 3.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>      <td> 9150.8519</td> <td> 3085.180</td> <td>    2.966</td> <td> 0.003</td> <td> 3103.431</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>      <td> 8865.9508</td> <td> 3234.107</td> <td>    2.741</td> <td> 0.006</td> <td> 2526.610</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>      <td> 1505.3236</td> <td> 3301.821</td> <td>    0.456</td> <td> 0.648</td> <td>-4966.748</td> <td> 7977.395</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>      <td> 9732.5905</td> <td> 3991.005</td> <td>    2.439</td> <td> 0.015</td> <td> 1909.615</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>      <td> 7456.0581</td> <td> 3068.111</td> <td>    2.430</td> <td> 0.015</td> <td> 1442.096</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>      <td> 4436.4667</td> <td> 3573.654</td> <td>    1.241</td> <td> 0.214</td> <td>-2568.438</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>      <td> 3238.1303</td> <td> 3079.092</td> <td>    1.052</td> <td> 0.293</td> <td>-2797.357</td> <td> 9273.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>      <td> 9349.7327</td> <td> 3138.244</td> <td>    2.979</td> <td> 0.003</td> <td> 3198.297</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>      <td> 8457.9972</td> <td> 3066.239</td> <td>    2.758</td> <td> 0.006</td> <td> 2447.702</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>      <td> 6215.9290</td> <td> 3018.667</td> <td>    2.059</td> <td> 0.039</td> <td>  298.884</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>      <td> 6751.7467</td> <td> 3158.717</td> <td>    2.137</td> <td> 0.033</td> <td>  560.181</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>      <td> 8553.9161</td> <td> 3269.146</td> <td>    2.617</td> <td> 0.009</td> <td> 2145.893</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>      <td> 1723.3876</td> <td> 3152.520</td> <td>    0.547</td> <td> 0.585</td> <td>-4456.030</td> <td> 7902.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>      <td> 9067.0759</td> <td> 4579.905</td> <td>    1.980</td> <td> 0.048</td> <td>   89.767</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>      <td> 9819.2990</td> <td> 3057.813</td> <td>    3.211</td> <td> 0.001</td> <td> 3825.522</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>      <td> 9430.8825</td> <td> 3170.373</td> <td>    2.975</td> <td> 0.003</td> <td> 3216.470</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>      <td> 7273.1684</td> <td> 3042.817</td> <td>    2.390</td> <td> 0.017</td> <td> 1308.786</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>      <td> 8897.8883</td> <td> 3135.083</td> <td>    2.838</td> <td> 0.005</td> <td> 2752.650</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>      <td> 6931.8955</td> <td> 3136.896</td> <td>    2.210</td> <td> 0.027</td> <td>  783.102</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>      <td>-1.495e+04</td> <td> 3233.948</td> <td>   -4.624</td> <td> 0.000</td> <td>-2.13e+04</td> <td>-8613.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>      <td> 8923.7108</td> <td> 3107.157</td> <td>    2.872</td> <td> 0.004</td> <td> 2833.211</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>      <td> 7129.5037</td> <td> 3384.544</td> <td>    2.106</td> <td> 0.035</td> <td>  495.283</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>      <td> 9304.1976</td> <td> 3123.447</td> <td>    2.979</td> <td> 0.003</td> <td> 3181.767</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>      <td> 9393.7191</td> <td> 3155.786</td> <td>    2.977</td> <td> 0.003</td> <td> 3207.900</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>      <td> 6253.3200</td> <td> 3022.501</td> <td>    2.069</td> <td> 0.039</td> <td>  328.759</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>      <td> 1.026e+04</td> <td> 3321.592</td> <td>    3.090</td> <td> 0.002</td> <td> 3753.317</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>      <td> 8767.4758</td> <td> 3234.267</td> <td>    2.711</td> <td> 0.007</td> <td> 2427.821</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>      <td> 5012.1399</td> <td> 3004.385</td> <td>    1.668</td> <td> 0.095</td> <td> -876.911</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>      <td> 9366.4810</td> <td> 3091.150</td> <td>    3.030</td> <td> 0.002</td> <td> 3307.357</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>      <td>-2443.1777</td> <td> 3001.938</td> <td>   -0.814</td> <td> 0.416</td> <td>-8327.432</td> <td> 3441.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>      <td> 8790.9387</td> <td> 3128.430</td> <td>    2.810</td> <td> 0.005</td> <td> 2658.740</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>      <td> 5620.5173</td> <td> 3072.543</td> <td>    1.829</td> <td> 0.067</td> <td> -402.134</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>      <td> 9969.7493</td> <td> 3156.641</td> <td>    3.158</td> <td> 0.002</td> <td> 3782.253</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>      <td> 1753.2079</td> <td> 3096.978</td> <td>    0.566</td> <td> 0.571</td> <td>-4317.340</td> <td> 7823.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>      <td> 6648.1472</td> <td> 3032.795</td> <td>    2.192</td> <td> 0.028</td> <td>  703.409</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>      <td> 5757.8853</td> <td> 3042.665</td> <td>    1.892</td> <td> 0.058</td> <td> -206.199</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>      <td>  1.24e+04</td> <td> 3192.708</td> <td>    3.884</td> <td> 0.000</td> <td> 6143.287</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>      <td> 9521.8682</td> <td> 3078.497</td> <td>    3.093</td> <td> 0.002</td> <td> 3487.546</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>      <td> 8424.9184</td> <td> 3204.025</td> <td>    2.629</td> <td> 0.009</td> <td> 2144.542</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>      <td> 6955.2953</td> <td> 3049.615</td> <td>    2.281</td> <td> 0.023</td> <td>  977.586</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>      <td> 7442.0282</td> <td> 3074.413</td> <td>    2.421</td> <td> 0.016</td> <td> 1415.711</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>      <td>-1.012e+04</td> <td> 2991.410</td> <td>   -3.382</td> <td> 0.001</td> <td> -1.6e+04</td> <td>-4253.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>      <td> 8157.1933</td> <td> 3199.316</td> <td>    2.550</td> <td> 0.011</td> <td> 1886.047</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>      <td> 1.069e+04</td> <td> 3172.643</td> <td>    3.370</td> <td> 0.001</td> <td> 4474.298</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>      <td> 2100.8383</td> <td> 3150.625</td> <td>    0.667</td> <td> 0.505</td> <td>-4074.865</td> <td> 8276.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>      <td> 8930.9265</td> <td> 3638.445</td> <td>    2.455</td> <td> 0.014</td> <td> 1799.022</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>      <td> 7656.2181</td> <td> 3156.951</td> <td>    2.425</td> <td> 0.015</td> <td> 1468.115</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>      <td>-6316.9730</td> <td> 3342.935</td> <td>   -1.890</td> <td> 0.059</td> <td>-1.29e+04</td> <td>  235.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>      <td>-1486.8453</td> <td> 3165.038</td> <td>   -0.470</td> <td> 0.639</td> <td>-7690.801</td> <td> 4717.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>      <td> 9092.3282</td> <td> 3091.617</td> <td>    2.941</td> <td> 0.003</td> <td> 3032.289</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>      <td> 9888.7824</td> <td> 3156.253</td> <td>    3.133</td> <td> 0.002</td> <td> 3702.048</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>      <td> 9592.9423</td> <td> 3114.281</td> <td>    3.080</td> <td> 0.002</td> <td> 3488.479</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>      <td> 5332.5335</td> <td> 3050.740</td> <td>    1.748</td> <td> 0.080</td> <td> -647.380</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>      <td> 1.002e+04</td> <td> 3143.649</td> <td>    3.186</td> <td> 0.001</td> <td> 3854.817</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>      <td> 7135.9390</td> <td> 3109.533</td> <td>    2.295</td> <td> 0.022</td> <td> 1040.782</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>      <td> 5907.8263</td> <td> 3555.339</td> <td>    1.662</td> <td> 0.097</td> <td>-1061.178</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>      <td> 7093.7298</td> <td> 3180.484</td> <td>    2.230</td> <td> 0.026</td> <td>  859.499</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>      <td> 5649.2334</td> <td> 3049.389</td> <td>    1.853</td> <td> 0.064</td> <td> -328.032</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>      <td> 3589.9167</td> <td> 3151.067</td> <td>    1.139</td> <td> 0.255</td> <td>-2586.652</td> <td> 9766.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>      <td> 1.015e+04</td> <td> 3202.762</td> <td>    3.170</td> <td> 0.002</td> <td> 3875.511</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>      <td> 5130.0212</td> <td> 3044.372</td> <td>    1.685</td> <td> 0.092</td> <td> -837.409</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>      <td>-2202.2944</td> <td> 3019.455</td> <td>   -0.729</td> <td> 0.466</td> <td>-8120.885</td> <td> 3716.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>      <td> 6667.9159</td> <td> 3236.736</td> <td>    2.060</td> <td> 0.039</td> <td>  323.422</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>      <td> 7346.0103</td> <td> 3043.721</td> <td>    2.413</td> <td> 0.016</td> <td> 1379.855</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>      <td> 9985.5502</td> <td> 3093.631</td> <td>    3.228</td> <td> 0.001</td> <td> 3921.565</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>      <td> 9750.2882</td> <td> 3258.525</td> <td>    2.992</td> <td> 0.003</td> <td> 3363.085</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>      <td> 9196.9702</td> <td> 3156.426</td> <td>    2.914</td> <td> 0.004</td> <td> 3009.895</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>      <td> 8782.9091</td> <td> 3076.771</td> <td>    2.855</td> <td> 0.004</td> <td> 2751.971</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>      <td> 6761.4473</td> <td> 3044.175</td> <td>    2.221</td> <td> 0.026</td> <td>  794.402</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>      <td>-3983.6259</td> <td> 3374.667</td> <td>   -1.180</td> <td> 0.238</td> <td>-1.06e+04</td> <td> 2631.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>      <td> 1.098e+04</td> <td> 3537.306</td> <td>    3.105</td> <td> 0.002</td> <td> 4051.291</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>      <td> 8838.8993</td> <td> 3110.041</td> <td>    2.842</td> <td> 0.004</td> <td> 2742.747</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>      <td> 7883.3944</td> <td> 3750.123</td> <td>    2.102</td> <td> 0.036</td> <td>  532.584</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>      <td> 2.055e+04</td> <td> 3130.331</td> <td>    6.565</td> <td> 0.000</td> <td> 1.44e+04</td> <td> 2.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>      <td> 5321.9245</td> <td> 3023.926</td> <td>    1.760</td> <td> 0.078</td> <td> -605.430</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>      <td>  2.91e+04</td> <td> 3348.461</td> <td>    8.689</td> <td> 0.000</td> <td> 2.25e+04</td> <td> 3.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>      <td>  288.7530</td> <td> 3183.883</td> <td>    0.091</td> <td> 0.928</td> <td>-5952.140</td> <td> 6529.646</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>      <td> 9857.0564</td> <td> 3160.749</td> <td>    3.119</td> <td> 0.002</td> <td> 3661.508</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>      <td> 6877.9877</td> <td> 3301.144</td> <td>    2.084</td> <td> 0.037</td> <td>  407.244</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>      <td> 2363.2893</td> <td> 3354.437</td> <td>    0.705</td> <td> 0.481</td> <td>-4211.916</td> <td> 8938.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>      <td> 9996.9128</td> <td> 3074.136</td> <td>    3.252</td> <td> 0.001</td> <td> 3971.140</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>      <td> 1.008e+04</td> <td> 3199.827</td> <td>    3.150</td> <td> 0.002</td> <td> 3805.949</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>      <td> 8703.5470</td> <td> 3078.394</td> <td>    2.827</td> <td> 0.005</td> <td> 2669.428</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>      <td> 4027.2377</td> <td> 3062.602</td> <td>    1.315</td> <td> 0.189</td> <td>-1975.927</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>      <td> 9234.4663</td> <td> 4423.761</td> <td>    2.087</td> <td> 0.037</td> <td>  563.222</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>      <td> 5022.8681</td> <td> 2990.914</td> <td>    1.679</td> <td> 0.093</td> <td> -839.778</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>      <td> 6865.4753</td> <td> 3330.626</td> <td>    2.061</td> <td> 0.039</td> <td>  336.943</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>      <td> 6724.1365</td> <td> 3040.648</td> <td>    2.211</td> <td> 0.027</td> <td>  764.005</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>      <td> 9368.7656</td> <td> 3104.748</td> <td>    3.018</td> <td> 0.003</td> <td> 3282.988</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>      <td> 1.007e+04</td> <td> 3139.976</td> <td>    3.206</td> <td> 0.001</td> <td> 3910.883</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>      <td> 6032.6233</td> <td> 3129.293</td> <td>    1.928</td> <td> 0.054</td> <td> -101.267</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>      <td> 1.005e+04</td> <td> 3163.140</td> <td>    3.177</td> <td> 0.001</td> <td> 3847.994</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>      <td>  1.08e+04</td> <td> 3255.498</td> <td>    3.319</td> <td> 0.001</td> <td> 4422.683</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>      <td> 9180.3611</td> <td> 3099.787</td> <td>    2.962</td> <td> 0.003</td> <td> 3104.308</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>      <td> 9396.6270</td> <td> 3137.971</td> <td>    2.994</td> <td> 0.003</td> <td> 3245.728</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>      <td> 9958.6402</td> <td> 3172.396</td> <td>    3.139</td> <td> 0.002</td> <td> 3740.263</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>      <td> 2878.0832</td> <td> 3278.995</td> <td>    0.878</td> <td> 0.380</td> <td>-3549.244</td> <td> 9305.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>      <td> 7955.4694</td> <td> 3090.496</td> <td>    2.574</td> <td> 0.010</td> <td> 1897.629</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>      <td> 7012.8960</td> <td> 3062.779</td> <td>    2.290</td> <td> 0.022</td> <td> 1009.384</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>      <td>-1.071e+04</td> <td> 3041.870</td> <td>   -3.522</td> <td> 0.000</td> <td>-1.67e+04</td> <td>-4751.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>      <td>  237.8112</td> <td> 3143.690</td> <td>    0.076</td> <td> 0.940</td> <td>-5924.298</td> <td> 6399.920</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>      <td> 1.047e+04</td> <td> 3372.333</td> <td>    3.105</td> <td> 0.002</td> <td> 3861.201</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>      <td> 7304.7780</td> <td> 3039.777</td> <td>    2.403</td> <td> 0.016</td> <td> 1346.353</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>      <td> 6403.1328</td> <td> 3012.256</td> <td>    2.126</td> <td> 0.034</td> <td>  498.654</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>      <td> 1965.6327</td> <td> 3401.698</td> <td>    0.578</td> <td> 0.563</td> <td>-4702.212</td> <td> 8633.478</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>      <td> 6269.8156</td> <td> 3082.415</td> <td>    2.034</td> <td> 0.042</td> <td>  227.814</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>      <td> 5595.9253</td> <td> 3023.816</td> <td>    1.851</td> <td> 0.064</td> <td> -331.214</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>      <td> 3815.5590</td> <td> 3108.453</td> <td>    1.227</td> <td> 0.220</td> <td>-2277.481</td> <td> 9908.599</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>      <td> 9293.0465</td> <td> 4448.428</td> <td>    2.089</td> <td> 0.037</td> <td>  573.452</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>      <td> 8593.1758</td> <td> 3436.473</td> <td>    2.501</td> <td> 0.012</td> <td> 1857.168</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>      <td> 9960.1426</td> <td> 3212.901</td> <td>    3.100</td> <td> 0.002</td> <td> 3662.368</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>      <td> 8369.7659</td> <td> 3096.656</td> <td>    2.703</td> <td> 0.007</td> <td> 2299.850</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>      <td> 5236.9855</td> <td> 4093.728</td> <td>    1.279</td> <td> 0.201</td> <td>-2787.343</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>      <td> 3177.1857</td> <td> 2977.132</td> <td>    1.067</td> <td> 0.286</td> <td>-2658.445</td> <td> 9012.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>      <td>  345.3862</td> <td> 2964.765</td> <td>    0.116</td> <td> 0.907</td> <td>-5466.004</td> <td> 6156.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>      <td> 9669.4538</td> <td> 3102.778</td> <td>    3.116</td> <td> 0.002</td> <td> 3587.537</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>      <td> 1.017e+04</td> <td> 3631.753</td> <td>    2.800</td> <td> 0.005</td> <td> 3049.714</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>      <td> 1.082e+04</td> <td> 3195.981</td> <td>    3.387</td> <td> 0.001</td> <td> 4559.228</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>      <td> 9826.5015</td> <td> 3129.155</td> <td>    3.140</td> <td> 0.002</td> <td> 3692.884</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>      <td> 4757.1723</td> <td> 3068.861</td> <td>    1.550</td> <td> 0.121</td> <td>-1258.262</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>      <td> 5449.7947</td> <td> 3213.596</td> <td>    1.696</td> <td> 0.090</td> <td> -849.341</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>      <td> 6091.4889</td> <td> 4097.403</td> <td>    1.487</td> <td> 0.137</td> <td>-1940.043</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>      <td> 4314.0795</td> <td> 3025.653</td> <td>    1.426</td> <td> 0.154</td> <td>-1616.660</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>      <td> 8571.9786</td> <td> 3107.010</td> <td>    2.759</td> <td> 0.006</td> <td> 2481.767</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>      <td> 7832.2604</td> <td> 3050.287</td> <td>    2.568</td> <td> 0.010</td> <td> 1853.235</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>      <td>-1.284e+04</td> <td> 4266.321</td> <td>   -3.009</td> <td> 0.003</td> <td>-2.12e+04</td> <td>-4476.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>      <td> 1.011e+04</td> <td> 3506.811</td> <td>    2.883</td> <td> 0.004</td> <td> 3237.019</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>      <td> 6355.8057</td> <td> 3158.698</td> <td>    2.012</td> <td> 0.044</td> <td>  164.278</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>      <td> 4594.2014</td> <td> 3082.822</td> <td>    1.490</td> <td> 0.136</td> <td>-1448.598</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>      <td> 1.241e+04</td> <td> 3064.660</td> <td>    4.049</td> <td> 0.000</td> <td> 6402.210</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>      <td> 4269.6200</td> <td> 2986.010</td> <td>    1.430</td> <td> 0.153</td> <td>-1583.412</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>      <td> -216.2534</td> <td> 3044.454</td> <td>   -0.071</td> <td> 0.943</td> <td>-6183.846</td> <td> 5751.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>      <td>-1.792e+04</td> <td> 4211.763</td> <td>   -4.256</td> <td> 0.000</td> <td>-2.62e+04</td> <td>-9668.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>      <td> 7000.0639</td> <td> 3121.546</td> <td>    2.242</td> <td> 0.025</td> <td>  881.359</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>      <td> 4487.5163</td> <td> 3762.106</td> <td>    1.193</td> <td> 0.233</td> <td>-2886.783</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>      <td> 3721.4346</td> <td> 3033.177</td> <td>    1.227</td> <td> 0.220</td> <td>-2224.052</td> <td> 9666.921</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>      <td> 3023.8387</td> <td> 4071.341</td> <td>    0.743</td> <td> 0.458</td> <td>-4956.608</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>      <td> 1.003e+04</td> <td> 3221.664</td> <td>    3.113</td> <td> 0.002</td> <td> 3715.041</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>      <td> 6673.9738</td> <td> 3111.149</td> <td>    2.145</td> <td> 0.032</td> <td>  575.650</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>      <td> 5948.1118</td> <td> 3049.133</td> <td>    1.951</td> <td> 0.051</td> <td>  -28.652</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>      <td>-5201.1049</td> <td> 5261.293</td> <td>   -0.989</td> <td> 0.323</td> <td>-1.55e+04</td> <td> 5111.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>      <td> 5978.4574</td> <td> 3032.475</td> <td>    1.971</td> <td> 0.049</td> <td>   34.345</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>      <td> 8360.8550</td> <td> 3113.996</td> <td>    2.685</td> <td> 0.007</td> <td> 2256.949</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>      <td> 9695.4952</td> <td> 3081.120</td> <td>    3.147</td> <td> 0.002</td> <td> 3656.031</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>      <td> 1673.7655</td> <td> 2960.502</td> <td>    0.565</td> <td> 0.572</td> <td>-4129.267</td> <td> 7476.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>      <td> 1462.5312</td> <td> 3123.254</td> <td>    0.468</td> <td> 0.640</td> <td>-4659.520</td> <td> 7584.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>      <td> 9161.1397</td> <td> 3123.661</td> <td>    2.933</td> <td> 0.003</td> <td> 3038.290</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>      <td> 2489.7585</td> <td> 2963.761</td> <td>    0.840</td> <td> 0.401</td> <td>-3319.664</td> <td> 8299.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>      <td> 9912.5502</td> <td> 3147.378</td> <td>    3.149</td> <td> 0.002</td> <td> 3743.211</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>      <td> 7202.1314</td> <td> 3024.139</td> <td>    2.382</td> <td> 0.017</td> <td> 1274.359</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>      <td> 1707.7949</td> <td> 3109.095</td> <td>    0.549</td> <td> 0.583</td> <td>-4386.503</td> <td> 7802.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>      <td> 9088.3459</td> <td> 3223.230</td> <td>    2.820</td> <td> 0.005</td> <td> 2770.327</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>      <td>-3.452e+04</td> <td> 3202.790</td> <td>  -10.779</td> <td> 0.000</td> <td>-4.08e+04</td> <td>-2.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>      <td> 1.056e+04</td> <td> 3244.249</td> <td>    3.254</td> <td> 0.001</td> <td> 4198.886</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>      <td> 2994.0482</td> <td> 3000.566</td> <td>    0.998</td> <td> 0.318</td> <td>-2887.517</td> <td> 8875.613</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>      <td> 5697.1012</td> <td> 3683.632</td> <td>    1.547</td> <td> 0.122</td> <td>-1523.378</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>      <td> 7227.4542</td> <td> 3090.223</td> <td>    2.339</td> <td> 0.019</td> <td> 1170.149</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>      <td> 1.018e+04</td> <td> 3189.909</td> <td>    3.192</td> <td> 0.001</td> <td> 3928.483</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21744.289</td> <th>  Durbin-Watson:     </th>   <td>   0.548</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>50237524.155</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>10.223</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>302.434</td>  <th>  Cond. No.          </th>   <td>2.00e+07</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large,  2e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.665    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.645    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      33.22    \\\\\n",
       "\\textbf{Date:}             & Wed, 16 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     01:58:42     & \\textbf{  Log-Likelihood:    } & -1.4157e+05   \\\\\n",
       "\\textbf{No. Observations:} &       13385      & \\textbf{  AIC:               } &  2.847e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       12629      & \\textbf{  BIC:               } &  2.903e+05    \\\\\n",
       "\\textbf{Df Model:}         &         755      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                     & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}       &   -9940.5765  &     2385.347     &    -4.167  &         0.000        &    -1.46e+04    &    -5264.935     \\\\\n",
       "\\textbf{gspilltecIV} &       0.1002  &        0.027     &     3.746  &         0.000        &        0.048    &        0.153     \\\\\n",
       "\\textbf{gspillsicIV} &       0.3399  &        0.049     &     6.902  &         0.000        &        0.243    &        0.436     \\\\\n",
       "\\textbf{pat\\_count}  &     -30.6018  &        1.838     &   -16.652  &         0.000        &      -34.204    &      -26.999     \\\\\n",
       "\\textbf{rsales}      &       0.7812  &        0.037     &    21.055  &         0.000        &        0.708    &        0.854     \\\\\n",
       "\\textbf{rppent}      &       0.6108  &        0.084     &     7.234  &         0.000        &        0.445    &        0.776     \\\\\n",
       "\\textbf{emp}         &      18.0641  &        7.147     &     2.527  &         0.012        &        4.054    &       32.074     \\\\\n",
       "\\textbf{rxrd}        &      18.5941  &        0.614     &    30.295  &         0.000        &       17.391    &       19.797     \\\\\n",
       "\\textbf{1981}        &    -425.5119  &      618.180     &    -0.688  &         0.491        &    -1637.239    &      786.215     \\\\\n",
       "\\textbf{1982}        &    -295.2372  &      616.239     &    -0.479  &         0.632        &    -1503.159    &      912.685     \\\\\n",
       "\\textbf{1983}        &    -294.1870  &      609.557     &    -0.483  &         0.629        &    -1489.011    &      900.637     \\\\\n",
       "\\textbf{1984}        &    -812.2257  &      607.964     &    -1.336  &         0.182        &    -2003.927    &      379.476     \\\\\n",
       "\\textbf{1985}        &    -929.1950  &      609.921     &    -1.523  &         0.128        &    -2124.732    &      266.343     \\\\\n",
       "\\textbf{1986}        &   -1184.5891  &      607.667     &    -1.949  &         0.051        &    -2375.708    &        6.530     \\\\\n",
       "\\textbf{1987}        &   -1372.3041  &      607.937     &    -2.257  &         0.024        &    -2563.953    &     -180.655     \\\\\n",
       "\\textbf{1988}        &   -1686.4878  &      609.953     &    -2.765  &         0.006        &    -2882.088    &     -490.888     \\\\\n",
       "\\textbf{1989}        &   -1550.4891  &      610.972     &    -2.538  &         0.011        &    -2748.088    &     -352.891     \\\\\n",
       "\\textbf{1990}        &   -2052.0735  &      612.118     &    -3.352  &         0.001        &    -3251.918    &     -852.228     \\\\\n",
       "\\textbf{1991}        &   -1629.8854  &      614.732     &    -2.651  &         0.008        &    -2834.854    &     -424.917     \\\\\n",
       "\\textbf{1992}        &   -1754.6866  &      617.583     &    -2.841  &         0.005        &    -2965.243    &     -544.130     \\\\\n",
       "\\textbf{1993}        &   -1685.0176  &      621.230     &    -2.712  &         0.007        &    -2902.723    &     -467.312     \\\\\n",
       "\\textbf{1994}        &   -1916.7490  &      627.407     &    -3.055  &         0.002        &    -3146.563    &     -686.935     \\\\\n",
       "\\textbf{1995}        &   -1380.6021  &      636.986     &    -2.167  &         0.030        &    -2629.191    &     -132.013     \\\\\n",
       "\\textbf{1996}        &   -1098.4502  &      651.099     &    -1.687  &         0.092        &    -2374.702    &      177.802     \\\\\n",
       "\\textbf{1997}        &    -692.3174  &      667.994     &    -1.036  &         0.300        &    -2001.686    &      617.051     \\\\\n",
       "\\textbf{1998}        &    -575.0406  &      687.698     &    -0.836  &         0.403        &    -1923.033    &      772.951     \\\\\n",
       "\\textbf{1999}        &     142.3167  &      710.226     &     0.200  &         0.841        &    -1249.835    &     1534.468     \\\\\n",
       "\\textbf{2000}        &    -207.4580  &      738.322     &    -0.281  &         0.779        &    -1654.682    &     1239.766     \\\\\n",
       "\\textbf{2001}        &   -2283.3834  &      768.945     &    -2.970  &         0.003        &    -3790.633    &     -776.134     \\\\\n",
       "\\textbf{10005.0}     &    8596.1312  &     3076.471     &     2.794  &         0.005        &     2565.781    &     1.46e+04     \\\\\n",
       "\\textbf{10006.0}     &    8240.3993  &     3516.122     &     2.344  &         0.019        &     1348.266    &     1.51e+04     \\\\\n",
       "\\textbf{10008.0}     &    7558.3544  &     3040.531     &     2.486  &         0.013        &     1598.452    &     1.35e+04     \\\\\n",
       "\\textbf{10016.0}     &    8488.3062  &     3060.494     &     2.774  &         0.006        &     2489.273    &     1.45e+04     \\\\\n",
       "\\textbf{10030.0}     &    9963.3568  &     3135.789     &     3.177  &         0.001        &     3816.734    &     1.61e+04     \\\\\n",
       "\\textbf{1004.0}      &    9403.1634  &     3133.966     &     3.000  &         0.003        &     3260.115    &     1.55e+04     \\\\\n",
       "\\textbf{10056.0}     &    7589.4824  &     3061.708     &     2.479  &         0.013        &     1588.069    &     1.36e+04     \\\\\n",
       "\\textbf{10085.0}     &    4090.3185  &     2997.861     &     1.364  &         0.172        &    -1785.945    &     9966.582     \\\\\n",
       "\\textbf{10092.0}     &    9204.1913  &     5393.483     &     1.707  &         0.088        &    -1367.854    &     1.98e+04     \\\\\n",
       "\\textbf{10097.0}     &    2206.4047  &     2993.452     &     0.737  &         0.461        &    -3661.215    &     8074.024     \\\\\n",
       "\\textbf{1010.0}      &    8207.1099  &     5379.135     &     1.526  &         0.127        &    -2336.812    &     1.88e+04     \\\\\n",
       "\\textbf{10109.0}     &    1.097e+04  &     3193.217     &     3.436  &         0.001        &     4711.604    &     1.72e+04     \\\\\n",
       "\\textbf{10115.0}     &    7957.3563  &     3058.537     &     2.602  &         0.009        &     1962.159    &      1.4e+04     \\\\\n",
       "\\textbf{10124.0}     &     1.11e+04  &     3199.490     &     3.470  &         0.001        &     4829.168    &     1.74e+04     \\\\\n",
       "\\textbf{1013.0}      &    4769.1715  &     2983.003     &     1.599  &         0.110        &    -1077.968    &     1.06e+04     \\\\\n",
       "\\textbf{10150.0}     &    3170.2099  &     3465.956     &     0.915  &         0.360        &    -3623.590    &     9964.010     \\\\\n",
       "\\textbf{10159.0}     &    3378.9006  &     4384.246     &     0.771  &         0.441        &    -5214.887    &      1.2e+04     \\\\\n",
       "\\textbf{10174.0}     &    1.021e+04  &     3389.760     &     3.012  &         0.003        &     3565.375    &     1.69e+04     \\\\\n",
       "\\textbf{10185.0}     &    8313.1068  &     3338.064     &     2.490  &         0.013        &     1769.995    &     1.49e+04     \\\\\n",
       "\\textbf{10195.0}     &    2593.7222  &     3141.189     &     0.826  &         0.409        &    -3563.486    &     8750.931     \\\\\n",
       "\\textbf{10198.0}     &    9933.5369  &     3131.022     &     3.173  &         0.002        &     3796.259    &     1.61e+04     \\\\\n",
       "\\textbf{10215.0}     &     1.08e+04  &     3193.341     &     3.381  &         0.001        &     4537.838    &     1.71e+04     \\\\\n",
       "\\textbf{10232.0}     &    6925.4903  &     3326.104     &     2.082  &         0.037        &      405.821    &     1.34e+04     \\\\\n",
       "\\textbf{10236.0}     &    9750.4628  &     3137.458     &     3.108  &         0.002        &     3600.569    &     1.59e+04     \\\\\n",
       "\\textbf{10286.0}     &    8274.2845  &     3121.219     &     2.651  &         0.008        &     2156.221    &     1.44e+04     \\\\\n",
       "\\textbf{10301.0}     &   -1.476e+04  &     3014.308     &    -4.895  &         0.000        &    -2.07e+04    &    -8846.626     \\\\\n",
       "\\textbf{10312.0}     &    9198.1294  &     3124.523     &     2.944  &         0.003        &     3073.591    &     1.53e+04     \\\\\n",
       "\\textbf{10332.0}     &    3868.1997  &     4061.176     &     0.952  &         0.341        &    -4092.322    &     1.18e+04     \\\\\n",
       "\\textbf{1036.0}      &    7245.0618  &     3261.623     &     2.221  &         0.026        &      851.786    &     1.36e+04     \\\\\n",
       "\\textbf{10374.0}     &    8066.1980  &     3065.104     &     2.632  &         0.009        &     2058.129    &     1.41e+04     \\\\\n",
       "\\textbf{10386.0}     &    4140.6626  &     2986.789     &     1.386  &         0.166        &    -1713.897    &     9995.222     \\\\\n",
       "\\textbf{10391.0}     &    1064.5542  &     3026.844     &     0.352  &         0.725        &    -4868.521    &     6997.629     \\\\\n",
       "\\textbf{10407.0}     &    4895.6611  &     3017.572     &     1.622  &         0.105        &    -1019.237    &     1.08e+04     \\\\\n",
       "\\textbf{10420.0}     &    8765.7651  &     3082.897     &     2.843  &         0.004        &     2722.820    &     1.48e+04     \\\\\n",
       "\\textbf{10422.0}     &    6959.5604  &     3252.126     &     2.140  &         0.032        &      584.899    &     1.33e+04     \\\\\n",
       "\\textbf{10426.0}     &    9660.2255  &     3299.137     &     2.928  &         0.003        &     3193.415    &     1.61e+04     \\\\\n",
       "\\textbf{10441.0}     &    1.026e+04  &     3164.254     &     3.241  &         0.001        &     4053.796    &     1.65e+04     \\\\\n",
       "\\textbf{1045.0}      &   -1654.6262  &     3243.965     &    -0.510  &         0.610        &    -8013.291    &     4704.038     \\\\\n",
       "\\textbf{10453.0}     &    4005.2538  &     2991.151     &     1.339  &         0.181        &    -1857.856    &     9868.364     \\\\\n",
       "\\textbf{10482.0}     &    -1.46e+04  &     3475.563     &    -4.201  &         0.000        &    -2.14e+04    &    -7789.183     \\\\\n",
       "\\textbf{10498.0}     &    9301.1887  &     3158.846     &     2.944  &         0.003        &     3109.370    &     1.55e+04     \\\\\n",
       "\\textbf{10499.0}     &     290.8155  &     3069.162     &     0.095  &         0.925        &    -5725.207    &     6306.838     \\\\\n",
       "\\textbf{10511.0}     &    1.071e+04  &     3287.724     &     3.258  &         0.001        &     4268.117    &     1.72e+04     \\\\\n",
       "\\textbf{10519.0}     &   -5675.1567  &     2981.921     &    -1.903  &         0.057        &    -1.15e+04    &      169.862     \\\\\n",
       "\\textbf{10530.0}     &    5301.0836  &     3009.281     &     1.762  &         0.078        &     -597.565    &     1.12e+04     \\\\\n",
       "\\textbf{10537.0}     &    6186.0975  &     3392.545     &     1.823  &         0.068        &     -463.806    &     1.28e+04     \\\\\n",
       "\\textbf{10540.0}     &    7576.5476  &     3032.964     &     2.498  &         0.012        &     1631.477    &     1.35e+04     \\\\\n",
       "\\textbf{10541.0}     &    8964.6580  &     3229.126     &     2.776  &         0.006        &     2635.080    &     1.53e+04     \\\\\n",
       "\\textbf{10550.0}     &    6593.4131  &     6048.079     &     1.090  &         0.276        &    -5261.741    &     1.84e+04     \\\\\n",
       "\\textbf{10553.0}     &    3851.8113  &     3153.072     &     1.222  &         0.222        &    -2328.689    &        1e+04     \\\\\n",
       "\\textbf{10565.0}     &    1.053e+04  &     3138.295     &     3.355  &         0.001        &     4378.941    &     1.67e+04     \\\\\n",
       "\\textbf{10580.0}     &     1.08e+04  &     3213.498     &     3.360  &         0.001        &     4498.657    &     1.71e+04     \\\\\n",
       "\\textbf{10581.0}     &    7665.2631  &     3106.263     &     2.468  &         0.014        &     1576.515    &     1.38e+04     \\\\\n",
       "\\textbf{10588.0}     &    1176.5368  &     2963.147     &     0.397  &         0.691        &    -4631.681    &     6984.754     \\\\\n",
       "\\textbf{10597.0}     &    8900.5001  &     3128.719     &     2.845  &         0.004        &     2767.736    &      1.5e+04     \\\\\n",
       "\\textbf{10599.0}     &    9271.0496  &     3139.906     &     2.953  &         0.003        &     3116.357    &     1.54e+04     \\\\\n",
       "\\textbf{10618.0}     &    8556.4419  &     3093.780     &     2.766  &         0.006        &     2492.162    &     1.46e+04     \\\\\n",
       "\\textbf{10656.0}     &    8664.1385  &     3068.114     &     2.824  &         0.005        &     2650.168    &     1.47e+04     \\\\\n",
       "\\textbf{10658.0}     &    8540.1624  &     3064.938     &     2.786  &         0.005        &     2532.418    &     1.45e+04     \\\\\n",
       "\\textbf{10726.0}     &    1.264e+04  &     3268.775     &     3.866  &         0.000        &     6231.058    &      1.9e+04     \\\\\n",
       "\\textbf{10734.0}     &    8389.8503  &     3727.109     &     2.251  &         0.024        &     1084.150    &     1.57e+04     \\\\\n",
       "\\textbf{10735.0}     &    1.002e+04  &     3173.342     &     3.159  &         0.002        &     3803.931    &     1.62e+04     \\\\\n",
       "\\textbf{10764.0}     &    1.051e+04  &     3276.657     &     3.207  &         0.001        &     4084.421    &     1.69e+04     \\\\\n",
       "\\textbf{10777.0}     &    8530.3531  &     3067.334     &     2.781  &         0.005        &     2517.913    &     1.45e+04     \\\\\n",
       "\\textbf{1078.0}      &    5453.6061  &     3082.313     &     1.769  &         0.077        &     -588.196    &     1.15e+04     \\\\\n",
       "\\textbf{10793.0}     &    8093.7684  &     3137.200     &     2.580  &         0.010        &     1944.380    &     1.42e+04     \\\\\n",
       "\\textbf{10816.0}     &    7496.2748  &     3087.546     &     2.428  &         0.015        &     1444.216    &     1.35e+04     \\\\\n",
       "\\textbf{10839.0}     &    9481.3260  &     3097.020     &     3.061  &         0.002        &     3410.697    &     1.56e+04     \\\\\n",
       "\\textbf{10857.0}     &   -2182.7305  &     3058.140     &    -0.714  &         0.475        &    -8177.148    &     3811.687     \\\\\n",
       "\\textbf{10867.0}     &    4643.9565  &     3339.296     &     1.391  &         0.164        &    -1901.571    &     1.12e+04     \\\\\n",
       "\\textbf{10906.0}     &    8932.4951  &     3098.783     &     2.883  &         0.004        &     2858.410    &      1.5e+04     \\\\\n",
       "\\textbf{10950.0}     &    9034.0200  &     4140.834     &     2.182  &         0.029        &      917.357    &     1.72e+04     \\\\\n",
       "\\textbf{10983.0}     &   -2.383e+04  &     3227.575     &    -7.383  &         0.000        &    -3.02e+04    &    -1.75e+04     \\\\\n",
       "\\textbf{1099.0}      &    8536.2893  &     3106.895     &     2.748  &         0.006        &     2446.304    &     1.46e+04     \\\\\n",
       "\\textbf{10991.0}     &    6926.2901  &     3614.928     &     1.916  &         0.055        &     -159.519    &      1.4e+04     \\\\\n",
       "\\textbf{11012.0}     &    7880.6911  &     3137.824     &     2.512  &         0.012        &     1730.080    &      1.4e+04     \\\\\n",
       "\\textbf{11038.0}     &    3251.3206  &     3219.125     &     1.010  &         0.313        &    -3058.654    &     9561.295     \\\\\n",
       "\\textbf{1104.0}      &    9488.5996  &     3138.344     &     3.023  &         0.003        &     3336.969    &     1.56e+04     \\\\\n",
       "\\textbf{11060.0}     &    9235.2591  &     3132.146     &     2.949  &         0.003        &     3095.777    &     1.54e+04     \\\\\n",
       "\\textbf{11094.0}     &    8684.5592  &     3089.265     &     2.811  &         0.005        &     2629.130    &     1.47e+04     \\\\\n",
       "\\textbf{11096.0}     &    6740.1208  &     3028.318     &     2.226  &         0.026        &      804.158    &     1.27e+04     \\\\\n",
       "\\textbf{11113.0}     &    9316.9328  &     3367.569     &     2.767  &         0.006        &     2715.986    &     1.59e+04     \\\\\n",
       "\\textbf{1115.0}      &    7875.2019  &     3125.463     &     2.520  &         0.012        &     1748.819    &      1.4e+04     \\\\\n",
       "\\textbf{11161.0}     &    6631.5622  &     3048.603     &     2.175  &         0.030        &      655.838    &     1.26e+04     \\\\\n",
       "\\textbf{11225.0}     &    1.031e+04  &     3223.600     &     3.198  &         0.001        &     3991.623    &     1.66e+04     \\\\\n",
       "\\textbf{11228.0}     &    1.001e+04  &     3111.122     &     3.217  &         0.001        &     3909.102    &     1.61e+04     \\\\\n",
       "\\textbf{11236.0}     &    4820.4345  &     4564.449     &     1.056  &         0.291        &    -4126.579    &     1.38e+04     \\\\\n",
       "\\textbf{11288.0}     &     726.5183  &     3139.333     &     0.231  &         0.817        &    -5427.050    &     6880.087     \\\\\n",
       "\\textbf{11312.0}     &     624.6433  &     3085.276     &     0.202  &         0.840        &    -5422.965    &     6672.252     \\\\\n",
       "\\textbf{11361.0}     &    8139.1393  &     3054.443     &     2.665  &         0.008        &     2151.966    &     1.41e+04     \\\\\n",
       "\\textbf{11399.0}     &    1883.4788  &     2984.861     &     0.631  &         0.528        &    -3967.302    &     7734.260     \\\\\n",
       "\\textbf{114303.0}    &   -8981.4766  &     5402.255     &    -1.663  &         0.096        &    -1.96e+04    &     1607.764     \\\\\n",
       "\\textbf{11456.0}     &    3026.3818  &     3078.672     &     0.983  &         0.326        &    -3008.282    &     9061.046     \\\\\n",
       "\\textbf{11465.0}     &    4113.0085  &     3092.455     &     1.330  &         0.184        &    -1948.673    &     1.02e+04     \\\\\n",
       "\\textbf{11502.0}     &    9343.9264  &     3174.590     &     2.943  &         0.003        &     3121.249    &     1.56e+04     \\\\\n",
       "\\textbf{11506.0}     &    4190.2588  &     3073.981     &     1.363  &         0.173        &    -1835.210    &     1.02e+04     \\\\\n",
       "\\textbf{11537.0}     &    9104.8465  &     3090.809     &     2.946  &         0.003        &     3046.392    &     1.52e+04     \\\\\n",
       "\\textbf{11566.0}     &    1.069e+04  &     3188.150     &     3.352  &         0.001        &     4436.526    &     1.69e+04     \\\\\n",
       "\\textbf{11573.0}     &    8122.9750  &     3053.915     &     2.660  &         0.008        &     2136.837    &     1.41e+04     \\\\\n",
       "\\textbf{11580.0}     &    4660.7676  &     3401.528     &     1.370  &         0.171        &    -2006.744    &     1.13e+04     \\\\\n",
       "\\textbf{11600.0}     &    9955.4797  &     3181.578     &     3.129  &         0.002        &     3719.104    &     1.62e+04     \\\\\n",
       "\\textbf{11609.0}     &    1.339e+04  &     3141.316     &     4.263  &         0.000        &     7234.942    &     1.95e+04     \\\\\n",
       "\\textbf{1161.0}      &   -1142.7795  &     2963.636     &    -0.386  &         0.700        &    -6951.956    &     4666.397     \\\\\n",
       "\\textbf{11636.0}     &   -1.033e+04  &     3195.420     &    -3.234  &         0.001        &    -1.66e+04    &    -4069.451     \\\\\n",
       "\\textbf{11670.0}     &    1.045e+04  &     3195.585     &     3.270  &         0.001        &     4186.263    &     1.67e+04     \\\\\n",
       "\\textbf{11678.0}     &    -172.3965  &     3111.159     &    -0.055  &         0.956        &    -6270.740    &     5925.947     \\\\\n",
       "\\textbf{11682.0}     &    8157.6451  &     3195.335     &     2.553  &         0.011        &     1894.303    &     1.44e+04     \\\\\n",
       "\\textbf{11694.0}     &    9735.1845  &     3265.189     &     2.982  &         0.003        &     3334.919    &     1.61e+04     \\\\\n",
       "\\textbf{11720.0}     &    2342.1807  &     3978.963     &     0.589  &         0.556        &    -5457.192    &     1.01e+04     \\\\\n",
       "\\textbf{11721.0}     &   -3708.6926  &     3442.247     &    -1.077  &         0.281        &    -1.05e+04    &     3038.635     \\\\\n",
       "\\textbf{11722.0}     &    7939.5465  &     3308.424     &     2.400  &         0.016        &     1454.532    &     1.44e+04     \\\\\n",
       "\\textbf{11793.0}     &    6392.4048  &     6063.166     &     1.054  &         0.292        &    -5492.321    &     1.83e+04     \\\\\n",
       "\\textbf{11797.0}     &    1.098e+04  &     3552.518     &     3.091  &         0.002        &     4015.910    &     1.79e+04     \\\\\n",
       "\\textbf{11914.0}     &    9253.8306  &     3755.065     &     2.464  &         0.014        &     1893.333    &     1.66e+04     \\\\\n",
       "\\textbf{1209.0}      &    6916.3882  &     3021.090     &     2.289  &         0.022        &      994.593    &     1.28e+04     \\\\\n",
       "\\textbf{12136.0}     &   -5842.3430  &     3290.133     &    -1.776  &         0.076        &    -1.23e+04    &      606.816     \\\\\n",
       "\\textbf{12141.0}     &    8.981e+04  &     3291.619     &    27.284  &         0.000        &     8.34e+04    &     9.63e+04     \\\\\n",
       "\\textbf{12181.0}     &    6468.7957  &     4281.785     &     1.511  &         0.131        &    -1924.153    &     1.49e+04     \\\\\n",
       "\\textbf{12215.0}     &    -753.1168  &     3219.104     &    -0.234  &         0.815        &    -7063.050    &     5556.816     \\\\\n",
       "\\textbf{12216.0}     &    3866.8554  &     3252.277     &     1.189  &         0.234        &    -2508.101    &     1.02e+04     \\\\\n",
       "\\textbf{12256.0}     &    2682.4219  &     3240.915     &     0.828  &         0.408        &    -3670.264    &     9035.107     \\\\\n",
       "\\textbf{12262.0}     &    9564.5090  &     3423.360     &     2.794  &         0.005        &     2854.203    &     1.63e+04     \\\\\n",
       "\\textbf{12389.0}     &    8986.4050  &     3304.204     &     2.720  &         0.007        &     2509.663    &     1.55e+04     \\\\\n",
       "\\textbf{1239.0}      &    6577.3859  &     3030.471     &     2.170  &         0.030        &      637.203    &     1.25e+04     \\\\\n",
       "\\textbf{12390.0}     &    7461.3813  &     3602.948     &     2.071  &         0.038        &      399.057    &     1.45e+04     \\\\\n",
       "\\textbf{12397.0}     &    6292.8548  &     5345.675     &     1.177  &         0.239        &    -4185.481    &     1.68e+04     \\\\\n",
       "\\textbf{1243.0}      &    4184.8534  &     3158.124     &     1.325  &         0.185        &    -2005.548    &     1.04e+04     \\\\\n",
       "\\textbf{12548.0}     &    9038.3456  &     3632.299     &     2.488  &         0.013        &     1918.488    &     1.62e+04     \\\\\n",
       "\\textbf{12570.0}     &    8969.7124  &     3499.499     &     2.563  &         0.010        &     2110.162    &     1.58e+04     \\\\\n",
       "\\textbf{12581.0}     &    6220.9585  &     3679.891     &     1.691  &         0.091        &     -992.187    &     1.34e+04     \\\\\n",
       "\\textbf{12592.0}     &    7907.3965  &     3465.117     &     2.282  &         0.023        &     1115.241    &     1.47e+04     \\\\\n",
       "\\textbf{12604.0}     &    7117.2488  &     6104.480     &     1.166  &         0.244        &    -4848.459    &     1.91e+04     \\\\\n",
       "\\textbf{12656.0}     &    1.071e+04  &     3513.018     &     3.048  &         0.002        &     3822.505    &     1.76e+04     \\\\\n",
       "\\textbf{12679.0}     &   -8243.1620  &     3377.006     &    -2.441  &         0.015        &    -1.49e+04    &    -1623.718     \\\\\n",
       "\\textbf{1278.0}      &    9559.3089  &     3276.729     &     2.917  &         0.004        &     3136.423    &      1.6e+04     \\\\\n",
       "\\textbf{12788.0}     &   -2571.9810  &     3720.888     &    -0.691  &         0.489        &    -9865.486    &     4721.524     \\\\\n",
       "\\textbf{1283.0}      &    9778.5694  &     3188.479     &     3.067  &         0.002        &     3528.666    &      1.6e+04     \\\\\n",
       "\\textbf{1297.0}      &    8851.0971  &     3160.094     &     2.801  &         0.005        &     2656.833    &      1.5e+04     \\\\\n",
       "\\textbf{12992.0}     &    1.022e+04  &     3469.470     &     2.945  &         0.003        &     3415.928    &      1.7e+04     \\\\\n",
       "\\textbf{13135.0}     &    5736.5602  &     3444.412     &     1.665  &         0.096        &    -1015.010    &     1.25e+04     \\\\\n",
       "\\textbf{1327.0}      &    2830.7666  &     3184.260     &     0.889  &         0.374        &    -3410.866    &     9072.399     \\\\\n",
       "\\textbf{13282.0}     &    3707.5355  &     5340.406     &     0.694  &         0.488        &    -6760.470    &     1.42e+04     \\\\\n",
       "\\textbf{1334.0}      &    1959.2153  &     3392.421     &     0.578  &         0.564        &    -4690.446    &     8608.876     \\\\\n",
       "\\textbf{13351.0}     &    3934.9513  &     3942.379     &     0.998  &         0.318        &    -3792.710    &     1.17e+04     \\\\\n",
       "\\textbf{13365.0}     &   -7664.6310  &     3538.787     &    -2.166  &         0.030        &    -1.46e+04    &     -728.070     \\\\\n",
       "\\textbf{13369.0}     &    6192.2784  &     3353.245     &     1.847  &         0.065        &     -380.592    &     1.28e+04     \\\\\n",
       "\\textbf{13406.0}     &    9511.9068  &     3422.019     &     2.780  &         0.005        &     2804.230    &     1.62e+04     \\\\\n",
       "\\textbf{13407.0}     &    3435.2039  &     3325.530     &     1.033  &         0.302        &    -3083.341    &     9953.749     \\\\\n",
       "\\textbf{13417.0}     &    1.054e+04  &     3627.974     &     2.906  &         0.004        &     3431.229    &     1.77e+04     \\\\\n",
       "\\textbf{13525.0}     &    2730.9588  &     3557.222     &     0.768  &         0.443        &    -4241.736    &     9703.654     \\\\\n",
       "\\textbf{13554.0}     &     1.11e+04  &     3515.274     &     3.158  &         0.002        &     4209.150    &      1.8e+04     \\\\\n",
       "\\textbf{1359.0}      &     231.6921  &     3271.221     &     0.071  &         0.944        &    -6180.398    &     6643.783     \\\\\n",
       "\\textbf{13623.0}     &    7757.8946  &     3398.861     &     2.282  &         0.022        &     1095.611    &     1.44e+04     \\\\\n",
       "\\textbf{1372.0}      &    2969.1064  &     3094.408     &     0.960  &         0.337        &    -3096.404    &     9034.616     \\\\\n",
       "\\textbf{1380.0}      &    2428.4690  &     3086.422     &     0.787  &         0.431        &    -3621.386    &     8478.324     \\\\\n",
       "\\textbf{13923.0}     &    9004.3585  &     3723.204     &     2.418  &         0.016        &     1706.314    &     1.63e+04     \\\\\n",
       "\\textbf{13932.0}     &    9906.5945  &     4225.421     &     2.345  &         0.019        &     1624.127    &     1.82e+04     \\\\\n",
       "\\textbf{13941.0}     &    -775.7343  &     3345.291     &    -0.232  &         0.817        &    -7333.012    &     5781.544     \\\\\n",
       "\\textbf{1397.0}      &    7611.2849  &     3344.240     &     2.276  &         0.023        &     1056.066    &     1.42e+04     \\\\\n",
       "\\textbf{14064.0}     &    7822.3534  &     3374.212     &     2.318  &         0.020        &     1208.386    &     1.44e+04     \\\\\n",
       "\\textbf{14084.0}     &    7161.9191  &     3367.382     &     2.127  &         0.033        &      561.340    &     1.38e+04     \\\\\n",
       "\\textbf{14324.0}     &    2450.1067  &     3416.939     &     0.717  &         0.473        &    -4247.612    &     9147.825     \\\\\n",
       "\\textbf{14462.0}     &    5896.9695  &     3424.853     &     1.722  &         0.085        &     -816.263    &     1.26e+04     \\\\\n",
       "\\textbf{1447.0}      &    1.291e+04  &     4617.436     &     2.796  &         0.005        &     3861.195    &      2.2e+04     \\\\\n",
       "\\textbf{14531.0}     &    6013.3098  &        1e+04     &     0.600  &         0.549        &    -1.36e+04    &     2.57e+04     \\\\\n",
       "\\textbf{14593.0}     &    9717.6801  &     3546.928     &     2.740  &         0.006        &     2765.162    &     1.67e+04     \\\\\n",
       "\\textbf{14622.0}     &    8494.5972  &     7256.705     &     1.171  &         0.242        &    -5729.646    &     2.27e+04     \\\\\n",
       "\\textbf{1465.0}      &    9293.7357  &     3759.137     &     2.472  &         0.013        &     1925.257    &     1.67e+04     \\\\\n",
       "\\textbf{1468.0}      &    9813.2527  &     3591.620     &     2.732  &         0.006        &     2773.133    &     1.69e+04     \\\\\n",
       "\\textbf{14897.0}     &    8912.7989  &     5404.388     &     1.649  &         0.099        &    -1680.623    &     1.95e+04     \\\\\n",
       "\\textbf{14954.0}     &    9310.1232  &     3588.857     &     2.594  &         0.009        &     2275.418    &     1.63e+04     \\\\\n",
       "\\textbf{1496.0}      &    1.018e+04  &     3142.118     &     3.241  &         0.001        &     4023.346    &     1.63e+04     \\\\\n",
       "\\textbf{15267.0}     &    8561.1899  &     3545.299     &     2.415  &         0.016        &     1611.865    &     1.55e+04     \\\\\n",
       "\\textbf{15354.0}     &    3951.6292  &     3545.461     &     1.115  &         0.265        &    -2998.013    &     1.09e+04     \\\\\n",
       "\\textbf{1542.0}      &    8274.0285  &     3128.411     &     2.645  &         0.008        &     2141.867    &     1.44e+04     \\\\\n",
       "\\textbf{15459.0}     &    4755.0666  &     3491.964     &     1.362  &         0.173        &    -2089.713    &     1.16e+04     \\\\\n",
       "\\textbf{1554.0}      &    1.008e+04  &     3139.681     &     3.211  &         0.001        &     3928.295    &     1.62e+04     \\\\\n",
       "\\textbf{15708.0}     &   -8685.5979  &     3678.105     &    -2.361  &         0.018        &    -1.59e+04    &    -1475.954     \\\\\n",
       "\\textbf{15711.0}     &    7018.0335  &     3521.077     &     1.993  &         0.046        &      116.187    &     1.39e+04     \\\\\n",
       "\\textbf{15761.0}     &    9603.3202  &     4161.317     &     2.308  &         0.021        &     1446.507    &     1.78e+04     \\\\\n",
       "\\textbf{1581.0}      &   -2.338e+04  &     4066.536     &    -5.750  &         0.000        &    -3.14e+04    &    -1.54e+04     \\\\\n",
       "\\textbf{1593.0}      &    7360.1914  &     3039.605     &     2.421  &         0.015        &     1402.105    &     1.33e+04     \\\\\n",
       "\\textbf{1602.0}      &    1.541e+04  &     3144.958     &     4.899  &         0.000        &     9241.495    &     2.16e+04     \\\\\n",
       "\\textbf{1613.0}      &    9518.7977  &     3135.907     &     3.035  &         0.002        &     3371.944    &     1.57e+04     \\\\\n",
       "\\textbf{16188.0}     &    6141.7233  &     3600.100     &     1.706  &         0.088        &     -915.020    &     1.32e+04     \\\\\n",
       "\\textbf{1632.0}      &    2730.6569  &     2967.596     &     0.920  &         0.358        &    -3086.281    &     8547.595     \\\\\n",
       "\\textbf{1633.0}      &    7645.1535  &     3073.458     &     2.487  &         0.013        &     1620.710    &     1.37e+04     \\\\\n",
       "\\textbf{1635.0}      &   -2279.6622  &     3207.382     &    -0.711  &         0.477        &    -8566.618    &     4007.294     \\\\\n",
       "\\textbf{16401.0}     &   -2106.7953  &     3535.255     &    -0.596  &         0.551        &    -9036.432    &     4822.842     \\\\\n",
       "\\textbf{16437.0}     &    2453.7358  &     4040.565     &     0.607  &         0.544        &    -5466.386    &     1.04e+04     \\\\\n",
       "\\textbf{1651.0}      &    5550.2093  &     3015.031     &     1.841  &         0.066        &     -359.709    &     1.15e+04     \\\\\n",
       "\\textbf{1655.0}      &    9615.4742  &     3116.290     &     3.086  &         0.002        &     3507.073    &     1.57e+04     \\\\\n",
       "\\textbf{1663.0}      &    1.375e+04  &     3117.223     &     4.412  &         0.000        &     7643.430    &     1.99e+04     \\\\\n",
       "\\textbf{16710.0}     &    4741.7051  &     3561.723     &     1.331  &         0.183        &    -2239.813    &     1.17e+04     \\\\\n",
       "\\textbf{16729.0}     &    4221.0150  &     3488.991     &     1.210  &         0.226        &    -2617.938    &     1.11e+04     \\\\\n",
       "\\textbf{1690.0}      &   -9476.8601  &     3052.010     &    -3.105  &         0.002        &    -1.55e+04    &    -3494.457     \\\\\n",
       "\\textbf{1703.0}      &    7131.1531  &     3145.834     &     2.267  &         0.023        &      964.841    &     1.33e+04     \\\\\n",
       "\\textbf{17101.0}     &   -1.039e+04  &     1.02e+04     &    -1.021  &         0.307        &    -3.04e+04    &     9567.672     \\\\\n",
       "\\textbf{17202.0}     &    8565.2847  &     3580.793     &     2.392  &         0.017        &     1546.386    &     1.56e+04     \\\\\n",
       "\\textbf{1722.0}      &    7635.3384  &     3120.097     &     2.447  &         0.014        &     1519.474    &     1.38e+04     \\\\\n",
       "\\textbf{1728.0}      &    9786.0208  &     3133.400     &     3.123  &         0.002        &     3644.082    &     1.59e+04     \\\\\n",
       "\\textbf{1743.0}      &    9206.6002  &     4065.126     &     2.265  &         0.024        &     1238.335    &     1.72e+04     \\\\\n",
       "\\textbf{1754.0}      &    9356.9783  &     3245.442     &     2.883  &         0.004        &     2995.419    &     1.57e+04     \\\\\n",
       "\\textbf{1762.0}      &    3530.4584  &     3070.682     &     1.150  &         0.250        &    -2488.544    &     9549.461     \\\\\n",
       "\\textbf{1773.0}      &    9383.6572  &     3200.790     &     2.932  &         0.003        &     3109.623    &     1.57e+04     \\\\\n",
       "\\textbf{1786.0}      &   -2446.1653  &     3027.221     &    -0.808  &         0.419        &    -8379.979    &     3487.648     \\\\\n",
       "\\textbf{18100.0}     &    7610.5863  &     3547.409     &     2.145  &         0.032        &      657.126    &     1.46e+04     \\\\\n",
       "\\textbf{1820.0}      &    7035.3148  &     3094.333     &     2.274  &         0.023        &      969.953    &     1.31e+04     \\\\\n",
       "\\textbf{1848.0}      &   -1341.5977  &     3489.590     &    -0.384  &         0.701        &    -8181.724    &     5498.529     \\\\\n",
       "\\textbf{18654.0}     &    9308.6209  &     4398.704     &     2.116  &         0.034        &      686.493    &     1.79e+04     \\\\\n",
       "\\textbf{1875.0}      &    5422.0706  &     4559.415     &     1.189  &         0.234        &    -3515.075    &     1.44e+04     \\\\\n",
       "\\textbf{1884.0}      &    9556.3065  &     3257.766     &     2.933  &         0.003        &     3170.590    &     1.59e+04     \\\\\n",
       "\\textbf{1913.0}      &    5067.5445  &     3013.058     &     1.682  &         0.093        &     -838.507    &      1.1e+04     \\\\\n",
       "\\textbf{1919.0}      &    8460.0632  &     3225.767     &     2.623  &         0.009        &     2137.070    &     1.48e+04     \\\\\n",
       "\\textbf{1920.0}      &    5677.3684  &     2997.340     &     1.894  &         0.058        &     -197.872    &     1.16e+04     \\\\\n",
       "\\textbf{1968.0}      &    7609.1915  &     3040.834     &     2.502  &         0.012        &     1648.695    &     1.36e+04     \\\\\n",
       "\\textbf{1976.0}      &    1.067e+04  &     3116.261     &     3.425  &         0.001        &     4565.380    &     1.68e+04     \\\\\n",
       "\\textbf{1981.0}      &    9170.2560  &     3119.840     &     2.939  &         0.003        &     3054.895    &     1.53e+04     \\\\\n",
       "\\textbf{1988.0}      &      67.7686  &     3946.416     &     0.017  &         0.986        &    -7667.806    &     7803.344     \\\\\n",
       "\\textbf{1992.0}      &    7764.2033  &     3046.140     &     2.549  &         0.011        &     1793.305    &     1.37e+04     \\\\\n",
       "\\textbf{2008.0}      &    8115.3533  &     3045.881     &     2.664  &         0.008        &     2144.965    &     1.41e+04     \\\\\n",
       "\\textbf{2033.0}      &    9191.5623  &     3578.712     &     2.568  &         0.010        &     2176.744    &     1.62e+04     \\\\\n",
       "\\textbf{2044.0}      &    7375.6500  &     3054.171     &     2.415  &         0.016        &     1389.011    &     1.34e+04     \\\\\n",
       "\\textbf{2049.0}      &    7958.9282  &     3057.521     &     2.603  &         0.009        &     1965.724    &      1.4e+04     \\\\\n",
       "\\textbf{2061.0}      &    1.067e+04  &     3184.425     &     3.350  &         0.001        &     4425.887    &     1.69e+04     \\\\\n",
       "\\textbf{20779.0}     &    5.298e+04  &     3626.010     &    14.611  &         0.000        &     4.59e+04    &     6.01e+04     \\\\\n",
       "\\textbf{2085.0}      &   -2797.9037  &     3091.351     &    -0.905  &         0.365        &    -8857.420    &     3261.613     \\\\\n",
       "\\textbf{2086.0}      &    6298.7693  &     3040.273     &     2.072  &         0.038        &      339.373    &     1.23e+04     \\\\\n",
       "\\textbf{2111.0}      &    7111.0506  &     3027.347     &     2.349  &         0.019        &     1176.992    &      1.3e+04     \\\\\n",
       "\\textbf{21204.0}     &    5065.4342  &     3773.658     &     1.342  &         0.180        &    -2331.508    &     1.25e+04     \\\\\n",
       "\\textbf{21238.0}     &    9630.2308  &     3715.731     &     2.592  &         0.010        &     2346.833    &     1.69e+04     \\\\\n",
       "\\textbf{2124.0}      &    7814.7292  &     3156.973     &     2.475  &         0.013        &     1626.582    &      1.4e+04     \\\\\n",
       "\\textbf{2146.0}      &    1.782e+04  &     3967.047     &     4.493  &         0.000        &        1e+04    &     2.56e+04     \\\\\n",
       "\\textbf{21496.0}     &   -9707.4783  &     3795.661     &    -2.558  &         0.011        &    -1.71e+04    &    -2267.406     \\\\\n",
       "\\textbf{2154.0}      &    8531.2191  &     3092.395     &     2.759  &         0.006        &     2469.654    &     1.46e+04     \\\\\n",
       "\\textbf{2176.0}      &    4.917e+04  &     3655.506     &    13.450  &         0.000        &      4.2e+04    &     5.63e+04     \\\\\n",
       "\\textbf{2188.0}      &    1.064e+04  &     3272.692     &     3.252  &         0.001        &     4229.171    &     1.71e+04     \\\\\n",
       "\\textbf{2189.0}      &    1987.7047  &     3044.824     &     0.653  &         0.514        &    -3980.612    &     7956.022     \\\\\n",
       "\\textbf{2220.0}      &    8121.7655  &     3066.803     &     2.648  &         0.008        &     2110.366    &     1.41e+04     \\\\\n",
       "\\textbf{22205.0}     &    1.097e+04  &     3742.158     &     2.932  &         0.003        &     3635.194    &     1.83e+04     \\\\\n",
       "\\textbf{2226.0}      &    6564.1694  &     6056.783     &     1.084  &         0.278        &    -5308.045    &     1.84e+04     \\\\\n",
       "\\textbf{2230.0}      &    9663.4059  &     3448.942     &     2.802  &         0.005        &     2902.955    &     1.64e+04     \\\\\n",
       "\\textbf{22325.0}     &    2036.3731  &     3623.720     &     0.562  &         0.574        &    -5066.669    &     9139.415     \\\\\n",
       "\\textbf{2255.0}      &    7276.8607  &     3077.041     &     2.365  &         0.018        &     1245.393    &     1.33e+04     \\\\\n",
       "\\textbf{22619.0}     &    9459.5788  &     3864.126     &     2.448  &         0.014        &     1885.306    &      1.7e+04     \\\\\n",
       "\\textbf{2267.0}      &    1604.9439  &     3063.270     &     0.524  &         0.600        &    -4399.530    &     7609.418     \\\\\n",
       "\\textbf{22815.0}     &    4875.5667  &     3570.634     &     1.365  &         0.172        &    -2123.419    &     1.19e+04     \\\\\n",
       "\\textbf{2285.0}      &   -2.061e+04  &     3176.312     &    -6.490  &         0.000        &    -2.68e+04    &    -1.44e+04     \\\\\n",
       "\\textbf{2290.0}      &    4219.0616  &     3064.579     &     1.377  &         0.169        &    -1787.979    &     1.02e+04     \\\\\n",
       "\\textbf{2295.0}      &     1.02e+04  &     4631.605     &     2.202  &         0.028        &     1118.124    &     1.93e+04     \\\\\n",
       "\\textbf{2316.0}      &    3932.7481  &     3268.816     &     1.203  &         0.229        &    -2474.628    &     1.03e+04     \\\\\n",
       "\\textbf{23220.0}     &    7708.2043  &     3613.811     &     2.133  &         0.033        &      624.587    &     1.48e+04     \\\\\n",
       "\\textbf{23224.0}     &   -1416.5488  &     3999.474     &    -0.354  &         0.723        &    -9256.124    &     6423.027     \\\\\n",
       "\\textbf{2343.0}      &     371.2913  &     5445.150     &     0.068  &         0.946        &    -1.03e+04    &      1.1e+04     \\\\\n",
       "\\textbf{2352.0}      &    7033.5174  &     3195.255     &     2.201  &         0.028        &      770.332    &     1.33e+04     \\\\\n",
       "\\textbf{23700.0}     &   -4104.2067  &     4540.832     &    -0.904  &         0.366        &     -1.3e+04    &     4796.514     \\\\\n",
       "\\textbf{2390.0}      &    1.022e+04  &     3146.886     &     3.249  &         0.001        &     4054.843    &     1.64e+04     \\\\\n",
       "\\textbf{2393.0}      &    5256.7276  &     3021.333     &     1.740  &         0.082        &     -665.543    &     1.12e+04     \\\\\n",
       "\\textbf{2403.0}      &    1.544e+04  &     3116.842     &     4.954  &         0.000        &     9331.361    &     2.16e+04     \\\\\n",
       "\\textbf{2435.0}      &    1.204e+04  &     3215.430     &     3.744  &         0.000        &     5734.523    &     1.83e+04     \\\\\n",
       "\\textbf{2444.0}      &    5543.3070  &     3031.522     &     1.829  &         0.067        &     -398.936    &     1.15e+04     \\\\\n",
       "\\textbf{2448.0}      &    7626.0071  &     3053.285     &     2.498  &         0.013        &     1641.104    &     1.36e+04     \\\\\n",
       "\\textbf{2469.0}      &    9427.6259  &     4407.154     &     2.139  &         0.032        &      788.935    &     1.81e+04     \\\\\n",
       "\\textbf{24720.0}     &    8470.8001  &     3726.615     &     2.273  &         0.023        &     1166.069    &     1.58e+04     \\\\\n",
       "\\textbf{24800.0}     &    2679.9470  &     3896.093     &     0.688  &         0.492        &    -4956.987    &     1.03e+04     \\\\\n",
       "\\textbf{2482.0}      &    1.036e+04  &     3154.774     &     3.283  &         0.001        &     4172.094    &     1.65e+04     \\\\\n",
       "\\textbf{24969.0}     &     1.01e+04  &     4399.409     &     2.296  &         0.022        &     1476.724    &     1.87e+04     \\\\\n",
       "\\textbf{2498.0}      &    2822.1067  &     3205.564     &     0.880  &         0.379        &    -3461.286    &     9105.499     \\\\\n",
       "\\textbf{2504.0}      &   -5740.6155  &     3125.446     &    -1.837  &         0.066        &    -1.19e+04    &      385.734     \\\\\n",
       "\\textbf{2508.0}      &    9551.1213  &     3313.820     &     2.882  &         0.004        &     3055.530    &      1.6e+04     \\\\\n",
       "\\textbf{25124.0}     &    8839.2099  &     3850.901     &     2.295  &         0.022        &     1290.860    &     1.64e+04     \\\\\n",
       "\\textbf{2518.0}      &    9462.2606  &     3118.179     &     3.035  &         0.002        &     3350.156    &     1.56e+04     \\\\\n",
       "\\textbf{25224.0}     &    9181.2871  &     7262.529     &     1.264  &         0.206        &    -5054.373    &     2.34e+04     \\\\\n",
       "\\textbf{25279.0}     &    8309.3827  &     3823.370     &     2.173  &         0.030        &      814.996    &     1.58e+04     \\\\\n",
       "\\textbf{2537.0}      &   -1331.6614  &     3141.364     &    -0.424  &         0.672        &    -7489.212    &     4825.889     \\\\\n",
       "\\textbf{2538.0}      &    1.059e+04  &     4051.182     &     2.615  &         0.009        &     2653.302    &     1.85e+04     \\\\\n",
       "\\textbf{25389.0}     &    1.016e+04  &     6108.961     &     1.663  &         0.096        &    -1815.943    &     2.21e+04     \\\\\n",
       "\\textbf{2547.0}      &    1402.5130  &     3210.761     &     0.437  &         0.662        &    -4891.066    &     7696.092     \\\\\n",
       "\\textbf{2553.0}      &    9104.4999  &     3154.705     &     2.886  &         0.004        &     2920.799    &     1.53e+04     \\\\\n",
       "\\textbf{2574.0}      &    2817.6652  &     3621.446     &     0.778  &         0.437        &    -4280.919    &     9916.249     \\\\\n",
       "\\textbf{25747.0}     &    8887.4249  &     3853.652     &     2.306  &         0.021        &     1333.682    &     1.64e+04     \\\\\n",
       "\\textbf{2577.0}      &    7333.8930  &     3033.785     &     2.417  &         0.016        &     1387.213    &     1.33e+04     \\\\\n",
       "\\textbf{2593.0}      &    7608.5207  &     3056.291     &     2.489  &         0.013        &     1617.727    &     1.36e+04     \\\\\n",
       "\\textbf{2596.0}      &    7104.2843  &     3165.531     &     2.244  &         0.025        &      899.363    &     1.33e+04     \\\\\n",
       "\\textbf{2663.0}      &    1.201e+04  &     3109.709     &     3.862  &         0.000        &     5914.315    &     1.81e+04     \\\\\n",
       "\\textbf{2771.0}      &    7170.4426  &     3157.640     &     2.271  &         0.023        &      980.989    &     1.34e+04     \\\\\n",
       "\\textbf{2787.0}      &    8843.2302  &     3108.248     &     2.845  &         0.004        &     2750.591    &     1.49e+04     \\\\\n",
       "\\textbf{2797.0}      &   -1136.7600  &     3047.444     &    -0.373  &         0.709        &    -7110.213    &     4836.693     \\\\\n",
       "\\textbf{2802.0}      &    9981.2391  &     3145.382     &     3.173  &         0.002        &     3815.813    &     1.61e+04     \\\\\n",
       "\\textbf{2817.0}      &   -2423.3733  &     3105.183     &    -0.780  &         0.435        &    -8510.003    &     3663.257     \\\\\n",
       "\\textbf{28678.0}     &   -6800.8821  &     3938.721     &    -1.727  &         0.084        &    -1.45e+04    &      919.610     \\\\\n",
       "\\textbf{28701.0}     &    7289.6069  &     3122.789     &     2.334  &         0.020        &     1168.466    &     1.34e+04     \\\\\n",
       "\\textbf{28742.0}     &   -4379.2282  &     4023.812     &    -1.088  &         0.276        &    -1.23e+04    &     3508.053     \\\\\n",
       "\\textbf{2888.0}      &    8333.3928  &     3213.797     &     2.593  &         0.010        &     2033.862    &     1.46e+04     \\\\\n",
       "\\textbf{2897.0}      &    9522.3975  &     3838.939     &     2.480  &         0.013        &     1997.494    &      1.7e+04     \\\\\n",
       "\\textbf{2917.0}      &    4179.7509  &     3174.337     &     1.317  &         0.188        &    -2042.431    &     1.04e+04     \\\\\n",
       "\\textbf{29392.0}     &   -4484.0845  &     3895.740     &    -1.151  &         0.250        &    -1.21e+04    &     3152.158     \\\\\n",
       "\\textbf{2950.0}      &   -8939.5631  &     4197.529     &    -2.130  &         0.033        &    -1.72e+04    &     -711.769     \\\\\n",
       "\\textbf{2951.0}      &    1.053e+04  &     3554.152     &     2.964  &         0.003        &     3567.561    &     1.75e+04     \\\\\n",
       "\\textbf{2953.0}      &    8670.7332  &     3081.692     &     2.814  &         0.005        &     2630.148    &     1.47e+04     \\\\\n",
       "\\textbf{2960.0}      &    6927.1661  &     3807.516     &     1.819  &         0.069        &     -536.143    &     1.44e+04     \\\\\n",
       "\\textbf{2975.0}      &    4129.9550  &     3074.147     &     1.343  &         0.179        &    -1895.841    &     1.02e+04     \\\\\n",
       "\\textbf{2982.0}      &    8607.4496  &     3114.061     &     2.764  &         0.006        &     2503.416    &     1.47e+04     \\\\\n",
       "\\textbf{2991.0}      &   -2091.3592  &     3611.129     &    -0.579  &         0.563        &    -9169.721    &     4987.002     \\\\\n",
       "\\textbf{3011.0}      &     352.0291  &     3228.393     &     0.109  &         0.913        &    -5976.112    &     6680.170     \\\\\n",
       "\\textbf{3015.0}      &    1.066e+04  &     3189.868     &     3.341  &         0.001        &     4404.847    &     1.69e+04     \\\\\n",
       "\\textbf{3026.0}      &    8297.5216  &     3114.849     &     2.664  &         0.008        &     2191.944    &     1.44e+04     \\\\\n",
       "\\textbf{3031.0}      &   -7156.8279  &     3963.813     &    -1.806  &         0.071        &    -1.49e+04    &      612.847     \\\\\n",
       "\\textbf{3062.0}      &    1.013e+04  &     3216.055     &     3.150  &         0.002        &     3827.484    &     1.64e+04     \\\\\n",
       "\\textbf{3093.0}      &    4614.1396  &     3380.925     &     1.365  &         0.172        &    -2012.986    &     1.12e+04     \\\\\n",
       "\\textbf{3107.0}      &    8261.4341  &     4593.460     &     1.799  &         0.072        &     -742.444    &     1.73e+04     \\\\\n",
       "\\textbf{3121.0}      &    1.075e+04  &     3100.529     &     3.467  &         0.001        &     4670.905    &     1.68e+04     \\\\\n",
       "\\textbf{3126.0}      &    8639.8575  &     3074.886     &     2.810  &         0.005        &     2612.613    &     1.47e+04     \\\\\n",
       "\\textbf{3144.0}      &    6.167e+04  &     3143.112     &    19.622  &         0.000        &     5.55e+04    &     6.78e+04     \\\\\n",
       "\\textbf{3156.0}      &    9057.0292  &     3556.046     &     2.547  &         0.011        &     2086.640    &      1.6e+04     \\\\\n",
       "\\textbf{3157.0}      &    8487.6305  &     3086.887     &     2.750  &         0.006        &     2436.864    &     1.45e+04     \\\\\n",
       "\\textbf{3170.0}      &    1.112e+04  &     3070.843     &     3.621  &         0.000        &     5098.953    &     1.71e+04     \\\\\n",
       "\\textbf{3178.0}      &    4719.2301  &     3279.399     &     1.439  &         0.150        &    -1708.889    &     1.11e+04     \\\\\n",
       "\\textbf{3206.0}      &    5242.4391  &     3326.387     &     1.576  &         0.115        &    -1277.785    &     1.18e+04     \\\\\n",
       "\\textbf{3229.0}      &    6940.2745  &     3201.204     &     2.168  &         0.030        &      665.428    &     1.32e+04     \\\\\n",
       "\\textbf{3235.0}      &    8557.6434  &     3259.016     &     2.626  &         0.009        &     2169.477    &     1.49e+04     \\\\\n",
       "\\textbf{3246.0}      &    8959.3648  &     3141.052     &     2.852  &         0.004        &     2802.426    &     1.51e+04     \\\\\n",
       "\\textbf{3248.0}      &    8795.1454  &     3144.903     &     2.797  &         0.005        &     2630.657    &      1.5e+04     \\\\\n",
       "\\textbf{3282.0}      &   -1.882e+04  &     3179.239     &    -5.919  &         0.000        &     -2.5e+04    &    -1.26e+04     \\\\\n",
       "\\textbf{3362.0}      &    1467.2473  &     3430.530     &     0.428  &         0.669        &    -5257.112    &     8191.606     \\\\\n",
       "\\textbf{3372.0}      &    8967.9006  &     3615.991     &     2.480  &         0.013        &     1880.009    &     1.61e+04     \\\\\n",
       "\\textbf{3422.0}      &    8150.3780  &     3116.503     &     2.615  &         0.009        &     2041.559    &     1.43e+04     \\\\\n",
       "\\textbf{3497.0}      &    3305.8909  &     3020.187     &     1.095  &         0.274        &    -2614.134    &     9225.916     \\\\\n",
       "\\textbf{3502.0}      &    4413.0364  &     3019.260     &     1.462  &         0.144        &    -1505.172    &     1.03e+04     \\\\\n",
       "\\textbf{3504.0}      &    7330.3624  &     3693.007     &     1.985  &         0.047        &       91.508    &     1.46e+04     \\\\\n",
       "\\textbf{3505.0}      &    6509.4875  &     3039.592     &     2.142  &         0.032        &      551.425    &     1.25e+04     \\\\\n",
       "\\textbf{3532.0}      &    9367.0102  &     3036.064     &     3.085  &         0.002        &     3415.864    &     1.53e+04     \\\\\n",
       "\\textbf{3574.0}      &    9567.2283  &     4941.074     &     1.936  &         0.053        &     -118.027    &     1.93e+04     \\\\\n",
       "\\textbf{3580.0}      &    5668.8773  &     3023.668     &     1.875  &         0.061        &     -257.971    &     1.16e+04     \\\\\n",
       "\\textbf{3612.0}      &    1.077e+04  &     3190.707     &     3.376  &         0.001        &     4518.562    &      1.7e+04     \\\\\n",
       "\\textbf{3619.0}      &    7583.6331  &     3125.470     &     2.426  &         0.015        &     1457.238    &     1.37e+04     \\\\\n",
       "\\textbf{3622.0}      &    1.024e+04  &     3247.561     &     3.154  &         0.002        &     3877.236    &     1.66e+04     \\\\\n",
       "\\textbf{3639.0}      &     701.7362  &     2978.785     &     0.236  &         0.814        &    -5137.134    &     6540.606     \\\\\n",
       "\\textbf{3650.0}      &     -56.5141  &     3013.203     &    -0.019  &         0.985        &    -5962.849    &     5849.821     \\\\\n",
       "\\textbf{3662.0}      &    6515.3615  &     3036.473     &     2.146  &         0.032        &      563.413    &     1.25e+04     \\\\\n",
       "\\textbf{3734.0}      &   -4540.5498  &     3009.478     &    -1.509  &         0.131        &    -1.04e+04    &     1358.485     \\\\\n",
       "\\textbf{3735.0}      &    5998.0243  &     3363.381     &     1.783  &         0.075        &     -594.714    &     1.26e+04     \\\\\n",
       "\\textbf{3761.0}      &    4676.2483  &     3019.021     &     1.549  &         0.121        &    -1241.491    &     1.06e+04     \\\\\n",
       "\\textbf{3779.0}      &   -1115.3957  &     3351.604     &    -0.333  &         0.739        &    -7685.049    &     5454.258     \\\\\n",
       "\\textbf{3781.0}      &    3388.6729  &     3657.813     &     0.926  &         0.354        &    -3781.196    &     1.06e+04     \\\\\n",
       "\\textbf{3782.0}      &    -945.5519  &     3097.134     &    -0.305  &         0.760        &    -7016.405    &     5125.302     \\\\\n",
       "\\textbf{3786.0}      &    8059.2754  &     3095.380     &     2.604  &         0.009        &     1991.860    &     1.41e+04     \\\\\n",
       "\\textbf{3796.0}      &   -1921.4741  &     3437.318     &    -0.559  &         0.576        &    -8659.140    &     4816.192     \\\\\n",
       "\\textbf{3821.0}      &    9374.5415  &     3182.442     &     2.946  &         0.003        &     3136.471    &     1.56e+04     \\\\\n",
       "\\textbf{3835.0}      &    1623.3259  &     3068.940     &     0.529  &         0.597        &    -4392.263    &     7638.914     \\\\\n",
       "\\textbf{3839.0}      &    5038.7123  &     3787.676     &     1.330  &         0.183        &    -2385.707    &     1.25e+04     \\\\\n",
       "\\textbf{3840.0}      &    3096.0560  &     3106.065     &     0.997  &         0.319        &    -2992.302    &     9184.414     \\\\\n",
       "\\textbf{3895.0}      &    9436.3688  &     3115.660     &     3.029  &         0.002        &     3329.202    &     1.55e+04     \\\\\n",
       "\\textbf{3908.0}      &    6499.5003  &     4150.425     &     1.566  &         0.117        &    -1635.964    &     1.46e+04     \\\\\n",
       "\\textbf{3911.0}      &    3978.4256  &     3062.447     &     1.299  &         0.194        &    -2024.435    &     9981.286     \\\\\n",
       "\\textbf{3917.0}      &    9644.8467  &     3201.184     &     3.013  &         0.003        &     3370.040    &     1.59e+04     \\\\\n",
       "\\textbf{3946.0}      &     1.02e+04  &     3163.781     &     3.224  &         0.001        &     3997.651    &     1.64e+04     \\\\\n",
       "\\textbf{3971.0}      &    8857.9912  &     3190.900     &     2.776  &         0.006        &     2603.342    &     1.51e+04     \\\\\n",
       "\\textbf{3980.0}      &    1.889e+04  &     3114.804     &     6.066  &         0.000        &     1.28e+04    &      2.5e+04     \\\\\n",
       "\\textbf{4034.0}      &    5433.4828  &     3023.927     &     1.797  &         0.072        &     -493.873    &     1.14e+04     \\\\\n",
       "\\textbf{4036.0}      &    1.006e+04  &     3132.064     &     3.210  &         0.001        &     3915.798    &     1.62e+04     \\\\\n",
       "\\textbf{4040.0}      &    4402.5134  &     3085.061     &     1.427  &         0.154        &    -1644.674    &     1.04e+04     \\\\\n",
       "\\textbf{4058.0}      &    8496.1476  &     3074.049     &     2.764  &         0.006        &     2470.545    &     1.45e+04     \\\\\n",
       "\\textbf{4060.0}      &   -8502.7681  &     3085.688     &    -2.756  &         0.006        &    -1.46e+04    &    -2454.351     \\\\\n",
       "\\textbf{4062.0}      &    1.245e+04  &     3186.197     &     3.906  &         0.000        &     6200.250    &     1.87e+04     \\\\\n",
       "\\textbf{4077.0}      &    8049.1131  &     4325.812     &     1.861  &         0.063        &     -430.135    &     1.65e+04     \\\\\n",
       "\\textbf{4087.0}      &   -1.922e+04  &     3417.870     &    -5.622  &         0.000        &    -2.59e+04    &    -1.25e+04     \\\\\n",
       "\\textbf{4091.0}      &    7764.4934  &     3536.914     &     2.195  &         0.028        &      831.604    &     1.47e+04     \\\\\n",
       "\\textbf{4127.0}      &    3991.2384  &     2983.946     &     1.338  &         0.181        &    -1857.749    &     9840.226     \\\\\n",
       "\\textbf{4138.0}      &    1.012e+04  &     3751.511     &     2.696  &         0.007        &     2761.757    &     1.75e+04     \\\\\n",
       "\\textbf{4162.0}      &    8848.7229  &     3711.438     &     2.384  &         0.017        &     1573.741    &     1.61e+04     \\\\\n",
       "\\textbf{4186.0}      &    1.029e+04  &     3147.338     &     3.271  &         0.001        &     4124.177    &     1.65e+04     \\\\\n",
       "\\textbf{4194.0}      &    -206.5396  &     3254.206     &    -0.063  &         0.949        &    -6585.277    &     6172.197     \\\\\n",
       "\\textbf{4199.0}      &   -3080.7950  &     2986.731     &    -1.031  &         0.302        &    -8935.242    &     2773.652     \\\\\n",
       "\\textbf{4213.0}      &    9040.9619  &     3063.929     &     2.951  &         0.003        &     3035.197    &      1.5e+04     \\\\\n",
       "\\textbf{4222.0}      &    -427.7763  &     3036.691     &    -0.141  &         0.888        &    -6380.151    &     5524.598     \\\\\n",
       "\\textbf{4223.0}      &    8795.2266  &     3075.179     &     2.860  &         0.004        &     2767.409    &     1.48e+04     \\\\\n",
       "\\textbf{4251.0}      &    9920.6121  &     3135.291     &     3.164  &         0.002        &     3774.966    &     1.61e+04     \\\\\n",
       "\\textbf{4265.0}      &    8311.0668  &     3331.549     &     2.495  &         0.013        &     1780.724    &     1.48e+04     \\\\\n",
       "\\textbf{4274.0}      &    7457.4585  &     3194.105     &     2.335  &         0.020        &     1196.528    &     1.37e+04     \\\\\n",
       "\\textbf{4321.0}      &    5939.8774  &     3058.043     &     1.942  &         0.052        &      -54.352    &     1.19e+04     \\\\\n",
       "\\textbf{4335.0}      &    7276.7703  &     4582.157     &     1.588  &         0.112        &    -1704.953    &     1.63e+04     \\\\\n",
       "\\textbf{4340.0}      &    6243.3898  &     3049.262     &     2.048  &         0.041        &      266.372    &     1.22e+04     \\\\\n",
       "\\textbf{4371.0}      &    6746.0318  &     3081.412     &     2.189  &         0.029        &      705.997    &     1.28e+04     \\\\\n",
       "\\textbf{4415.0}      &    9237.0407  &     3230.539     &     2.859  &         0.004        &     2904.693    &     1.56e+04     \\\\\n",
       "\\textbf{4450.0}      &    7979.6767  &     3101.329     &     2.573  &         0.010        &     1900.602    &     1.41e+04     \\\\\n",
       "\\textbf{4476.0}      &     675.3081  &     3174.296     &     0.213  &         0.832        &    -5546.793    &     6897.409     \\\\\n",
       "\\textbf{4510.0}      &    2475.6995  &     3026.171     &     0.818  &         0.413        &    -3456.054    &     8407.453     \\\\\n",
       "\\textbf{4520.0}      &    7980.2863  &     3051.589     &     2.615  &         0.009        &     1998.708    &      1.4e+04     \\\\\n",
       "\\textbf{4551.0}      &    8011.4429  &     7270.601     &     1.102  &         0.271        &    -6240.039    &     2.23e+04     \\\\\n",
       "\\textbf{4568.0}      &    8842.8576  &     3229.403     &     2.738  &         0.006        &     2512.738    &     1.52e+04     \\\\\n",
       "\\textbf{4579.0}      &    1.065e+04  &     3189.762     &     3.338  &         0.001        &     4393.423    &     1.69e+04     \\\\\n",
       "\\textbf{4585.0}      &    1.003e+04  &     3206.938     &     3.129  &         0.002        &     3747.041    &     1.63e+04     \\\\\n",
       "\\textbf{4595.0}      &    6865.7401  &     3039.451     &     2.259  &         0.024        &      907.954    &     1.28e+04     \\\\\n",
       "\\textbf{4600.0}      &    1012.0246  &     3200.037     &     0.316  &         0.752        &    -5260.535    &     7284.584     \\\\\n",
       "\\textbf{4607.0}      &    9896.4315  &     3136.909     &     3.155  &         0.002        &     3747.613    &      1.6e+04     \\\\\n",
       "\\textbf{4608.0}      &    1926.7708  &     3084.280     &     0.625  &         0.532        &    -4118.886    &     7972.428     \\\\\n",
       "\\textbf{4622.0}      &    5050.9157  &     3012.296     &     1.677  &         0.094        &     -853.642    &      1.1e+04     \\\\\n",
       "\\textbf{4623.0}      &    9205.2343  &     3264.415     &     2.820  &         0.005        &     2806.486    &     1.56e+04     \\\\\n",
       "\\textbf{4768.0}      &    8149.0733  &     3123.950     &     2.609  &         0.009        &     2025.657    &     1.43e+04     \\\\\n",
       "\\textbf{4771.0}      &     1.02e+04  &     3154.399     &     3.233  &         0.001        &     4014.036    &     1.64e+04     \\\\\n",
       "\\textbf{4800.0}      &    8510.9355  &     3222.793     &     2.641  &         0.008        &     2193.771    &     1.48e+04     \\\\\n",
       "\\textbf{4802.0}      &    9756.0632  &     3122.659     &     3.124  &         0.002        &     3635.177    &     1.59e+04     \\\\\n",
       "\\textbf{4807.0}      &    9311.2841  &     3209.003     &     2.902  &         0.004        &     3021.150    &     1.56e+04     \\\\\n",
       "\\textbf{4839.0}      &    -1.39e+05  &     4400.348     &   -31.598  &         0.000        &    -1.48e+05    &     -1.3e+05     \\\\\n",
       "\\textbf{4843.0}      &    -155.0920  &     3300.774     &    -0.047  &         0.963        &    -6625.111    &     6314.927     \\\\\n",
       "\\textbf{4881.0}      &    7349.9526  &     3033.927     &     2.423  &         0.015        &     1402.995    &     1.33e+04     \\\\\n",
       "\\textbf{4900.0}      &    8056.7145  &     3130.004     &     2.574  &         0.010        &     1921.431    &     1.42e+04     \\\\\n",
       "\\textbf{4926.0}      &    6951.4250  &     3036.486     &     2.289  &         0.022        &      999.451    &     1.29e+04     \\\\\n",
       "\\textbf{4941.0}      &    7701.0234  &     3119.428     &     2.469  &         0.014        &     1586.472    &     1.38e+04     \\\\\n",
       "\\textbf{4961.0}      &   -6713.3878  &     3845.668     &    -1.746  &         0.081        &    -1.43e+04    &      824.706     \\\\\n",
       "\\textbf{4988.0}      &    1.555e+04  &     3152.418     &     4.932  &         0.000        &     9368.460    &     2.17e+04     \\\\\n",
       "\\textbf{4993.0}      &    1.078e+04  &     3191.719     &     3.379  &         0.001        &     4527.925    &      1.7e+04     \\\\\n",
       "\\textbf{5018.0}      &    2756.6116  &     3051.145     &     0.903  &         0.366        &    -3224.095    &     8737.318     \\\\\n",
       "\\textbf{5020.0}      &    1703.6436  &     3206.794     &     0.531  &         0.595        &    -4582.160    &     7989.448     \\\\\n",
       "\\textbf{5027.0}      &    5583.6057  &     3073.165     &     1.817  &         0.069        &     -440.265    &     1.16e+04     \\\\\n",
       "\\textbf{5032.0}      &    8848.7731  &     3093.419     &     2.861  &         0.004        &     2785.202    &     1.49e+04     \\\\\n",
       "\\textbf{5043.0}      &    5765.7251  &     3056.378     &     1.886  &         0.059        &     -225.240    &     1.18e+04     \\\\\n",
       "\\textbf{5046.0}      &   -3742.7011  &     3008.827     &    -1.244  &         0.214        &    -9640.459    &     2155.057     \\\\\n",
       "\\textbf{5047.0}      &    4.951e+04  &     3945.666     &    12.549  &         0.000        &     4.18e+04    &     5.72e+04     \\\\\n",
       "\\textbf{5065.0}      &    9943.6682  &     3565.116     &     2.789  &         0.005        &     2955.499    &     1.69e+04     \\\\\n",
       "\\textbf{5071.0}      &    8402.8199  &     3506.244     &     2.397  &         0.017        &     1530.049    &     1.53e+04     \\\\\n",
       "\\textbf{5073.0}      &    -2.03e+05  &     6327.061     &   -32.087  &         0.000        &    -2.15e+05    &    -1.91e+05     \\\\\n",
       "\\textbf{5087.0}      &    4808.3060  &     3031.181     &     1.586  &         0.113        &    -1133.268    &     1.07e+04     \\\\\n",
       "\\textbf{5109.0}      &        1e+04  &     3156.702     &     3.169  &         0.002        &     3815.966    &     1.62e+04     \\\\\n",
       "\\textbf{5116.0}      &     331.7479  &     3230.260     &     0.103  &         0.918        &    -6000.052    &     6663.548     \\\\\n",
       "\\textbf{5122.0}      &    5093.7234  &     3033.485     &     1.679  &         0.093        &     -852.368    &      1.1e+04     \\\\\n",
       "\\textbf{5134.0}      &    1619.9886  &     3164.783     &     0.512  &         0.609        &    -4583.467    &     7823.444     \\\\\n",
       "\\textbf{5142.0}      &    6629.5952  &     3817.878     &     1.736  &         0.083        &     -854.026    &     1.41e+04     \\\\\n",
       "\\textbf{5165.0}      &    7201.9002  &     3418.120     &     2.107  &         0.035        &      501.866    &     1.39e+04     \\\\\n",
       "\\textbf{5169.0}      &    1.921e+04  &     3111.749     &     6.173  &         0.000        &     1.31e+04    &     2.53e+04     \\\\\n",
       "\\textbf{5174.0}      &    6861.1596  &     3315.866     &     2.069  &         0.039        &      361.558    &     1.34e+04     \\\\\n",
       "\\textbf{5179.0}      &    9522.8653  &     3123.992     &     3.048  &         0.002        &     3399.366    &     1.56e+04     \\\\\n",
       "\\textbf{5181.0}      &    1.022e+04  &     3247.843     &     3.148  &         0.002        &     3857.092    &     1.66e+04     \\\\\n",
       "\\textbf{5187.0}      &    1.051e+04  &     3595.928     &     2.924  &         0.003        &     3466.116    &     1.76e+04     \\\\\n",
       "\\textbf{5229.0}      &    1832.7007  &     2976.839     &     0.616  &         0.538        &    -4002.356    &     7667.757     \\\\\n",
       "\\textbf{5234.0}      &   -5223.8465  &     3133.458     &    -1.667  &         0.096        &    -1.14e+04    &      918.206     \\\\\n",
       "\\textbf{5237.0}      &    9043.0280  &     3101.586     &     2.916  &         0.004        &     2963.448    &     1.51e+04     \\\\\n",
       "\\textbf{5252.0}      &    8212.2020  &     3060.018     &     2.684  &         0.007        &     2214.102    &     1.42e+04     \\\\\n",
       "\\textbf{5254.0}      &    8446.7502  &     3097.320     &     2.727  &         0.006        &     2375.532    &     1.45e+04     \\\\\n",
       "\\textbf{5306.0}      &    6465.5650  &     3027.709     &     2.135  &         0.033        &      530.795    &     1.24e+04     \\\\\n",
       "\\textbf{5338.0}      &    9319.2221  &     3098.305     &     3.008  &         0.003        &     3246.074    &     1.54e+04     \\\\\n",
       "\\textbf{5377.0}      &    1.009e+04  &     3165.731     &     3.188  &         0.001        &     3885.677    &     1.63e+04     \\\\\n",
       "\\textbf{5439.0}      &    7334.3884  &     3132.147     &     2.342  &         0.019        &     1194.905    &     1.35e+04     \\\\\n",
       "\\textbf{5456.0}      &    1.067e+04  &     3219.404     &     3.313  &         0.001        &     4356.183    &      1.7e+04     \\\\\n",
       "\\textbf{5464.0}      &    8046.9137  &     3717.507     &     2.165  &         0.030        &      760.036    &     1.53e+04     \\\\\n",
       "\\textbf{5476.0}      &    1.038e+04  &     3188.810     &     3.254  &         0.001        &     4125.428    &     1.66e+04     \\\\\n",
       "\\textbf{5492.0}      &   -7692.0384  &     3028.409     &    -2.540  &         0.011        &    -1.36e+04    &    -1755.897     \\\\\n",
       "\\textbf{5496.0}      &    8328.8408  &     3078.327     &     2.706  &         0.007        &     2294.852    &     1.44e+04     \\\\\n",
       "\\textbf{5505.0}      &    9584.8474  &     3145.488     &     3.047  &         0.002        &     3419.213    &     1.58e+04     \\\\\n",
       "\\textbf{5518.0}      &    8040.5569  &     3394.234     &     2.369  &         0.018        &     1387.343    &     1.47e+04     \\\\\n",
       "\\textbf{5520.0}      &    7141.4059  &     3085.889     &     2.314  &         0.021        &     1092.595    &     1.32e+04     \\\\\n",
       "\\textbf{5545.0}      &    9850.2695  &     3257.198     &     3.024  &         0.002        &     3465.667    &     1.62e+04     \\\\\n",
       "\\textbf{5568.0}      &      1.2e+04  &     3131.140     &     3.834  &         0.000        &     5866.525    &     1.81e+04     \\\\\n",
       "\\textbf{5569.0}      &    1.006e+04  &     3173.656     &     3.171  &         0.002        &     3841.616    &     1.63e+04     \\\\\n",
       "\\textbf{5578.0}      &    9572.4372  &     3115.964     &     3.072  &         0.002        &     3464.675    &     1.57e+04     \\\\\n",
       "\\textbf{5581.0}      &    9414.8809  &     3097.341     &     3.040  &         0.002        &     3343.622    &     1.55e+04     \\\\\n",
       "\\textbf{5589.0}      &    2400.9953  &     2973.079     &     0.808  &         0.419        &    -3426.690    &     8228.681     \\\\\n",
       "\\textbf{5597.0}      &     1.13e+04  &     3648.090     &     3.097  &         0.002        &     4145.936    &     1.84e+04     \\\\\n",
       "\\textbf{5606.0}      &   -2.745e+04  &     3152.384     &    -8.709  &         0.000        &    -3.36e+04    &    -2.13e+04     \\\\\n",
       "\\textbf{5639.0}      &    1.082e+04  &     3159.665     &     3.425  &         0.001        &     4628.029    &      1.7e+04     \\\\\n",
       "\\textbf{5667.0}      &    7446.7098  &     3218.672     &     2.314  &         0.021        &     1137.624    &     1.38e+04     \\\\\n",
       "\\textbf{5690.0}      &    9706.2182  &     3117.215     &     3.114  &         0.002        &     3596.004    &     1.58e+04     \\\\\n",
       "\\textbf{5709.0}      &    9020.8914  &     3130.883     &     2.881  &         0.004        &     2883.886    &     1.52e+04     \\\\\n",
       "\\textbf{5726.0}      &    9123.8740  &     3144.647     &     2.901  &         0.004        &     2959.887    &     1.53e+04     \\\\\n",
       "\\textbf{5764.0}      &    7723.0628  &     3018.519     &     2.559  &         0.011        &     1806.306    &     1.36e+04     \\\\\n",
       "\\textbf{5772.0}      &    8901.7882  &     3086.811     &     2.884  &         0.004        &     2851.170    &      1.5e+04     \\\\\n",
       "\\textbf{5860.0}      &   -1.948e+04  &     3110.857     &    -6.261  &         0.000        &    -2.56e+04    &    -1.34e+04     \\\\\n",
       "\\textbf{5878.0}      &    1.119e+04  &     3037.648     &     3.682  &         0.000        &     5231.276    &     1.71e+04     \\\\\n",
       "\\textbf{5903.0}      &    4917.5431  &     3123.826     &     1.574  &         0.115        &    -1205.631    &      1.1e+04     \\\\\n",
       "\\textbf{5905.0}      &    6472.3316  &     3072.948     &     2.106  &         0.035        &      448.886    &     1.25e+04     \\\\\n",
       "\\textbf{5959.0}      &    4032.3371  &     3074.491     &     1.312  &         0.190        &    -1994.131    &     1.01e+04     \\\\\n",
       "\\textbf{6008.0}      &    2.842e+04  &     3030.015     &     9.380  &         0.000        &     2.25e+04    &     3.44e+04     \\\\\n",
       "\\textbf{6034.0}      &    5915.9539  &     3166.969     &     1.868  &         0.062        &     -291.787    &     1.21e+04     \\\\\n",
       "\\textbf{6035.0}      &    4321.0088  &     3624.230     &     1.192  &         0.233        &    -2783.033    &     1.14e+04     \\\\\n",
       "\\textbf{6036.0}      &     672.3068  &     2994.346     &     0.225  &         0.822        &    -5197.066    &     6541.680     \\\\\n",
       "\\textbf{6039.0}      &    9107.9457  &     3133.532     &     2.907  &         0.004        &     2965.747    &     1.53e+04     \\\\\n",
       "\\textbf{6044.0}      &    1.053e+04  &     3386.931     &     3.110  &         0.002        &     3894.343    &     1.72e+04     \\\\\n",
       "\\textbf{6066.0}      &    -2.49e+04  &     4424.278     &    -5.629  &         0.000        &    -3.36e+04    &    -1.62e+04     \\\\\n",
       "\\textbf{6078.0}      &    9970.6042  &     3077.931     &     3.239  &         0.001        &     3937.391    &      1.6e+04     \\\\\n",
       "\\textbf{6081.0}      &   -8372.7134  &     2993.920     &    -2.797  &         0.005        &    -1.42e+04    &    -2504.175     \\\\\n",
       "\\textbf{60893.0}     &   -4392.3442  &     4603.659     &    -0.954  &         0.340        &    -1.34e+04    &     4631.526     \\\\\n",
       "\\textbf{6097.0}      &    1.012e+04  &     3186.388     &     3.177  &         0.001        &     3876.500    &     1.64e+04     \\\\\n",
       "\\textbf{6102.0}      &    8471.8648  &     3120.976     &     2.714  &         0.007        &     2354.277    &     1.46e+04     \\\\\n",
       "\\textbf{6104.0}      &   -1015.6692  &     3148.374     &    -0.323  &         0.747        &    -7186.960    &     5155.622     \\\\\n",
       "\\textbf{6109.0}      &    1845.7093  &     2960.815     &     0.623  &         0.533        &    -3957.937    &     7649.356     \\\\\n",
       "\\textbf{6127.0}      &    3170.7530  &     3399.610     &     0.933  &         0.351        &    -3492.999    &     9834.505     \\\\\n",
       "\\textbf{61552.0}     &   -3744.5941  &     4251.815     &    -0.881  &         0.378        &    -1.21e+04    &     4589.610     \\\\\n",
       "\\textbf{6158.0}      &    6412.2005  &     3180.992     &     2.016  &         0.044        &      176.973    &     1.26e+04     \\\\\n",
       "\\textbf{6171.0}      &    9057.6581  &     3095.055     &     2.926  &         0.003        &     2990.880    &     1.51e+04     \\\\\n",
       "\\textbf{61780.0}     &    7728.4892  &     4929.729     &     1.568  &         0.117        &    -1934.529    &     1.74e+04     \\\\\n",
       "\\textbf{6207.0}      &    9047.4137  &     3117.371     &     2.902  &         0.004        &     2936.893    &     1.52e+04     \\\\\n",
       "\\textbf{6214.0}      &    9507.7433  &     3104.135     &     3.063  &         0.002        &     3423.167    &     1.56e+04     \\\\\n",
       "\\textbf{6216.0}      &    9878.3520  &     3184.210     &     3.102  &         0.002        &     3636.816    &     1.61e+04     \\\\\n",
       "\\textbf{62221.0}     &    9234.8611  &     4394.723     &     2.101  &         0.036        &      620.537    &     1.78e+04     \\\\\n",
       "\\textbf{6259.0}      &    5439.7175  &     3440.702     &     1.581  &         0.114        &    -1304.581    &     1.22e+04     \\\\\n",
       "\\textbf{62599.0}     &   -2.283e+04  &     4958.737     &    -4.603  &         0.000        &    -3.25e+04    &    -1.31e+04     \\\\\n",
       "\\textbf{6266.0}      &    9238.7331  &     3091.436     &     2.988  &         0.003        &     3179.048    &     1.53e+04     \\\\\n",
       "\\textbf{6268.0}      &    1962.6459  &     3058.799     &     0.642  &         0.521        &    -4033.064    &     7958.356     \\\\\n",
       "\\textbf{6288.0}      &    8168.8157  &     3114.238     &     2.623  &         0.009        &     2064.437    &     1.43e+04     \\\\\n",
       "\\textbf{6297.0}      &    9274.3327  &     3198.409     &     2.900  &         0.004        &     3004.965    &     1.55e+04     \\\\\n",
       "\\textbf{6307.0}      &   -1.469e+04  &     3698.147     &    -3.973  &         0.000        &    -2.19e+04    &    -7443.955     \\\\\n",
       "\\textbf{6313.0}      &    8992.0347  &     4390.677     &     2.048  &         0.041        &      385.640    &     1.76e+04     \\\\\n",
       "\\textbf{6314.0}      &    9736.1987  &     3151.801     &     3.089  &         0.002        &     3558.191    &     1.59e+04     \\\\\n",
       "\\textbf{6326.0}      &    3826.7169  &     2984.925     &     1.282  &         0.200        &    -2024.190    &     9677.624     \\\\\n",
       "\\textbf{6349.0}      &    8643.9164  &     3087.391     &     2.800  &         0.005        &     2592.161    &     1.47e+04     \\\\\n",
       "\\textbf{6357.0}      &    1.032e+04  &     3297.883     &     3.130  &         0.002        &     3857.899    &     1.68e+04     \\\\\n",
       "\\textbf{6375.0}      &    1.413e+04  &     3134.685     &     4.507  &         0.000        &     7984.330    &     2.03e+04     \\\\\n",
       "\\textbf{6376.0}      &    9471.0692  &     3147.599     &     3.009  &         0.003        &     3301.297    &     1.56e+04     \\\\\n",
       "\\textbf{6379.0}      &   -3775.7784  &     6131.764     &    -0.616  &         0.538        &    -1.58e+04    &     8243.410     \\\\\n",
       "\\textbf{6386.0}      &    9629.8217  &     3110.678     &     3.096  &         0.002        &     3532.421    &     1.57e+04     \\\\\n",
       "\\textbf{6403.0}      &    5780.6448  &     3183.328     &     1.816  &         0.069        &     -459.161    &      1.2e+04     \\\\\n",
       "\\textbf{6410.0}      &    1.062e+04  &     3209.167     &     3.309  &         0.001        &     4327.639    &     1.69e+04     \\\\\n",
       "\\textbf{6416.0}      &    4480.9728  &     3166.668     &     1.415  &         0.157        &    -1726.177    &     1.07e+04     \\\\\n",
       "\\textbf{6424.0}      &    9498.0653  &     3132.089     &     3.033  &         0.002        &     3358.695    &     1.56e+04     \\\\\n",
       "\\textbf{6433.0}      &    9589.1419  &     3170.423     &     3.025  &         0.002        &     3374.632    &     1.58e+04     \\\\\n",
       "\\textbf{6435.0}      &    9723.4153  &     3042.211     &     3.196  &         0.001        &     3760.220    &     1.57e+04     \\\\\n",
       "\\textbf{6492.0}      &    6260.6289  &     3090.244     &     2.026  &         0.043        &      203.281    &     1.23e+04     \\\\\n",
       "\\textbf{6497.0}      &    2903.0101  &     3215.604     &     0.903  &         0.367        &    -3400.062    &     9206.082     \\\\\n",
       "\\textbf{6500.0}      &    8402.1659  &     7302.670     &     1.151  &         0.250        &    -5912.176    &     2.27e+04     \\\\\n",
       "\\textbf{6509.0}      &    8464.7557  &     3077.538     &     2.750  &         0.006        &     2432.313    &     1.45e+04     \\\\\n",
       "\\textbf{6527.0}      &    1.055e+04  &     3417.715     &     3.086  &         0.002        &     3848.357    &     1.72e+04     \\\\\n",
       "\\textbf{6528.0}      &    8661.1370  &     3343.926     &     2.590  &         0.010        &     2106.534    &     1.52e+04     \\\\\n",
       "\\textbf{6531.0}      &     -25.9974  &     3078.405     &    -0.008  &         0.993        &    -6060.139    &     6008.144     \\\\\n",
       "\\textbf{6532.0}      &    5885.9647  &     3095.765     &     1.901  &         0.057        &     -182.205    &      1.2e+04     \\\\\n",
       "\\textbf{6543.0}      &    9929.9352  &     3144.078     &     3.158  &         0.002        &     3767.065    &     1.61e+04     \\\\\n",
       "\\textbf{6548.0}      &    9593.2367  &     3169.925     &     3.026  &         0.002        &     3379.702    &     1.58e+04     \\\\\n",
       "\\textbf{6550.0}      &    9616.0676  &     3359.843     &     2.862  &         0.004        &     3030.264    &     1.62e+04     \\\\\n",
       "\\textbf{6552.0}      &    9638.8646  &     3275.149     &     2.943  &         0.003        &     3219.075    &     1.61e+04     \\\\\n",
       "\\textbf{6565.0}      &    4782.1084  &     3198.665     &     1.495  &         0.135        &    -1487.761    &     1.11e+04     \\\\\n",
       "\\textbf{6571.0}      &    9305.0305  &     3110.487     &     2.992  &         0.003        &     3208.004    &     1.54e+04     \\\\\n",
       "\\textbf{6573.0}      &    9082.6070  &     3092.638     &     2.937  &         0.003        &     3020.566    &     1.51e+04     \\\\\n",
       "\\textbf{6641.0}      &    7749.3873  &     5382.286     &     1.440  &         0.150        &    -2800.711    &     1.83e+04     \\\\\n",
       "\\textbf{6649.0}      &    1.053e+04  &     3159.208     &     3.335  &         0.001        &     4342.190    &     1.67e+04     \\\\\n",
       "\\textbf{6730.0}      &    8500.3738  &     3089.775     &     2.751  &         0.006        &     2443.946    &     1.46e+04     \\\\\n",
       "\\textbf{6731.0}      &    6466.6645  &     3134.642     &     2.063  &         0.039        &      322.291    &     1.26e+04     \\\\\n",
       "\\textbf{6742.0}      &    9550.9622  &     4649.788     &     2.054  &         0.040        &      436.672    &     1.87e+04     \\\\\n",
       "\\textbf{6745.0}      &    1.009e+04  &     3200.253     &     3.154  &         0.002        &     3820.559    &     1.64e+04     \\\\\n",
       "\\textbf{6756.0}      &    9834.8380  &     3160.429     &     3.112  &         0.002        &     3639.917    &      1.6e+04     \\\\\n",
       "\\textbf{6765.0}      &   -4414.1271  &     3022.513     &    -1.460  &         0.144        &    -1.03e+04    &     1510.458     \\\\\n",
       "\\textbf{6768.0}      &    1.156e+04  &     3234.772     &     3.575  &         0.000        &     5223.749    &     1.79e+04     \\\\\n",
       "\\textbf{6774.0}      &    -1.45e+04  &     3423.679     &    -4.235  &         0.000        &    -2.12e+04    &    -7789.918     \\\\\n",
       "\\textbf{6797.0}      &    1.062e+04  &     3560.156     &     2.982  &         0.003        &     3636.640    &     1.76e+04     \\\\\n",
       "\\textbf{6803.0}      &    9790.5158  &     3152.629     &     3.106  &         0.002        &     3610.883    &      1.6e+04     \\\\\n",
       "\\textbf{6821.0}      &    8916.7458  &     3111.850     &     2.865  &         0.004        &     2817.047    &      1.5e+04     \\\\\n",
       "\\textbf{6830.0}      &    8333.4226  &     3124.559     &     2.667  &         0.008        &     2208.812    &     1.45e+04     \\\\\n",
       "\\textbf{6845.0}      &    6582.6117  &     3033.170     &     2.170  &         0.030        &      637.138    &     1.25e+04     \\\\\n",
       "\\textbf{6848.0}      &    9265.8494  &     3380.966     &     2.741  &         0.006        &     2638.642    &     1.59e+04     \\\\\n",
       "\\textbf{6873.0}      &    5262.2493  &     3710.128     &     1.418  &         0.156        &    -2010.164    &     1.25e+04     \\\\\n",
       "\\textbf{6900.0}      &    7470.5391  &     3040.133     &     2.457  &         0.014        &     1511.417    &     1.34e+04     \\\\\n",
       "\\textbf{6908.0}      &    7574.9222  &     3037.300     &     2.494  &         0.013        &     1621.353    &     1.35e+04     \\\\\n",
       "\\textbf{6994.0}      &    8255.8589  &     3119.283     &     2.647  &         0.008        &     2141.590    &     1.44e+04     \\\\\n",
       "\\textbf{7045.0}      &    1844.4045  &     3736.226     &     0.494  &         0.622        &    -5479.167    &     9167.976     \\\\\n",
       "\\textbf{7065.0}      &    1.408e+04  &     3067.167     &     4.591  &         0.000        &     8070.340    &     2.01e+04     \\\\\n",
       "\\textbf{7085.0}      &     1.13e+04  &     3081.095     &     3.669  &         0.000        &     5264.878    &     1.73e+04     \\\\\n",
       "\\textbf{7107.0}      &    8052.7198  &     3211.390     &     2.508  &         0.012        &     1757.908    &     1.43e+04     \\\\\n",
       "\\textbf{7116.0}      &    1.032e+04  &     3179.999     &     3.245  &         0.001        &     4086.849    &     1.66e+04     \\\\\n",
       "\\textbf{7117.0}      &    1.055e+04  &     3949.975     &     2.671  &         0.008        &     2809.099    &     1.83e+04     \\\\\n",
       "\\textbf{7121.0}      &    8775.8315  &     3133.720     &     2.800  &         0.005        &     2633.264    &     1.49e+04     \\\\\n",
       "\\textbf{7127.0}      &    6420.8313  &     3243.158     &     1.980  &         0.048        &       63.750    &     1.28e+04     \\\\\n",
       "\\textbf{7139.0}      &    8727.2476  &     3099.294     &     2.816  &         0.005        &     2652.161    &     1.48e+04     \\\\\n",
       "\\textbf{7146.0}      &    9720.1875  &     3126.664     &     3.109  &         0.002        &     3591.451    &     1.58e+04     \\\\\n",
       "\\textbf{7163.0}      &    1.279e+04  &     3134.484     &     4.081  &         0.000        &     6646.405    &     1.89e+04     \\\\\n",
       "\\textbf{7180.0}      &    6160.9922  &     3087.312     &     1.996  &         0.046        &      109.392    &     1.22e+04     \\\\\n",
       "\\textbf{7183.0}      &    6762.3251  &     3128.386     &     2.162  &         0.031        &      630.214    &     1.29e+04     \\\\\n",
       "\\textbf{7228.0}      &    1.764e+04  &     3168.523     &     5.567  &         0.000        &     1.14e+04    &     2.38e+04     \\\\\n",
       "\\textbf{7232.0}      &    8154.6539  &     4154.025     &     1.963  &         0.050        &       12.134    &     1.63e+04     \\\\\n",
       "\\textbf{7250.0}      &    6766.0899  &     3377.887     &     2.003  &         0.045        &      144.918    &     1.34e+04     \\\\\n",
       "\\textbf{7257.0}      &    2.971e+04  &     3115.326     &     9.536  &         0.000        &     2.36e+04    &     3.58e+04     \\\\\n",
       "\\textbf{7260.0}      &    9150.8519  &     3085.180     &     2.966  &         0.003        &     3103.431    &     1.52e+04     \\\\\n",
       "\\textbf{7267.0}      &    8865.9508  &     3234.107     &     2.741  &         0.006        &     2526.610    &     1.52e+04     \\\\\n",
       "\\textbf{7268.0}      &    1505.3236  &     3301.821     &     0.456  &         0.648        &    -4966.748    &     7977.395     \\\\\n",
       "\\textbf{7281.0}      &    9732.5905  &     3991.005     &     2.439  &         0.015        &     1909.615    &     1.76e+04     \\\\\n",
       "\\textbf{7291.0}      &    7456.0581  &     3068.111     &     2.430  &         0.015        &     1442.096    &     1.35e+04     \\\\\n",
       "\\textbf{7343.0}      &    4436.4667  &     3573.654     &     1.241  &         0.214        &    -2568.438    &     1.14e+04     \\\\\n",
       "\\textbf{7346.0}      &    3238.1303  &     3079.092     &     1.052  &         0.293        &    -2797.357    &     9273.617     \\\\\n",
       "\\textbf{7401.0}      &    9349.7327  &     3138.244     &     2.979  &         0.003        &     3198.297    &     1.55e+04     \\\\\n",
       "\\textbf{7409.0}      &    8457.9972  &     3066.239     &     2.758  &         0.006        &     2447.702    &     1.45e+04     \\\\\n",
       "\\textbf{7420.0}      &    6215.9290  &     3018.667     &     2.059  &         0.039        &      298.884    &     1.21e+04     \\\\\n",
       "\\textbf{7435.0}      &    6751.7467  &     3158.717     &     2.137  &         0.033        &      560.181    &     1.29e+04     \\\\\n",
       "\\textbf{7466.0}      &    8553.9161  &     3269.146     &     2.617  &         0.009        &     2145.893    &      1.5e+04     \\\\\n",
       "\\textbf{7486.0}      &    1723.3876  &     3152.520     &     0.547  &         0.585        &    -4456.030    &     7902.805     \\\\\n",
       "\\textbf{7503.0}      &    9067.0759  &     4579.905     &     1.980  &         0.048        &       89.767    &      1.8e+04     \\\\\n",
       "\\textbf{7506.0}      &    9819.2990  &     3057.813     &     3.211  &         0.001        &     3825.522    &     1.58e+04     \\\\\n",
       "\\textbf{7537.0}      &    9430.8825  &     3170.373     &     2.975  &         0.003        &     3216.470    &     1.56e+04     \\\\\n",
       "\\textbf{7549.0}      &    7273.1684  &     3042.817     &     2.390  &         0.017        &     1308.786    &     1.32e+04     \\\\\n",
       "\\textbf{7554.0}      &    8897.8883  &     3135.083     &     2.838  &         0.005        &     2752.650    &      1.5e+04     \\\\\n",
       "\\textbf{7557.0}      &    6931.8955  &     3136.896     &     2.210  &         0.027        &      783.102    &     1.31e+04     \\\\\n",
       "\\textbf{7585.0}      &   -1.495e+04  &     3233.948     &    -4.624  &         0.000        &    -2.13e+04    &    -8613.193     \\\\\n",
       "\\textbf{7602.0}      &    8923.7108  &     3107.157     &     2.872  &         0.004        &     2833.211    &      1.5e+04     \\\\\n",
       "\\textbf{7620.0}      &    7129.5037  &     3384.544     &     2.106  &         0.035        &      495.283    &     1.38e+04     \\\\\n",
       "\\textbf{7636.0}      &    9304.1976  &     3123.447     &     2.979  &         0.003        &     3181.767    &     1.54e+04     \\\\\n",
       "\\textbf{7646.0}      &    9393.7191  &     3155.786     &     2.977  &         0.003        &     3207.900    &     1.56e+04     \\\\\n",
       "\\textbf{7658.0}      &    6253.3200  &     3022.501     &     2.069  &         0.039        &      328.759    &     1.22e+04     \\\\\n",
       "\\textbf{7683.0}      &    1.026e+04  &     3321.592     &     3.090  &         0.002        &     3753.317    &     1.68e+04     \\\\\n",
       "\\textbf{7685.0}      &    8767.4758  &     3234.267     &     2.711  &         0.007        &     2427.821    &     1.51e+04     \\\\\n",
       "\\textbf{7692.0}      &    5012.1399  &     3004.385     &     1.668  &         0.095        &     -876.911    &     1.09e+04     \\\\\n",
       "\\textbf{7762.0}      &    9366.4810  &     3091.150     &     3.030  &         0.002        &     3307.357    &     1.54e+04     \\\\\n",
       "\\textbf{7772.0}      &   -2443.1777  &     3001.938     &    -0.814  &         0.416        &    -8327.432    &     3441.076     \\\\\n",
       "\\textbf{7773.0}      &    8790.9387  &     3128.430     &     2.810  &         0.005        &     2658.740    &     1.49e+04     \\\\\n",
       "\\textbf{7777.0}      &    5620.5173  &     3072.543     &     1.829  &         0.067        &     -402.134    &     1.16e+04     \\\\\n",
       "\\textbf{7835.0}      &    9969.7493  &     3156.641     &     3.158  &         0.002        &     3782.253    &     1.62e+04     \\\\\n",
       "\\textbf{7873.0}      &    1753.2079  &     3096.978     &     0.566  &         0.571        &    -4317.340    &     7823.755     \\\\\n",
       "\\textbf{7883.0}      &    6648.1472  &     3032.795     &     2.192  &         0.028        &      703.409    &     1.26e+04     \\\\\n",
       "\\textbf{7904.0}      &    5757.8853  &     3042.665     &     1.892  &         0.058        &     -206.199    &     1.17e+04     \\\\\n",
       "\\textbf{7906.0}      &     1.24e+04  &     3192.708     &     3.884  &         0.000        &     6143.287    &     1.87e+04     \\\\\n",
       "\\textbf{7921.0}      &    9521.8682  &     3078.497     &     3.093  &         0.002        &     3487.546    &     1.56e+04     \\\\\n",
       "\\textbf{7923.0}      &    8424.9184  &     3204.025     &     2.629  &         0.009        &     2144.542    &     1.47e+04     \\\\\n",
       "\\textbf{7935.0}      &    6955.2953  &     3049.615     &     2.281  &         0.023        &      977.586    &     1.29e+04     \\\\\n",
       "\\textbf{7938.0}      &    7442.0282  &     3074.413     &     2.421  &         0.016        &     1415.711    &     1.35e+04     \\\\\n",
       "\\textbf{7985.0}      &   -1.012e+04  &     2991.410     &    -3.382  &         0.001        &     -1.6e+04    &    -4253.115     \\\\\n",
       "\\textbf{8014.0}      &    8157.1933  &     3199.316     &     2.550  &         0.011        &     1886.047    &     1.44e+04     \\\\\n",
       "\\textbf{8030.0}      &    1.069e+04  &     3172.643     &     3.370  &         0.001        &     4474.298    &     1.69e+04     \\\\\n",
       "\\textbf{8046.0}      &    2100.8383  &     3150.625     &     0.667  &         0.505        &    -4074.865    &     8276.542     \\\\\n",
       "\\textbf{8047.0}      &    8930.9265  &     3638.445     &     2.455  &         0.014        &     1799.022    &     1.61e+04     \\\\\n",
       "\\textbf{8062.0}      &    7656.2181  &     3156.951     &     2.425  &         0.015        &     1468.115    &     1.38e+04     \\\\\n",
       "\\textbf{8068.0}      &   -6316.9730  &     3342.935     &    -1.890  &         0.059        &    -1.29e+04    &      235.687     \\\\\n",
       "\\textbf{8087.0}      &   -1486.8453  &     3165.038     &    -0.470  &         0.639        &    -7690.801    &     4717.110     \\\\\n",
       "\\textbf{8095.0}      &    9092.3282  &     3091.617     &     2.941  &         0.003        &     3032.289    &     1.52e+04     \\\\\n",
       "\\textbf{8096.0}      &    9888.7824  &     3156.253     &     3.133  &         0.002        &     3702.048    &     1.61e+04     \\\\\n",
       "\\textbf{8109.0}      &    9592.9423  &     3114.281     &     3.080  &         0.002        &     3488.479    &     1.57e+04     \\\\\n",
       "\\textbf{8123.0}      &    5332.5335  &     3050.740     &     1.748  &         0.080        &     -647.380    &     1.13e+04     \\\\\n",
       "\\textbf{8150.0}      &    1.002e+04  &     3143.649     &     3.186  &         0.001        &     3854.817    &     1.62e+04     \\\\\n",
       "\\textbf{8163.0}      &    7135.9390  &     3109.533     &     2.295  &         0.022        &     1040.782    &     1.32e+04     \\\\\n",
       "\\textbf{8176.0}      &    5907.8263  &     3555.339     &     1.662  &         0.097        &    -1061.178    &     1.29e+04     \\\\\n",
       "\\textbf{8202.0}      &    7093.7298  &     3180.484     &     2.230  &         0.026        &      859.499    &     1.33e+04     \\\\\n",
       "\\textbf{8214.0}      &    5649.2334  &     3049.389     &     1.853  &         0.064        &     -328.032    &     1.16e+04     \\\\\n",
       "\\textbf{8215.0}      &    3589.9167  &     3151.067     &     1.139  &         0.255        &    -2586.652    &     9766.486     \\\\\n",
       "\\textbf{8219.0}      &    1.015e+04  &     3202.762     &     3.170  &         0.002        &     3875.511    &     1.64e+04     \\\\\n",
       "\\textbf{8247.0}      &    5130.0212  &     3044.372     &     1.685  &         0.092        &     -837.409    &     1.11e+04     \\\\\n",
       "\\textbf{8253.0}      &   -2202.2944  &     3019.455     &    -0.729  &         0.466        &    -8120.885    &     3716.296     \\\\\n",
       "\\textbf{8290.0}      &    6667.9159  &     3236.736     &     2.060  &         0.039        &      323.422    &      1.3e+04     \\\\\n",
       "\\textbf{8293.0}      &    7346.0103  &     3043.721     &     2.413  &         0.016        &     1379.855    &     1.33e+04     \\\\\n",
       "\\textbf{8304.0}      &    9985.5502  &     3093.631     &     3.228  &         0.001        &     3921.565    &      1.6e+04     \\\\\n",
       "\\textbf{8334.0}      &    9750.2882  &     3258.525     &     2.992  &         0.003        &     3363.085    &     1.61e+04     \\\\\n",
       "\\textbf{8348.0}      &    9196.9702  &     3156.426     &     2.914  &         0.004        &     3009.895    &     1.54e+04     \\\\\n",
       "\\textbf{8357.0}      &    8782.9091  &     3076.771     &     2.855  &         0.004        &     2751.971    &     1.48e+04     \\\\\n",
       "\\textbf{8358.0}      &    6761.4473  &     3044.175     &     2.221  &         0.026        &      794.402    &     1.27e+04     \\\\\n",
       "\\textbf{8446.0}      &   -3983.6259  &     3374.667     &    -1.180  &         0.238        &    -1.06e+04    &     2631.234     \\\\\n",
       "\\textbf{8460.0}      &    1.098e+04  &     3537.306     &     3.105  &         0.002        &     4051.291    &     1.79e+04     \\\\\n",
       "\\textbf{8463.0}      &    8838.8993  &     3110.041     &     2.842  &         0.004        &     2742.747    &     1.49e+04     \\\\\n",
       "\\textbf{8479.0}      &    7883.3944  &     3750.123     &     2.102  &         0.036        &      532.584    &     1.52e+04     \\\\\n",
       "\\textbf{8530.0}      &    2.055e+04  &     3130.331     &     6.565  &         0.000        &     1.44e+04    &     2.67e+04     \\\\\n",
       "\\textbf{8536.0}      &    5321.9245  &     3023.926     &     1.760  &         0.078        &     -605.430    &     1.12e+04     \\\\\n",
       "\\textbf{8543.0}      &     2.91e+04  &     3348.461     &     8.689  &         0.000        &     2.25e+04    &     3.57e+04     \\\\\n",
       "\\textbf{8549.0}      &     288.7530  &     3183.883     &     0.091  &         0.928        &    -5952.140    &     6529.646     \\\\\n",
       "\\textbf{8551.0}      &    9857.0564  &     3160.749     &     3.119  &         0.002        &     3661.508    &     1.61e+04     \\\\\n",
       "\\textbf{8559.0}      &    6877.9877  &     3301.144     &     2.084  &         0.037        &      407.244    &     1.33e+04     \\\\\n",
       "\\textbf{8573.0}      &    2363.2893  &     3354.437     &     0.705  &         0.481        &    -4211.916    &     8938.495     \\\\\n",
       "\\textbf{8606.0}      &    9996.9128  &     3074.136     &     3.252  &         0.001        &     3971.140    &      1.6e+04     \\\\\n",
       "\\textbf{8607.0}      &    1.008e+04  &     3199.827     &     3.150  &         0.002        &     3805.949    &     1.64e+04     \\\\\n",
       "\\textbf{8648.0}      &    8703.5470  &     3078.394     &     2.827  &         0.005        &     2669.428    &     1.47e+04     \\\\\n",
       "\\textbf{8657.0}      &    4027.2377  &     3062.602     &     1.315  &         0.189        &    -1975.927    &        1e+04     \\\\\n",
       "\\textbf{8675.0}      &    9234.4663  &     4423.761     &     2.087  &         0.037        &      563.222    &     1.79e+04     \\\\\n",
       "\\textbf{8681.0}      &    5022.8681  &     2990.914     &     1.679  &         0.093        &     -839.778    &     1.09e+04     \\\\\n",
       "\\textbf{8687.0}      &    6865.4753  &     3330.626     &     2.061  &         0.039        &      336.943    &     1.34e+04     \\\\\n",
       "\\textbf{8692.0}      &    6724.1365  &     3040.648     &     2.211  &         0.027        &      764.005    &     1.27e+04     \\\\\n",
       "\\textbf{8699.0}      &    9368.7656  &     3104.748     &     3.018  &         0.003        &     3282.988    &     1.55e+04     \\\\\n",
       "\\textbf{8717.0}      &    1.007e+04  &     3139.976     &     3.206  &         0.001        &     3910.883    &     1.62e+04     \\\\\n",
       "\\textbf{8759.0}      &    6032.6233  &     3129.293     &     1.928  &         0.054        &     -101.267    &     1.22e+04     \\\\\n",
       "\\textbf{8762.0}      &    1.005e+04  &     3163.140     &     3.177  &         0.001        &     3847.994    &     1.62e+04     \\\\\n",
       "\\textbf{8819.0}      &     1.08e+04  &     3255.498     &     3.319  &         0.001        &     4422.683    &     1.72e+04     \\\\\n",
       "\\textbf{8850.0}      &    9180.3611  &     3099.787     &     2.962  &         0.003        &     3104.308    &     1.53e+04     \\\\\n",
       "\\textbf{8852.0}      &    9396.6270  &     3137.971     &     2.994  &         0.003        &     3245.728    &     1.55e+04     \\\\\n",
       "\\textbf{8859.0}      &    9958.6402  &     3172.396     &     3.139  &         0.002        &     3740.263    &     1.62e+04     \\\\\n",
       "\\textbf{8867.0}      &    2878.0832  &     3278.995     &     0.878  &         0.380        &    -3549.244    &     9305.411     \\\\\n",
       "\\textbf{8881.0}      &    7955.4694  &     3090.496     &     2.574  &         0.010        &     1897.629    &      1.4e+04     \\\\\n",
       "\\textbf{8958.0}      &    7012.8960  &     3062.779     &     2.290  &         0.022        &     1009.384    &      1.3e+04     \\\\\n",
       "\\textbf{8972.0}      &   -1.071e+04  &     3041.870     &    -3.522  &         0.000        &    -1.67e+04    &    -4751.155     \\\\\n",
       "\\textbf{8990.0}      &     237.8112  &     3143.690     &     0.076  &         0.940        &    -5924.298    &     6399.920     \\\\\n",
       "\\textbf{9004.0}      &    1.047e+04  &     3372.333     &     3.105  &         0.002        &     3861.201    &     1.71e+04     \\\\\n",
       "\\textbf{9016.0}      &    7304.7780  &     3039.777     &     2.403  &         0.016        &     1346.353    &     1.33e+04     \\\\\n",
       "\\textbf{9048.0}      &    6403.1328  &     3012.256     &     2.126  &         0.034        &      498.654    &     1.23e+04     \\\\\n",
       "\\textbf{9051.0}      &    1965.6327  &     3401.698     &     0.578  &         0.563        &    -4702.212    &     8633.478     \\\\\n",
       "\\textbf{9071.0}      &    6269.8156  &     3082.415     &     2.034  &         0.042        &      227.814    &     1.23e+04     \\\\\n",
       "\\textbf{9112.0}      &    5595.9253  &     3023.816     &     1.851  &         0.064        &     -331.214    &     1.15e+04     \\\\\n",
       "\\textbf{9114.0}      &    3815.5590  &     3108.453     &     1.227  &         0.220        &    -2277.481    &     9908.599     \\\\\n",
       "\\textbf{9132.0}      &    9293.0465  &     4448.428     &     2.089  &         0.037        &      573.452    &      1.8e+04     \\\\\n",
       "\\textbf{9173.0}      &    8593.1758  &     3436.473     &     2.501  &         0.012        &     1857.168    &     1.53e+04     \\\\\n",
       "\\textbf{9180.0}      &    9960.1426  &     3212.901     &     3.100  &         0.002        &     3662.368    &     1.63e+04     \\\\\n",
       "\\textbf{9186.0}      &    8369.7659  &     3096.656     &     2.703  &         0.007        &     2299.850    &     1.44e+04     \\\\\n",
       "\\textbf{9191.0}      &    5236.9855  &     4093.728     &     1.279  &         0.201        &    -2787.343    &     1.33e+04     \\\\\n",
       "\\textbf{9216.0}      &    3177.1857  &     2977.132     &     1.067  &         0.286        &    -2658.445    &     9012.817     \\\\\n",
       "\\textbf{9217.0}      &     345.3862  &     2964.765     &     0.116  &         0.907        &    -5466.004    &     6156.777     \\\\\n",
       "\\textbf{9225.0}      &    9669.4538  &     3102.778     &     3.116  &         0.002        &     3587.537    &     1.58e+04     \\\\\n",
       "\\textbf{9230.0}      &    1.017e+04  &     3631.753     &     2.800  &         0.005        &     3049.714    &     1.73e+04     \\\\\n",
       "\\textbf{9259.0}      &    1.082e+04  &     3195.981     &     3.387  &         0.001        &     4559.228    &     1.71e+04     \\\\\n",
       "\\textbf{9293.0}      &    9826.5015  &     3129.155     &     3.140  &         0.002        &     3692.884    &      1.6e+04     \\\\\n",
       "\\textbf{9299.0}      &    4757.1723  &     3068.861     &     1.550  &         0.121        &    -1258.262    &     1.08e+04     \\\\\n",
       "\\textbf{9308.0}      &    5449.7947  &     3213.596     &     1.696  &         0.090        &     -849.341    &     1.17e+04     \\\\\n",
       "\\textbf{9311.0}      &    6091.4889  &     4097.403     &     1.487  &         0.137        &    -1940.043    &     1.41e+04     \\\\\n",
       "\\textbf{9313.0}      &    4314.0795  &     3025.653     &     1.426  &         0.154        &    -1616.660    &     1.02e+04     \\\\\n",
       "\\textbf{9325.0}      &    8571.9786  &     3107.010     &     2.759  &         0.006        &     2481.767    &     1.47e+04     \\\\\n",
       "\\textbf{9332.0}      &    7832.2604  &     3050.287     &     2.568  &         0.010        &     1853.235    &     1.38e+04     \\\\\n",
       "\\textbf{9340.0}      &   -1.284e+04  &     4266.321     &    -3.009  &         0.003        &    -2.12e+04    &    -4476.619     \\\\\n",
       "\\textbf{9372.0}      &    1.011e+04  &     3506.811     &     2.883  &         0.004        &     3237.019    &      1.7e+04     \\\\\n",
       "\\textbf{9411.0}      &    6355.8057  &     3158.698     &     2.012  &         0.044        &      164.278    &     1.25e+04     \\\\\n",
       "\\textbf{9459.0}      &    4594.2014  &     3082.822     &     1.490  &         0.136        &    -1448.598    &     1.06e+04     \\\\\n",
       "\\textbf{9465.0}      &    1.241e+04  &     3064.660     &     4.049  &         0.000        &     6402.210    &     1.84e+04     \\\\\n",
       "\\textbf{9472.0}      &    4269.6200  &     2986.010     &     1.430  &         0.153        &    -1583.412    &     1.01e+04     \\\\\n",
       "\\textbf{9483.0}      &    -216.2534  &     3044.454     &    -0.071  &         0.943        &    -6183.846    &     5751.340     \\\\\n",
       "\\textbf{9563.0}      &   -1.792e+04  &     4211.763     &    -4.256  &         0.000        &    -2.62e+04    &    -9668.157     \\\\\n",
       "\\textbf{9590.0}      &    7000.0639  &     3121.546     &     2.242  &         0.025        &      881.359    &     1.31e+04     \\\\\n",
       "\\textbf{9598.0}      &    4487.5163  &     3762.106     &     1.193  &         0.233        &    -2886.783    &     1.19e+04     \\\\\n",
       "\\textbf{9599.0}      &    3721.4346  &     3033.177     &     1.227  &         0.220        &    -2224.052    &     9666.921     \\\\\n",
       "\\textbf{9602.0}      &    3023.8387  &     4071.341     &     0.743  &         0.458        &    -4956.608    &      1.1e+04     \\\\\n",
       "\\textbf{9619.0}      &    1.003e+04  &     3221.664     &     3.113  &         0.002        &     3715.041    &     1.63e+04     \\\\\n",
       "\\textbf{9643.0}      &    6673.9738  &     3111.149     &     2.145  &         0.032        &      575.650    &     1.28e+04     \\\\\n",
       "\\textbf{9650.0}      &    5948.1118  &     3049.133     &     1.951  &         0.051        &      -28.652    &     1.19e+04     \\\\\n",
       "\\textbf{9653.0}      &   -5201.1049  &     5261.293     &    -0.989  &         0.323        &    -1.55e+04    &     5111.828     \\\\\n",
       "\\textbf{9667.0}      &    5978.4574  &     3032.475     &     1.971  &         0.049        &       34.345    &     1.19e+04     \\\\\n",
       "\\textbf{9698.0}      &    8360.8550  &     3113.996     &     2.685  &         0.007        &     2256.949    &     1.45e+04     \\\\\n",
       "\\textbf{9699.0}      &    9695.4952  &     3081.120     &     3.147  &         0.002        &     3656.031    &     1.57e+04     \\\\\n",
       "\\textbf{9719.0}      &    1673.7655  &     2960.502     &     0.565  &         0.572        &    -4129.267    &     7476.798     \\\\\n",
       "\\textbf{9742.0}      &    1462.5312  &     3123.254     &     0.468  &         0.640        &    -4659.520    &     7584.583     \\\\\n",
       "\\textbf{9761.0}      &    9161.1397  &     3123.661     &     2.933  &         0.003        &     3038.290    &     1.53e+04     \\\\\n",
       "\\textbf{9771.0}      &    2489.7585  &     2963.761     &     0.840  &         0.401        &    -3319.664    &     8299.181     \\\\\n",
       "\\textbf{9772.0}      &    9912.5502  &     3147.378     &     3.149  &         0.002        &     3743.211    &     1.61e+04     \\\\\n",
       "\\textbf{9778.0}      &    7202.1314  &     3024.139     &     2.382  &         0.017        &     1274.359    &     1.31e+04     \\\\\n",
       "\\textbf{9799.0}      &    1707.7949  &     3109.095     &     0.549  &         0.583        &    -4386.503    &     7802.093     \\\\\n",
       "\\textbf{9815.0}      &    9088.3459  &     3223.230     &     2.820  &         0.005        &     2770.327    &     1.54e+04     \\\\\n",
       "\\textbf{9818.0}      &   -3.452e+04  &     3202.790     &   -10.779  &         0.000        &    -4.08e+04    &    -2.82e+04     \\\\\n",
       "\\textbf{9837.0}      &    1.056e+04  &     3244.249     &     3.254  &         0.001        &     4198.886    &     1.69e+04     \\\\\n",
       "\\textbf{9922.0}      &    2994.0482  &     3000.566     &     0.998  &         0.318        &    -2887.517    &     8875.613     \\\\\n",
       "\\textbf{9954.0}      &    5697.1012  &     3683.632     &     1.547  &         0.122        &    -1523.378    &     1.29e+04     \\\\\n",
       "\\textbf{9963.0}      &    7227.4542  &     3090.223     &     2.339  &         0.019        &     1170.149    &     1.33e+04     \\\\\n",
       "\\textbf{9988.0}      &    1.018e+04  &     3189.909     &     3.192  &         0.001        &     3928.483    &     1.64e+04     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21744.289 & \\textbf{  Durbin-Watson:     } &      0.548    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 50237524.155  \\\\\n",
       "\\textbf{Skew:}          &   10.223  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  302.434  & \\textbf{  Cond. No.          } &   2.00e+07    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large,  2e+07. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.665\n",
       "Model:                            OLS   Adj. R-squared:                  0.645\n",
       "Method:                 Least Squares   F-statistic:                     33.22\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        01:58:42   Log-Likelihood:            -1.4157e+05\n",
       "No. Observations:               13385   AIC:                         2.847e+05\n",
       "Df Residuals:                   12629   BIC:                         2.903e+05\n",
       "Df Model:                         755                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const       -9940.5765   2385.347     -4.167      0.000   -1.46e+04   -5264.935\n",
       "gspilltecIV     0.1002      0.027      3.746      0.000       0.048       0.153\n",
       "gspillsicIV     0.3399      0.049      6.902      0.000       0.243       0.436\n",
       "pat_count     -30.6018      1.838    -16.652      0.000     -34.204     -26.999\n",
       "rsales          0.7812      0.037     21.055      0.000       0.708       0.854\n",
       "rppent          0.6108      0.084      7.234      0.000       0.445       0.776\n",
       "emp            18.0641      7.147      2.527      0.012       4.054      32.074\n",
       "rxrd           18.5941      0.614     30.295      0.000      17.391      19.797\n",
       "1981         -425.5119    618.180     -0.688      0.491   -1637.239     786.215\n",
       "1982         -295.2372    616.239     -0.479      0.632   -1503.159     912.685\n",
       "1983         -294.1870    609.557     -0.483      0.629   -1489.011     900.637\n",
       "1984         -812.2257    607.964     -1.336      0.182   -2003.927     379.476\n",
       "1985         -929.1950    609.921     -1.523      0.128   -2124.732     266.343\n",
       "1986        -1184.5891    607.667     -1.949      0.051   -2375.708       6.530\n",
       "1987        -1372.3041    607.937     -2.257      0.024   -2563.953    -180.655\n",
       "1988        -1686.4878    609.953     -2.765      0.006   -2882.088    -490.888\n",
       "1989        -1550.4891    610.972     -2.538      0.011   -2748.088    -352.891\n",
       "1990        -2052.0735    612.118     -3.352      0.001   -3251.918    -852.228\n",
       "1991        -1629.8854    614.732     -2.651      0.008   -2834.854    -424.917\n",
       "1992        -1754.6866    617.583     -2.841      0.005   -2965.243    -544.130\n",
       "1993        -1685.0176    621.230     -2.712      0.007   -2902.723    -467.312\n",
       "1994        -1916.7490    627.407     -3.055      0.002   -3146.563    -686.935\n",
       "1995        -1380.6021    636.986     -2.167      0.030   -2629.191    -132.013\n",
       "1996        -1098.4502    651.099     -1.687      0.092   -2374.702     177.802\n",
       "1997         -692.3174    667.994     -1.036      0.300   -2001.686     617.051\n",
       "1998         -575.0406    687.698     -0.836      0.403   -1923.033     772.951\n",
       "1999          142.3167    710.226      0.200      0.841   -1249.835    1534.468\n",
       "2000         -207.4580    738.322     -0.281      0.779   -1654.682    1239.766\n",
       "2001        -2283.3834    768.945     -2.970      0.003   -3790.633    -776.134\n",
       "10005.0      8596.1312   3076.471      2.794      0.005    2565.781    1.46e+04\n",
       "10006.0      8240.3993   3516.122      2.344      0.019    1348.266    1.51e+04\n",
       "10008.0      7558.3544   3040.531      2.486      0.013    1598.452    1.35e+04\n",
       "10016.0      8488.3062   3060.494      2.774      0.006    2489.273    1.45e+04\n",
       "10030.0      9963.3568   3135.789      3.177      0.001    3816.734    1.61e+04\n",
       "1004.0       9403.1634   3133.966      3.000      0.003    3260.115    1.55e+04\n",
       "10056.0      7589.4824   3061.708      2.479      0.013    1588.069    1.36e+04\n",
       "10085.0      4090.3185   2997.861      1.364      0.172   -1785.945    9966.582\n",
       "10092.0      9204.1913   5393.483      1.707      0.088   -1367.854    1.98e+04\n",
       "10097.0      2206.4047   2993.452      0.737      0.461   -3661.215    8074.024\n",
       "1010.0       8207.1099   5379.135      1.526      0.127   -2336.812    1.88e+04\n",
       "10109.0      1.097e+04   3193.217      3.436      0.001    4711.604    1.72e+04\n",
       "10115.0      7957.3563   3058.537      2.602      0.009    1962.159     1.4e+04\n",
       "10124.0       1.11e+04   3199.490      3.470      0.001    4829.168    1.74e+04\n",
       "1013.0       4769.1715   2983.003      1.599      0.110   -1077.968    1.06e+04\n",
       "10150.0      3170.2099   3465.956      0.915      0.360   -3623.590    9964.010\n",
       "10159.0      3378.9006   4384.246      0.771      0.441   -5214.887     1.2e+04\n",
       "10174.0      1.021e+04   3389.760      3.012      0.003    3565.375    1.69e+04\n",
       "10185.0      8313.1068   3338.064      2.490      0.013    1769.995    1.49e+04\n",
       "10195.0      2593.7222   3141.189      0.826      0.409   -3563.486    8750.931\n",
       "10198.0      9933.5369   3131.022      3.173      0.002    3796.259    1.61e+04\n",
       "10215.0       1.08e+04   3193.341      3.381      0.001    4537.838    1.71e+04\n",
       "10232.0      6925.4903   3326.104      2.082      0.037     405.821    1.34e+04\n",
       "10236.0      9750.4628   3137.458      3.108      0.002    3600.569    1.59e+04\n",
       "10286.0      8274.2845   3121.219      2.651      0.008    2156.221    1.44e+04\n",
       "10301.0     -1.476e+04   3014.308     -4.895      0.000   -2.07e+04   -8846.626\n",
       "10312.0      9198.1294   3124.523      2.944      0.003    3073.591    1.53e+04\n",
       "10332.0      3868.1997   4061.176      0.952      0.341   -4092.322    1.18e+04\n",
       "1036.0       7245.0618   3261.623      2.221      0.026     851.786    1.36e+04\n",
       "10374.0      8066.1980   3065.104      2.632      0.009    2058.129    1.41e+04\n",
       "10386.0      4140.6626   2986.789      1.386      0.166   -1713.897    9995.222\n",
       "10391.0      1064.5542   3026.844      0.352      0.725   -4868.521    6997.629\n",
       "10407.0      4895.6611   3017.572      1.622      0.105   -1019.237    1.08e+04\n",
       "10420.0      8765.7651   3082.897      2.843      0.004    2722.820    1.48e+04\n",
       "10422.0      6959.5604   3252.126      2.140      0.032     584.899    1.33e+04\n",
       "10426.0      9660.2255   3299.137      2.928      0.003    3193.415    1.61e+04\n",
       "10441.0      1.026e+04   3164.254      3.241      0.001    4053.796    1.65e+04\n",
       "1045.0      -1654.6262   3243.965     -0.510      0.610   -8013.291    4704.038\n",
       "10453.0      4005.2538   2991.151      1.339      0.181   -1857.856    9868.364\n",
       "10482.0      -1.46e+04   3475.563     -4.201      0.000   -2.14e+04   -7789.183\n",
       "10498.0      9301.1887   3158.846      2.944      0.003    3109.370    1.55e+04\n",
       "10499.0       290.8155   3069.162      0.095      0.925   -5725.207    6306.838\n",
       "10511.0      1.071e+04   3287.724      3.258      0.001    4268.117    1.72e+04\n",
       "10519.0     -5675.1567   2981.921     -1.903      0.057   -1.15e+04     169.862\n",
       "10530.0      5301.0836   3009.281      1.762      0.078    -597.565    1.12e+04\n",
       "10537.0      6186.0975   3392.545      1.823      0.068    -463.806    1.28e+04\n",
       "10540.0      7576.5476   3032.964      2.498      0.012    1631.477    1.35e+04\n",
       "10541.0      8964.6580   3229.126      2.776      0.006    2635.080    1.53e+04\n",
       "10550.0      6593.4131   6048.079      1.090      0.276   -5261.741    1.84e+04\n",
       "10553.0      3851.8113   3153.072      1.222      0.222   -2328.689       1e+04\n",
       "10565.0      1.053e+04   3138.295      3.355      0.001    4378.941    1.67e+04\n",
       "10580.0       1.08e+04   3213.498      3.360      0.001    4498.657    1.71e+04\n",
       "10581.0      7665.2631   3106.263      2.468      0.014    1576.515    1.38e+04\n",
       "10588.0      1176.5368   2963.147      0.397      0.691   -4631.681    6984.754\n",
       "10597.0      8900.5001   3128.719      2.845      0.004    2767.736     1.5e+04\n",
       "10599.0      9271.0496   3139.906      2.953      0.003    3116.357    1.54e+04\n",
       "10618.0      8556.4419   3093.780      2.766      0.006    2492.162    1.46e+04\n",
       "10656.0      8664.1385   3068.114      2.824      0.005    2650.168    1.47e+04\n",
       "10658.0      8540.1624   3064.938      2.786      0.005    2532.418    1.45e+04\n",
       "10726.0      1.264e+04   3268.775      3.866      0.000    6231.058     1.9e+04\n",
       "10734.0      8389.8503   3727.109      2.251      0.024    1084.150    1.57e+04\n",
       "10735.0      1.002e+04   3173.342      3.159      0.002    3803.931    1.62e+04\n",
       "10764.0      1.051e+04   3276.657      3.207      0.001    4084.421    1.69e+04\n",
       "10777.0      8530.3531   3067.334      2.781      0.005    2517.913    1.45e+04\n",
       "1078.0       5453.6061   3082.313      1.769      0.077    -588.196    1.15e+04\n",
       "10793.0      8093.7684   3137.200      2.580      0.010    1944.380    1.42e+04\n",
       "10816.0      7496.2748   3087.546      2.428      0.015    1444.216    1.35e+04\n",
       "10839.0      9481.3260   3097.020      3.061      0.002    3410.697    1.56e+04\n",
       "10857.0     -2182.7305   3058.140     -0.714      0.475   -8177.148    3811.687\n",
       "10867.0      4643.9565   3339.296      1.391      0.164   -1901.571    1.12e+04\n",
       "10906.0      8932.4951   3098.783      2.883      0.004    2858.410     1.5e+04\n",
       "10950.0      9034.0200   4140.834      2.182      0.029     917.357    1.72e+04\n",
       "10983.0     -2.383e+04   3227.575     -7.383      0.000   -3.02e+04   -1.75e+04\n",
       "1099.0       8536.2893   3106.895      2.748      0.006    2446.304    1.46e+04\n",
       "10991.0      6926.2901   3614.928      1.916      0.055    -159.519     1.4e+04\n",
       "11012.0      7880.6911   3137.824      2.512      0.012    1730.080     1.4e+04\n",
       "11038.0      3251.3206   3219.125      1.010      0.313   -3058.654    9561.295\n",
       "1104.0       9488.5996   3138.344      3.023      0.003    3336.969    1.56e+04\n",
       "11060.0      9235.2591   3132.146      2.949      0.003    3095.777    1.54e+04\n",
       "11094.0      8684.5592   3089.265      2.811      0.005    2629.130    1.47e+04\n",
       "11096.0      6740.1208   3028.318      2.226      0.026     804.158    1.27e+04\n",
       "11113.0      9316.9328   3367.569      2.767      0.006    2715.986    1.59e+04\n",
       "1115.0       7875.2019   3125.463      2.520      0.012    1748.819     1.4e+04\n",
       "11161.0      6631.5622   3048.603      2.175      0.030     655.838    1.26e+04\n",
       "11225.0      1.031e+04   3223.600      3.198      0.001    3991.623    1.66e+04\n",
       "11228.0      1.001e+04   3111.122      3.217      0.001    3909.102    1.61e+04\n",
       "11236.0      4820.4345   4564.449      1.056      0.291   -4126.579    1.38e+04\n",
       "11288.0       726.5183   3139.333      0.231      0.817   -5427.050    6880.087\n",
       "11312.0       624.6433   3085.276      0.202      0.840   -5422.965    6672.252\n",
       "11361.0      8139.1393   3054.443      2.665      0.008    2151.966    1.41e+04\n",
       "11399.0      1883.4788   2984.861      0.631      0.528   -3967.302    7734.260\n",
       "114303.0    -8981.4766   5402.255     -1.663      0.096   -1.96e+04    1607.764\n",
       "11456.0      3026.3818   3078.672      0.983      0.326   -3008.282    9061.046\n",
       "11465.0      4113.0085   3092.455      1.330      0.184   -1948.673    1.02e+04\n",
       "11502.0      9343.9264   3174.590      2.943      0.003    3121.249    1.56e+04\n",
       "11506.0      4190.2588   3073.981      1.363      0.173   -1835.210    1.02e+04\n",
       "11537.0      9104.8465   3090.809      2.946      0.003    3046.392    1.52e+04\n",
       "11566.0      1.069e+04   3188.150      3.352      0.001    4436.526    1.69e+04\n",
       "11573.0      8122.9750   3053.915      2.660      0.008    2136.837    1.41e+04\n",
       "11580.0      4660.7676   3401.528      1.370      0.171   -2006.744    1.13e+04\n",
       "11600.0      9955.4797   3181.578      3.129      0.002    3719.104    1.62e+04\n",
       "11609.0      1.339e+04   3141.316      4.263      0.000    7234.942    1.95e+04\n",
       "1161.0      -1142.7795   2963.636     -0.386      0.700   -6951.956    4666.397\n",
       "11636.0     -1.033e+04   3195.420     -3.234      0.001   -1.66e+04   -4069.451\n",
       "11670.0      1.045e+04   3195.585      3.270      0.001    4186.263    1.67e+04\n",
       "11678.0      -172.3965   3111.159     -0.055      0.956   -6270.740    5925.947\n",
       "11682.0      8157.6451   3195.335      2.553      0.011    1894.303    1.44e+04\n",
       "11694.0      9735.1845   3265.189      2.982      0.003    3334.919    1.61e+04\n",
       "11720.0      2342.1807   3978.963      0.589      0.556   -5457.192    1.01e+04\n",
       "11721.0     -3708.6926   3442.247     -1.077      0.281   -1.05e+04    3038.635\n",
       "11722.0      7939.5465   3308.424      2.400      0.016    1454.532    1.44e+04\n",
       "11793.0      6392.4048   6063.166      1.054      0.292   -5492.321    1.83e+04\n",
       "11797.0      1.098e+04   3552.518      3.091      0.002    4015.910    1.79e+04\n",
       "11914.0      9253.8306   3755.065      2.464      0.014    1893.333    1.66e+04\n",
       "1209.0       6916.3882   3021.090      2.289      0.022     994.593    1.28e+04\n",
       "12136.0     -5842.3430   3290.133     -1.776      0.076   -1.23e+04     606.816\n",
       "12141.0      8.981e+04   3291.619     27.284      0.000    8.34e+04    9.63e+04\n",
       "12181.0      6468.7957   4281.785      1.511      0.131   -1924.153    1.49e+04\n",
       "12215.0      -753.1168   3219.104     -0.234      0.815   -7063.050    5556.816\n",
       "12216.0      3866.8554   3252.277      1.189      0.234   -2508.101    1.02e+04\n",
       "12256.0      2682.4219   3240.915      0.828      0.408   -3670.264    9035.107\n",
       "12262.0      9564.5090   3423.360      2.794      0.005    2854.203    1.63e+04\n",
       "12389.0      8986.4050   3304.204      2.720      0.007    2509.663    1.55e+04\n",
       "1239.0       6577.3859   3030.471      2.170      0.030     637.203    1.25e+04\n",
       "12390.0      7461.3813   3602.948      2.071      0.038     399.057    1.45e+04\n",
       "12397.0      6292.8548   5345.675      1.177      0.239   -4185.481    1.68e+04\n",
       "1243.0       4184.8534   3158.124      1.325      0.185   -2005.548    1.04e+04\n",
       "12548.0      9038.3456   3632.299      2.488      0.013    1918.488    1.62e+04\n",
       "12570.0      8969.7124   3499.499      2.563      0.010    2110.162    1.58e+04\n",
       "12581.0      6220.9585   3679.891      1.691      0.091    -992.187    1.34e+04\n",
       "12592.0      7907.3965   3465.117      2.282      0.023    1115.241    1.47e+04\n",
       "12604.0      7117.2488   6104.480      1.166      0.244   -4848.459    1.91e+04\n",
       "12656.0      1.071e+04   3513.018      3.048      0.002    3822.505    1.76e+04\n",
       "12679.0     -8243.1620   3377.006     -2.441      0.015   -1.49e+04   -1623.718\n",
       "1278.0       9559.3089   3276.729      2.917      0.004    3136.423     1.6e+04\n",
       "12788.0     -2571.9810   3720.888     -0.691      0.489   -9865.486    4721.524\n",
       "1283.0       9778.5694   3188.479      3.067      0.002    3528.666     1.6e+04\n",
       "1297.0       8851.0971   3160.094      2.801      0.005    2656.833     1.5e+04\n",
       "12992.0      1.022e+04   3469.470      2.945      0.003    3415.928     1.7e+04\n",
       "13135.0      5736.5602   3444.412      1.665      0.096   -1015.010    1.25e+04\n",
       "1327.0       2830.7666   3184.260      0.889      0.374   -3410.866    9072.399\n",
       "13282.0      3707.5355   5340.406      0.694      0.488   -6760.470    1.42e+04\n",
       "1334.0       1959.2153   3392.421      0.578      0.564   -4690.446    8608.876\n",
       "13351.0      3934.9513   3942.379      0.998      0.318   -3792.710    1.17e+04\n",
       "13365.0     -7664.6310   3538.787     -2.166      0.030   -1.46e+04    -728.070\n",
       "13369.0      6192.2784   3353.245      1.847      0.065    -380.592    1.28e+04\n",
       "13406.0      9511.9068   3422.019      2.780      0.005    2804.230    1.62e+04\n",
       "13407.0      3435.2039   3325.530      1.033      0.302   -3083.341    9953.749\n",
       "13417.0      1.054e+04   3627.974      2.906      0.004    3431.229    1.77e+04\n",
       "13525.0      2730.9588   3557.222      0.768      0.443   -4241.736    9703.654\n",
       "13554.0       1.11e+04   3515.274      3.158      0.002    4209.150     1.8e+04\n",
       "1359.0        231.6921   3271.221      0.071      0.944   -6180.398    6643.783\n",
       "13623.0      7757.8946   3398.861      2.282      0.022    1095.611    1.44e+04\n",
       "1372.0       2969.1064   3094.408      0.960      0.337   -3096.404    9034.616\n",
       "1380.0       2428.4690   3086.422      0.787      0.431   -3621.386    8478.324\n",
       "13923.0      9004.3585   3723.204      2.418      0.016    1706.314    1.63e+04\n",
       "13932.0      9906.5945   4225.421      2.345      0.019    1624.127    1.82e+04\n",
       "13941.0      -775.7343   3345.291     -0.232      0.817   -7333.012    5781.544\n",
       "1397.0       7611.2849   3344.240      2.276      0.023    1056.066    1.42e+04\n",
       "14064.0      7822.3534   3374.212      2.318      0.020    1208.386    1.44e+04\n",
       "14084.0      7161.9191   3367.382      2.127      0.033     561.340    1.38e+04\n",
       "14324.0      2450.1067   3416.939      0.717      0.473   -4247.612    9147.825\n",
       "14462.0      5896.9695   3424.853      1.722      0.085    -816.263    1.26e+04\n",
       "1447.0       1.291e+04   4617.436      2.796      0.005    3861.195     2.2e+04\n",
       "14531.0      6013.3098      1e+04      0.600      0.549   -1.36e+04    2.57e+04\n",
       "14593.0      9717.6801   3546.928      2.740      0.006    2765.162    1.67e+04\n",
       "14622.0      8494.5972   7256.705      1.171      0.242   -5729.646    2.27e+04\n",
       "1465.0       9293.7357   3759.137      2.472      0.013    1925.257    1.67e+04\n",
       "1468.0       9813.2527   3591.620      2.732      0.006    2773.133    1.69e+04\n",
       "14897.0      8912.7989   5404.388      1.649      0.099   -1680.623    1.95e+04\n",
       "14954.0      9310.1232   3588.857      2.594      0.009    2275.418    1.63e+04\n",
       "1496.0       1.018e+04   3142.118      3.241      0.001    4023.346    1.63e+04\n",
       "15267.0      8561.1899   3545.299      2.415      0.016    1611.865    1.55e+04\n",
       "15354.0      3951.6292   3545.461      1.115      0.265   -2998.013    1.09e+04\n",
       "1542.0       8274.0285   3128.411      2.645      0.008    2141.867    1.44e+04\n",
       "15459.0      4755.0666   3491.964      1.362      0.173   -2089.713    1.16e+04\n",
       "1554.0       1.008e+04   3139.681      3.211      0.001    3928.295    1.62e+04\n",
       "15708.0     -8685.5979   3678.105     -2.361      0.018   -1.59e+04   -1475.954\n",
       "15711.0      7018.0335   3521.077      1.993      0.046     116.187    1.39e+04\n",
       "15761.0      9603.3202   4161.317      2.308      0.021    1446.507    1.78e+04\n",
       "1581.0      -2.338e+04   4066.536     -5.750      0.000   -3.14e+04   -1.54e+04\n",
       "1593.0       7360.1914   3039.605      2.421      0.015    1402.105    1.33e+04\n",
       "1602.0       1.541e+04   3144.958      4.899      0.000    9241.495    2.16e+04\n",
       "1613.0       9518.7977   3135.907      3.035      0.002    3371.944    1.57e+04\n",
       "16188.0      6141.7233   3600.100      1.706      0.088    -915.020    1.32e+04\n",
       "1632.0       2730.6569   2967.596      0.920      0.358   -3086.281    8547.595\n",
       "1633.0       7645.1535   3073.458      2.487      0.013    1620.710    1.37e+04\n",
       "1635.0      -2279.6622   3207.382     -0.711      0.477   -8566.618    4007.294\n",
       "16401.0     -2106.7953   3535.255     -0.596      0.551   -9036.432    4822.842\n",
       "16437.0      2453.7358   4040.565      0.607      0.544   -5466.386    1.04e+04\n",
       "1651.0       5550.2093   3015.031      1.841      0.066    -359.709    1.15e+04\n",
       "1655.0       9615.4742   3116.290      3.086      0.002    3507.073    1.57e+04\n",
       "1663.0       1.375e+04   3117.223      4.412      0.000    7643.430    1.99e+04\n",
       "16710.0      4741.7051   3561.723      1.331      0.183   -2239.813    1.17e+04\n",
       "16729.0      4221.0150   3488.991      1.210      0.226   -2617.938    1.11e+04\n",
       "1690.0      -9476.8601   3052.010     -3.105      0.002   -1.55e+04   -3494.457\n",
       "1703.0       7131.1531   3145.834      2.267      0.023     964.841    1.33e+04\n",
       "17101.0     -1.039e+04   1.02e+04     -1.021      0.307   -3.04e+04    9567.672\n",
       "17202.0      8565.2847   3580.793      2.392      0.017    1546.386    1.56e+04\n",
       "1722.0       7635.3384   3120.097      2.447      0.014    1519.474    1.38e+04\n",
       "1728.0       9786.0208   3133.400      3.123      0.002    3644.082    1.59e+04\n",
       "1743.0       9206.6002   4065.126      2.265      0.024    1238.335    1.72e+04\n",
       "1754.0       9356.9783   3245.442      2.883      0.004    2995.419    1.57e+04\n",
       "1762.0       3530.4584   3070.682      1.150      0.250   -2488.544    9549.461\n",
       "1773.0       9383.6572   3200.790      2.932      0.003    3109.623    1.57e+04\n",
       "1786.0      -2446.1653   3027.221     -0.808      0.419   -8379.979    3487.648\n",
       "18100.0      7610.5863   3547.409      2.145      0.032     657.126    1.46e+04\n",
       "1820.0       7035.3148   3094.333      2.274      0.023     969.953    1.31e+04\n",
       "1848.0      -1341.5977   3489.590     -0.384      0.701   -8181.724    5498.529\n",
       "18654.0      9308.6209   4398.704      2.116      0.034     686.493    1.79e+04\n",
       "1875.0       5422.0706   4559.415      1.189      0.234   -3515.075    1.44e+04\n",
       "1884.0       9556.3065   3257.766      2.933      0.003    3170.590    1.59e+04\n",
       "1913.0       5067.5445   3013.058      1.682      0.093    -838.507     1.1e+04\n",
       "1919.0       8460.0632   3225.767      2.623      0.009    2137.070    1.48e+04\n",
       "1920.0       5677.3684   2997.340      1.894      0.058    -197.872    1.16e+04\n",
       "1968.0       7609.1915   3040.834      2.502      0.012    1648.695    1.36e+04\n",
       "1976.0       1.067e+04   3116.261      3.425      0.001    4565.380    1.68e+04\n",
       "1981.0       9170.2560   3119.840      2.939      0.003    3054.895    1.53e+04\n",
       "1988.0         67.7686   3946.416      0.017      0.986   -7667.806    7803.344\n",
       "1992.0       7764.2033   3046.140      2.549      0.011    1793.305    1.37e+04\n",
       "2008.0       8115.3533   3045.881      2.664      0.008    2144.965    1.41e+04\n",
       "2033.0       9191.5623   3578.712      2.568      0.010    2176.744    1.62e+04\n",
       "2044.0       7375.6500   3054.171      2.415      0.016    1389.011    1.34e+04\n",
       "2049.0       7958.9282   3057.521      2.603      0.009    1965.724     1.4e+04\n",
       "2061.0       1.067e+04   3184.425      3.350      0.001    4425.887    1.69e+04\n",
       "20779.0      5.298e+04   3626.010     14.611      0.000    4.59e+04    6.01e+04\n",
       "2085.0      -2797.9037   3091.351     -0.905      0.365   -8857.420    3261.613\n",
       "2086.0       6298.7693   3040.273      2.072      0.038     339.373    1.23e+04\n",
       "2111.0       7111.0506   3027.347      2.349      0.019    1176.992     1.3e+04\n",
       "21204.0      5065.4342   3773.658      1.342      0.180   -2331.508    1.25e+04\n",
       "21238.0      9630.2308   3715.731      2.592      0.010    2346.833    1.69e+04\n",
       "2124.0       7814.7292   3156.973      2.475      0.013    1626.582     1.4e+04\n",
       "2146.0       1.782e+04   3967.047      4.493      0.000       1e+04    2.56e+04\n",
       "21496.0     -9707.4783   3795.661     -2.558      0.011   -1.71e+04   -2267.406\n",
       "2154.0       8531.2191   3092.395      2.759      0.006    2469.654    1.46e+04\n",
       "2176.0       4.917e+04   3655.506     13.450      0.000     4.2e+04    5.63e+04\n",
       "2188.0       1.064e+04   3272.692      3.252      0.001    4229.171    1.71e+04\n",
       "2189.0       1987.7047   3044.824      0.653      0.514   -3980.612    7956.022\n",
       "2220.0       8121.7655   3066.803      2.648      0.008    2110.366    1.41e+04\n",
       "22205.0      1.097e+04   3742.158      2.932      0.003    3635.194    1.83e+04\n",
       "2226.0       6564.1694   6056.783      1.084      0.278   -5308.045    1.84e+04\n",
       "2230.0       9663.4059   3448.942      2.802      0.005    2902.955    1.64e+04\n",
       "22325.0      2036.3731   3623.720      0.562      0.574   -5066.669    9139.415\n",
       "2255.0       7276.8607   3077.041      2.365      0.018    1245.393    1.33e+04\n",
       "22619.0      9459.5788   3864.126      2.448      0.014    1885.306     1.7e+04\n",
       "2267.0       1604.9439   3063.270      0.524      0.600   -4399.530    7609.418\n",
       "22815.0      4875.5667   3570.634      1.365      0.172   -2123.419    1.19e+04\n",
       "2285.0      -2.061e+04   3176.312     -6.490      0.000   -2.68e+04   -1.44e+04\n",
       "2290.0       4219.0616   3064.579      1.377      0.169   -1787.979    1.02e+04\n",
       "2295.0        1.02e+04   4631.605      2.202      0.028    1118.124    1.93e+04\n",
       "2316.0       3932.7481   3268.816      1.203      0.229   -2474.628    1.03e+04\n",
       "23220.0      7708.2043   3613.811      2.133      0.033     624.587    1.48e+04\n",
       "23224.0     -1416.5488   3999.474     -0.354      0.723   -9256.124    6423.027\n",
       "2343.0        371.2913   5445.150      0.068      0.946   -1.03e+04     1.1e+04\n",
       "2352.0       7033.5174   3195.255      2.201      0.028     770.332    1.33e+04\n",
       "23700.0     -4104.2067   4540.832     -0.904      0.366    -1.3e+04    4796.514\n",
       "2390.0       1.022e+04   3146.886      3.249      0.001    4054.843    1.64e+04\n",
       "2393.0       5256.7276   3021.333      1.740      0.082    -665.543    1.12e+04\n",
       "2403.0       1.544e+04   3116.842      4.954      0.000    9331.361    2.16e+04\n",
       "2435.0       1.204e+04   3215.430      3.744      0.000    5734.523    1.83e+04\n",
       "2444.0       5543.3070   3031.522      1.829      0.067    -398.936    1.15e+04\n",
       "2448.0       7626.0071   3053.285      2.498      0.013    1641.104    1.36e+04\n",
       "2469.0       9427.6259   4407.154      2.139      0.032     788.935    1.81e+04\n",
       "24720.0      8470.8001   3726.615      2.273      0.023    1166.069    1.58e+04\n",
       "24800.0      2679.9470   3896.093      0.688      0.492   -4956.987    1.03e+04\n",
       "2482.0       1.036e+04   3154.774      3.283      0.001    4172.094    1.65e+04\n",
       "24969.0       1.01e+04   4399.409      2.296      0.022    1476.724    1.87e+04\n",
       "2498.0       2822.1067   3205.564      0.880      0.379   -3461.286    9105.499\n",
       "2504.0      -5740.6155   3125.446     -1.837      0.066   -1.19e+04     385.734\n",
       "2508.0       9551.1213   3313.820      2.882      0.004    3055.530     1.6e+04\n",
       "25124.0      8839.2099   3850.901      2.295      0.022    1290.860    1.64e+04\n",
       "2518.0       9462.2606   3118.179      3.035      0.002    3350.156    1.56e+04\n",
       "25224.0      9181.2871   7262.529      1.264      0.206   -5054.373    2.34e+04\n",
       "25279.0      8309.3827   3823.370      2.173      0.030     814.996    1.58e+04\n",
       "2537.0      -1331.6614   3141.364     -0.424      0.672   -7489.212    4825.889\n",
       "2538.0       1.059e+04   4051.182      2.615      0.009    2653.302    1.85e+04\n",
       "25389.0      1.016e+04   6108.961      1.663      0.096   -1815.943    2.21e+04\n",
       "2547.0       1402.5130   3210.761      0.437      0.662   -4891.066    7696.092\n",
       "2553.0       9104.4999   3154.705      2.886      0.004    2920.799    1.53e+04\n",
       "2574.0       2817.6652   3621.446      0.778      0.437   -4280.919    9916.249\n",
       "25747.0      8887.4249   3853.652      2.306      0.021    1333.682    1.64e+04\n",
       "2577.0       7333.8930   3033.785      2.417      0.016    1387.213    1.33e+04\n",
       "2593.0       7608.5207   3056.291      2.489      0.013    1617.727    1.36e+04\n",
       "2596.0       7104.2843   3165.531      2.244      0.025     899.363    1.33e+04\n",
       "2663.0       1.201e+04   3109.709      3.862      0.000    5914.315    1.81e+04\n",
       "2771.0       7170.4426   3157.640      2.271      0.023     980.989    1.34e+04\n",
       "2787.0       8843.2302   3108.248      2.845      0.004    2750.591    1.49e+04\n",
       "2797.0      -1136.7600   3047.444     -0.373      0.709   -7110.213    4836.693\n",
       "2802.0       9981.2391   3145.382      3.173      0.002    3815.813    1.61e+04\n",
       "2817.0      -2423.3733   3105.183     -0.780      0.435   -8510.003    3663.257\n",
       "28678.0     -6800.8821   3938.721     -1.727      0.084   -1.45e+04     919.610\n",
       "28701.0      7289.6069   3122.789      2.334      0.020    1168.466    1.34e+04\n",
       "28742.0     -4379.2282   4023.812     -1.088      0.276   -1.23e+04    3508.053\n",
       "2888.0       8333.3928   3213.797      2.593      0.010    2033.862    1.46e+04\n",
       "2897.0       9522.3975   3838.939      2.480      0.013    1997.494     1.7e+04\n",
       "2917.0       4179.7509   3174.337      1.317      0.188   -2042.431    1.04e+04\n",
       "29392.0     -4484.0845   3895.740     -1.151      0.250   -1.21e+04    3152.158\n",
       "2950.0      -8939.5631   4197.529     -2.130      0.033   -1.72e+04    -711.769\n",
       "2951.0       1.053e+04   3554.152      2.964      0.003    3567.561    1.75e+04\n",
       "2953.0       8670.7332   3081.692      2.814      0.005    2630.148    1.47e+04\n",
       "2960.0       6927.1661   3807.516      1.819      0.069    -536.143    1.44e+04\n",
       "2975.0       4129.9550   3074.147      1.343      0.179   -1895.841    1.02e+04\n",
       "2982.0       8607.4496   3114.061      2.764      0.006    2503.416    1.47e+04\n",
       "2991.0      -2091.3592   3611.129     -0.579      0.563   -9169.721    4987.002\n",
       "3011.0        352.0291   3228.393      0.109      0.913   -5976.112    6680.170\n",
       "3015.0       1.066e+04   3189.868      3.341      0.001    4404.847    1.69e+04\n",
       "3026.0       8297.5216   3114.849      2.664      0.008    2191.944    1.44e+04\n",
       "3031.0      -7156.8279   3963.813     -1.806      0.071   -1.49e+04     612.847\n",
       "3062.0       1.013e+04   3216.055      3.150      0.002    3827.484    1.64e+04\n",
       "3093.0       4614.1396   3380.925      1.365      0.172   -2012.986    1.12e+04\n",
       "3107.0       8261.4341   4593.460      1.799      0.072    -742.444    1.73e+04\n",
       "3121.0       1.075e+04   3100.529      3.467      0.001    4670.905    1.68e+04\n",
       "3126.0       8639.8575   3074.886      2.810      0.005    2612.613    1.47e+04\n",
       "3144.0       6.167e+04   3143.112     19.622      0.000    5.55e+04    6.78e+04\n",
       "3156.0       9057.0292   3556.046      2.547      0.011    2086.640     1.6e+04\n",
       "3157.0       8487.6305   3086.887      2.750      0.006    2436.864    1.45e+04\n",
       "3170.0       1.112e+04   3070.843      3.621      0.000    5098.953    1.71e+04\n",
       "3178.0       4719.2301   3279.399      1.439      0.150   -1708.889    1.11e+04\n",
       "3206.0       5242.4391   3326.387      1.576      0.115   -1277.785    1.18e+04\n",
       "3229.0       6940.2745   3201.204      2.168      0.030     665.428    1.32e+04\n",
       "3235.0       8557.6434   3259.016      2.626      0.009    2169.477    1.49e+04\n",
       "3246.0       8959.3648   3141.052      2.852      0.004    2802.426    1.51e+04\n",
       "3248.0       8795.1454   3144.903      2.797      0.005    2630.657     1.5e+04\n",
       "3282.0      -1.882e+04   3179.239     -5.919      0.000    -2.5e+04   -1.26e+04\n",
       "3362.0       1467.2473   3430.530      0.428      0.669   -5257.112    8191.606\n",
       "3372.0       8967.9006   3615.991      2.480      0.013    1880.009    1.61e+04\n",
       "3422.0       8150.3780   3116.503      2.615      0.009    2041.559    1.43e+04\n",
       "3497.0       3305.8909   3020.187      1.095      0.274   -2614.134    9225.916\n",
       "3502.0       4413.0364   3019.260      1.462      0.144   -1505.172    1.03e+04\n",
       "3504.0       7330.3624   3693.007      1.985      0.047      91.508    1.46e+04\n",
       "3505.0       6509.4875   3039.592      2.142      0.032     551.425    1.25e+04\n",
       "3532.0       9367.0102   3036.064      3.085      0.002    3415.864    1.53e+04\n",
       "3574.0       9567.2283   4941.074      1.936      0.053    -118.027    1.93e+04\n",
       "3580.0       5668.8773   3023.668      1.875      0.061    -257.971    1.16e+04\n",
       "3612.0       1.077e+04   3190.707      3.376      0.001    4518.562     1.7e+04\n",
       "3619.0       7583.6331   3125.470      2.426      0.015    1457.238    1.37e+04\n",
       "3622.0       1.024e+04   3247.561      3.154      0.002    3877.236    1.66e+04\n",
       "3639.0        701.7362   2978.785      0.236      0.814   -5137.134    6540.606\n",
       "3650.0        -56.5141   3013.203     -0.019      0.985   -5962.849    5849.821\n",
       "3662.0       6515.3615   3036.473      2.146      0.032     563.413    1.25e+04\n",
       "3734.0      -4540.5498   3009.478     -1.509      0.131   -1.04e+04    1358.485\n",
       "3735.0       5998.0243   3363.381      1.783      0.075    -594.714    1.26e+04\n",
       "3761.0       4676.2483   3019.021      1.549      0.121   -1241.491    1.06e+04\n",
       "3779.0      -1115.3957   3351.604     -0.333      0.739   -7685.049    5454.258\n",
       "3781.0       3388.6729   3657.813      0.926      0.354   -3781.196    1.06e+04\n",
       "3782.0       -945.5519   3097.134     -0.305      0.760   -7016.405    5125.302\n",
       "3786.0       8059.2754   3095.380      2.604      0.009    1991.860    1.41e+04\n",
       "3796.0      -1921.4741   3437.318     -0.559      0.576   -8659.140    4816.192\n",
       "3821.0       9374.5415   3182.442      2.946      0.003    3136.471    1.56e+04\n",
       "3835.0       1623.3259   3068.940      0.529      0.597   -4392.263    7638.914\n",
       "3839.0       5038.7123   3787.676      1.330      0.183   -2385.707    1.25e+04\n",
       "3840.0       3096.0560   3106.065      0.997      0.319   -2992.302    9184.414\n",
       "3895.0       9436.3688   3115.660      3.029      0.002    3329.202    1.55e+04\n",
       "3908.0       6499.5003   4150.425      1.566      0.117   -1635.964    1.46e+04\n",
       "3911.0       3978.4256   3062.447      1.299      0.194   -2024.435    9981.286\n",
       "3917.0       9644.8467   3201.184      3.013      0.003    3370.040    1.59e+04\n",
       "3946.0        1.02e+04   3163.781      3.224      0.001    3997.651    1.64e+04\n",
       "3971.0       8857.9912   3190.900      2.776      0.006    2603.342    1.51e+04\n",
       "3980.0       1.889e+04   3114.804      6.066      0.000    1.28e+04     2.5e+04\n",
       "4034.0       5433.4828   3023.927      1.797      0.072    -493.873    1.14e+04\n",
       "4036.0       1.006e+04   3132.064      3.210      0.001    3915.798    1.62e+04\n",
       "4040.0       4402.5134   3085.061      1.427      0.154   -1644.674    1.04e+04\n",
       "4058.0       8496.1476   3074.049      2.764      0.006    2470.545    1.45e+04\n",
       "4060.0      -8502.7681   3085.688     -2.756      0.006   -1.46e+04   -2454.351\n",
       "4062.0       1.245e+04   3186.197      3.906      0.000    6200.250    1.87e+04\n",
       "4077.0       8049.1131   4325.812      1.861      0.063    -430.135    1.65e+04\n",
       "4087.0      -1.922e+04   3417.870     -5.622      0.000   -2.59e+04   -1.25e+04\n",
       "4091.0       7764.4934   3536.914      2.195      0.028     831.604    1.47e+04\n",
       "4127.0       3991.2384   2983.946      1.338      0.181   -1857.749    9840.226\n",
       "4138.0       1.012e+04   3751.511      2.696      0.007    2761.757    1.75e+04\n",
       "4162.0       8848.7229   3711.438      2.384      0.017    1573.741    1.61e+04\n",
       "4186.0       1.029e+04   3147.338      3.271      0.001    4124.177    1.65e+04\n",
       "4194.0       -206.5396   3254.206     -0.063      0.949   -6585.277    6172.197\n",
       "4199.0      -3080.7950   2986.731     -1.031      0.302   -8935.242    2773.652\n",
       "4213.0       9040.9619   3063.929      2.951      0.003    3035.197     1.5e+04\n",
       "4222.0       -427.7763   3036.691     -0.141      0.888   -6380.151    5524.598\n",
       "4223.0       8795.2266   3075.179      2.860      0.004    2767.409    1.48e+04\n",
       "4251.0       9920.6121   3135.291      3.164      0.002    3774.966    1.61e+04\n",
       "4265.0       8311.0668   3331.549      2.495      0.013    1780.724    1.48e+04\n",
       "4274.0       7457.4585   3194.105      2.335      0.020    1196.528    1.37e+04\n",
       "4321.0       5939.8774   3058.043      1.942      0.052     -54.352    1.19e+04\n",
       "4335.0       7276.7703   4582.157      1.588      0.112   -1704.953    1.63e+04\n",
       "4340.0       6243.3898   3049.262      2.048      0.041     266.372    1.22e+04\n",
       "4371.0       6746.0318   3081.412      2.189      0.029     705.997    1.28e+04\n",
       "4415.0       9237.0407   3230.539      2.859      0.004    2904.693    1.56e+04\n",
       "4450.0       7979.6767   3101.329      2.573      0.010    1900.602    1.41e+04\n",
       "4476.0        675.3081   3174.296      0.213      0.832   -5546.793    6897.409\n",
       "4510.0       2475.6995   3026.171      0.818      0.413   -3456.054    8407.453\n",
       "4520.0       7980.2863   3051.589      2.615      0.009    1998.708     1.4e+04\n",
       "4551.0       8011.4429   7270.601      1.102      0.271   -6240.039    2.23e+04\n",
       "4568.0       8842.8576   3229.403      2.738      0.006    2512.738    1.52e+04\n",
       "4579.0       1.065e+04   3189.762      3.338      0.001    4393.423    1.69e+04\n",
       "4585.0       1.003e+04   3206.938      3.129      0.002    3747.041    1.63e+04\n",
       "4595.0       6865.7401   3039.451      2.259      0.024     907.954    1.28e+04\n",
       "4600.0       1012.0246   3200.037      0.316      0.752   -5260.535    7284.584\n",
       "4607.0       9896.4315   3136.909      3.155      0.002    3747.613     1.6e+04\n",
       "4608.0       1926.7708   3084.280      0.625      0.532   -4118.886    7972.428\n",
       "4622.0       5050.9157   3012.296      1.677      0.094    -853.642     1.1e+04\n",
       "4623.0       9205.2343   3264.415      2.820      0.005    2806.486    1.56e+04\n",
       "4768.0       8149.0733   3123.950      2.609      0.009    2025.657    1.43e+04\n",
       "4771.0        1.02e+04   3154.399      3.233      0.001    4014.036    1.64e+04\n",
       "4800.0       8510.9355   3222.793      2.641      0.008    2193.771    1.48e+04\n",
       "4802.0       9756.0632   3122.659      3.124      0.002    3635.177    1.59e+04\n",
       "4807.0       9311.2841   3209.003      2.902      0.004    3021.150    1.56e+04\n",
       "4839.0       -1.39e+05   4400.348    -31.598      0.000   -1.48e+05    -1.3e+05\n",
       "4843.0       -155.0920   3300.774     -0.047      0.963   -6625.111    6314.927\n",
       "4881.0       7349.9526   3033.927      2.423      0.015    1402.995    1.33e+04\n",
       "4900.0       8056.7145   3130.004      2.574      0.010    1921.431    1.42e+04\n",
       "4926.0       6951.4250   3036.486      2.289      0.022     999.451    1.29e+04\n",
       "4941.0       7701.0234   3119.428      2.469      0.014    1586.472    1.38e+04\n",
       "4961.0      -6713.3878   3845.668     -1.746      0.081   -1.43e+04     824.706\n",
       "4988.0       1.555e+04   3152.418      4.932      0.000    9368.460    2.17e+04\n",
       "4993.0       1.078e+04   3191.719      3.379      0.001    4527.925     1.7e+04\n",
       "5018.0       2756.6116   3051.145      0.903      0.366   -3224.095    8737.318\n",
       "5020.0       1703.6436   3206.794      0.531      0.595   -4582.160    7989.448\n",
       "5027.0       5583.6057   3073.165      1.817      0.069    -440.265    1.16e+04\n",
       "5032.0       8848.7731   3093.419      2.861      0.004    2785.202    1.49e+04\n",
       "5043.0       5765.7251   3056.378      1.886      0.059    -225.240    1.18e+04\n",
       "5046.0      -3742.7011   3008.827     -1.244      0.214   -9640.459    2155.057\n",
       "5047.0       4.951e+04   3945.666     12.549      0.000    4.18e+04    5.72e+04\n",
       "5065.0       9943.6682   3565.116      2.789      0.005    2955.499    1.69e+04\n",
       "5071.0       8402.8199   3506.244      2.397      0.017    1530.049    1.53e+04\n",
       "5073.0       -2.03e+05   6327.061    -32.087      0.000   -2.15e+05   -1.91e+05\n",
       "5087.0       4808.3060   3031.181      1.586      0.113   -1133.268    1.07e+04\n",
       "5109.0           1e+04   3156.702      3.169      0.002    3815.966    1.62e+04\n",
       "5116.0        331.7479   3230.260      0.103      0.918   -6000.052    6663.548\n",
       "5122.0       5093.7234   3033.485      1.679      0.093    -852.368     1.1e+04\n",
       "5134.0       1619.9886   3164.783      0.512      0.609   -4583.467    7823.444\n",
       "5142.0       6629.5952   3817.878      1.736      0.083    -854.026    1.41e+04\n",
       "5165.0       7201.9002   3418.120      2.107      0.035     501.866    1.39e+04\n",
       "5169.0       1.921e+04   3111.749      6.173      0.000    1.31e+04    2.53e+04\n",
       "5174.0       6861.1596   3315.866      2.069      0.039     361.558    1.34e+04\n",
       "5179.0       9522.8653   3123.992      3.048      0.002    3399.366    1.56e+04\n",
       "5181.0       1.022e+04   3247.843      3.148      0.002    3857.092    1.66e+04\n",
       "5187.0       1.051e+04   3595.928      2.924      0.003    3466.116    1.76e+04\n",
       "5229.0       1832.7007   2976.839      0.616      0.538   -4002.356    7667.757\n",
       "5234.0      -5223.8465   3133.458     -1.667      0.096   -1.14e+04     918.206\n",
       "5237.0       9043.0280   3101.586      2.916      0.004    2963.448    1.51e+04\n",
       "5252.0       8212.2020   3060.018      2.684      0.007    2214.102    1.42e+04\n",
       "5254.0       8446.7502   3097.320      2.727      0.006    2375.532    1.45e+04\n",
       "5306.0       6465.5650   3027.709      2.135      0.033     530.795    1.24e+04\n",
       "5338.0       9319.2221   3098.305      3.008      0.003    3246.074    1.54e+04\n",
       "5377.0       1.009e+04   3165.731      3.188      0.001    3885.677    1.63e+04\n",
       "5439.0       7334.3884   3132.147      2.342      0.019    1194.905    1.35e+04\n",
       "5456.0       1.067e+04   3219.404      3.313      0.001    4356.183     1.7e+04\n",
       "5464.0       8046.9137   3717.507      2.165      0.030     760.036    1.53e+04\n",
       "5476.0       1.038e+04   3188.810      3.254      0.001    4125.428    1.66e+04\n",
       "5492.0      -7692.0384   3028.409     -2.540      0.011   -1.36e+04   -1755.897\n",
       "5496.0       8328.8408   3078.327      2.706      0.007    2294.852    1.44e+04\n",
       "5505.0       9584.8474   3145.488      3.047      0.002    3419.213    1.58e+04\n",
       "5518.0       8040.5569   3394.234      2.369      0.018    1387.343    1.47e+04\n",
       "5520.0       7141.4059   3085.889      2.314      0.021    1092.595    1.32e+04\n",
       "5545.0       9850.2695   3257.198      3.024      0.002    3465.667    1.62e+04\n",
       "5568.0         1.2e+04   3131.140      3.834      0.000    5866.525    1.81e+04\n",
       "5569.0       1.006e+04   3173.656      3.171      0.002    3841.616    1.63e+04\n",
       "5578.0       9572.4372   3115.964      3.072      0.002    3464.675    1.57e+04\n",
       "5581.0       9414.8809   3097.341      3.040      0.002    3343.622    1.55e+04\n",
       "5589.0       2400.9953   2973.079      0.808      0.419   -3426.690    8228.681\n",
       "5597.0        1.13e+04   3648.090      3.097      0.002    4145.936    1.84e+04\n",
       "5606.0      -2.745e+04   3152.384     -8.709      0.000   -3.36e+04   -2.13e+04\n",
       "5639.0       1.082e+04   3159.665      3.425      0.001    4628.029     1.7e+04\n",
       "5667.0       7446.7098   3218.672      2.314      0.021    1137.624    1.38e+04\n",
       "5690.0       9706.2182   3117.215      3.114      0.002    3596.004    1.58e+04\n",
       "5709.0       9020.8914   3130.883      2.881      0.004    2883.886    1.52e+04\n",
       "5726.0       9123.8740   3144.647      2.901      0.004    2959.887    1.53e+04\n",
       "5764.0       7723.0628   3018.519      2.559      0.011    1806.306    1.36e+04\n",
       "5772.0       8901.7882   3086.811      2.884      0.004    2851.170     1.5e+04\n",
       "5860.0      -1.948e+04   3110.857     -6.261      0.000   -2.56e+04   -1.34e+04\n",
       "5878.0       1.119e+04   3037.648      3.682      0.000    5231.276    1.71e+04\n",
       "5903.0       4917.5431   3123.826      1.574      0.115   -1205.631     1.1e+04\n",
       "5905.0       6472.3316   3072.948      2.106      0.035     448.886    1.25e+04\n",
       "5959.0       4032.3371   3074.491      1.312      0.190   -1994.131    1.01e+04\n",
       "6008.0       2.842e+04   3030.015      9.380      0.000    2.25e+04    3.44e+04\n",
       "6034.0       5915.9539   3166.969      1.868      0.062    -291.787    1.21e+04\n",
       "6035.0       4321.0088   3624.230      1.192      0.233   -2783.033    1.14e+04\n",
       "6036.0        672.3068   2994.346      0.225      0.822   -5197.066    6541.680\n",
       "6039.0       9107.9457   3133.532      2.907      0.004    2965.747    1.53e+04\n",
       "6044.0       1.053e+04   3386.931      3.110      0.002    3894.343    1.72e+04\n",
       "6066.0       -2.49e+04   4424.278     -5.629      0.000   -3.36e+04   -1.62e+04\n",
       "6078.0       9970.6042   3077.931      3.239      0.001    3937.391     1.6e+04\n",
       "6081.0      -8372.7134   2993.920     -2.797      0.005   -1.42e+04   -2504.175\n",
       "60893.0     -4392.3442   4603.659     -0.954      0.340   -1.34e+04    4631.526\n",
       "6097.0       1.012e+04   3186.388      3.177      0.001    3876.500    1.64e+04\n",
       "6102.0       8471.8648   3120.976      2.714      0.007    2354.277    1.46e+04\n",
       "6104.0      -1015.6692   3148.374     -0.323      0.747   -7186.960    5155.622\n",
       "6109.0       1845.7093   2960.815      0.623      0.533   -3957.937    7649.356\n",
       "6127.0       3170.7530   3399.610      0.933      0.351   -3492.999    9834.505\n",
       "61552.0     -3744.5941   4251.815     -0.881      0.378   -1.21e+04    4589.610\n",
       "6158.0       6412.2005   3180.992      2.016      0.044     176.973    1.26e+04\n",
       "6171.0       9057.6581   3095.055      2.926      0.003    2990.880    1.51e+04\n",
       "61780.0      7728.4892   4929.729      1.568      0.117   -1934.529    1.74e+04\n",
       "6207.0       9047.4137   3117.371      2.902      0.004    2936.893    1.52e+04\n",
       "6214.0       9507.7433   3104.135      3.063      0.002    3423.167    1.56e+04\n",
       "6216.0       9878.3520   3184.210      3.102      0.002    3636.816    1.61e+04\n",
       "62221.0      9234.8611   4394.723      2.101      0.036     620.537    1.78e+04\n",
       "6259.0       5439.7175   3440.702      1.581      0.114   -1304.581    1.22e+04\n",
       "62599.0     -2.283e+04   4958.737     -4.603      0.000   -3.25e+04   -1.31e+04\n",
       "6266.0       9238.7331   3091.436      2.988      0.003    3179.048    1.53e+04\n",
       "6268.0       1962.6459   3058.799      0.642      0.521   -4033.064    7958.356\n",
       "6288.0       8168.8157   3114.238      2.623      0.009    2064.437    1.43e+04\n",
       "6297.0       9274.3327   3198.409      2.900      0.004    3004.965    1.55e+04\n",
       "6307.0      -1.469e+04   3698.147     -3.973      0.000   -2.19e+04   -7443.955\n",
       "6313.0       8992.0347   4390.677      2.048      0.041     385.640    1.76e+04\n",
       "6314.0       9736.1987   3151.801      3.089      0.002    3558.191    1.59e+04\n",
       "6326.0       3826.7169   2984.925      1.282      0.200   -2024.190    9677.624\n",
       "6349.0       8643.9164   3087.391      2.800      0.005    2592.161    1.47e+04\n",
       "6357.0       1.032e+04   3297.883      3.130      0.002    3857.899    1.68e+04\n",
       "6375.0       1.413e+04   3134.685      4.507      0.000    7984.330    2.03e+04\n",
       "6376.0       9471.0692   3147.599      3.009      0.003    3301.297    1.56e+04\n",
       "6379.0      -3775.7784   6131.764     -0.616      0.538   -1.58e+04    8243.410\n",
       "6386.0       9629.8217   3110.678      3.096      0.002    3532.421    1.57e+04\n",
       "6403.0       5780.6448   3183.328      1.816      0.069    -459.161     1.2e+04\n",
       "6410.0       1.062e+04   3209.167      3.309      0.001    4327.639    1.69e+04\n",
       "6416.0       4480.9728   3166.668      1.415      0.157   -1726.177    1.07e+04\n",
       "6424.0       9498.0653   3132.089      3.033      0.002    3358.695    1.56e+04\n",
       "6433.0       9589.1419   3170.423      3.025      0.002    3374.632    1.58e+04\n",
       "6435.0       9723.4153   3042.211      3.196      0.001    3760.220    1.57e+04\n",
       "6492.0       6260.6289   3090.244      2.026      0.043     203.281    1.23e+04\n",
       "6497.0       2903.0101   3215.604      0.903      0.367   -3400.062    9206.082\n",
       "6500.0       8402.1659   7302.670      1.151      0.250   -5912.176    2.27e+04\n",
       "6509.0       8464.7557   3077.538      2.750      0.006    2432.313    1.45e+04\n",
       "6527.0       1.055e+04   3417.715      3.086      0.002    3848.357    1.72e+04\n",
       "6528.0       8661.1370   3343.926      2.590      0.010    2106.534    1.52e+04\n",
       "6531.0        -25.9974   3078.405     -0.008      0.993   -6060.139    6008.144\n",
       "6532.0       5885.9647   3095.765      1.901      0.057    -182.205     1.2e+04\n",
       "6543.0       9929.9352   3144.078      3.158      0.002    3767.065    1.61e+04\n",
       "6548.0       9593.2367   3169.925      3.026      0.002    3379.702    1.58e+04\n",
       "6550.0       9616.0676   3359.843      2.862      0.004    3030.264    1.62e+04\n",
       "6552.0       9638.8646   3275.149      2.943      0.003    3219.075    1.61e+04\n",
       "6565.0       4782.1084   3198.665      1.495      0.135   -1487.761    1.11e+04\n",
       "6571.0       9305.0305   3110.487      2.992      0.003    3208.004    1.54e+04\n",
       "6573.0       9082.6070   3092.638      2.937      0.003    3020.566    1.51e+04\n",
       "6641.0       7749.3873   5382.286      1.440      0.150   -2800.711    1.83e+04\n",
       "6649.0       1.053e+04   3159.208      3.335      0.001    4342.190    1.67e+04\n",
       "6730.0       8500.3738   3089.775      2.751      0.006    2443.946    1.46e+04\n",
       "6731.0       6466.6645   3134.642      2.063      0.039     322.291    1.26e+04\n",
       "6742.0       9550.9622   4649.788      2.054      0.040     436.672    1.87e+04\n",
       "6745.0       1.009e+04   3200.253      3.154      0.002    3820.559    1.64e+04\n",
       "6756.0       9834.8380   3160.429      3.112      0.002    3639.917     1.6e+04\n",
       "6765.0      -4414.1271   3022.513     -1.460      0.144   -1.03e+04    1510.458\n",
       "6768.0       1.156e+04   3234.772      3.575      0.000    5223.749    1.79e+04\n",
       "6774.0       -1.45e+04   3423.679     -4.235      0.000   -2.12e+04   -7789.918\n",
       "6797.0       1.062e+04   3560.156      2.982      0.003    3636.640    1.76e+04\n",
       "6803.0       9790.5158   3152.629      3.106      0.002    3610.883     1.6e+04\n",
       "6821.0       8916.7458   3111.850      2.865      0.004    2817.047     1.5e+04\n",
       "6830.0       8333.4226   3124.559      2.667      0.008    2208.812    1.45e+04\n",
       "6845.0       6582.6117   3033.170      2.170      0.030     637.138    1.25e+04\n",
       "6848.0       9265.8494   3380.966      2.741      0.006    2638.642    1.59e+04\n",
       "6873.0       5262.2493   3710.128      1.418      0.156   -2010.164    1.25e+04\n",
       "6900.0       7470.5391   3040.133      2.457      0.014    1511.417    1.34e+04\n",
       "6908.0       7574.9222   3037.300      2.494      0.013    1621.353    1.35e+04\n",
       "6994.0       8255.8589   3119.283      2.647      0.008    2141.590    1.44e+04\n",
       "7045.0       1844.4045   3736.226      0.494      0.622   -5479.167    9167.976\n",
       "7065.0       1.408e+04   3067.167      4.591      0.000    8070.340    2.01e+04\n",
       "7085.0        1.13e+04   3081.095      3.669      0.000    5264.878    1.73e+04\n",
       "7107.0       8052.7198   3211.390      2.508      0.012    1757.908    1.43e+04\n",
       "7116.0       1.032e+04   3179.999      3.245      0.001    4086.849    1.66e+04\n",
       "7117.0       1.055e+04   3949.975      2.671      0.008    2809.099    1.83e+04\n",
       "7121.0       8775.8315   3133.720      2.800      0.005    2633.264    1.49e+04\n",
       "7127.0       6420.8313   3243.158      1.980      0.048      63.750    1.28e+04\n",
       "7139.0       8727.2476   3099.294      2.816      0.005    2652.161    1.48e+04\n",
       "7146.0       9720.1875   3126.664      3.109      0.002    3591.451    1.58e+04\n",
       "7163.0       1.279e+04   3134.484      4.081      0.000    6646.405    1.89e+04\n",
       "7180.0       6160.9922   3087.312      1.996      0.046     109.392    1.22e+04\n",
       "7183.0       6762.3251   3128.386      2.162      0.031     630.214    1.29e+04\n",
       "7228.0       1.764e+04   3168.523      5.567      0.000    1.14e+04    2.38e+04\n",
       "7232.0       8154.6539   4154.025      1.963      0.050      12.134    1.63e+04\n",
       "7250.0       6766.0899   3377.887      2.003      0.045     144.918    1.34e+04\n",
       "7257.0       2.971e+04   3115.326      9.536      0.000    2.36e+04    3.58e+04\n",
       "7260.0       9150.8519   3085.180      2.966      0.003    3103.431    1.52e+04\n",
       "7267.0       8865.9508   3234.107      2.741      0.006    2526.610    1.52e+04\n",
       "7268.0       1505.3236   3301.821      0.456      0.648   -4966.748    7977.395\n",
       "7281.0       9732.5905   3991.005      2.439      0.015    1909.615    1.76e+04\n",
       "7291.0       7456.0581   3068.111      2.430      0.015    1442.096    1.35e+04\n",
       "7343.0       4436.4667   3573.654      1.241      0.214   -2568.438    1.14e+04\n",
       "7346.0       3238.1303   3079.092      1.052      0.293   -2797.357    9273.617\n",
       "7401.0       9349.7327   3138.244      2.979      0.003    3198.297    1.55e+04\n",
       "7409.0       8457.9972   3066.239      2.758      0.006    2447.702    1.45e+04\n",
       "7420.0       6215.9290   3018.667      2.059      0.039     298.884    1.21e+04\n",
       "7435.0       6751.7467   3158.717      2.137      0.033     560.181    1.29e+04\n",
       "7466.0       8553.9161   3269.146      2.617      0.009    2145.893     1.5e+04\n",
       "7486.0       1723.3876   3152.520      0.547      0.585   -4456.030    7902.805\n",
       "7503.0       9067.0759   4579.905      1.980      0.048      89.767     1.8e+04\n",
       "7506.0       9819.2990   3057.813      3.211      0.001    3825.522    1.58e+04\n",
       "7537.0       9430.8825   3170.373      2.975      0.003    3216.470    1.56e+04\n",
       "7549.0       7273.1684   3042.817      2.390      0.017    1308.786    1.32e+04\n",
       "7554.0       8897.8883   3135.083      2.838      0.005    2752.650     1.5e+04\n",
       "7557.0       6931.8955   3136.896      2.210      0.027     783.102    1.31e+04\n",
       "7585.0      -1.495e+04   3233.948     -4.624      0.000   -2.13e+04   -8613.193\n",
       "7602.0       8923.7108   3107.157      2.872      0.004    2833.211     1.5e+04\n",
       "7620.0       7129.5037   3384.544      2.106      0.035     495.283    1.38e+04\n",
       "7636.0       9304.1976   3123.447      2.979      0.003    3181.767    1.54e+04\n",
       "7646.0       9393.7191   3155.786      2.977      0.003    3207.900    1.56e+04\n",
       "7658.0       6253.3200   3022.501      2.069      0.039     328.759    1.22e+04\n",
       "7683.0       1.026e+04   3321.592      3.090      0.002    3753.317    1.68e+04\n",
       "7685.0       8767.4758   3234.267      2.711      0.007    2427.821    1.51e+04\n",
       "7692.0       5012.1399   3004.385      1.668      0.095    -876.911    1.09e+04\n",
       "7762.0       9366.4810   3091.150      3.030      0.002    3307.357    1.54e+04\n",
       "7772.0      -2443.1777   3001.938     -0.814      0.416   -8327.432    3441.076\n",
       "7773.0       8790.9387   3128.430      2.810      0.005    2658.740    1.49e+04\n",
       "7777.0       5620.5173   3072.543      1.829      0.067    -402.134    1.16e+04\n",
       "7835.0       9969.7493   3156.641      3.158      0.002    3782.253    1.62e+04\n",
       "7873.0       1753.2079   3096.978      0.566      0.571   -4317.340    7823.755\n",
       "7883.0       6648.1472   3032.795      2.192      0.028     703.409    1.26e+04\n",
       "7904.0       5757.8853   3042.665      1.892      0.058    -206.199    1.17e+04\n",
       "7906.0        1.24e+04   3192.708      3.884      0.000    6143.287    1.87e+04\n",
       "7921.0       9521.8682   3078.497      3.093      0.002    3487.546    1.56e+04\n",
       "7923.0       8424.9184   3204.025      2.629      0.009    2144.542    1.47e+04\n",
       "7935.0       6955.2953   3049.615      2.281      0.023     977.586    1.29e+04\n",
       "7938.0       7442.0282   3074.413      2.421      0.016    1415.711    1.35e+04\n",
       "7985.0      -1.012e+04   2991.410     -3.382      0.001    -1.6e+04   -4253.115\n",
       "8014.0       8157.1933   3199.316      2.550      0.011    1886.047    1.44e+04\n",
       "8030.0       1.069e+04   3172.643      3.370      0.001    4474.298    1.69e+04\n",
       "8046.0       2100.8383   3150.625      0.667      0.505   -4074.865    8276.542\n",
       "8047.0       8930.9265   3638.445      2.455      0.014    1799.022    1.61e+04\n",
       "8062.0       7656.2181   3156.951      2.425      0.015    1468.115    1.38e+04\n",
       "8068.0      -6316.9730   3342.935     -1.890      0.059   -1.29e+04     235.687\n",
       "8087.0      -1486.8453   3165.038     -0.470      0.639   -7690.801    4717.110\n",
       "8095.0       9092.3282   3091.617      2.941      0.003    3032.289    1.52e+04\n",
       "8096.0       9888.7824   3156.253      3.133      0.002    3702.048    1.61e+04\n",
       "8109.0       9592.9423   3114.281      3.080      0.002    3488.479    1.57e+04\n",
       "8123.0       5332.5335   3050.740      1.748      0.080    -647.380    1.13e+04\n",
       "8150.0       1.002e+04   3143.649      3.186      0.001    3854.817    1.62e+04\n",
       "8163.0       7135.9390   3109.533      2.295      0.022    1040.782    1.32e+04\n",
       "8176.0       5907.8263   3555.339      1.662      0.097   -1061.178    1.29e+04\n",
       "8202.0       7093.7298   3180.484      2.230      0.026     859.499    1.33e+04\n",
       "8214.0       5649.2334   3049.389      1.853      0.064    -328.032    1.16e+04\n",
       "8215.0       3589.9167   3151.067      1.139      0.255   -2586.652    9766.486\n",
       "8219.0       1.015e+04   3202.762      3.170      0.002    3875.511    1.64e+04\n",
       "8247.0       5130.0212   3044.372      1.685      0.092    -837.409    1.11e+04\n",
       "8253.0      -2202.2944   3019.455     -0.729      0.466   -8120.885    3716.296\n",
       "8290.0       6667.9159   3236.736      2.060      0.039     323.422     1.3e+04\n",
       "8293.0       7346.0103   3043.721      2.413      0.016    1379.855    1.33e+04\n",
       "8304.0       9985.5502   3093.631      3.228      0.001    3921.565     1.6e+04\n",
       "8334.0       9750.2882   3258.525      2.992      0.003    3363.085    1.61e+04\n",
       "8348.0       9196.9702   3156.426      2.914      0.004    3009.895    1.54e+04\n",
       "8357.0       8782.9091   3076.771      2.855      0.004    2751.971    1.48e+04\n",
       "8358.0       6761.4473   3044.175      2.221      0.026     794.402    1.27e+04\n",
       "8446.0      -3983.6259   3374.667     -1.180      0.238   -1.06e+04    2631.234\n",
       "8460.0       1.098e+04   3537.306      3.105      0.002    4051.291    1.79e+04\n",
       "8463.0       8838.8993   3110.041      2.842      0.004    2742.747    1.49e+04\n",
       "8479.0       7883.3944   3750.123      2.102      0.036     532.584    1.52e+04\n",
       "8530.0       2.055e+04   3130.331      6.565      0.000    1.44e+04    2.67e+04\n",
       "8536.0       5321.9245   3023.926      1.760      0.078    -605.430    1.12e+04\n",
       "8543.0        2.91e+04   3348.461      8.689      0.000    2.25e+04    3.57e+04\n",
       "8549.0        288.7530   3183.883      0.091      0.928   -5952.140    6529.646\n",
       "8551.0       9857.0564   3160.749      3.119      0.002    3661.508    1.61e+04\n",
       "8559.0       6877.9877   3301.144      2.084      0.037     407.244    1.33e+04\n",
       "8573.0       2363.2893   3354.437      0.705      0.481   -4211.916    8938.495\n",
       "8606.0       9996.9128   3074.136      3.252      0.001    3971.140     1.6e+04\n",
       "8607.0       1.008e+04   3199.827      3.150      0.002    3805.949    1.64e+04\n",
       "8648.0       8703.5470   3078.394      2.827      0.005    2669.428    1.47e+04\n",
       "8657.0       4027.2377   3062.602      1.315      0.189   -1975.927       1e+04\n",
       "8675.0       9234.4663   4423.761      2.087      0.037     563.222    1.79e+04\n",
       "8681.0       5022.8681   2990.914      1.679      0.093    -839.778    1.09e+04\n",
       "8687.0       6865.4753   3330.626      2.061      0.039     336.943    1.34e+04\n",
       "8692.0       6724.1365   3040.648      2.211      0.027     764.005    1.27e+04\n",
       "8699.0       9368.7656   3104.748      3.018      0.003    3282.988    1.55e+04\n",
       "8717.0       1.007e+04   3139.976      3.206      0.001    3910.883    1.62e+04\n",
       "8759.0       6032.6233   3129.293      1.928      0.054    -101.267    1.22e+04\n",
       "8762.0       1.005e+04   3163.140      3.177      0.001    3847.994    1.62e+04\n",
       "8819.0        1.08e+04   3255.498      3.319      0.001    4422.683    1.72e+04\n",
       "8850.0       9180.3611   3099.787      2.962      0.003    3104.308    1.53e+04\n",
       "8852.0       9396.6270   3137.971      2.994      0.003    3245.728    1.55e+04\n",
       "8859.0       9958.6402   3172.396      3.139      0.002    3740.263    1.62e+04\n",
       "8867.0       2878.0832   3278.995      0.878      0.380   -3549.244    9305.411\n",
       "8881.0       7955.4694   3090.496      2.574      0.010    1897.629     1.4e+04\n",
       "8958.0       7012.8960   3062.779      2.290      0.022    1009.384     1.3e+04\n",
       "8972.0      -1.071e+04   3041.870     -3.522      0.000   -1.67e+04   -4751.155\n",
       "8990.0        237.8112   3143.690      0.076      0.940   -5924.298    6399.920\n",
       "9004.0       1.047e+04   3372.333      3.105      0.002    3861.201    1.71e+04\n",
       "9016.0       7304.7780   3039.777      2.403      0.016    1346.353    1.33e+04\n",
       "9048.0       6403.1328   3012.256      2.126      0.034     498.654    1.23e+04\n",
       "9051.0       1965.6327   3401.698      0.578      0.563   -4702.212    8633.478\n",
       "9071.0       6269.8156   3082.415      2.034      0.042     227.814    1.23e+04\n",
       "9112.0       5595.9253   3023.816      1.851      0.064    -331.214    1.15e+04\n",
       "9114.0       3815.5590   3108.453      1.227      0.220   -2277.481    9908.599\n",
       "9132.0       9293.0465   4448.428      2.089      0.037     573.452     1.8e+04\n",
       "9173.0       8593.1758   3436.473      2.501      0.012    1857.168    1.53e+04\n",
       "9180.0       9960.1426   3212.901      3.100      0.002    3662.368    1.63e+04\n",
       "9186.0       8369.7659   3096.656      2.703      0.007    2299.850    1.44e+04\n",
       "9191.0       5236.9855   4093.728      1.279      0.201   -2787.343    1.33e+04\n",
       "9216.0       3177.1857   2977.132      1.067      0.286   -2658.445    9012.817\n",
       "9217.0        345.3862   2964.765      0.116      0.907   -5466.004    6156.777\n",
       "9225.0       9669.4538   3102.778      3.116      0.002    3587.537    1.58e+04\n",
       "9230.0       1.017e+04   3631.753      2.800      0.005    3049.714    1.73e+04\n",
       "9259.0       1.082e+04   3195.981      3.387      0.001    4559.228    1.71e+04\n",
       "9293.0       9826.5015   3129.155      3.140      0.002    3692.884     1.6e+04\n",
       "9299.0       4757.1723   3068.861      1.550      0.121   -1258.262    1.08e+04\n",
       "9308.0       5449.7947   3213.596      1.696      0.090    -849.341    1.17e+04\n",
       "9311.0       6091.4889   4097.403      1.487      0.137   -1940.043    1.41e+04\n",
       "9313.0       4314.0795   3025.653      1.426      0.154   -1616.660    1.02e+04\n",
       "9325.0       8571.9786   3107.010      2.759      0.006    2481.767    1.47e+04\n",
       "9332.0       7832.2604   3050.287      2.568      0.010    1853.235    1.38e+04\n",
       "9340.0      -1.284e+04   4266.321     -3.009      0.003   -2.12e+04   -4476.619\n",
       "9372.0       1.011e+04   3506.811      2.883      0.004    3237.019     1.7e+04\n",
       "9411.0       6355.8057   3158.698      2.012      0.044     164.278    1.25e+04\n",
       "9459.0       4594.2014   3082.822      1.490      0.136   -1448.598    1.06e+04\n",
       "9465.0       1.241e+04   3064.660      4.049      0.000    6402.210    1.84e+04\n",
       "9472.0       4269.6200   2986.010      1.430      0.153   -1583.412    1.01e+04\n",
       "9483.0       -216.2534   3044.454     -0.071      0.943   -6183.846    5751.340\n",
       "9563.0      -1.792e+04   4211.763     -4.256      0.000   -2.62e+04   -9668.157\n",
       "9590.0       7000.0639   3121.546      2.242      0.025     881.359    1.31e+04\n",
       "9598.0       4487.5163   3762.106      1.193      0.233   -2886.783    1.19e+04\n",
       "9599.0       3721.4346   3033.177      1.227      0.220   -2224.052    9666.921\n",
       "9602.0       3023.8387   4071.341      0.743      0.458   -4956.608     1.1e+04\n",
       "9619.0       1.003e+04   3221.664      3.113      0.002    3715.041    1.63e+04\n",
       "9643.0       6673.9738   3111.149      2.145      0.032     575.650    1.28e+04\n",
       "9650.0       5948.1118   3049.133      1.951      0.051     -28.652    1.19e+04\n",
       "9653.0      -5201.1049   5261.293     -0.989      0.323   -1.55e+04    5111.828\n",
       "9667.0       5978.4574   3032.475      1.971      0.049      34.345    1.19e+04\n",
       "9698.0       8360.8550   3113.996      2.685      0.007    2256.949    1.45e+04\n",
       "9699.0       9695.4952   3081.120      3.147      0.002    3656.031    1.57e+04\n",
       "9719.0       1673.7655   2960.502      0.565      0.572   -4129.267    7476.798\n",
       "9742.0       1462.5312   3123.254      0.468      0.640   -4659.520    7584.583\n",
       "9761.0       9161.1397   3123.661      2.933      0.003    3038.290    1.53e+04\n",
       "9771.0       2489.7585   2963.761      0.840      0.401   -3319.664    8299.181\n",
       "9772.0       9912.5502   3147.378      3.149      0.002    3743.211    1.61e+04\n",
       "9778.0       7202.1314   3024.139      2.382      0.017    1274.359    1.31e+04\n",
       "9799.0       1707.7949   3109.095      0.549      0.583   -4386.503    7802.093\n",
       "9815.0       9088.3459   3223.230      2.820      0.005    2770.327    1.54e+04\n",
       "9818.0      -3.452e+04   3202.790    -10.779      0.000   -4.08e+04   -2.82e+04\n",
       "9837.0       1.056e+04   3244.249      3.254      0.001    4198.886    1.69e+04\n",
       "9922.0       2994.0482   3000.566      0.998      0.318   -2887.517    8875.613\n",
       "9954.0       5697.1012   3683.632      1.547      0.122   -1523.378    1.29e+04\n",
       "9963.0       7227.4542   3090.223      2.339      0.019    1170.149    1.33e+04\n",
       "9988.0       1.018e+04   3189.909      3.192      0.001    3928.483    1.64e+04\n",
       "==============================================================================\n",
       "Omnibus:                    21744.289   Durbin-Watson:                   0.548\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         50237524.155\n",
       "Skew:                          10.223   Prob(JB):                         0.00\n",
       "Kurtosis:                     302.434   Cond. No.                     2.00e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large,  2e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_vars = df[x_vars]\n",
    "x_vars = sm.add_constant(x_vars)\n",
    "x_vars = x_vars.astype(float) # converts categorical booleans to floats\n",
    "\n",
    "lin_reg = sm.OLS(y_var,x_vars).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59501cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td colspan=\"1\"><em>Dependent variable: rmkvaf</em></td></tr><tr><td style=\"text-align:left\"></td><tr><td style=\"text-align:left\"></td><td>(1)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "\n",
       "<tr><td style=\"text-align:left\">const</td><td>-9940.577<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(2385.347)</td></tr>\n",
       "<tr><td style=\"text-align:left\">gspilltecIV</td><td>0.100<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.027)</td></tr>\n",
       "<tr><td style=\"text-align:left\">gspillsicIV</td><td>0.340<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.049)</td></tr>\n",
       "<tr><td style=\"text-align:left\">pat_count</td><td>-30.602<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(1.838)</td></tr>\n",
       "<tr><td style=\"text-align:left\">rsales</td><td>0.781<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.037)</td></tr>\n",
       "<tr><td style=\"text-align:left\">rppent</td><td>0.611<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.084)</td></tr>\n",
       "<tr><td style=\"text-align:left\">emp</td><td>18.064<sup>**</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(7.147)</td></tr>\n",
       "<tr><td style=\"text-align:left\">rxrd</td><td>18.594<sup>***</sup></td></tr>\n",
       "<tr><td style=\"text-align:left\"></td><td>(0.614)</td></tr>\n",
       "\n",
       "<tr><td style=\"text-align: left\">Firm & Time Effects</td><td>Yes</td></tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr>\n",
       "<tr><td style=\"text-align: left\">Observations</td><td>13385</td></tr><tr><td style=\"text-align: left\">R<sup>2</sup></td><td>0.665</td></tr><tr><td style=\"text-align: left\">Adjusted R<sup>2</sup></td><td>0.645</td></tr><tr><td style=\"text-align: left\">Residual Std. Error</td><td>9771.830 (df=12629)</td></tr><tr><td style=\"text-align: left\">F Statistic</td><td>33.221<sup>***</sup> (df=755; 12629)</td></tr>\n",
       "<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td><td colspan=\"1\" style=\"text-align: right\"><sup>*</sup>p&lt;0.1; <sup>**</sup>p&lt;0.05; <sup>***</sup>p&lt;0.01</td></tr></table>"
      ],
      "text/plain": [
       "<stargazer.stargazer.Stargazer at 0x7f5fdbe41cd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export results, omit fixed effects\n",
    "stargazer = Stargazer([lin_reg])\n",
    "\n",
    "main_vars = [col for col in x_vars.columns if col not in fixed_effects]\n",
    "cov_labels = {'rmkvaf': 'Market Value',\n",
    "             'pat_count': 'Patent Count',\n",
    "             'rsales': 'Sales',\n",
    "             'rppent': '',\n",
    "             'emp': 'Employment',\n",
    "             'rxrd': 'R&D Expenditures'}\n",
    "\n",
    "\n",
    "stargazer.covariate_order(main_vars)\n",
    "stargazer.add_line(\"Firm & Time Effects\", ['Yes'])\n",
    "#stargazer.rename_covariates(cov_labels)\n",
    "stargazer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94e65e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!htbp] \\centering\n",
      "\\begin{tabular}{@{\\extracolsep{5pt}}lc}\n",
      "\\\\[-1.8ex]\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "& \\multicolumn{1}{c}{\\textit{Dependent variable: rmkvaf}} \\\n",
      "\\cr \\cline{2-2}\n",
      "\\\\[-1.8ex] & (1) \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " const & -9940.577$^{***}$ \\\\\n",
      "& (2385.347) \\\\\n",
      " gspilltecIV & 0.100$^{***}$ \\\\\n",
      "& (0.027) \\\\\n",
      " gspillsicIV & 0.340$^{***}$ \\\\\n",
      "& (0.049) \\\\\n",
      " pat_count & -30.602$^{***}$ \\\\\n",
      "& (1.838) \\\\\n",
      " rsales & 0.781$^{***}$ \\\\\n",
      "& (0.037) \\\\\n",
      " rppent & 0.611$^{***}$ \\\\\n",
      "& (0.084) \\\\\n",
      " emp & 18.064$^{**}$ \\\\\n",
      "& (7.147) \\\\\n",
      " rxrd & 18.594$^{***}$ \\\\\n",
      "& (0.614) \\\\\n",
      " Firm & Time Effects & Yes \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Observations & 13385 \\\\\n",
      " $R^2$ & 0.665 \\\\\n",
      " Adjusted $R^2$ & 0.645 \\\\\n",
      " Residual Std. Error & 9771.830 (df=12629) \\\\\n",
      " F Statistic & 33.221$^{***}$ (df=755; 12629) \\\\\n",
      "\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "\\textit{Note:} & \\multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
      "\\end{tabular}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# render to latex\n",
    "print(stargazer.render_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbb04b",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0ad6a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# design = MS(df.columns.drop([col for col in df.columns if col not in x_vars.columns])).fit(df)\n",
    "# NOTE: including all of the fixed effects seems to kill the notebook. Running Ridge/LASSO on subset excluding FE's\n",
    "\n",
    "# Check: outlier in dataset driving a split in reg tree.\n",
    "## Drop two outlier firms: i = 5047, i = 12141, i = 6008\n",
    "df = df.loc[(df['i'] != '5047.0') & (df['i'] != '12141.0') & (df['i'] != '6008.0')]\n",
    "\n",
    "design = MS(df.columns.drop([col for col in df.columns if col not in x_vars.columns or col in fixed_effects])).fit(df)\n",
    "Y = np.array(df['rmkvaf'])\n",
    "X = design.transform(df)\n",
    "\n",
    "D = design.fit_transform(df)\n",
    "D = D.drop('intercept', axis=1)\n",
    "X = np.asarray(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "643f4695-8889-4a0f-8fd1-e7abe27e3d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>rmkvaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>20779.0</td>\n",
       "      <td>436988.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9175</th>\n",
       "      <td>8530.0</td>\n",
       "      <td>271724.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9176</th>\n",
       "      <td>8530.0</td>\n",
       "      <td>228610.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7418</th>\n",
       "      <td>7257.0</td>\n",
       "      <td>202124.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>20779.0</td>\n",
       "      <td>194111.171875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11426</th>\n",
       "      <td>10215.0</td>\n",
       "      <td>0.756740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10890</th>\n",
       "      <td>9799.0</td>\n",
       "      <td>0.718651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9447</th>\n",
       "      <td>8681.0</td>\n",
       "      <td>0.634273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10891</th>\n",
       "      <td>9799.0</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10892</th>\n",
       "      <td>9799.0</td>\n",
       "      <td>0.432155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             i         rmkvaf\n",
       "904    20779.0  436988.531250\n",
       "9175    8530.0  271724.750000\n",
       "9176    8530.0  228610.859375\n",
       "7418    7257.0  202124.515625\n",
       "903    20779.0  194111.171875\n",
       "...        ...            ...\n",
       "11426  10215.0       0.756740\n",
       "10890   9799.0       0.718651\n",
       "9447    8681.0       0.634273\n",
       "10891   9799.0       0.617000\n",
       "10892   9799.0       0.432155\n",
       "\n",
       "[13325 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['i','rmkvaf']].sort_values('rmkvaf', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "968dedb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ModelSpec(terms=Index([&#x27;rxrd&#x27;, &#x27;pat_count&#x27;, &#x27;rsales&#x27;, &#x27;rppent&#x27;, &#x27;emp&#x27;, &#x27;gspilltecIV&#x27;,\n",
       "       &#x27;gspillsicIV&#x27;],\n",
       "      dtype=&#x27;object&#x27;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ModelSpec</label><div class=\"sk-toggleable__content\"><pre>ModelSpec(terms=Index([&#x27;rxrd&#x27;, &#x27;pat_count&#x27;, &#x27;rsales&#x27;, &#x27;rppent&#x27;, &#x27;emp&#x27;, &#x27;gspilltecIV&#x27;,\n",
       "       &#x27;gspillsicIV&#x27;],\n",
       "      dtype=&#x27;object&#x27;))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ModelSpec(terms=Index(['rxrd', 'pat_count', 'rsales', 'rppent', 'emp', 'gspilltecIV',\n",
       "       'gspillsicIV'],\n",
       "      dtype='object'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6996449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1097622458263.0627, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1097582592759.0806, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1097532301523.779, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1097287928278.9343, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1097160671205.5079, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1097000226857.8716, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1096797985075.5941, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1096543128226.2659, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1096222080776.6653, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1095817830767.4758, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1095309098831.6775, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1094669329146.029, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1093865477861.7535, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1092856580086.3606, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1091592089381.1823, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1090010008379.0867, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1088034871573.5668, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1085575709348.579, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1082524224690.4636, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1078753558370.0934, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1074118206320.2416, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1068455871174.8545, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1061592236580.0118, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1053349760721.1654, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1043561450382.8282, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1032090009162.562, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1018851580344.3143, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1003841505405.8765, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 987157411344.5521, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 969013309233.9867, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 949738353518.6455, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 929756390102.0393, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 909547319239.4812, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 889597049624.498, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 870346886024.6908, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 852153510263.9482, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 835267070262.4497, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 819829073989.0403, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 805886387525.1746, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 793414535420.8507, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 782343133947.8167, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 772577943030.8448, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 764016577236.3497, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756557398060.3286, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 750102904742.2596, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 744559790655.4528, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 739837788505.7505, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 735848784403.1954, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 732506844689.9487, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 729729116868.3756, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 727437204256.852, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 725558555503.5277, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 724027534585.2634, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 722786015892.2922, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 721783499166.7552, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 720976829563.4169, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 720329642920.7789, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 719811653588.1552, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 719397880466.3943, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 719067879710.983, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718805027390.9995, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718595875459.7067, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718429590192.3008, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718297473033.9185, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718192558411.4476, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718109280357.261, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 718043198837.4747, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717990776778.7076, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717949199467.1333, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717916228940.6221, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717890087018.1964, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717869361602.1312, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717852931790.8408, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717839908132.1796, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717829585022.8748, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717821402826.8296, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717814917754.3455, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717809777928.9146, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717805704380.7697, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717802475959.7673, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717799917363.3588, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717797889639.066, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717796282651.7646, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717795009110.0918, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717793999830.1788, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717793199980.6025, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717792566105.6345, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717792063765.6301, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717791665666.6669, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717791350178.1057, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717791100157.5858, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790902019.8363, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790744998.5765, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790620561.6069, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790521947.2557, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790443796.9402, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790381864.081, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 717790332783.2946, tolerance: 219554957.16334796\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEUAAAAVCAYAAAAQAyPeAAAACXBIWXMAAA7EAAAOxAGVKw4bAAADIUlEQVRYCd2Y7XHTQBCGRYYClNCB6MAhFRB3YNwBcQfkp/0vEzogVMAkHQAVMLgDuwOMOzDvs74Vh3QRR6yxSXbmtHer9/ZLezenKzabTRG36XRaxeOn3k/Fe1RENJvN3mk4cJHGldqFWumyJ8iJkbhrekYlQHoxEjsTvzTBb9mtjxN8LfxxQr6TSDr5MNg9VX+dUib5dZD/EH+pdi3ZsonNwQlDUojlhvmWFA2ohK/ipwidNMYwDraMSXaudinMnfjOFHz4KEUrtVdq2D2WvJUUyb7r3ZXbDnORDdWvfc3FaV4RsK/F188RiAj+g/UaD4GGDREKKmTivSQk6CL4N6H/xzJG5iSbF+qXsW31+cr4Qgzmby7O9Ya55GHie8pYSqx0IhDdb42xDynViQ/2zEncPGETX8/lF1UP5eIMHOInD+WRHuwldckZIjz0rlUJkpHNqxi35z7LliXWJI+B91AubovePtExplIoty9bWfdTCWHZDMRTX6p7cg9vZderoEvbSS4uoYQ8DEkKm9oiAUiJqBLaoegkGG5tvpFDJC4XF02zLnmoSApKUuVoKH+EKmHNZlWVzzsAf5FpM4UjD5YUstqVebfBxurr1mX75l0fz6uDc0suruk/8ZVUSi6xIXcZy9XzYJyq1D8e1d0kly1zcU0FGluBkBQCdYUJnJ1HeF+prZOA/QpZvvjSJK8UX965uFgPca5ICiWTMhKD2YyheytFX6czsdvpvTw5/rs/sUJO4/OoSnJxsQ4SuyQpc7Wz+E2i7wEnKyUk5Kc4R+0+yDdB//q1TtngkLkSZzkbBftjDd4GUZGLc3zglliO+Z/UyGoXkTgSwqmxRXKAY7ZtUuIcwZPJa01sCDTP/fAD2G3Q+1k8PnHjPKdqPiYbK5z/FvyMKRfnc7A7sbsU3Sks1Aa73p1Ix0it3FXPIebL70ptgW2WD8SBrI9/Ga4eHlQl5sVhH1yZ2MHUkhJKk8uWv22497qtuaVeUsqPjkLcxG9L1CuFQPirTF4fZEbJDd37TOz/BiPueqXUN294GTI2esTB/XOyFSt3N3fi9Wn9F1Hku17ZY8bCAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle \\left( 7, \\  100\\right)$"
      ],
      "text/plain": [
       "(7, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs = X - X.mean(0)[None,:]\n",
    "X_scale = X.std(0)\n",
    "Xs = Xs / X_scale[None,:]\n",
    "lambdas = 10**np.linspace(8, -2, 100) / Y.std()\n",
    "soln_array = lm.ElasticNet.path(Xs,\n",
    "                                 Y,\n",
    "                                 l1_ratio=0.,\n",
    "                                 alphas=lambdas)[1]\n",
    "soln_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12466d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rxrd</th>\n",
       "      <th>pat_count</th>\n",
       "      <th>rsales</th>\n",
       "      <th>rppent</th>\n",
       "      <th>emp</th>\n",
       "      <th>gspilltecIV</th>\n",
       "      <th>gspillsicIV</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative log(lambda)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-8.997627</th>\n",
       "      <td>0.892316</td>\n",
       "      <td>0.424800</td>\n",
       "      <td>0.842053</td>\n",
       "      <td>0.725879</td>\n",
       "      <td>0.597987</td>\n",
       "      <td>0.388765</td>\n",
       "      <td>0.325032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.765043</th>\n",
       "      <td>1.125856</td>\n",
       "      <td>0.535961</td>\n",
       "      <td>1.062429</td>\n",
       "      <td>0.915848</td>\n",
       "      <td>0.754468</td>\n",
       "      <td>0.490504</td>\n",
       "      <td>0.410105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.532459</th>\n",
       "      <td>1.420481</td>\n",
       "      <td>0.676185</td>\n",
       "      <td>1.340440</td>\n",
       "      <td>1.155497</td>\n",
       "      <td>0.951864</td>\n",
       "      <td>0.618847</td>\n",
       "      <td>0.517433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.299874</th>\n",
       "      <td>1.792144</td>\n",
       "      <td>0.853057</td>\n",
       "      <td>1.691136</td>\n",
       "      <td>1.457798</td>\n",
       "      <td>1.200849</td>\n",
       "      <td>0.780738</td>\n",
       "      <td>0.652830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8.067290</th>\n",
       "      <td>2.260952</td>\n",
       "      <td>1.076129</td>\n",
       "      <td>2.133482</td>\n",
       "      <td>1.839098</td>\n",
       "      <td>1.514876</td>\n",
       "      <td>0.984929</td>\n",
       "      <td>0.823623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.097886</th>\n",
       "      <td>4847.562838</td>\n",
       "      <td>-405.321162</td>\n",
       "      <td>2395.633459</td>\n",
       "      <td>1591.688181</td>\n",
       "      <td>-1205.604869</td>\n",
       "      <td>167.101411</td>\n",
       "      <td>1176.284323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.330471</th>\n",
       "      <td>4847.567994</td>\n",
       "      <td>-405.322699</td>\n",
       "      <td>2395.633438</td>\n",
       "      <td>1591.688347</td>\n",
       "      <td>-1205.608144</td>\n",
       "      <td>167.100739</td>\n",
       "      <td>1176.284323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.563055</th>\n",
       "      <td>4847.572079</td>\n",
       "      <td>-405.323918</td>\n",
       "      <td>2395.633421</td>\n",
       "      <td>1591.688479</td>\n",
       "      <td>-1205.610740</td>\n",
       "      <td>167.100207</td>\n",
       "      <td>1176.284324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.795639</th>\n",
       "      <td>4847.575318</td>\n",
       "      <td>-405.324883</td>\n",
       "      <td>2395.633408</td>\n",
       "      <td>1591.688583</td>\n",
       "      <td>-1205.612797</td>\n",
       "      <td>167.099785</td>\n",
       "      <td>1176.284324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.028224</th>\n",
       "      <td>4847.577884</td>\n",
       "      <td>-405.325648</td>\n",
       "      <td>2395.633397</td>\n",
       "      <td>1591.688666</td>\n",
       "      <td>-1205.614427</td>\n",
       "      <td>167.099450</td>\n",
       "      <td>1176.284324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             rxrd   pat_count       rsales       rppent  \\\n",
       "negative log(lambda)                                                      \n",
       "-8.997627                0.892316    0.424800     0.842053     0.725879   \n",
       "-8.765043                1.125856    0.535961     1.062429     0.915848   \n",
       "-8.532459                1.420481    0.676185     1.340440     1.155497   \n",
       "-8.299874                1.792144    0.853057     1.691136     1.457798   \n",
       "-8.067290                2.260952    1.076129     2.133482     1.839098   \n",
       "...                           ...         ...          ...          ...   \n",
       " 13.097886            4847.562838 -405.321162  2395.633459  1591.688181   \n",
       " 13.330471            4847.567994 -405.322699  2395.633438  1591.688347   \n",
       " 13.563055            4847.572079 -405.323918  2395.633421  1591.688479   \n",
       " 13.795639            4847.575318 -405.324883  2395.633408  1591.688583   \n",
       " 14.028224            4847.577884 -405.325648  2395.633397  1591.688666   \n",
       "\n",
       "                              emp  gspilltecIV  gspillsicIV  \n",
       "negative log(lambda)                                         \n",
       "-8.997627                0.597987     0.388765     0.325032  \n",
       "-8.765043                0.754468     0.490504     0.410105  \n",
       "-8.532459                0.951864     0.618847     0.517433  \n",
       "-8.299874                1.200849     0.780738     0.652830  \n",
       "-8.067290                1.514876     0.984929     0.823623  \n",
       "...                           ...          ...          ...  \n",
       " 13.097886           -1205.604869   167.101411  1176.284323  \n",
       " 13.330471           -1205.608144   167.100739  1176.284323  \n",
       " 13.563055           -1205.610740   167.100207  1176.284324  \n",
       " 13.795639           -1205.612797   167.099785  1176.284324  \n",
       " 14.028224           -1205.614427   167.099450  1176.284324  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))\n",
    "soln_path.index.name = 'negative log(lambda)'\n",
    "\n",
    "soln_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "222ea61d-8649-47d5-9913-dbf843cdde9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_fig, ax = plt.subplots(figsize=(8,8))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficients', fontsize=20)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# save figure\n",
    "plt.savefig('ridge_coef.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba1608",
   "metadata": {},
   "source": [
    "## Cross-Validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baaa9c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.698e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.502e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.564e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.116e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.894e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.698e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.501e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.564e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.115e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.894e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.698e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.501e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.564e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.115e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.893e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.697e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.500e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.563e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.115e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.893e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.696e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.500e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.562e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.114e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.892e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.695e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.499e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.562e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.113e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.892e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.694e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.498e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.561e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.112e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.891e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.693e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.497e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.559e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.111e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.889e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.691e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.495e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.558e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.109e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.888e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.689e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.493e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.556e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.107e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.886e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.686e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.491e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.553e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.104e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.883e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.683e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.487e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.550e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.101e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.880e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.679e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.484e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.546e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.097e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.876e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.673e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.479e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.541e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.092e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.872e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.666e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.472e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.534e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.085e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.865e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.658e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.464e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.526e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.077e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.858e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.647e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.455e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.516e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.067e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.848e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.634e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.442e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.054e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.836e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.617e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.427e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.487e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.038e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.821e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.596e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.408e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.467e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.019e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.802e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.570e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.384e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.443e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.994e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.779e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.538e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.355e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.413e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.963e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.750e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.498e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.319e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.375e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.926e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.715e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.450e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.274e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.330e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.880e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.672e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.392e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.221e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.274e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.825e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.619e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.322e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.157e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.208e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.758e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.556e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.239e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.081e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.129e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.679e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.481e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.142e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.992e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.037e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.587e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.394e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.029e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.889e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.930e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.480e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.292e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.902e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.773e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.809e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.359e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.177e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.760e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.643e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.674e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.224e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.049e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.607e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.503e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.528e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.078e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.910e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.443e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.354e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.372e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.922e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.762e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.274e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.199e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.210e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.761e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.608e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.103e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.043e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.047e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.598e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.453e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.935e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.889e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.886e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.436e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.299e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.772e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.741e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.730e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.281e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.150e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.618e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.601e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.583e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.133e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.010e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.476e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.471e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.446e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.996e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.879e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.346e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.352e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.322e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.871e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.760e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.228e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.245e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.210e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.757e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.652e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.124e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.149e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.109e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.655e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.555e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.030e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.063e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.021e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.564e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.469e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.948e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.988e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.943e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.484e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.393e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.877e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.922e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.875e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.413e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.327e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.814e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.864e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.815e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.351e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.269e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.760e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.813e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.764e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.297e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.219e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.713e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.770e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.721e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.251e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.176e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.674e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.733e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.684e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.211e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.139e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.640e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.701e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.653e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.178e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.108e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.612e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.675e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.627e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.150e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.082e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.589e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.653e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.605e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.126e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.061e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.570e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.635e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.587e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.107e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.043e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.554e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.620e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.573e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.091e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.029e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.541e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.608e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.561e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.078e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.017e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.531e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.598e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.551e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.067e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.007e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.523e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.590e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.543e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.059e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.000e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.516e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.583e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.537e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.052e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.993e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.510e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.578e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.532e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.046e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.988e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.506e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.574e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.528e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.042e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.984e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.503e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.571e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.525e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.038e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.981e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.500e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.568e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.523e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.036e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.978e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.498e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.566e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.520e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.033e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.976e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.496e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.564e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.519e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.032e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.975e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.494e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.563e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.518e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.030e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.974e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.493e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.562e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.517e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.029e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.973e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.492e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.561e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.516e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.028e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.492e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.560e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.515e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.027e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.971e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.491e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.560e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.515e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.027e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.971e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.491e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.559e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.514e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.026e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.970e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.559e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.514e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.026e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.970e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.559e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.514e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.026e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.970e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.490e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.489e+11, tolerance: 1.740e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.558e+11, tolerance: 1.701e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.513e+11, tolerance: 1.513e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.025e+11, tolerance: 1.623e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.969e+11, tolerance: 1.579e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.458e+11, tolerance: 2.039e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869826287367.8448, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869790761166.3677, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869745944639.9281, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869689412420.6233, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869618108395.422, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869528183110.4602, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869414790110.5598, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869271831346.6857, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 869091639715.147, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 868864584476.9196, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 868218498629.7336, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 867765406935.7318, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 867195703115.2474, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 866480035823.805, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 865582049719.963, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 864456936033.4359, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 863049812799.1011, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 861293996514.261, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 859109290172.406, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 856400506536.4086, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 853056576011.53, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 848950755107.9131, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 843942639072.7588, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 837882848510.794, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 830621321129.2244, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 822019963846.3278, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811969842853.6812, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 800411977582.4825, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 787359186274.8766, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 772914652728.2882, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 757281690958.6362, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 740759543195.3231, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 723722615210.2457, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 706585010366.2926, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 689757003667.7996, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 673603011145.9363, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 658410152507.386, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 644372876061.2927, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 631594084419.8286, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 620098937463.5056, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 609855370679.973, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 600795470347.54, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 592833456936.6901, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 585878193010.355, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 579840102736.5327, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 574633734750.995, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 570177769246.6417, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 566394147467.4489, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 563207444528.3628, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 560544939057.8539, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 558337315225.6691, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556519675822.3036, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555032520014.8164, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 553822446027.4701, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 552842478294.1228, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 552052030123.9502, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 551416576682.1167, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 550907134616.5184, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 550499638870.0391, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 550174288451.878, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549914911142.5471, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549708377647.4849, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549544080660.3044, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549413483804.5198, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549309738778.25385, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549227365269.7114, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549161986445.76855, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549110112324.5811, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549068963603.6097, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549036329171.4103, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549010451357.47076, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548989933832.27155, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548973667883.30493, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548960773524.41064, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548950552531.8815, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548942451040.9233, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548936029787.1447, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548930940449.77026, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548926906857.77344, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548923710066.92633, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548921176515.2545, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548919168624.9124, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548917577347.13635, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548916316249.6429, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548915316828.16644, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548914524789.11334, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548913897102.5191, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548913399665.87665, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548913005452.2077, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548912693042.34454, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548912445461.446, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548912249256.9174, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548912093767.6168, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548911970544.65375, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548911872892.3449, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548911795504.41705, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548911734175.7176, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548911685573.72327, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548911647057.4395, tolerance: 173992407.607251\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 850148495565.4346, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 850116106792.5237, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 850075248164.173, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 850023708514.6371, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849958701643.0793, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849876718081.5232, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849773339427.1764, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849643006244.7684, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849478728659.2225, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849271726649.3964, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 849010984811.8335, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 848682704186.2887, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 848269631990.6367, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 847750249441.5076, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 847097799293.3256, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 846279139977.5219, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 845253424787.3964, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 843970626079.6783, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 842369960927.9214, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 840378332311.8745, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 837908985563.3373, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 834860698737.8872, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 831117977372.5637, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 826552894803.504, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 821029370161.2079, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 814410731035.6235, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806571245990.6736, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 797411783874.1235, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 786878742419.1819, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 774983914251.231, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 761821341643.1592, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 747576131947.2844, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 732520542112.6641, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 716994987955.3362, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 701375693293.2131, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 686035037027.6952, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 671303296059.6826, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 657440059955.6854, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 644620300160.1892, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 632935518038.7654, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 622406527795.4402, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 613002465011.1118, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 604660653016.2261, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 597303360585.9879, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 590849418402.4075, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 585220478376.5789, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 580343006664.4098, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 576147725728.1184, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 572568185778.163, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 569539649297.3967, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 566998812224.8191, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 564884333036.6602, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 563137838096.5941, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 561705014053.8441, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 560536494218.2529, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 559588392028.9576, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 558822460989.0457, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 558205939081.062, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557711168355.8235, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557315081306.3076, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556998630037.208, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556746213339.8416, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556545137027.2313, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556385127050.5374, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556257903554.2899, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556156816600.6892, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556076539902.1008, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556012816650.1897, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555962250680.3822, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555922136211.4186, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555890319871.2937, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555865089414.6738, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555845084300.6666, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555829224046.0416, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555816650952.2583, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555806684405.4906, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555798784463.3286, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555792522873.734, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555787560029.9232, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555783626658.3154, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555780509275.5859, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555778038644.1693, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555776080611.2017, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555774528840.8705, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555773299050.0425, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555772324437.1069, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555771552057.4946, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555770939950.1024, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555770454859.3342, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555770070429.35, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555769765772.7441, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555769524336.0696, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555769333000.6921, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555769181370.0771, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555769061205.0288, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555768965976.0461, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555768890508.5609, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555768830701.7742, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555768783305.86, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 555768745745.3717, tolerance: 170054451.5504226\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756429971272.8939, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756399386291.8805, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756360802175.5592, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756312130064.753, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756250737528.4619, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756173308238.8473, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756075666086.5016, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 755952555119.565, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 755797364838.7308, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 755601788276.591, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 755355397999.1035, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 755045122841.9657, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 754654606134.2405, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 754163424899.018, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 753546149952.8733, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 752771230392.7434, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 751799694879.9923, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 750583679681.4187, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 749064824191.765, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 747172624625.656, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 744822912714.1238, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 741916735115.3896, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 738340053580.667, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 733964858208.5405, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 728652458395.6538, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 722259826641.7338, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 714649812716.8392, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 705705672484.2123, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 695349518524.1165, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 683562945671.088, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 670406402601.198, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 656032425358.9238, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 640687490642.2561, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 624698803240.8785, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 608445939437.9137, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 592321902485.6882, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 576691848057.7954, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 561858663832.9738, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548042216692.9524, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 535374581448.45776, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523909003207.9317, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513637428507.4411, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504510757710.99744, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496457084291.3463, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 489395194359.07635, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 483242653673.9199, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 477919348651.66724, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 473348127517.6686, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469454233946.79785, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 466164765813.385, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463408745876.87555, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461117839229.09344, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 459227439934.2414, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457677777507.7236, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456414776517.43994, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 455390538933.93976, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 454563439155.47003, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 453897896973.84906, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 453363924450.22156, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 452936542132.70306, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 452595143199.8814, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 452322862136.90283, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 452105983928.80273, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451933413286.35455, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451796211639.1191, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451687201966.6385, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451600637045.36176, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451531924425.97156, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451477400649.1667, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451434147280.8172, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451399841902.7752, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451372637973.74054, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451351068317.57715, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451333967813.432, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451320411606.3758, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451309665810.36707, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451301148233.016, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451294397119.3541, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451289046299.17914, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451284805439.7732, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451281444363.6371, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451278780600.22284, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451276669507.7077, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451274996436.68884, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451273670514.87463, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451272619718.3565, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451271786963.658, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451271127009.48926, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451270604000.6086, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451270189520.9464, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451269861050.34906, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451269600741.41876, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451269394450.0251, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451269230966.9963, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451269101409.0923, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451268998736.4166, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451268917369.9238, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451268852888.272, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451268801787.6055, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451268761291.1569, tolerance: 151309366.91366094\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811576507257.9117, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811544200373.8899, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811503444541.2633, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811452033751.9979, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811387188126.6423, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811305405877.0795, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811202277820.3407, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 811072255408.3435, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 810908361321.0933, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 810701829509.1035, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 810441659245.0438, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 810114065434.2155, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 809701805480.6979, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 809183362012.8024, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 808531961739.1206, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 807714415228.7245, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 806689772919.7266, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 805407812713.3334, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 803807409017.1233, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 801814888345.7378, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 799342559658.6896, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 796287724624.1521, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 792532625124.7667, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 782386781788.7372, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 775711624891.9635, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 767785691880.0886, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 758498349223.5032, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 747782317564.5596, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 735634438856.7441, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 722134215257.4009, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 707455010921.0767, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691862809681.8374, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675699457013.0526, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 659351292322.0494, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 643208715663.4777, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 627625480014.4329, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 612886705445.9225, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 599191625489.7389, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 586652356955.605, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 575305622299.8491, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 565131854103.8694, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 556075826852.8726, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 548064327656.8072, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 541018491854.1902, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 534860492261.1704, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 529515758803.15674, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524912589456.55225, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520980938011.15155, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 517651589480.3713, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 514856213893.85236, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512528217068.12744, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 510604015835.2862, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509024330701.82916, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 507735204126.9883, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 506688608627.2699, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505842637557.37573, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505161348765.1085, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504614361744.8905, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504176307769.6442, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503826215178.3989, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503546889640.4275, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503324328166.7467, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503147188670.83887, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503006324575.661, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502894385812.38074, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502805482651.97955, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502734906229.5433, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502678898583.6457, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502634464949.52203, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502599221499.2632, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502571272440.56824, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502549111195.5907, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502531541183.2328, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502517612468.26324, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502506571195.1385, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502497819286.8044, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502490882362.6925, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502485384223.57275, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502481026574.0647, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502477572917.1954, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502474835768.4105, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502472666508.75073, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502470947334.65436, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502469584872.8059, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502468505116.3303, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502467649409.72406, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502466971265.53864, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502466433840.7327, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502466007936.293, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502465670411.49884, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502465402927.1111, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502465190949.19666, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502465022959.6261, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502464889830.33307, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502464784327.3659, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502464700717.8953, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502464634458.7119, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502464581949.3763, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502464540336.583, tolerance: 162339990.65153286\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 789418382953.7173, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 789389506049.59, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 789353076648.3147, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 789249157973.936, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 789176051826.1652, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 789083860799.4662, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 788967621915.3064, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 788821092953.1854, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 788636429421.2988, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 788403783913.8997, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 788110811559.6951, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 787742063282.2771, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 787278247343.5273, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 786695339961.2717, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 785963529030.1345, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 785045983208.533, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 783897454937.9991, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 782462754647.0634, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 780675180107.6663, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 778455056286.3558, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 775708643354.5835, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 772327806621.6211, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 768191005429.1339, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 763166322843.4991, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 757117367046.7859, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 749912829048.1698, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 741440140575.3557, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 731622898333.6343, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 720440450088.0492, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 707946442723.9181, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 694281726647.725, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 679676616015.9154, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 664438910322.9763, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 648927454375.1791, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 633515417849.5056, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 618551064408.0045, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 604324770188.103, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 591048886048.2253, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 578852782150.7124, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 567791033642.5731, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 557859864762.3317, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 549016267743.6491, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 541195252593.76483, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 534322601378.1079, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528322474947.14056, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523120705222.90027, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518645359201.1116, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 514826201194.4665, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 511594227353.4084, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 508881816412.6382, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 506623509070.13794, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504757135406.37885, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 503224950189.96466, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 501974521240.7457, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500959248072.5053, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500138501940.63745, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 499477449083.12476, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498946647992.88727, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498521511742.8657, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 498181711315.34424, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497910575687.3178, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497694525065.1165, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497522557861.5043, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497385800437.573, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497277120901.255, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497190803567.0346, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497122278193.7724, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497067897104.35474, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497024753195.30505, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496990532271.0285, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496963393822.07336, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496941875143.63525, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496924814462.019, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496911289450.4367, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496900568147.62354, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496892069836.79803, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496885333900.9964, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496879995052.0984, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496875763643.91907, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496872410035.2427, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496869752175.51825, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496867645752.69403, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496865976376.7478, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496864653379.70795, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496863604898.8208, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496862773977.84064, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496862115475.98425, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496861593617.5, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496861180049.1625, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496860852300.5585, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496860592563.65564, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496860386725.5058, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496860223601.60645, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496860094328.27936, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496859991881.1001, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496859910693.3037, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496859846353.2431, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496859795364.78485, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496859754957.2609, tolerance: 157905743.84411123\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.458e+11, tolerance: 2.039e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.412e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.423e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.361e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.164e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.906e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.372e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.327e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.133e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.874e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.338e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.359e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.299e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.107e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.846e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.309e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.333e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.275e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.084e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.823e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.312e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.256e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.066e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.804e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.263e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.293e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.051e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.278e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.774e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.232e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.265e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.027e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.763e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.255e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.205e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.754e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.246e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.198e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.012e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.239e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.006e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.741e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.196e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.233e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.002e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.736e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.191e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.228e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.732e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.187e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.224e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.995e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.729e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.183e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.221e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.993e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.726e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.180e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.219e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.176e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.991e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.724e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.178e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.217e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.989e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.177e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.215e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.988e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.175e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.987e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.720e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.174e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.212e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.986e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.719e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.211e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.172e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.985e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.718e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.169e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.171e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.210e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.984e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.717e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.209e+11, tolerance: 1.056e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e+11, tolerance: 1.057e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e+11, tolerance: 9.926e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e+11, tolerance: 9.298e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.170e+11, tolerance: 1.051e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+11, tolerance: 1.272e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528048475261.1476, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528028712201.3733, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528003779853.7872, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527972328039.55585, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527932655056.85516, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527882616831.1157, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527819512981.9874, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527739944177.0779, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527639633910.0223, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527513206424.5934, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527353910938.11755, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527153280678.2027, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526900713704.603, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526582961350.8442, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526183509915.6277, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525681842822.88916, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525052575198.462, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524264462701.0988, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523279304385.8978, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522050789253.4084, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520523382711.05853, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518631417403.5329, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 516298646118.0012, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513438630856.7906, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509956468325.56885, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505752454308.95483, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 500728304455.72943, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494796381283.3492, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 487891915369.5767, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 479987377783.83386, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 471107016026.51575, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461338397088.52405, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 450837160573.1199, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 439821717136.91034, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 417326936099.3899, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 406407524612.5433, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 396035711204.03156, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 386391292764.2608, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 377588098700.65955, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 369676463825.0127, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362653792964.12823, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 356479216265.63873, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 351088600855.63184, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 346407288032.43677, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 342359261871.9338, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 338872611254.71686, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335881927139.9824, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333328639476.0495, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 331160300375.7219, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329329583248.9241, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327793440420.2401, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326512572078.0876, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325451174874.47906, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324576862390.36115, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323860647324.16943, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323276906376.4171, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322803285794.5584, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322420535620.9771, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322112280500.87134, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321864745283.44434, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321666456792.3605, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321507941514.9297, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321381434805.5795, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321280612257.72455, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321200349295.871, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321136511341.2756, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321085774261.93506, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321045473139.6256, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321013476475.1238, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320988082590.7134, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320967934986.65875, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320951953618.0829, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320939279376.3065, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320929229415.40283, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320921261318.5991, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320914944427.2076, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320909936945.90155, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320905967689.3468, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320902821547.4742, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320900327923.1949, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320898351542.0104, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320896785151.3644, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320895543724.1404, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320894559858.11847, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320893780125.83386, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320893162179.1743, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320892672453.09174, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320892284344.8071, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891976770.1332, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891733019.01526, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891539848.2595, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891386762.37573, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891265443.57776, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891169299.97174, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891093107.4647, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320891032725.99866, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890984874.5831, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890946953.0737, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320890916900.8732, tolerance: 105624797.09384269\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528448074352.54614, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528427023256.1906, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528400466065.59424, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528366964687.70917, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528324706651.54724, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528271408371.6509, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528204193844.44086, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528119442792.71893, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 528012600967.84644, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527877943822.0877, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527708283110.30475, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527494604261.1042, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 527225620756.6161, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526887230612.8238, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 526461859931.5777, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525927680333.1277, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525257692328.4793, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524418677530.03534, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523370042128.7293, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522062606437.23, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520437445565.906, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 518424959647.8668, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 515944451803.10645, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 512904615742.24835, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 509205467494.5067, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 504742360129.68207, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 499412727894.03046, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493126014503.15674, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 485816732477.45984, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 477459702431.0634, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 468085302972.58325, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457791345455.77686, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 446747567214.08594, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435189385358.10583, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 423399794749.2046, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 411681660508.6588, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 400325849048.0717, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 389582146651.75653, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 379638947727.57336, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 370614745393.61316, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 362560947768.7969, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 355472911563.22363, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 349305034756.4879, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 343986135882.7355, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 339432547580.5959, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335557722893.6556, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 332278275616.3448, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329517082133.2898, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 327204364078.7225, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325277641555.0235, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323681226790.79376, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322365650129.9352, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321287174041.3808, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320407403965.502, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319692945566.3684, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319115056836.1172, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318649267087.4754, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318274959992.5095, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317974934182.2276, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317734961259.1421, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317543360222.10895, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317390602770.00543, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317268958412.0658, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317172183309.6808, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317095252950.44354, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317034136200.00104, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316985606809.7544, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316947087808.3547, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316916524109.341, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316892278911.45886, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316873049894.24744, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316857801712.4983, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316845711802.20276, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316836126989.19214, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316828528820.82166, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316822505913.719, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316817731927.2277, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316813948036.62396, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316810948998.599, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316808572080.1489, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316806688267.1168, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316805195285.9068, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316804012066.2944, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316803074348.8467, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316802331201.10596, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316801742254.9727, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316801275516.2793, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800905628.2238, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800612494.7311, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800380189.23584, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800196089.70905, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316800050193.1062, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799934571.9959, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799842943.897, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799770329.9782, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799712784.55383, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799667180.7024, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799631040.3784, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799602399.75616, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316799579702.53064, tolerance: 105705701.29629055\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496224986600.4098, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496205483430.9915, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496180879003.4755, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496149840938.634, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496110689986.6315, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 496061310390.4424, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495999037480.6409, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495920516952.20795, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495821529061.52606, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495696769590.49835, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495539577886.29675, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495341600682.1602, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 495092378905.4019, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 494778843591.30176, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493889735660.7309, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 493268900421.383, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 492491401369.00775, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491519592349.6038, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 490307852466.6972, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 488801501847.7192, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 486935925685.1531, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484636163024.79175, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 481817331611.39325, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 478386383823.75006, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 474245787459.3388, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 469299735751.27167, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463463318789.9132, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456674624265.47943, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 448908907999.72864, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 440192844084.81586, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 430615723679.48834, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 420333869024.571, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 409565095646.47437, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 398572108690.56445, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 387636845872.0059, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 377030784428.063, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 366987691583.3198, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 357684456234.36755, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 349232917263.3622, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 341682315799.8093, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 335029500154.01135, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329232987932.6658, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324227325738.5381, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319935310083.5559, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316276934726.40247, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 313175002174.43695, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 310558007527.37164, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 308361167670.9948, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 306526422077.2857, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 305002010165.59644, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 303741968330.2816, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 302705680357.5455, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301857493473.99133, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 301166367399.2714, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 300605523846.75024, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 300152079950.3474, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299786664561.76294, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299493025795.1905, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299257641896.0869, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299069347401.27576, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298918984384.26514, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298799085551.5159, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298703592851.4171, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298627612546.65295, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298567205612.6084, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298519210909.2883, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298481097764.13794, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298450844272.167, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298426837629.19275, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298407793039.61993, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298392688084.16736, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298380709825.3931, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298371212324.08496, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298363682610.52576, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298357713487.4239, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298352981830.81036, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298349231301.4792, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298346258585.6534, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298343902454.12134, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298342035068.4843, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298340555076.8617, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298339382133.15106, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298338452547.7907, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298337715837.4362, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298337131988.3244, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336669286.10706, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336302595.1166, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298336011994.1294, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335781694.89966, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335599184.86536, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335454547.6318, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335339924.37463, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335249086.95166, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335177099.56274, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335120050.60815, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335074840.17255, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335039011.6082, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298335010618.03815, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 298334988116.5913, tolerance: 99259900.83227727\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464810599896.45087, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464793248944.3599, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464771359282.1457, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464743745231.46216, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464708912299.84937, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464664977298.2786, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464609568120.04706, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464539698195.30743, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464451609528.50726, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464340576943.7874, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464200664725.12036, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 464024425303.44995, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463802528141.80493, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 463523305745.13684, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462731118530.4985, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 462177624567.70447, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 461484070627.82294, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 460616576597.31976, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 459533956330.6653, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 458186645907.33246, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 456515769959.38416, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 454452559853.37103, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 451918441172.80896, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 448826226105.8657, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 445082953945.3695, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435275733091.1002, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 429056514965.6001, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 421899449009.2604, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 413811417944.61755, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 404856105040.9058, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 395160735594.5588, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 384914151700.114, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 374354369600.08203, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 363746523291.4193, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 353355170277.946, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 343416943320.09064, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 334119451052.6597, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 325590184068.5088, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317896029116.958, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 311051219264.0425, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 305030072647.7236, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 299780828765.7506, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 295237831707.45074, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 291330618478.0862, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 287989659762.0391, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 285149306799.46454, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 282748856213.29626, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 280732627601.9095, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 279049706436.2962, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 277653700527.2541, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 276502612203.1458, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 275558789940.3094, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 274788881594.73468, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 274163727141.08127, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273658163603.8505, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273250745449.65015, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272923401877.61023, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272661058881.1206, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272451252538.23804, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272283754610.40863, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272150224978.3077, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 272043899344.33215, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271959315727.6475, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271892079752.48126, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271838666460.20462, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271796255107.8321, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271762592874.1055, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271735883335.48764, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271714695806.80237, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271697892027.12494, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271684567118.49487, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271674002195.96768, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271665626429.67087, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271658986737.10428, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271653723611.2165, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271649551867.52618, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271646245325.2189, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271643624628.56247, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271641547571.22223, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271639901413.12088, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271638596782.02603, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271637562834.60263, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271636743417.80136, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271636094024.43405, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271635579379.00717, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271635171523.58615, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634848300.29248, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634592148.3221, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634389150.34995, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634228276.72906, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271634100786.3789, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633999751.98294, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633919683.6565, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633856230.68973, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633805945.17883, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633766094.70123, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633734513.83478, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 271633709486.50262, tolerance: 92975378.35672489\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525301147711.6022, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525281103704.23224, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525255816918.97766, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525223917979.2684, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525183680985.7675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525132931373.5039, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525068930356.60675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524988230250.0947, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524886493707.8172, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524758268479.14954, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524596707695.49316, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524393224033.7979, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 524137064538.2692, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523814791725.0607, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 523409656382.71454, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522900849085.8078, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 522262622225.7143, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 521463284378.8002, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 520464087008.6203, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 519218053774.1407, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 517668849923.09174, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 515749858422.83264, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 513383724044.3558, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 510482744685.9554, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 506950617287.7621, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 502686149632.5061, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 497589565085.6275, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 491571857999.71405, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484567190093.43774, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 476547476356.84705, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 467537148423.8904, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 457624897377.5301, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 446968545937.3337, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435789736487.481, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 424357193694.17426, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 412960531183.66077, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 401879706287.51886, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 391356808311.0815, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 381576070446.9056, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 372655219803.94714, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 364647866256.41864, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 357553995189.92377, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 351334510979.3379, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 345926085410.1604, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 341253718481.08026, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 337239788704.96814, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 333809526177.61835, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 330893588419.35187, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 328428730675.2572, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 326357518668.32117, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 324627772068.48505, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 323192104831.1432, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 322007663582.5347, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 321036012065.2327, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 320243061470.459, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319598963566.3247, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 319077924137.76794, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318657931428.97186, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318320417550.4367, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 318049880283.31525, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317833492658.004, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317660722649.13605, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317522978640.6069, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317413289921.7065, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317326026279.58093, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317256656993.96765, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317201547103.4444, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317157787435.8938, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317123054289.6646, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317095494558.7486, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317073632313.3988, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317056293231.3007, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317042543726.2156, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317031642080.089, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317022999316.27167, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317016147938.5145, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317010716996.50073, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317006412224.1675, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317003000235.5371, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 317000295959.7698, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316998152658.17615, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316996453996.834, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316995107754.17334, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316994040828.00507, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316993195274.67456, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316992525167.6508, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991994106.4376, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991573241.433, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316991239708.0435, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990975385.35114, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990765912.1365, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990599906.94055, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990468349.87195, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990364092.65015, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990281470.27295, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990215993.257, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990164103.718, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990122982.0697, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990090393.818, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: UserWarning: Coordinate descent without L1 regularization may lead to unexpected results and is discouraged. Set l1_ratio > 0 to add L1 regularization.\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:614: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 316990064568.1554, tolerance: 105075546.26559137\n",
      "  model = cd_fast.enet_coordinate_descent_gram(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/linear_model/_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.877e+11, tolerance: 1.272e+08 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Using K-fold CV w/ K=5\n",
    "K = 5\n",
    "kfold = skm.KFold(K,\n",
    "                  random_state=0,\n",
    "                  shuffle=True)\n",
    "\n",
    "scaler = StandardScaler(with_mean=True,  with_std=True)\n",
    "ridge = lm.ElasticNet(alpha=lambdas[59], l1_ratio=0)\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('ridge', ridge)])\n",
    "\n",
    "param_grid = {'ridge__alpha': lambdas}\n",
    "\n",
    "grid = skm.GridSearchCV(pipe,\n",
    "                        param_grid,\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error') \n",
    "grid.fit(X, Y)\n",
    "grid.best_params_['ridge__alpha']\n",
    "grid.best_estimator_\n",
    "\n",
    "ridgeCV = lm.ElasticNetCV(alphas=lambdas,\n",
    "                           l1_ratio=0,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('ridge', ridgeCV)])\n",
    "pipeCV.fit(X, Y)\n",
    "\n",
    "tuned_ridge = pipeCV.named_steps['ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75bc1025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results\n",
    "ridge_fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(lambdas),\n",
    "            -grid.cv_results_['mean_test_score'],\n",
    "            yerr=grid.cv_results_['std_test_score'] / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\n",
    "#ax.set_ylim([80000000,220000000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('ridge_cv.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490865a",
   "metadata": {},
   "source": [
    "# LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b2be136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 27.1687845272156$"
      ],
      "text/plain": [
       "27.168784527215628"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lassoCV = lm.ElasticNetCV(n_alphas=100,\n",
    "                           l1_ratio=1,\n",
    "                           cv=kfold)\n",
    "pipeCV = Pipeline(steps=[('scaler', scaler),\n",
    "                         ('lasso', lassoCV)])\n",
    "pipeCV.fit(X, Y)\n",
    "tuned_lasso = pipeCV.named_steps['lasso']\n",
    "tuned_lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47176817",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas, soln_array = lm.Lasso.path(Xs,\n",
    "                                    Y,\n",
    "                                    l1_ratio=1,\n",
    "                                    n_alphas=100)[:2]\n",
    "soln_path = pd.DataFrame(soln_array.T,\n",
    "                         columns=D.columns,\n",
    "                         index=-np.log(lambdas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2afef1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_fig, ax = plt.subplots(figsize=(10,5))\n",
    "soln_path.plot(ax=ax, legend=False)\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Standardized coefficiients', fontsize=20)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('lasso_coef.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974f5c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5180e80",
   "metadata": {},
   "source": [
    "## Cross-Validation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a1f1fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lassoCV_fig, ax = plt.subplots(figsize=(8,8))\n",
    "ax.errorbar(-np.log(tuned_lasso.alphas_),\n",
    "            tuned_lasso.mse_path_.mean(1),\n",
    "            yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\n",
    "ax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\n",
    "#ax.set_ylim([50000,250000])\n",
    "ax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\n",
    "ax.set_ylabel('Cross-validated MSE', fontsize=20)\n",
    "\n",
    "# save figure\n",
    "plt.savefig('lasso_cv.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1b75988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASSO:  27.168784527215628\n",
      "Ridge:  0.028384053913137267\n"
     ]
    }
   ],
   "source": [
    "# comparing lambdas\n",
    "\n",
    "# tuned alphas\n",
    "print(\"LASSO: \", tuned_lasso.alpha_)\n",
    "print(\"Ridge: \", tuned_ridge.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3baffa1e-9741-4994-945d-015423124e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.3020686865971527\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(tuned_lasso.alpha_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d9b6b-1ad0-4d57-ac8f-897ca16dfcee",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ffeb132-3aaf-4c21-bd39-1a0010f835f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting sample\n",
    "(X_train,\n",
    " X_test,\n",
    " y_train,\n",
    " y_test) = skm.train_test_split(X,\n",
    "                                df['rmkvaf'],\n",
    "                                test_size=0.3,\n",
    "                                random_state=0)\n",
    "feature_names = list(D.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ada53644-59b1-48d7-a3ae-a28d1735227d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg = DTR(max_depth=3)\n",
    "reg.fit(X_train, y_train)\n",
    "ax = plt.subplots(figsize=(12,12))[1]\n",
    "plot_tree(reg,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a54d3d71-4eee-469d-8950-450d5145c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best pruning for tree\n",
    "ccp_path = reg.cost_complexity_pruning_path(X_train, y_train)\n",
    "kfold = skm.KFold(5,\n",
    "                  shuffle=True,\n",
    "                  random_state=10)\n",
    "grid = skm.GridSearchCV(reg,\n",
    "                        {'ccp_alpha': ccp_path.ccp_alphas},\n",
    "                        refit=True, # retrains on best alpha\n",
    "                        cv=kfold,\n",
    "                        scoring='neg_mean_squared_error')\n",
    "G = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c5188d5-2c25-4f27-a6fc-190d4ae450c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJ8AAAAPCAYAAAD6fR2jAAAACXBIWXMAAA7EAAAOxAGVKw4bAAAHC0lEQVRoBdWa23EVORBAZykC8LIRABkYk4HJYFkiwGTAFl/4j4IMgAh4ZABEwCMD2AgAZ+A9R1bLGl3duRrq/tBVsqRWP9Wtlu7AdH5+Pj1+/PjQvm7gDmg3atzoeN/yRvX+znTuNe2EdvA7+1HbvisPrk4X8OH09PSA4Zc8dyzcuugu/kJzg9GDjJPG+VPw7zMuun3LS3LR8zQr+E5/k6bubxlXOnDPy2SarjG+D+6swk3Mtf9RhXP+BnzrSyIBv1NmJWs2HNR1CJM6nkM/48+TM/B/xgJj9/7fPD+i/+EcfMQwL8071tXzgD7imAiyvMAtxbYIhCfiEbhX4Gr9i3kQyafhgoYZzLe0JwgqAWOsQToXBk6M/wb3jv4uTZ6AfctT12eEa1PSQ689n+nv0FICBg68AXymMfT69B/9LVqdqCZu8SXTmnwHtOKLc9bUPSJTMT0Y0XUbRhO/tjFkHTOIRJuwycTTnjtBwNhEiP3oHqBM+4a+ThDlrYmt9OpXjvmQdNE7t1kUAhbzIJLvC8x3g2NLfwL+BDqTLYITTlpBAif7XuWhT91tUlgJ1Gm1iCC8ZHwNfEo8xhNjbfnEsNAxV54J1cJ9EB9otS9DMltBMV+jC9rwI9gncAbavrbJRGsPjomgXyZAqZCMC7D+sEzmgzWxlVMdVrmIvzgTuD04i3lwRa5B8LSc5ZZYUO78V2GNPA/G7LRmpR/pj7FDxwUrcbsB4uWt6TydG4GWsAOjMjusCTWqS196sFE1IbISfq38Dj6TwUOaEjaQ9uC8AWbxq9aHY4Ec90NZLyp+5XsDje5pYo3KV8vpjhGsY7MTBU5DBKvKKlgpz82eOZuVRaKZWHEKf3QM8Y0oHNGkM9AP4XlH75MhDpEVpfgCPpJ6RCasXRjVVVe2JAj92vOkI1UfDiu7W5Kwu8bfg372bIpF8Gtia8X11ok9CzGr+5J8CEtXGxL+onlyfF95IrrAmgmRyj/jjcTYlzzk9Daytcmr1g0Rf61dZK5PQqoI0L21Mffw/GTse8oKVT8pJvDDMuHtgnpsLC7qapnh0VYTrLz1ggbctieSFWlifRY35l635VCFnG099Eux9QB/g0Zd92gebPeu+2MNuq15FclngF9DmLKZXsct65bSqCigSvnWOB/IOvmJ1sI+5UUyJdtaRXmuPsEga1sLKSggg27CLyueAXFzPERWUfsWhmW2jDFfoStY7LWlZ09NU8bo0EfjNktW8OI8RHFLFJ52kGWMxFbWI+iLLsYeYr8quF8Bi3lwRSoYTLISXMYaatJtnBbWfEQ+o3n6XtH8heWpLsB8r/KK4O2DqGz+YJjQXxKQsUEJ30oAwGuzeE+tvsaBm/kCflgmtF1YoSvxQ68tx/Szg98Vfol8w9AqW35s5SU/q2zcTJdslyPoFmPLuskkWJFbma/Bv6xoJsaLeZCSL4nb/GOgbiDAjegCa2a5AUyfKLpEl8hfldd7b4XUqIrpTYc92nKdZlXzTWfl0P54zKfkA2+1c2N8A3mF+FCOq6zdwCGZ8Hdhja5KgO+qclAqfHeIDouEfoQPiS7r3iggXSENEt6l2PZs8+uByem1vAQlD66ixEe3b6bZB+WKW4ET6+nqop+9J1j6RLPS2Dx5+5YX765kBzpqCFzZDPSfQdB+hojrK+icm6QF4NN2q+BXWvIlFgdlBnnbr9KVma2+YWsrbzbHNg+S8Zv90mTuoTug3ykHmtHYRizc422QihUyd+bBFSSYqRHEWmCqKgiJZDOzvWJ7tDXfvuUpO67FWo/jqHy7ric39z22u3nab1A2NhCcgfLEh1yGW6HI3EbxK7oyjwHcsK/VA61JepO+VDzGcVsp4zZzb6XSwGm3V7q4OJSjsYU1xWIpByLZd+bBVYS9wIjycFR6Bk9/HVQ3IwUwCHKvEiFo9y1P2b5nYqOcB1itfaekQNEbjJe06xXOjdKXVNnF52aQYqNYLiB9+DJBs1Nm4WwGWY/6hnRl9tjPpeeGdplEJlcbO+01BvpW/Miy5fspnr4kLPPR2CrGa9x4tOD+6mvo3JkHVj7/mWb2LmAeX8JrA3XSUloAOh01WD5qdUDYt7wJ2T5uf2R9SQlj9f5DSz8IEvLifdcGzY3SvqjgkurXxjsVGq8wP7fUSWkF2SlTe2jnNKtIDWt0yadfQuznxaz6iw5t0i91pv2OHlwdi4qrDJUfOgI5GtsJPd4MJm8pBoyV18ZiZx784f9CgFln4gR55bjZPsZnG8DcClInpHx+gY9sZ5reh3uVl2XqoA5r03fabdoTdNdJNTGPTZFeMMlm9okEZ+V4RKsTaxvtqEzfiwamfXOu0eXemcD3kWOgNwC8eqTrgTfBxvsdnAVGHmMoKPsj+Pg38KHYJk7+wOeexB6bM71YLObB/zLTGaY+c5zOAAAAAElFTkSuQmCC\n",
      "text/latex": [
       "$\\displaystyle 53530989.8724365$"
      ],
      "text/plain": [
       "53530989.87243649"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get prediction error\n",
    "best_ = grid.best_estimator_\n",
    "tree_mse = np.mean((y_test - best_.predict(X_test))**2)\n",
    "tree_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d6c2481-aa18-457f-a9a1-27259aea898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output pruned Regression Tree\n",
    "ax = plt.subplots(figsize=(12,12))[1]\n",
    "plot_tree(G.best_estimator_,\n",
    "          feature_names=feature_names,\n",
    "          ax=ax);\n",
    "plt.savefig(\"pruned_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3747128-9922-488c-9dfa-12e96b229624",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cc80acc-02f1-49f6-944a-f2e5827b4160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 17713049.0842962$"
      ],
      "text/plain": [
       "17713049.08429623"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_tree = RF(max_features=3, # Random forest model\n",
    "               random_state=0).fit(X_train, y_train)\n",
    "y_hat_RF = RF_tree.predict(X_test)\n",
    "rf_mse = np.mean((y_test - y_hat_RF)**2)\n",
    "rf_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67960627-6d72-410c-8d6a-a1901ec6d5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rxrd</th>\n",
       "      <td>0.303054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsales</th>\n",
       "      <td>0.230557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rppent</th>\n",
       "      <td>0.142841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gspillsicIV</th>\n",
       "      <td>0.111545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emp</th>\n",
       "      <td>0.105906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gspilltecIV</th>\n",
       "      <td>0.066526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pat_count</th>\n",
       "      <td>0.039571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             importance\n",
       "rxrd           0.303054\n",
       "rsales         0.230557\n",
       "rppent         0.142841\n",
       "gspillsicIV    0.111545\n",
       "emp            0.105906\n",
       "gspilltecIV    0.066526\n",
       "pat_count      0.039571"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(\n",
    "    {'importance':RF_tree.feature_importances_},\n",
    "    index=feature_names) # Must include feature importance in report\n",
    "feature_imp = feature_imp.sort_values(by='importance', ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f163025-fb68-4c6d-b8af-7ba3d15d1593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Importance of Random Forest Regressors for Firm Market Value')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 750x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(feature_imp.sort_values(by='importance', ascending=False).to_latex()) # print latex output\n",
    "\n",
    "# plot importance values\n",
    "fig = plt.figure(figsize=(7.5,5))\n",
    "\n",
    "plt.bar(feature_imp.index, feature_imp.importance, color ='darkblue', \n",
    "        width = 0.4)\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Importance of Random Forest Regressors for Firm Market Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7ebdd-af00-4928-a015-4d2ebd4e752b",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "466f8c68-ccdd-49f6-8504-b67e067e42cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=7, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=7, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features=7, random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = RF(max_features=X_train.shape[1], random_state=0)\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f09cbe84-2c6a-4584-9218-44dd31228cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/latex": [
       "$\\displaystyle 16896083.2258455$"
      ],
      "text/plain": [
       "16896083.225845523"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplots(figsize=(8,8))[1]\n",
    "y_hat_bag = bag.predict(X_test)\n",
    "ax.scatter(y_hat_bag, y_test)\n",
    "bag_mse = np.mean((y_test - y_hat_bag)**2)\n",
    "bag_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c9684-f038-4369-82c1-d56d2e81d11e",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "365151ed-46e6-49e6-8335-fbdabde150ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n",
       "                          random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n",
       "                          random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.001, n_estimators=5000,\n",
       "                          random_state=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost = GBR(n_estimators=5000, # Gradient Boosting Regressor\n",
    "                   learning_rate=0.001,\n",
    "                   max_depth=3,\n",
    "                   random_state=0)\n",
    "boost.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b8112cd-eb77-409f-bd3d-3555d401f00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_error = np.zeros_like(boost.train_score_)\n",
    "for idx, y_ in enumerate(boost.staged_predict(X_test)):\n",
    "   test_error[idx] = np.mean((y_test - y_)**2)\n",
    "\n",
    "plot_idx = np.arange(boost.train_score_.shape[0])\n",
    "ax = plt.subplots(figsize=(8,8))[1]\n",
    "ax.plot(plot_idx,\n",
    "        boost.train_score_,\n",
    "        'b',\n",
    "        label='Training')\n",
    "ax.plot(plot_idx,\n",
    "        test_error,\n",
    "        'r',\n",
    "        label='Test')\n",
    "ax.legend();\n",
    "\n",
    "boost_mse = min(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84442aec-7647-410a-9f3e-676e2057043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 53809940.0\n",
      "Prev. MSE: 40865396.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XG Boost\n",
    "y = df['rmkvaf']\n",
    "# Convert the data into XGBoost's DMatrix format\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the XGBoost model with the optimal number of boosting rounds\n",
    "model = xgb.train(params, dtrain, num_boost_round= 10)\n",
    "\n",
    "# Make predictions \n",
    "y_pred = model.predict(dtrain)\n",
    "\n",
    "# Calculate and print the Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Prev. MSE: 40865396.0\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y, y_pred, c='grey', alpha=0.3)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)\n",
    "plt.xlabel('True Y')\n",
    "plt.ylabel('Predicted Y')\n",
    "plt.title('Y vs Predicted Y (Y hat)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb857974-d09b-4241-8bfa-6d6bac1c6adf",
   "metadata": {},
   "source": [
    "# Comparing Test Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27eecd93-a25d-470b-a91e-a121cea264f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree:  53530989.87243649\n",
      "Bag:  16896083.225845523\n",
      "Random Forest:  17713049.08429623\n",
      "Boost:  32678662.643215105\n",
      "\\begin{tabular}{llr}\n",
      "\\toprule\n",
      " & model & MSE \\\\\n",
      "\\midrule\n",
      "0 & Tree & 53530989.872436 \\\\\n",
      "1 & Bag & 16896083.225846 \\\\\n",
      "2 & Random Forest & 17713049.084296 \\\\\n",
      "3 & Boost & 32678662.643215 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tree\n",
    "print('Tree: ', tree_mse)\n",
    "# Bag\n",
    "print('Bag: ', bag_mse)\n",
    "# Random Forest\n",
    "print('Random Forest: ', rf_mse)\n",
    "# Boost\n",
    "print('Boost: ', boost_mse)\n",
    "\n",
    "# Make table\n",
    "error_tbl = {\n",
    "    'model' : ['Tree', 'Bag', 'Random Forest', 'Boost'],\n",
    "    'MSE' : [tree_mse, bag_mse, rf_mse, boost_mse]\n",
    "}\n",
    "error_tbl = pd.DataFrame(data=error_tbl)\n",
    "print(error_tbl.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5400d0-4f99-4c11-b548-8d3c3648a1f4",
   "metadata": {},
   "source": [
    "# Predicting Market value of spillovers across years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dff34c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gspilltecIV :  4809.362016403995\n",
      "gspillsicIV :  -352.0895602447208\n"
     ]
    }
   ],
   "source": [
    "# Use Optimal LASSO coefficients - remember to rescale from their standardization back to unit values.\n",
    "print(x_vars.columns[1],': ',tuned_lasso.coef_[0]) # index 0 for x_vars is constant term\n",
    "print(x_vars.columns[2],': ',tuned_lasso.coef_[1]) # so 1st variable is at index 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c697a390-1f24-4821-a943-9e16da9d6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling lasso coefficients\n",
    "coef_tec = tuned_lasso.coef_[0]/df['gspilltecIV'].std()\n",
    "coef_sic = tuned_lasso.coef_[1]/df['gspillsicIV'].std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7615d301-c511-430d-a2db-2869e63855d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['firmval_tec'] = df['gspilltecIV']*coef_tec\n",
    "df['firmval_sic'] = df['gspillsicIV']*coef_sic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11e9621a-9efd-40b1-9f36-1c1b45389142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmkvaf</th>\n",
       "      <th>firmval_tec</th>\n",
       "      <th>firmval_sic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2314.207764</td>\n",
       "      <td>5975.001465</td>\n",
       "      <td>-391.839294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1288.770630</td>\n",
       "      <td>5833.845703</td>\n",
       "      <td>-382.459259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1756.920532</td>\n",
       "      <td>5747.703125</td>\n",
       "      <td>-376.848724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8698.285156</td>\n",
       "      <td>5714.716797</td>\n",
       "      <td>-372.497253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10707.480469</td>\n",
       "      <td>5784.362793</td>\n",
       "      <td>-376.836273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13380</th>\n",
       "      <td>1200.901245</td>\n",
       "      <td>9066.891602</td>\n",
       "      <td>-1239.664551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13381</th>\n",
       "      <td>1264.767578</td>\n",
       "      <td>9276.417969</td>\n",
       "      <td>-1268.360474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13382</th>\n",
       "      <td>1033.856079</td>\n",
       "      <td>9522.323242</td>\n",
       "      <td>-1296.008911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13383</th>\n",
       "      <td>981.391479</td>\n",
       "      <td>9800.909180</td>\n",
       "      <td>-1327.834595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13384</th>\n",
       "      <td>1259.013672</td>\n",
       "      <td>10024.870117</td>\n",
       "      <td>-1355.650146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13325 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rmkvaf   firmval_tec  firmval_sic\n",
       "0       2314.207764   5975.001465  -391.839294\n",
       "1       1288.770630   5833.845703  -382.459259\n",
       "2       1756.920532   5747.703125  -376.848724\n",
       "3       8698.285156   5714.716797  -372.497253\n",
       "4      10707.480469   5784.362793  -376.836273\n",
       "...             ...           ...          ...\n",
       "13380   1200.901245   9066.891602 -1239.664551\n",
       "13381   1264.767578   9276.417969 -1268.360474\n",
       "13382   1033.856079   9522.323242 -1296.008911\n",
       "13383    981.391479   9800.909180 -1327.834595\n",
       "13384   1259.013672  10024.870117 -1355.650146\n",
       "\n",
       "[13325 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['rmkvaf','firmval_tec','firmval_sic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f01af06-eb37-4951-a1e9-6adc1b239424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45665948.0\n",
      "84164990.0\n"
     ]
    }
   ],
   "source": [
    "print(df['rmkvaf'].sum())\n",
    "print(df['firmval_tec'].sum())\n",
    "# Generate predictions for each firm\n",
    "\n",
    "# Aggregate across years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d937715-bc35-41dd-89b6-3139b370c84c",
   "metadata": {},
   "source": [
    "# Impact of spillovers on firm value over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1b5a3fc-da96-4a20-940c-03d155fd1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include confidence intervals. Use 2 or 5 year intervals if need more statistical power\n",
    "# Try with PLR DML to compare results - separate bar i.e. for each year, one OLS bar and one DML bar\n",
    "# Separate plots for tech/product market spillovers\n",
    "\n",
    "# build year-spillover interaction terms\n",
    "years = df.year.sort_values().unique()\n",
    "years = years[1:] # remove first year as ref category\n",
    "\n",
    "spillovers = ['gspilltecIV','gspillsicIV']\n",
    "\n",
    "for spillover in spillovers:\n",
    "    for year in years:\n",
    "        col_name = f\"{spillover}X{year}\"\n",
    "        x_vars[col_name] = x_vars[spillover]*x_vars[year]\n",
    "\n",
    "# drop reference year dummy\n",
    "#x_vars = x_vars.drop(columns=['1981'])\n",
    "#fixed_effects.remove('1981')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1451932-16b5-45cb-ae75-c2c0afa2e331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.669</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.648</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   31.93</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:02:58</td>     <th>  Log-Likelihood:    </th> <td>-1.4150e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13385</td>      <th>  AIC:               </th>  <td>2.846e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 12587</td>      <th>  BIC:               </th>  <td>2.906e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   797</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-1.381e+04</td> <td> 4167.202</td> <td>   -3.315</td> <td> 0.001</td> <td> -2.2e+04</td> <td>-5645.690</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th>      <td>    0.1986</td> <td>    0.145</td> <td>    1.368</td> <td> 0.171</td> <td>   -0.086</td> <td>    0.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th>      <td>    0.6659</td> <td>    0.233</td> <td>    2.852</td> <td> 0.004</td> <td>    0.208</td> <td>    1.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -27.0286</td> <td>    1.911</td> <td>  -14.141</td> <td> 0.000</td> <td>  -30.775</td> <td>  -23.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.7667</td> <td>    0.037</td> <td>   20.685</td> <td> 0.000</td> <td>    0.694</td> <td>    0.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.6251</td> <td>    0.084</td> <td>    7.408</td> <td> 0.000</td> <td>    0.460</td> <td>    0.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>   15.4383</td> <td>    7.160</td> <td>    2.156</td> <td> 0.031</td> <td>    1.404</td> <td>   29.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>   18.8001</td> <td>    0.613</td> <td>   30.654</td> <td> 0.000</td> <td>   17.598</td> <td>   20.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981</th>             <td>  109.6592</td> <td> 1080.584</td> <td>    0.101</td> <td> 0.919</td> <td>-2008.450</td> <td> 2227.768</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>             <td> -233.2389</td> <td> 1074.271</td> <td>   -0.217</td> <td> 0.828</td> <td>-2338.975</td> <td> 1872.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>             <td> -128.6822</td> <td> 1065.010</td> <td>   -0.121</td> <td> 0.904</td> <td>-2216.264</td> <td> 1958.899</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>             <td>   98.8470</td> <td> 1059.772</td> <td>    0.093</td> <td> 0.926</td> <td>-1978.467</td> <td> 2176.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>             <td>  500.7900</td> <td> 1057.845</td> <td>    0.473</td> <td> 0.636</td> <td>-1572.747</td> <td> 2574.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>             <td> 1025.5479</td> <td> 1047.722</td> <td>    0.979</td> <td> 0.328</td> <td>-1028.146</td> <td> 3079.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>             <td>  964.1585</td> <td> 1043.238</td> <td>    0.924</td> <td> 0.355</td> <td>-1080.748</td> <td> 3009.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>             <td> 1227.5616</td> <td> 1042.827</td> <td>    1.177</td> <td> 0.239</td> <td> -816.538</td> <td> 3271.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>             <td> 1409.3119</td> <td> 1037.820</td> <td>    1.358</td> <td> 0.175</td> <td> -624.973</td> <td> 3443.597</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>             <td> 1582.7353</td> <td> 1032.683</td> <td>    1.533</td> <td> 0.125</td> <td> -441.480</td> <td> 3606.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>             <td> 1616.4231</td> <td> 1032.392</td> <td>    1.566</td> <td> 0.117</td> <td> -407.222</td> <td> 3640.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>             <td> 1589.4573</td> <td> 1031.336</td> <td>    1.541</td> <td> 0.123</td> <td> -432.118</td> <td> 3611.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>             <td> 1205.1245</td> <td> 1027.138</td> <td>    1.173</td> <td> 0.241</td> <td> -808.222</td> <td> 3218.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>             <td> 1111.0146</td> <td> 1029.798</td> <td>    1.079</td> <td> 0.281</td> <td> -907.547</td> <td> 3129.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>             <td> 1413.9353</td> <td> 1028.108</td> <td>    1.375</td> <td> 0.169</td> <td> -601.312</td> <td> 3429.183</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>             <td> 1164.7069</td> <td> 1030.027</td> <td>    1.131</td> <td> 0.258</td> <td> -854.302</td> <td> 3183.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>             <td> 1327.6436</td> <td> 1034.933</td> <td>    1.283</td> <td> 0.200</td> <td> -700.983</td> <td> 3356.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>             <td>  626.1367</td> <td> 1038.418</td> <td>    0.603</td> <td> 0.547</td> <td>-1409.322</td> <td> 2661.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>             <td>-1672.8117</td> <td> 1048.945</td> <td>   -1.595</td> <td> 0.111</td> <td>-3728.904</td> <td>  383.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2000</th>             <td>   14.3278</td> <td> 1070.432</td> <td>    0.013</td> <td> 0.989</td> <td>-2083.883</td> <td> 2112.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2001</th>             <td> 1048.8243</td> <td> 1114.791</td> <td>    0.941</td> <td> 0.347</td> <td>-1136.337</td> <td> 3233.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>          <td> 1.075e+04</td> <td> 3748.461</td> <td>    2.868</td> <td> 0.004</td> <td> 3402.447</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>          <td>  1.05e+04</td> <td> 3980.492</td> <td>    2.637</td> <td> 0.008</td> <td> 2694.217</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>          <td> 9625.6040</td> <td> 3498.425</td> <td>    2.751</td> <td> 0.006</td> <td> 2768.158</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>          <td> 1.086e+04</td> <td> 3609.282</td> <td>    3.009</td> <td> 0.003</td> <td> 3784.555</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>          <td> 1.211e+04</td> <td> 4151.615</td> <td>    2.916</td> <td> 0.004</td> <td> 3968.672</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>           <td>  1.17e+04</td> <td> 3919.766</td> <td>    2.984</td> <td> 0.003</td> <td> 4013.670</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>          <td> 9375.3438</td> <td> 3684.022</td> <td>    2.545</td> <td> 0.011</td> <td> 2154.098</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>          <td> 4448.8290</td> <td> 3357.715</td> <td>    1.325</td> <td> 0.185</td> <td>-2132.805</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>          <td> 1.187e+04</td> <td> 5958.377</td> <td>    1.993</td> <td> 0.046</td> <td>  193.098</td> <td> 2.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>          <td> 3456.7486</td> <td> 3228.208</td> <td>    1.071</td> <td> 0.284</td> <td>-2871.032</td> <td> 9784.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>           <td> 1.067e+04</td> <td> 5738.714</td> <td>    1.860</td> <td> 0.063</td> <td> -574.920</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>          <td> 1.305e+04</td> <td> 4548.789</td> <td>    2.868</td> <td> 0.004</td> <td> 4130.891</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>          <td> 8975.8676</td> <td> 3778.522</td> <td>    2.375</td> <td> 0.018</td> <td> 1569.388</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>          <td> 1.317e+04</td> <td> 4577.903</td> <td>    2.877</td> <td> 0.004</td> <td> 4198.916</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>           <td> 5844.5024</td> <td> 3184.674</td> <td>    1.835</td> <td> 0.067</td> <td> -397.944</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>          <td> 4379.0875</td> <td> 3779.417</td> <td>    1.159</td> <td> 0.247</td> <td>-3029.145</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>          <td>  973.0308</td> <td> 6088.181</td> <td>    0.160</td> <td> 0.873</td> <td> -1.1e+04</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>          <td> 1.223e+04</td> <td> 4299.729</td> <td>    2.844</td> <td> 0.004</td> <td> 3800.706</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>          <td> 9342.5104</td> <td> 4437.384</td> <td>    2.105</td> <td> 0.035</td> <td>  644.562</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>          <td> 1185.3886</td> <td> 4591.466</td> <td>    0.258</td> <td> 0.796</td> <td>-7814.585</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>          <td>  1.21e+04</td> <td> 4116.091</td> <td>    2.940</td> <td> 0.003</td> <td> 4034.264</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>          <td> 1.288e+04</td> <td> 4540.119</td> <td>    2.837</td> <td> 0.005</td> <td> 3982.728</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>          <td> 6637.9380</td> <td> 4756.976</td> <td>    1.395</td> <td> 0.163</td> <td>-2686.461</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>          <td> 1.171e+04</td> <td> 4184.951</td> <td>    2.798</td> <td> 0.005</td> <td> 3507.701</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>          <td> 1.045e+04</td> <td> 3617.866</td> <td>    2.889</td> <td> 0.004</td> <td> 3359.886</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>          <td>-1.587e+04</td> <td> 3086.762</td> <td>   -5.140</td> <td> 0.000</td> <td>-2.19e+04</td> <td>-9815.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>          <td> 1.152e+04</td> <td> 3852.606</td> <td>    2.990</td> <td> 0.003</td> <td> 3967.037</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>          <td> 6845.6324</td> <td> 4814.039</td> <td>    1.422</td> <td> 0.155</td> <td>-2590.617</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>           <td> 8479.1608</td> <td> 4222.035</td> <td>    2.008</td> <td> 0.045</td> <td>  203.328</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>          <td> 9888.7034</td> <td> 3720.204</td> <td>    2.658</td> <td> 0.008</td> <td> 2596.536</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>          <td> 5773.3219</td> <td> 3175.639</td> <td>    1.818</td> <td> 0.069</td> <td> -451.414</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>          <td> 2387.3954</td> <td> 3277.528</td> <td>    0.728</td> <td> 0.466</td> <td>-4037.059</td> <td> 8811.850</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>          <td> 5287.9053</td> <td> 3527.954</td> <td>    1.499</td> <td> 0.134</td> <td>-1627.422</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>          <td> 9770.2423</td> <td> 3967.970</td> <td>    2.462</td> <td> 0.014</td> <td> 1992.416</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>          <td> 8393.9708</td> <td> 3804.080</td> <td>    2.207</td> <td> 0.027</td> <td>  937.394</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>          <td> 1.138e+04</td> <td> 4573.690</td> <td>    2.488</td> <td> 0.013</td> <td> 2416.339</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>          <td> 1.227e+04</td> <td> 4355.586</td> <td>    2.818</td> <td> 0.005</td> <td> 3735.499</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>           <td>  961.9430</td> <td> 3789.693</td> <td>    0.254</td> <td> 0.800</td> <td>-6466.434</td> <td> 8390.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>          <td> 5689.5884</td> <td> 3192.668</td> <td>    1.782</td> <td> 0.075</td> <td> -568.527</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>          <td>-1.372e+04</td> <td> 3822.485</td> <td>   -3.589</td> <td> 0.000</td> <td>-2.12e+04</td> <td>-6228.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>          <td> 1.137e+04</td> <td> 4141.570</td> <td>    2.745</td> <td> 0.006</td> <td> 3251.953</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>          <td> -212.4259</td> <td> 3309.795</td> <td>   -0.064</td> <td> 0.949</td> <td>-6700.128</td> <td> 6275.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>          <td> 1.257e+04</td> <td> 4628.344</td> <td>    2.716</td> <td> 0.007</td> <td> 3500.005</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>          <td>-6240.6283</td> <td> 3068.227</td> <td>   -2.034</td> <td> 0.042</td> <td>-1.23e+04</td> <td> -226.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>          <td> 7396.4535</td> <td> 3303.701</td> <td>    2.239</td> <td> 0.025</td> <td>  920.696</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>          <td> 8373.8467</td> <td> 3619.936</td> <td>    2.313</td> <td> 0.021</td> <td> 1278.221</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>          <td> 9487.4398</td> <td> 3460.376</td> <td>    2.742</td> <td> 0.006</td> <td> 2704.575</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>          <td> 1.118e+04</td> <td> 3862.112</td> <td>    2.894</td> <td> 0.004</td> <td> 3605.598</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>          <td> 7515.1246</td> <td> 6151.801</td> <td>    1.222</td> <td> 0.222</td> <td>-4543.343</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>          <td> 5242.3296</td> <td> 3365.410</td> <td>    1.558</td> <td> 0.119</td> <td>-1354.387</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>          <td> 1.274e+04</td> <td> 4159.576</td> <td>    3.062</td> <td> 0.002</td> <td> 4583.444</td> <td> 2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>          <td> 1.283e+04</td> <td> 4341.856</td> <td>    2.956</td> <td> 0.003</td> <td> 4322.812</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>          <td> 9797.9005</td> <td> 3963.747</td> <td>    2.472</td> <td> 0.013</td> <td> 2028.352</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>          <td>  855.6779</td> <td> 3101.920</td> <td>    0.276</td> <td> 0.783</td> <td>-5224.558</td> <td> 6935.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>          <td> 1.105e+04</td> <td> 3914.511</td> <td>    2.823</td> <td> 0.005</td> <td> 3378.337</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>          <td> 1.158e+04</td> <td> 3962.497</td> <td>    2.922</td> <td> 0.003</td> <td> 3811.778</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>          <td> 1.051e+04</td> <td> 3872.700</td> <td>    2.715</td> <td> 0.007</td> <td> 2923.389</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>          <td> 1.107e+04</td> <td> 3644.052</td> <td>    3.039</td> <td> 0.002</td> <td> 3929.910</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>          <td> 1.097e+04</td> <td> 3625.002</td> <td>    3.027</td> <td> 0.002</td> <td> 3869.116</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>          <td> 1.442e+04</td> <td> 4401.151</td> <td>    3.276</td> <td> 0.001</td> <td> 5789.509</td> <td>  2.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>          <td> 9405.0171</td> <td> 4445.151</td> <td>    2.116</td> <td> 0.034</td> <td>  691.843</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>          <td> 1.206e+04</td> <td> 4437.109</td> <td>    2.718</td> <td> 0.007</td> <td> 3364.680</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>          <td> 1.242e+04</td> <td> 4557.772</td> <td>    2.724</td> <td> 0.006</td> <td> 3481.657</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>          <td> 1.093e+04</td> <td> 3624.387</td> <td>    3.015</td> <td> 0.003</td> <td> 3822.478</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>           <td> 2633.8775</td> <td> 3872.332</td> <td>    0.680</td> <td> 0.496</td> <td>-4956.484</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>          <td> 1.041e+04</td> <td> 4157.794</td> <td>    2.504</td> <td> 0.012</td> <td> 2259.333</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>          <td> 9848.3369</td> <td> 3792.306</td> <td>    2.597</td> <td> 0.009</td> <td> 2414.839</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>          <td> 1.179e+04</td> <td> 3862.003</td> <td>    3.052</td> <td> 0.002</td> <td> 4216.664</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>          <td> -845.3807</td> <td> 3281.642</td> <td>   -0.258</td> <td> 0.797</td> <td>-7277.900</td> <td> 5587.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>          <td> 6807.2531</td> <td> 4207.731</td> <td>    1.618</td> <td> 0.106</td> <td>-1440.540</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>          <td>  1.09e+04</td> <td> 3926.720</td> <td>    2.775</td> <td> 0.006</td> <td> 3199.498</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>          <td> 1.168e+04</td> <td> 4777.897</td> <td>    2.445</td> <td> 0.014</td> <td> 2317.340</td> <td>  2.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>          <td>-2.276e+04</td> <td> 3294.972</td> <td>   -6.907</td> <td> 0.000</td> <td>-2.92e+04</td> <td>-1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>           <td> 1.067e+04</td> <td> 3750.813</td> <td>    2.845</td> <td> 0.004</td> <td> 3318.973</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>          <td> 7301.2367</td> <td> 4789.870</td> <td>    1.524</td> <td> 0.127</td> <td>-2087.639</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>          <td> 9155.2709</td> <td> 4087.399</td> <td>    2.240</td> <td> 0.025</td> <td> 1143.346</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>          <td> 4372.6645</td> <td> 4278.944</td> <td>    1.022</td> <td> 0.307</td> <td>-4014.718</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>           <td> 1.125e+04</td> <td> 4231.136</td> <td>    2.660</td> <td> 0.008</td> <td> 2960.063</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>          <td> 1.158e+04</td> <td> 4129.190</td> <td>    2.804</td> <td> 0.005</td> <td> 3485.908</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>          <td>  1.07e+04</td> <td> 3840.633</td> <td>    2.787</td> <td> 0.005</td> <td> 3173.778</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>          <td> 8640.8067</td> <td> 3421.736</td> <td>    2.525</td> <td> 0.012</td> <td> 1933.682</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>          <td> 1.101e+04</td> <td> 4226.667</td> <td>    2.605</td> <td> 0.009</td> <td> 2724.173</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>           <td> 9631.4518</td> <td> 3696.163</td> <td>    2.606</td> <td> 0.009</td> <td> 2386.409</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>          <td> 7721.6527</td> <td> 3683.960</td> <td>    2.096</td> <td> 0.036</td> <td>  500.530</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>          <td> 1.232e+04</td> <td> 4390.224</td> <td>    2.807</td> <td> 0.005</td> <td> 3719.225</td> <td> 2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>          <td> 1.215e+04</td> <td> 3982.478</td> <td>    3.050</td> <td> 0.002</td> <td> 4342.074</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>          <td> 6991.3336</td> <td> 4824.075</td> <td>    1.449</td> <td> 0.147</td> <td>-2464.589</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>          <td>-2018.1984</td> <td> 4000.197</td> <td>   -0.505</td> <td> 0.614</td> <td>-9859.194</td> <td> 5822.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>          <td>  -34.4822</td> <td> 3301.898</td> <td>   -0.010</td> <td> 0.992</td> <td>-6506.705</td> <td> 6437.741</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>          <td> 1.062e+04</td> <td> 3541.132</td> <td>    2.998</td> <td> 0.003</td> <td> 3674.735</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>          <td> 2972.3845</td> <td> 3179.933</td> <td>    0.935</td> <td> 0.350</td> <td>-3260.769</td> <td> 9205.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>         <td>-1.046e+04</td> <td> 5391.406</td> <td>   -1.941</td> <td> 0.052</td> <td> -2.1e+04</td> <td>  104.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>          <td> 5467.4855</td> <td> 3454.479</td> <td>    1.583</td> <td> 0.114</td> <td>-1303.821</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>          <td> 6032.2158</td> <td> 3836.369</td> <td>    1.572</td> <td> 0.116</td> <td>-1487.653</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>          <td> 1.117e+04</td> <td> 4449.872</td> <td>    2.510</td> <td> 0.012</td> <td> 2448.510</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>          <td> 6270.2659</td> <td> 3372.318</td> <td>    1.859</td> <td> 0.063</td> <td> -339.991</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>          <td> 1.137e+04</td> <td> 3814.796</td> <td>    2.980</td> <td> 0.003</td> <td> 3891.850</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>          <td> 1.276e+04</td> <td> 4524.328</td> <td>    2.820</td> <td> 0.005</td> <td> 3890.205</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>          <td> 1.034e+04</td> <td> 3563.257</td> <td>    2.901</td> <td> 0.004</td> <td> 3353.251</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>          <td> 6296.8646</td> <td> 3705.782</td> <td>    1.699</td> <td> 0.089</td> <td> -967.034</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>          <td> 1.189e+04</td> <td> 4332.106</td> <td>    2.745</td> <td> 0.006</td> <td> 3401.731</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>          <td> 1.548e+04</td> <td> 4202.441</td> <td>    3.683</td> <td> 0.000</td> <td> 7241.117</td> <td> 2.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>           <td>-1188.8749</td> <td> 2976.195</td> <td>   -0.399</td> <td> 0.690</td> <td>-7022.672</td> <td> 4644.922</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>          <td>-1.074e+04</td> <td> 3653.852</td> <td>   -2.939</td> <td> 0.003</td> <td>-1.79e+04</td> <td>-3577.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>          <td> 1.238e+04</td> <td> 4581.034</td> <td>    2.702</td> <td> 0.007</td> <td> 3396.962</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>          <td>-1260.8304</td> <td> 3564.728</td> <td>   -0.354</td> <td> 0.724</td> <td>-8248.241</td> <td> 5726.581</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>          <td> 1.073e+04</td> <td> 3611.674</td> <td>    2.970</td> <td> 0.003</td> <td> 3648.552</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>          <td> 1.178e+04</td> <td> 4057.704</td> <td>    2.904</td> <td> 0.004</td> <td> 3830.393</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>          <td> 2309.2070</td> <td> 4719.750</td> <td>    0.489</td> <td> 0.625</td> <td>-6942.223</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>          <td>-6216.7588</td> <td> 4834.181</td> <td>   -1.286</td> <td> 0.198</td> <td>-1.57e+04</td> <td> 3258.974</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>          <td> 1.005e+04</td> <td> 3772.084</td> <td>    2.663</td> <td> 0.008</td> <td> 2652.948</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>          <td> 1.032e+04</td> <td> 6325.197</td> <td>    1.632</td> <td> 0.103</td> <td>-2074.750</td> <td> 2.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>          <td> 1.239e+04</td> <td> 4716.169</td> <td>    2.627</td> <td> 0.009</td> <td> 3143.742</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>          <td> 1.126e+04</td> <td> 4485.572</td> <td>    2.511</td> <td> 0.012</td> <td> 2470.812</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>           <td> 8441.1675</td> <td> 3331.506</td> <td>    2.534</td> <td> 0.011</td> <td> 1910.908</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>          <td>-7389.7179</td> <td> 3617.876</td> <td>   -2.043</td> <td> 0.041</td> <td>-1.45e+04</td> <td> -298.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>          <td>  8.96e+04</td> <td> 3470.701</td> <td>   25.815</td> <td> 0.000</td> <td> 8.28e+04</td> <td> 9.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>          <td> 8824.9469</td> <td> 4454.428</td> <td>    1.981</td> <td> 0.048</td> <td>   93.589</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>          <td> -639.9548</td> <td> 3284.031</td> <td>   -0.195</td> <td> 0.845</td> <td>-7077.156</td> <td> 5797.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>          <td> 3684.1706</td> <td> 3584.414</td> <td>    1.028</td> <td> 0.304</td> <td>-3341.827</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>          <td> 3350.1200</td> <td> 3467.654</td> <td>    0.966</td> <td> 0.334</td> <td>-3447.011</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>          <td> 1.074e+04</td> <td> 4592.115</td> <td>    2.339</td> <td> 0.019</td> <td> 1739.052</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>          <td> 1.193e+04</td> <td> 3704.182</td> <td>    3.222</td> <td> 0.001</td> <td> 4674.066</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>           <td> 8045.7087</td> <td> 3516.851</td> <td>    2.288</td> <td> 0.022</td> <td> 1152.144</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>          <td> 9333.9268</td> <td> 4088.151</td> <td>    2.283</td> <td> 0.022</td> <td> 1320.528</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>          <td> 8107.0205</td> <td> 5501.313</td> <td>    1.474</td> <td> 0.141</td> <td>-2676.391</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>           <td> 6447.7796</td> <td> 4080.880</td> <td>    1.580</td> <td> 0.114</td> <td>-1551.368</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>          <td> 1.097e+04</td> <td> 4265.673</td> <td>    2.571</td> <td> 0.010</td> <td> 2605.463</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>          <td> 1.033e+04</td> <td> 4387.962</td> <td>    2.353</td> <td> 0.019</td> <td> 1724.321</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>          <td> 8527.2526</td> <td> 4009.702</td> <td>    2.127</td> <td> 0.033</td> <td>  667.624</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>          <td> 9019.3177</td> <td> 4216.569</td> <td>    2.139</td> <td> 0.032</td> <td>  754.199</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>          <td> 8118.8529</td> <td> 6722.925</td> <td>    1.208</td> <td> 0.227</td> <td>-5059.104</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>          <td> 1.221e+04</td> <td> 4797.855</td> <td>    2.545</td> <td> 0.011</td> <td> 2806.048</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>          <td>-9893.9636</td> <td> 3978.706</td> <td>   -2.487</td> <td> 0.013</td> <td>-1.77e+04</td> <td>-2095.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>           <td> 1.164e+04</td> <td> 4567.652</td> <td>    2.549</td> <td> 0.011</td> <td> 2690.848</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>          <td>-5830.7039</td> <td> 6161.565</td> <td>   -0.946</td> <td> 0.344</td> <td>-1.79e+04</td> <td> 6246.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>           <td> 1.128e+04</td> <td> 4606.609</td> <td>    2.449</td> <td> 0.014</td> <td> 2252.426</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>           <td> 1.082e+04</td> <td> 3929.942</td> <td>    2.753</td> <td> 0.006</td> <td> 3117.433</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>          <td>   1.2e+04</td> <td> 4455.981</td> <td>    2.693</td> <td> 0.007</td> <td> 3265.796</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>          <td> 7128.1442</td> <td> 4048.838</td> <td>    1.761</td> <td> 0.078</td> <td> -808.196</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>           <td> 1639.9713</td> <td> 4435.025</td> <td>    0.370</td> <td> 0.712</td> <td>-7053.353</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>          <td> 4849.3715</td> <td> 5498.022</td> <td>    0.882</td> <td> 0.378</td> <td>-5927.589</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>           <td> -511.5826</td> <td> 5831.432</td> <td>   -0.088</td> <td> 0.930</td> <td>-1.19e+04</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>          <td> 6163.1783</td> <td> 4257.004</td> <td>    1.448</td> <td> 0.148</td> <td>-2181.199</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>          <td>-1.072e+04</td> <td> 4821.541</td> <td>   -2.223</td> <td> 0.026</td> <td>-2.02e+04</td> <td>-1268.772</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>          <td> 8931.5001</td> <td> 3699.764</td> <td>    2.414</td> <td> 0.016</td> <td> 1679.398</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>          <td> 1.141e+04</td> <td> 4169.202</td> <td>    2.737</td> <td> 0.006</td> <td> 3238.275</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>          <td> 4068.7423</td> <td> 3712.436</td> <td>    1.096</td> <td> 0.273</td> <td>-3208.198</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>          <td> 1.202e+04</td> <td> 4758.917</td> <td>    2.526</td> <td> 0.012</td> <td> 2692.416</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>          <td> 1258.2074</td> <td> 5299.053</td> <td>    0.237</td> <td> 0.812</td> <td>-9128.744</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>          <td> 1.269e+04</td> <td> 4749.625</td> <td>    2.673</td> <td> 0.008</td> <td> 3384.617</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>           <td>-3324.7685</td> <td> 5248.095</td> <td>   -0.634</td> <td> 0.526</td> <td>-1.36e+04</td> <td> 6962.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>          <td> 9061.8658</td> <td> 4120.385</td> <td>    2.199</td> <td> 0.028</td> <td>  985.282</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>           <td> 2057.8258</td> <td> 4237.981</td> <td>    0.486</td> <td> 0.627</td> <td>-6249.263</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>           <td> 3146.3459</td> <td> 3782.620</td> <td>    0.832</td> <td> 0.406</td> <td>-4268.166</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>          <td> 1.088e+04</td> <td> 4304.183</td> <td>    2.527</td> <td> 0.012</td> <td> 2439.468</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>          <td>  1.22e+04</td> <td> 5116.181</td> <td>    2.384</td> <td> 0.017</td> <td> 2168.201</td> <td> 2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>          <td> -596.2531</td> <td> 3355.204</td> <td>   -0.178</td> <td> 0.859</td> <td>-7172.965</td> <td> 5980.458</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>           <td> 9648.9416</td> <td> 3677.558</td> <td>    2.624</td> <td> 0.009</td> <td> 2440.368</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>          <td> 8774.2049</td> <td> 4340.256</td> <td>    2.022</td> <td> 0.043</td> <td>  266.642</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>          <td> 9923.8171</td> <td> 3771.402</td> <td>    2.631</td> <td> 0.009</td> <td> 2531.295</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>          <td> 1785.4300</td> <td> 3979.030</td> <td>    0.449</td> <td> 0.654</td> <td>-6014.076</td> <td> 9584.936</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>          <td> 8775.3024</td> <td> 3785.900</td> <td>    2.318</td> <td> 0.020</td> <td> 1354.361</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>           <td> 1.583e+04</td> <td> 5146.276</td> <td>    3.077</td> <td> 0.002</td> <td> 5747.016</td> <td> 2.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14531.0</th>          <td> 8978.9881</td> <td> 1.01e+04</td> <td>    0.886</td> <td> 0.375</td> <td>-1.09e+04</td> <td> 2.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>          <td> 1.146e+04</td> <td> 4575.502</td> <td>    2.505</td> <td> 0.012</td> <td> 2491.648</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>          <td> 1.118e+04</td> <td> 7405.549</td> <td>    1.510</td> <td> 0.131</td> <td>-3335.409</td> <td> 2.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>           <td> 1.075e+04</td> <td> 4781.058</td> <td>    2.248</td> <td> 0.025</td> <td> 1376.754</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>           <td> 1.188e+04</td> <td> 4462.287</td> <td>    2.663</td> <td> 0.008</td> <td> 3137.130</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>          <td> 9851.0172</td> <td> 5967.234</td> <td>    1.651</td> <td> 0.099</td> <td>-1845.671</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>          <td> 1.089e+04</td> <td> 4473.110</td> <td>    2.434</td> <td> 0.015</td> <td> 2119.784</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>           <td> 1.237e+04</td> <td> 4176.488</td> <td>    2.962</td> <td> 0.003</td> <td> 4186.049</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>          <td> 1.032e+04</td> <td> 4202.868</td> <td>    2.456</td> <td> 0.014</td> <td> 2083.660</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>          <td> 5330.6304</td> <td> 3750.916</td> <td>    1.421</td> <td> 0.155</td> <td>-2021.737</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>           <td> 1.025e+04</td> <td> 3687.124</td> <td>    2.780</td> <td> 0.005</td> <td> 3023.899</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>          <td> 7623.0653</td> <td> 3820.476</td> <td>    1.995</td> <td> 0.046</td> <td>  134.350</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>           <td> 1.221e+04</td> <td> 4168.390</td> <td>    2.928</td> <td> 0.003</td> <td> 4034.702</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>          <td>-1.173e+04</td> <td> 4739.990</td> <td>   -2.476</td> <td> 0.013</td> <td> -2.1e+04</td> <td>-2442.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>          <td> 9204.2718</td> <td> 4005.034</td> <td>    2.298</td> <td> 0.022</td> <td> 1353.794</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>          <td> 1.127e+04</td> <td> 4819.636</td> <td>    2.338</td> <td> 0.019</td> <td> 1821.834</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>           <td>-2.091e+04</td> <td> 4631.919</td> <td>   -4.515</td> <td> 0.000</td> <td>   -3e+04</td> <td>-1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>           <td> 9602.8522</td> <td> 3468.687</td> <td>    2.768</td> <td> 0.006</td> <td> 2803.697</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>           <td> 1.722e+04</td> <td> 3483.910</td> <td>    4.944</td> <td> 0.000</td> <td> 1.04e+04</td> <td> 2.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>           <td> 1.144e+04</td> <td> 4184.743</td> <td>    2.733</td> <td> 0.006</td> <td> 3235.570</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>          <td> 7072.9575</td> <td> 4187.940</td> <td>    1.689</td> <td> 0.091</td> <td>-1136.043</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>           <td> 2653.4280</td> <td> 3125.560</td> <td>    0.849</td> <td> 0.396</td> <td>-3473.146</td> <td> 8780.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>           <td> 9133.1549</td> <td> 3813.953</td> <td>    2.395</td> <td> 0.017</td> <td> 1657.226</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>           <td>-5567.8350</td> <td> 4976.379</td> <td>   -1.119</td> <td> 0.263</td> <td>-1.53e+04</td> <td> 4186.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>          <td> -662.3621</td> <td> 3678.802</td> <td>   -0.180</td> <td> 0.857</td> <td>-7873.375</td> <td> 6548.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>          <td> 4365.1379</td> <td> 4051.649</td> <td>    1.077</td> <td> 0.281</td> <td>-3576.712</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>           <td> 6305.5157</td> <td> 3460.812</td> <td>    1.822</td> <td> 0.068</td> <td> -478.204</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>           <td> 1.188e+04</td> <td> 4009.212</td> <td>    2.963</td> <td> 0.003</td> <td> 4021.892</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>           <td> 1.612e+04</td> <td> 3749.056</td> <td>    4.299</td> <td> 0.000</td> <td> 8769.535</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>          <td> 5981.0310</td> <td> 3906.403</td> <td>    1.531</td> <td> 0.126</td> <td>-1676.113</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>          <td> 5061.6838</td> <td> 3959.387</td> <td>    1.278</td> <td> 0.201</td> <td>-2699.318</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>           <td>-1.108e+04</td> <td> 3519.723</td> <td>   -3.149</td> <td> 0.002</td> <td> -1.8e+04</td> <td>-4184.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>           <td> 9217.5903</td> <td> 3557.851</td> <td>    2.591</td> <td> 0.010</td> <td> 2243.659</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17101.0</th>          <td>-1.638e+04</td> <td> 1.17e+04</td> <td>   -1.405</td> <td> 0.160</td> <td>-3.92e+04</td> <td> 6464.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>          <td> 9944.8151</td> <td> 4458.719</td> <td>    2.230</td> <td> 0.026</td> <td> 1205.045</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>           <td> 9966.2654</td> <td> 3919.741</td> <td>    2.543</td> <td> 0.011</td> <td> 2282.975</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>           <td> 1.199e+04</td> <td> 4123.710</td> <td>    2.907</td> <td> 0.004</td> <td> 3904.903</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>           <td>  1.14e+04</td> <td> 5013.948</td> <td>    2.274</td> <td> 0.023</td> <td> 1574.373</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>           <td> 1.127e+04</td> <td> 4027.944</td> <td>    2.797</td> <td> 0.005</td> <td> 3371.582</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>           <td> 6020.1626</td> <td> 3557.041</td> <td>    1.692</td> <td> 0.091</td> <td> -952.180</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>           <td> 1.144e+04</td> <td> 4568.557</td> <td>    2.504</td> <td> 0.012</td> <td> 2482.906</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>           <td>-3272.3191</td> <td> 3073.973</td> <td>   -1.065</td> <td> 0.287</td> <td>-9297.774</td> <td> 2753.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>          <td> 8898.7772</td> <td> 4275.979</td> <td>    2.081</td> <td> 0.037</td> <td>  517.206</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>           <td> 7869.9943</td> <td> 4068.111</td> <td>    1.935</td> <td> 0.053</td> <td> -104.124</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>           <td> -542.6371</td> <td> 3639.338</td> <td>   -0.149</td> <td> 0.881</td> <td>-7676.295</td> <td> 6591.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>          <td> 1.171e+04</td> <td> 5117.475</td> <td>    2.288</td> <td> 0.022</td> <td> 1680.301</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>           <td> 5894.0920</td> <td> 4997.948</td> <td>    1.179</td> <td> 0.238</td> <td>-3902.649</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>           <td> 1.117e+04</td> <td> 4392.864</td> <td>    2.543</td> <td> 0.011</td> <td> 2562.229</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>           <td> 7336.9598</td> <td> 3351.413</td> <td>    2.189</td> <td> 0.029</td> <td>  767.680</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>           <td> 1.059e+04</td> <td> 4046.792</td> <td>    2.616</td> <td> 0.009</td> <td> 2653.940</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>           <td> 7313.0108</td> <td> 3212.234</td> <td>    2.277</td> <td> 0.023</td> <td> 1016.543</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>           <td> 9900.3459</td> <td> 3469.706</td> <td>    2.853</td> <td> 0.004</td> <td> 3099.193</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>           <td> 1.247e+04</td> <td> 4073.086</td> <td>    3.062</td> <td> 0.002</td> <td> 4486.807</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>           <td> 1.105e+04</td> <td> 4077.664</td> <td>    2.710</td> <td> 0.007</td> <td> 3058.436</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>           <td> 2066.6708</td> <td> 4287.949</td> <td>    0.482</td> <td> 0.630</td> <td>-6338.363</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>           <td> 1.028e+04</td> <td> 3494.046</td> <td>    2.943</td> <td> 0.003</td> <td> 3434.591</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>           <td> 1.017e+04</td> <td> 3530.470</td> <td>    2.880</td> <td> 0.004</td> <td> 3248.358</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>           <td> 1.117e+04</td> <td> 4362.901</td> <td>    2.560</td> <td> 0.010</td> <td> 2616.168</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>           <td> 8626.2108</td> <td> 3719.509</td> <td>    2.319</td> <td> 0.020</td> <td> 1335.406</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>           <td> 1.006e+04</td> <td> 3610.499</td> <td>    2.787</td> <td> 0.005</td> <td> 2984.839</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>           <td> 1.278e+04</td> <td> 4481.478</td> <td>    2.851</td> <td> 0.004</td> <td> 3993.798</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>          <td> 5.394e+04</td> <td> 3923.620</td> <td>   13.748</td> <td> 0.000</td> <td> 4.63e+04</td> <td> 6.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>           <td>-5400.0504</td> <td> 4099.351</td> <td>   -1.317</td> <td> 0.188</td> <td>-1.34e+04</td> <td> 2635.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>           <td> 7832.7624</td> <td> 3450.472</td> <td>    2.270</td> <td> 0.023</td> <td> 1069.311</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>           <td> 8505.6464</td> <td> 3461.370</td> <td>    2.457</td> <td> 0.014</td> <td> 1720.834</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>          <td> 4498.8386</td> <td> 5310.844</td> <td>    0.847</td> <td> 0.397</td> <td>-5911.226</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>          <td> 1.083e+04</td> <td> 4828.430</td> <td>    2.243</td> <td> 0.025</td> <td> 1365.421</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>           <td> 1.018e+04</td> <td> 3588.996</td> <td>    2.836</td> <td> 0.005</td> <td> 3142.326</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>           <td> 1.769e+04</td> <td> 5276.450</td> <td>    3.354</td> <td> 0.001</td> <td> 7352.347</td> <td>  2.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>          <td> -1.28e+04</td> <td> 5019.607</td> <td>   -2.549</td> <td> 0.011</td> <td>-2.26e+04</td> <td>-2956.627</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>           <td> 1.055e+04</td> <td> 3883.931</td> <td>    2.716</td> <td> 0.007</td> <td> 2936.502</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>           <td> 5.108e+04</td> <td> 4320.049</td> <td>   11.825</td> <td> 0.000</td> <td> 4.26e+04</td> <td> 5.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>           <td> 1.253e+04</td> <td> 4513.709</td> <td>    2.776</td> <td> 0.006</td> <td> 3681.859</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>           <td> 4436.7599</td> <td> 3438.026</td> <td>    1.290</td> <td> 0.197</td> <td>-2302.296</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>           <td> 1.034e+04</td> <td> 3652.188</td> <td>    2.833</td> <td> 0.005</td> <td> 3185.997</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>          <td> 1.269e+04</td> <td> 4884.409</td> <td>    2.598</td> <td> 0.009</td> <td> 3114.998</td> <td> 2.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>           <td> 7783.8828</td> <td> 6251.292</td> <td>    1.245</td> <td> 0.213</td> <td>-4469.603</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>           <td> 1.047e+04</td> <td> 4520.037</td> <td>    2.316</td> <td> 0.021</td> <td> 1610.401</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>          <td>  671.7506</td> <td> 4354.809</td> <td>    0.154</td> <td> 0.877</td> <td>-7864.340</td> <td> 9207.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>           <td> 9504.4405</td> <td> 3720.917</td> <td>    2.554</td> <td> 0.011</td> <td> 2210.876</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>          <td> 1.145e+04</td> <td> 4546.012</td> <td>    2.518</td> <td> 0.012</td> <td> 2536.613</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>           <td>  954.6225</td> <td> 3444.320</td> <td>    0.277</td> <td> 0.782</td> <td>-5796.771</td> <td> 7706.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>          <td> 5747.8505</td> <td> 3998.387</td> <td>    1.438</td> <td> 0.151</td> <td>-2089.598</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>           <td>-1.948e+04</td> <td> 3317.110</td> <td>   -5.871</td> <td> 0.000</td> <td> -2.6e+04</td> <td> -1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>           <td> 6425.0318</td> <td> 3607.703</td> <td>    1.781</td> <td> 0.075</td> <td> -646.616</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>           <td> 1.256e+04</td> <td> 5597.325</td> <td>    2.244</td> <td> 0.025</td> <td> 1586.917</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>           <td> 5728.1236</td> <td> 3470.724</td> <td>    1.650</td> <td> 0.099</td> <td>-1075.024</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>          <td> 9924.7612</td> <td> 4113.115</td> <td>    2.413</td> <td> 0.016</td> <td> 1862.428</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>          <td>-3755.6038</td> <td> 6047.146</td> <td>   -0.621</td> <td> 0.535</td> <td>-1.56e+04</td> <td> 8097.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>           <td> -534.6446</td> <td> 6309.619</td> <td>   -0.085</td> <td> 0.932</td> <td>-1.29e+04</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>           <td> 9736.7758</td> <td> 3588.280</td> <td>    2.713</td> <td> 0.007</td> <td> 2703.200</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>          <td>-1938.1193</td> <td> 4766.712</td> <td>   -0.407</td> <td> 0.684</td> <td>-1.13e+04</td> <td> 7405.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>           <td> 1.238e+04</td> <td> 4216.522</td> <td>    2.936</td> <td> 0.003</td> <td> 4114.191</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>           <td> 7550.3051</td> <td> 3430.390</td> <td>    2.201</td> <td> 0.028</td> <td>  826.217</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>           <td> 1.248e+04</td> <td> 4112.791</td> <td>    3.034</td> <td> 0.002</td> <td> 4415.079</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>           <td>  1.42e+04</td> <td> 4525.795</td> <td>    3.138</td> <td> 0.002</td> <td> 5330.352</td> <td> 2.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>           <td> 7862.9049</td> <td> 3409.081</td> <td>    2.306</td> <td> 0.021</td> <td> 1180.587</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>           <td> 9317.7327</td> <td> 3635.420</td> <td>    2.563</td> <td> 0.010</td> <td> 2191.756</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>           <td> 1.172e+04</td> <td> 5182.428</td> <td>    2.262</td> <td> 0.024</td> <td> 1564.434</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>          <td> 1.086e+04</td> <td> 4248.909</td> <td>    2.557</td> <td> 0.011</td> <td> 2536.128</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>          <td>  522.9771</td> <td> 4749.139</td> <td>    0.110</td> <td> 0.912</td> <td>-8786.059</td> <td> 9832.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>           <td> 1.252e+04</td> <td> 4265.554</td> <td>    2.935</td> <td> 0.003</td> <td> 4156.786</td> <td> 2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>          <td> 1.161e+04</td> <td> 5237.807</td> <td>    2.217</td> <td> 0.027</td> <td> 1346.333</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>           <td> 2742.2213</td> <td> 3605.284</td> <td>    0.761</td> <td> 0.447</td> <td>-4324.684</td> <td> 9809.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>           <td>-3534.0958</td> <td> 4056.194</td> <td>   -0.871</td> <td> 0.384</td> <td>-1.15e+04</td> <td> 4416.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>           <td>  1.16e+04</td> <td> 4095.612</td> <td>    2.832</td> <td> 0.005</td> <td> 3570.062</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>          <td> 1.094e+04</td> <td> 4389.059</td> <td>    2.494</td> <td> 0.013</td> <td> 2341.169</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>           <td>  1.17e+04</td> <td> 4013.804</td> <td>    2.915</td> <td> 0.004</td> <td> 3832.399</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>          <td> 1.153e+04</td> <td> 7440.529</td> <td>    1.549</td> <td> 0.121</td> <td>-3059.387</td> <td> 2.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>          <td> 9297.6728</td> <td> 4410.080</td> <td>    2.108</td> <td> 0.035</td> <td>  653.244</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>           <td>-3686.1598</td> <td> 4295.083</td> <td>   -0.858</td> <td> 0.391</td> <td>-1.21e+04</td> <td> 4732.858</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>           <td> 1.152e+04</td> <td> 5113.381</td> <td>    2.252</td> <td> 0.024</td> <td> 1494.339</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>          <td> 1.123e+04</td> <td> 6651.487</td> <td>    1.688</td> <td> 0.091</td> <td>-1810.098</td> <td> 2.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>           <td> 4060.4291</td> <td> 3620.838</td> <td>    1.121</td> <td> 0.262</td> <td>-3036.965</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>           <td> 1.089e+04</td> <td> 4153.132</td> <td>    2.623</td> <td> 0.009</td> <td> 2753.170</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>           <td> 4823.8558</td> <td> 4406.699</td> <td>    1.095</td> <td> 0.274</td> <td>-3813.945</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>          <td> 1.101e+04</td> <td> 4450.960</td> <td>    2.474</td> <td> 0.013</td> <td> 2285.080</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>           <td> 9384.7662</td> <td> 3451.936</td> <td>    2.719</td> <td> 0.007</td> <td> 2618.446</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>           <td> 9890.8041</td> <td> 3570.057</td> <td>    2.770</td> <td> 0.006</td> <td> 2892.948</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>           <td> 7510.0147</td> <td> 4622.486</td> <td>    1.625</td> <td> 0.104</td> <td>-1550.762</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>           <td> 1.441e+04</td> <td> 3919.032</td> <td>    3.678</td> <td> 0.000</td> <td> 6732.007</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>           <td> 7729.5092</td> <td> 4553.521</td> <td>    1.697</td> <td> 0.090</td> <td>-1196.087</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>           <td>  1.09e+04</td> <td> 3973.049</td> <td>    2.743</td> <td> 0.006</td> <td> 3111.448</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>           <td>-2345.0266</td> <td> 3283.149</td> <td>   -0.714</td> <td> 0.475</td> <td>-8780.498</td> <td> 4090.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>           <td> 1.203e+04</td> <td> 4223.324</td> <td>    2.849</td> <td> 0.004</td> <td> 3753.698</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>           <td>    0.4896</td> <td> 3865.011</td> <td>    0.000</td> <td> 1.000</td> <td>-7575.522</td> <td> 7576.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>          <td>-8274.1760</td> <td> 4174.195</td> <td>   -1.982</td> <td> 0.047</td> <td>-1.65e+04</td> <td>  -92.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>          <td> 9223.2361</td> <td> 3878.259</td> <td>    2.378</td> <td> 0.017</td> <td> 1621.257</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>          <td>-5918.1481</td> <td> 4904.397</td> <td>   -1.207</td> <td> 0.228</td> <td>-1.55e+04</td> <td> 3695.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>           <td> 1.067e+04</td> <td> 3701.823</td> <td>    2.881</td> <td> 0.004</td> <td> 3410.014</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>           <td> 1.195e+04</td> <td> 4495.724</td> <td>    2.659</td> <td> 0.008</td> <td> 3141.271</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>           <td> 6165.4632</td> <td> 3795.123</td> <td>    1.625</td> <td> 0.104</td> <td>-1273.556</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>          <td>-3779.9320</td> <td> 3953.912</td> <td>   -0.956</td> <td> 0.339</td> <td>-1.15e+04</td> <td> 3970.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>           <td>-1.324e+04</td> <td> 6688.460</td> <td>   -1.980</td> <td> 0.048</td> <td>-2.64e+04</td> <td> -133.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>           <td> 1.223e+04</td> <td> 4616.162</td> <td>    2.649</td> <td> 0.008</td> <td> 3179.648</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>           <td> 1.061e+04</td> <td> 3801.196</td> <td>    2.792</td> <td> 0.005</td> <td> 3162.859</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>           <td> 9163.1119</td> <td> 4168.518</td> <td>    2.198</td> <td> 0.028</td> <td>  992.181</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>           <td> 4507.4799</td> <td> 3464.677</td> <td>    1.301</td> <td> 0.193</td> <td>-2283.815</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>           <td> 1.035e+04</td> <td> 4073.030</td> <td>    2.540</td> <td> 0.011</td> <td> 2363.511</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>           <td>-1437.3222</td> <td> 3931.504</td> <td>   -0.366</td> <td> 0.715</td> <td>-9143.669</td> <td> 6269.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>           <td>-1447.5958</td> <td> 4429.291</td> <td>   -0.327</td> <td> 0.744</td> <td>-1.01e+04</td> <td> 7234.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>           <td>  1.26e+04</td> <td> 4378.506</td> <td>    2.878</td> <td> 0.004</td> <td> 4018.449</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>           <td> 9786.3293</td> <td> 3882.487</td> <td>    2.521</td> <td> 0.012</td> <td> 2176.063</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>           <td>-9619.9936</td> <td> 5039.433</td> <td>   -1.909</td> <td> 0.056</td> <td>-1.95e+04</td> <td>  258.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>           <td> 1.265e+04</td> <td> 3725.803</td> <td>    3.394</td> <td> 0.001</td> <td> 5342.370</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>           <td> 3796.6585</td> <td> 4349.266</td> <td>    0.873</td> <td> 0.383</td> <td>-4728.567</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>           <td> 1.066e+04</td> <td> 4996.454</td> <td>    2.134</td> <td> 0.033</td> <td>  867.551</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>           <td> 1.278e+04</td> <td> 3913.903</td> <td>    3.266</td> <td> 0.001</td> <td> 5111.474</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>           <td> 1.102e+04</td> <td> 3683.022</td> <td>    2.992</td> <td> 0.003</td> <td> 3800.977</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>           <td> 6.396e+04</td> <td> 4031.384</td> <td>   15.866</td> <td> 0.000</td> <td> 5.61e+04</td> <td> 7.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>           <td> 1.045e+04</td> <td> 4497.387</td> <td>    2.323</td> <td> 0.020</td> <td> 1633.650</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>           <td> 1.036e+04</td> <td> 3840.485</td> <td>    2.697</td> <td> 0.007</td> <td> 2830.490</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>           <td> 1.222e+04</td> <td> 3803.160</td> <td>    3.213</td> <td> 0.001</td> <td> 4764.830</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>           <td> 3978.5918</td> <td> 4778.656</td> <td>    0.833</td> <td> 0.405</td> <td>-5388.302</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>           <td> 7525.3953</td> <td> 3817.285</td> <td>    1.971</td> <td> 0.049</td> <td>   42.935</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>           <td> 7539.4676</td> <td> 4373.199</td> <td>    1.724</td> <td> 0.085</td> <td>-1032.668</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>           <td> 1.102e+04</td> <td> 3756.998</td> <td>    2.934</td> <td> 0.003</td> <td> 3657.067</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>           <td> 1.099e+04</td> <td> 4195.025</td> <td>    2.620</td> <td> 0.009</td> <td> 2766.893</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>           <td> 1.107e+04</td> <td> 3776.058</td> <td>    2.933</td> <td> 0.003</td> <td> 3671.798</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>           <td>-2.004e+04</td> <td> 3435.208</td> <td>   -5.833</td> <td> 0.000</td> <td>-2.68e+04</td> <td>-1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>           <td> 3973.2246</td> <td> 4193.822</td> <td>    0.947</td> <td> 0.343</td> <td>-4247.305</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>           <td> 1.097e+04</td> <td> 4243.952</td> <td>    2.584</td> <td> 0.010</td> <td> 2648.966</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>           <td> 1.022e+04</td> <td> 3593.566</td> <td>    2.844</td> <td> 0.004</td> <td> 3174.659</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>           <td> 4718.9993</td> <td> 3142.124</td> <td>    1.502</td> <td> 0.133</td> <td>-1440.043</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>           <td> 4945.7255</td> <td> 3530.657</td> <td>    1.401</td> <td> 0.161</td> <td>-1974.900</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>           <td> 9538.6385</td> <td> 4075.855</td> <td>    2.340</td> <td> 0.019</td> <td> 1549.342</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>           <td> 8971.7211</td> <td> 3460.723</td> <td>    2.592</td> <td> 0.010</td> <td> 2188.177</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>           <td> 1.079e+04</td> <td> 3473.780</td> <td>    3.105</td> <td> 0.002</td> <td> 3977.426</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>           <td> 1.266e+04</td> <td> 5621.210</td> <td>    2.253</td> <td> 0.024</td> <td> 1645.503</td> <td> 2.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>           <td> 7108.9511</td> <td> 3449.114</td> <td>    2.061</td> <td> 0.039</td> <td>  348.162</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>           <td> 1.271e+04</td> <td> 4548.236</td> <td>    2.795</td> <td> 0.005</td> <td> 3795.224</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>           <td> 9717.4324</td> <td> 4083.473</td> <td>    2.380</td> <td> 0.017</td> <td> 1713.204</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>           <td> 1.223e+04</td> <td> 4314.302</td> <td>    2.836</td> <td> 0.005</td> <td> 3776.892</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>           <td>  -35.2982</td> <td> 3266.121</td> <td>   -0.011</td> <td> 0.991</td> <td>-6437.393</td> <td> 6366.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>           <td> 1972.6354</td> <td> 3363.711</td> <td>    0.586</td> <td> 0.558</td> <td>-4620.750</td> <td> 8566.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>           <td> 9054.5036</td> <td> 3495.631</td> <td>    2.590</td> <td> 0.010</td> <td> 2202.533</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>           <td>-5653.8420</td> <td> 3410.927</td> <td>   -1.658</td> <td> 0.097</td> <td>-1.23e+04</td> <td> 1032.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>           <td> 9184.4944</td> <td> 3795.950</td> <td>    2.420</td> <td> 0.016</td> <td> 1743.855</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>           <td> 6261.7669</td> <td> 3189.760</td> <td>    1.963</td> <td> 0.050</td> <td>    9.351</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>           <td>-4303.5751</td> <td> 5205.937</td> <td>   -0.827</td> <td> 0.408</td> <td>-1.45e+04</td> <td> 5900.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>           <td> 2109.6502</td> <td> 4872.186</td> <td>    0.433</td> <td> 0.665</td> <td>-7440.577</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>           <td>-2053.4891</td> <td> 3403.699</td> <td>   -0.603</td> <td> 0.546</td> <td>-8725.259</td> <td> 4618.281</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>           <td> 9489.0672</td> <td> 3988.705</td> <td>    2.379</td> <td> 0.017</td> <td> 1670.597</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>           <td>-5002.4715</td> <td> 5240.283</td> <td>   -0.955</td> <td> 0.340</td> <td>-1.53e+04</td> <td> 5269.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>           <td> 1.161e+04</td> <td> 4287.437</td> <td>    2.708</td> <td> 0.007</td> <td> 3206.709</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>           <td> 3602.0856</td> <td> 3562.673</td> <td>    1.011</td> <td> 0.312</td> <td>-3381.297</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>           <td> 7755.4892</td> <td> 4065.538</td> <td>    1.908</td> <td> 0.056</td> <td> -213.585</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>           <td> 2233.2361</td> <td> 4317.199</td> <td>    0.517</td> <td> 0.605</td> <td>-6229.132</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>           <td> 1.151e+04</td> <td> 4015.359</td> <td>    2.867</td> <td> 0.004</td> <td> 3640.571</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>           <td> 6511.3911</td> <td> 5260.917</td> <td>    1.238</td> <td> 0.216</td> <td>-3800.809</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>           <td> 5754.9750</td> <td> 3276.668</td> <td>    1.756</td> <td> 0.079</td> <td> -667.793</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>           <td> 1.148e+04</td> <td> 4187.146</td> <td>    2.742</td> <td> 0.006</td> <td> 3272.315</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>           <td> 1.227e+04</td> <td> 4348.144</td> <td>    2.822</td> <td> 0.005</td> <td> 3745.972</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>           <td>  1.08e+04</td> <td> 3856.276</td> <td>    2.801</td> <td> 0.005</td> <td> 3242.502</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>           <td> 2.076e+04</td> <td> 3714.901</td> <td>    5.588</td> <td> 0.000</td> <td> 1.35e+04</td> <td>  2.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>           <td> 7677.6628</td> <td> 3373.504</td> <td>    2.276</td> <td> 0.023</td> <td> 1065.082</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>           <td> 1.221e+04</td> <td> 4119.481</td> <td>    2.964</td> <td> 0.003</td> <td> 4134.874</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>           <td> 4321.4450</td> <td> 4087.621</td> <td>    1.057</td> <td> 0.290</td> <td>-3690.916</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>           <td> 1.029e+04</td> <td> 3779.927</td> <td>    2.722</td> <td> 0.006</td> <td> 2881.014</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>           <td>-7864.2560</td> <td> 3258.229</td> <td>   -2.414</td> <td> 0.016</td> <td>-1.43e+04</td> <td>-1477.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>           <td> 1.447e+04</td> <td> 4511.974</td> <td>    3.207</td> <td> 0.001</td> <td> 5624.335</td> <td> 2.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>           <td> 9410.6027</td> <td> 4925.705</td> <td>    1.911</td> <td> 0.056</td> <td> -244.530</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>           <td>-1.904e+04</td> <td> 3566.201</td> <td>   -5.338</td> <td> 0.000</td> <td> -2.6e+04</td> <td> -1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>           <td> 9316.1058</td> <td> 4179.475</td> <td>    2.229</td> <td> 0.026</td> <td> 1123.697</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>           <td> 4598.9212</td> <td> 3230.438</td> <td>    1.424</td> <td> 0.155</td> <td>-1733.230</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>           <td> 1.238e+04</td> <td> 4680.272</td> <td>    2.645</td> <td> 0.008</td> <td> 3206.805</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>           <td> 1.048e+04</td> <td> 4305.636</td> <td>    2.433</td> <td> 0.015</td> <td> 2035.387</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>           <td> 1.241e+04</td> <td> 4222.066</td> <td>    2.940</td> <td> 0.003</td> <td> 4136.346</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>           <td> -735.4906</td> <td> 3515.408</td> <td>   -0.209</td> <td> 0.834</td> <td>-7626.227</td> <td> 6155.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>           <td>-3459.6812</td> <td> 3114.452</td> <td>   -1.111</td> <td> 0.267</td> <td>-9564.483</td> <td> 2645.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>           <td> 1.125e+04</td> <td> 3649.832</td> <td>    3.083</td> <td> 0.002</td> <td> 4097.360</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>           <td>-2143.4353</td> <td> 3777.823</td> <td>   -0.567</td> <td> 0.570</td> <td>-9548.544</td> <td> 5261.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>           <td> 1.092e+04</td> <td> 3729.340</td> <td>    2.927</td> <td> 0.003</td> <td> 3606.865</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>           <td> 1.203e+04</td> <td> 4142.789</td> <td>    2.903</td> <td> 0.004</td> <td> 3906.307</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>           <td> 9581.9468</td> <td> 4108.600</td> <td>    2.332</td> <td> 0.020</td> <td> 1528.464</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>           <td> 9736.2323</td> <td> 3591.278</td> <td>    2.711</td> <td> 0.007</td> <td> 2696.779</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>           <td> 8285.3216</td> <td> 3383.445</td> <td>    2.449</td> <td> 0.014</td> <td> 1653.253</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>           <td> 9296.5716</td> <td> 4979.568</td> <td>    1.867</td> <td> 0.062</td> <td> -464.142</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>           <td> 7675.5694</td> <td> 3435.786</td> <td>    2.234</td> <td> 0.026</td> <td>  940.904</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>           <td> 9023.4305</td> <td> 3522.492</td> <td>    2.562</td> <td> 0.010</td> <td> 2118.810</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>           <td> 1.091e+04</td> <td> 4160.138</td> <td>    2.624</td> <td> 0.009</td> <td> 2760.450</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>           <td> 9206.4725</td> <td> 4054.859</td> <td>    2.270</td> <td> 0.023</td> <td> 1258.330</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>           <td>  699.4465</td> <td> 3334.112</td> <td>    0.210</td> <td> 0.834</td> <td>-5835.921</td> <td> 7234.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>           <td> 4574.8460</td> <td> 3383.453</td> <td>    1.352</td> <td> 0.176</td> <td>-2057.239</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>           <td> 1.052e+04</td> <td> 3525.801</td> <td>    2.985</td> <td> 0.003</td> <td> 3613.707</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>           <td> 9794.1435</td> <td> 7586.255</td> <td>    1.291</td> <td> 0.197</td> <td>-5076.073</td> <td> 2.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>           <td> 1.119e+04</td> <td> 3823.353</td> <td>    2.926</td> <td> 0.003</td> <td> 3693.045</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>           <td> 1.259e+04</td> <td> 4538.028</td> <td>    2.775</td> <td> 0.006</td> <td> 3697.287</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>           <td> 1.212e+04</td> <td> 4254.059</td> <td>    2.849</td> <td> 0.004</td> <td> 3779.535</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>           <td> 8302.6110</td> <td> 3564.836</td> <td>    2.329</td> <td> 0.020</td> <td> 1314.989</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>           <td> -353.5173</td> <td> 4049.300</td> <td>   -0.087</td> <td> 0.930</td> <td>-8290.763</td> <td> 7583.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>           <td> 1.197e+04</td> <td> 4164.987</td> <td>    2.874</td> <td> 0.004</td> <td> 3807.698</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>           <td>  570.1042</td> <td> 4185.709</td> <td>    0.136</td> <td> 0.892</td> <td>-7634.523</td> <td> 8774.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>           <td> 7234.5217</td> <td> 3341.365</td> <td>    2.165</td> <td> 0.030</td> <td>  684.936</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>           <td> 1.083e+04</td> <td> 4177.485</td> <td>    2.592</td> <td> 0.010</td> <td> 2639.526</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>           <td> 1.054e+04</td> <td> 3845.062</td> <td>    2.740</td> <td> 0.006</td> <td> 2998.522</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>           <td>  1.23e+04</td> <td> 4275.165</td> <td>    2.877</td> <td> 0.004</td> <td> 3920.863</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>           <td> 1.055e+04</td> <td> 4286.620</td> <td>    2.462</td> <td> 0.014</td> <td> 2151.440</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>           <td> 1.197e+04</td> <td> 4035.859</td> <td>    2.966</td> <td> 0.003</td> <td> 4059.365</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>           <td> 1.149e+04</td> <td> 3962.282</td> <td>    2.901</td> <td> 0.004</td> <td> 3727.073</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>           <td>-1.368e+05</td> <td> 4706.996</td> <td>  -29.062</td> <td> 0.000</td> <td>-1.46e+05</td> <td>-1.28e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>           <td>-3544.4727</td> <td> 5456.969</td> <td>   -0.650</td> <td> 0.516</td> <td>-1.42e+04</td> <td> 7152.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>           <td> 9402.2705</td> <td> 3445.968</td> <td>    2.728</td> <td> 0.006</td> <td> 2647.647</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>           <td> 9382.8163</td> <td> 4045.853</td> <td>    2.319</td> <td> 0.020</td> <td> 1452.327</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>           <td> 9212.0044</td> <td> 3448.176</td> <td>    2.672</td> <td> 0.008</td> <td> 2453.054</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>           <td> 9970.1650</td> <td> 3800.845</td> <td>    2.623</td> <td> 0.009</td> <td> 2519.929</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>           <td>-6740.0949</td> <td> 3831.719</td> <td>   -1.759</td> <td> 0.079</td> <td>-1.43e+04</td> <td>  770.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>           <td> 1.766e+04</td> <td> 4298.290</td> <td>    4.110</td> <td> 0.000</td> <td> 9239.322</td> <td> 2.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>           <td> 1.286e+04</td> <td> 4536.766</td> <td>    2.835</td> <td> 0.005</td> <td> 3968.498</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>           <td> 5292.8001</td> <td> 3685.801</td> <td>    1.436</td> <td> 0.151</td> <td>-1931.933</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>           <td>-1494.8542</td> <td> 4969.243</td> <td>   -0.301</td> <td> 0.764</td> <td>-1.12e+04</td> <td> 8245.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>           <td> 6774.1739</td> <td> 3369.587</td> <td>    2.010</td> <td> 0.044</td> <td>  169.270</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>           <td> 1.108e+04</td> <td> 3841.583</td> <td>    2.884</td> <td> 0.004</td> <td> 3549.242</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>           <td> 6882.4486</td> <td> 3529.631</td> <td>    1.950</td> <td> 0.051</td> <td>  -36.166</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>           <td>-2455.8570</td> <td> 3177.871</td> <td>   -0.773</td> <td> 0.440</td> <td>-8684.969</td> <td> 3773.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>           <td> 5.259e+04</td> <td> 4554.429</td> <td>   11.546</td> <td> 0.000</td> <td> 4.37e+04</td> <td> 6.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>           <td> 1.158e+04</td> <td> 4427.376</td> <td>    2.615</td> <td> 0.009</td> <td> 2898.115</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>           <td> 1.078e+04</td> <td> 4323.382</td> <td>    2.494</td> <td> 0.013</td> <td> 2310.134</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>           <td> -2.02e+05</td> <td> 6401.840</td> <td>  -31.546</td> <td> 0.000</td> <td>-2.14e+05</td> <td>-1.89e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>           <td> 6088.8350</td> <td> 3522.160</td> <td>    1.729</td> <td> 0.084</td> <td> -815.136</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>           <td> 1.217e+04</td> <td> 4304.921</td> <td>    2.826</td> <td> 0.005</td> <td> 3729.124</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>           <td>-1193.1184</td> <td> 4145.651</td> <td>   -0.288</td> <td> 0.774</td> <td>-9319.226</td> <td> 6932.989</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>           <td> 6279.1585</td> <td> 3312.645</td> <td>    1.896</td> <td> 0.058</td> <td> -214.130</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>           <td> 4252.6159</td> <td> 3634.799</td> <td>    1.170</td> <td> 0.242</td> <td>-2872.144</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>           <td> 8303.3545</td> <td> 4310.795</td> <td>    1.926</td> <td> 0.054</td> <td> -146.461</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>           <td> 7418.0199</td> <td> 4698.379</td> <td>    1.579</td> <td> 0.114</td> <td>-1791.519</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>           <td> 2.099e+04</td> <td> 4031.196</td> <td>    5.207</td> <td> 0.000</td> <td> 1.31e+04</td> <td> 2.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>           <td> 8015.5017</td> <td> 3945.068</td> <td>    2.032</td> <td> 0.042</td> <td>  282.567</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>           <td>  1.14e+04</td> <td> 4114.840</td> <td>    2.771</td> <td> 0.006</td> <td> 3335.369</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>           <td> 1.224e+04</td> <td> 4311.848</td> <td>    2.838</td> <td> 0.005</td> <td> 3786.337</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>           <td> 1.224e+04</td> <td> 4519.231</td> <td>    2.708</td> <td> 0.007</td> <td> 3379.174</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>           <td> 3035.1962</td> <td> 3090.895</td> <td>    0.982</td> <td> 0.326</td> <td>-3023.429</td> <td> 9093.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>           <td>-2691.0876</td> <td> 3571.956</td> <td>   -0.753</td> <td> 0.451</td> <td>-9692.666</td> <td> 4310.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>           <td> 1.093e+04</td> <td> 3947.487</td> <td>    2.769</td> <td> 0.006</td> <td> 3192.997</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>           <td> 1.027e+04</td> <td> 3622.943</td> <td>    2.835</td> <td> 0.005</td> <td> 3170.558</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>           <td> 1.069e+04</td> <td> 3659.795</td> <td>    2.920</td> <td> 0.004</td> <td> 3512.110</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>           <td> 8878.3146</td> <td> 3452.015</td> <td>    2.572</td> <td> 0.010</td> <td> 2111.840</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>           <td> 1.152e+04</td> <td> 3892.628</td> <td>    2.959</td> <td> 0.003</td> <td> 3886.700</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>           <td> 1.224e+04</td> <td> 4350.392</td> <td>    2.814</td> <td> 0.005</td> <td> 3713.543</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>           <td> 9178.7547</td> <td> 4056.182</td> <td>    2.263</td> <td> 0.024</td> <td> 1228.020</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>           <td> 1.282e+04</td> <td> 4549.255</td> <td>    2.818</td> <td> 0.005</td> <td> 3901.137</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>           <td> 1.027e+04</td> <td> 4237.773</td> <td>    2.423</td> <td> 0.015</td> <td> 1961.674</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>           <td> 1.204e+04</td> <td> 4595.709</td> <td>    2.620</td> <td> 0.009</td> <td> 3031.184</td> <td>  2.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>           <td>-1.033e+04</td> <td> 3427.634</td> <td>   -3.015</td> <td> 0.003</td> <td>-1.71e+04</td> <td>-3615.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>           <td> 1.046e+04</td> <td> 3768.357</td> <td>    2.776</td> <td> 0.006</td> <td> 3073.259</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>           <td> 1.179e+04</td> <td> 4246.204</td> <td>    2.777</td> <td> 0.005</td> <td> 3468.650</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>           <td> 9808.6268</td> <td> 4310.289</td> <td>    2.276</td> <td> 0.023</td> <td> 1359.804</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>           <td> 8554.6270</td> <td> 3689.535</td> <td>    2.319</td> <td> 0.020</td> <td> 1322.576</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>           <td>   1.2e+04</td> <td> 4089.836</td> <td>    2.933</td> <td> 0.003</td> <td> 3979.670</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>           <td> 1.447e+04</td> <td> 3861.174</td> <td>    3.747</td> <td> 0.000</td> <td> 6900.100</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>           <td> 1.217e+04</td> <td> 4187.067</td> <td>    2.906</td> <td> 0.004</td> <td> 3960.789</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>           <td> 1.152e+04</td> <td> 4034.562</td> <td>    2.856</td> <td> 0.004</td> <td> 3615.039</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>           <td> 1.133e+04</td> <td> 3917.795</td> <td>    2.892</td> <td> 0.004</td> <td> 3649.874</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>           <td> 3608.1835</td> <td> 3097.460</td> <td>    1.165</td> <td> 0.244</td> <td>-2463.310</td> <td> 9679.677</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>           <td> 1.356e+04</td> <td> 4297.825</td> <td>    3.156</td> <td> 0.002</td> <td> 5140.132</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>           <td>-2.837e+04</td> <td> 3149.020</td> <td>   -9.010</td> <td> 0.000</td> <td>-3.45e+04</td> <td>-2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>           <td> 1.289e+04</td> <td> 4328.275</td> <td>    2.978</td> <td> 0.003</td> <td> 4407.076</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>           <td> 7881.4811</td> <td> 4801.503</td> <td>    1.641</td> <td> 0.101</td> <td>-1530.197</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>           <td> 1.195e+04</td> <td> 4008.021</td> <td>    2.981</td> <td> 0.003</td> <td> 4089.735</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>           <td> 1.123e+04</td> <td> 4100.696</td> <td>    2.739</td> <td> 0.006</td> <td> 3192.150</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>           <td> 1.086e+04</td> <td> 4109.937</td> <td>    2.641</td> <td> 0.008</td> <td> 2798.903</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>           <td> 9713.0214</td> <td> 3363.224</td> <td>    2.888</td> <td> 0.004</td> <td> 3120.589</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>           <td> 1.116e+04</td> <td> 3785.991</td> <td>    2.948</td> <td> 0.003</td> <td> 3740.005</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>           <td>-2.048e+04</td> <td> 3302.986</td> <td>   -6.200</td> <td> 0.000</td> <td> -2.7e+04</td> <td> -1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>           <td> 1.323e+04</td> <td> 3467.510</td> <td>    3.814</td> <td> 0.000</td> <td> 6429.968</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>           <td> 5588.3979</td> <td> 3992.481</td> <td>    1.400</td> <td> 0.162</td> <td>-2237.473</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>           <td> 8895.7752</td> <td> 3610.913</td> <td>    2.464</td> <td> 0.014</td> <td> 1817.835</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>           <td> 6280.2646</td> <td> 3454.673</td> <td>    1.818</td> <td> 0.069</td> <td> -491.421</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>           <td> 2.856e+04</td> <td> 3084.708</td> <td>    9.258</td> <td> 0.000</td> <td> 2.25e+04</td> <td> 3.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>           <td> 8870.7729</td> <td> 3711.642</td> <td>    2.390</td> <td> 0.017</td> <td> 1595.389</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>           <td> 3860.7482</td> <td> 4533.466</td> <td>    0.852</td> <td> 0.394</td> <td>-5025.536</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>           <td>  981.9694</td> <td> 3092.145</td> <td>    0.318</td> <td> 0.751</td> <td>-5079.106</td> <td> 7043.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>           <td> 1.112e+04</td> <td> 3962.475</td> <td>    2.806</td> <td> 0.005</td> <td> 3350.540</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>           <td> 1.228e+04</td> <td> 4576.202</td> <td>    2.684</td> <td> 0.007</td> <td> 3311.930</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>           <td>-2.641e+04</td> <td> 4513.111</td> <td>   -5.851</td> <td> 0.000</td> <td>-3.53e+04</td> <td>-1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>           <td> 1.134e+04</td> <td> 3843.392</td> <td>    2.950</td> <td> 0.003</td> <td> 3805.519</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>           <td>-1.015e+04</td> <td> 3315.955</td> <td>   -3.062</td> <td> 0.002</td> <td>-1.67e+04</td> <td>-3654.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>          <td>-6784.4217</td> <td> 6312.243</td> <td>   -1.075</td> <td> 0.282</td> <td>-1.92e+04</td> <td> 5588.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>           <td> 1.162e+04</td> <td> 4377.402</td> <td>    2.655</td> <td> 0.008</td> <td> 3043.600</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>           <td> 1.069e+04</td> <td> 4037.788</td> <td>    2.646</td> <td> 0.008</td> <td> 2770.560</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>           <td> 1028.7449</td> <td> 3549.960</td> <td>    0.290</td> <td> 0.772</td> <td>-5929.718</td> <td> 7987.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>           <td> 1854.4705</td> <td> 3067.203</td> <td>    0.605</td> <td> 0.545</td> <td>-4157.716</td> <td> 7866.657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>           <td> 5567.8581</td> <td> 3841.809</td> <td>    1.449</td> <td> 0.147</td> <td>-1962.673</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>          <td>-3961.2146</td> <td> 4323.316</td> <td>   -0.916</td> <td> 0.360</td> <td>-1.24e+04</td> <td> 4513.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>           <td> 7855.3039</td> <td> 3597.807</td> <td>    2.183</td> <td> 0.029</td> <td>  803.053</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>           <td> 1.098e+04</td> <td> 3902.421</td> <td>    2.814</td> <td> 0.005</td> <td> 3333.242</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>          <td> 1.015e+04</td> <td> 5314.163</td> <td>    1.909</td> <td> 0.056</td> <td> -270.110</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>           <td> 1.081e+04</td> <td> 4080.983</td> <td>    2.648</td> <td> 0.008</td> <td> 2807.745</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>           <td> 1.179e+04</td> <td> 3902.095</td> <td>    3.020</td> <td> 0.003</td> <td> 4136.961</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>           <td> 1.181e+04</td> <td> 4533.594</td> <td>    2.605</td> <td> 0.009</td> <td> 2922.193</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>          <td> 1.167e+04</td> <td> 5095.014</td> <td>    2.290</td> <td> 0.022</td> <td> 1679.299</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>           <td> 4520.9981</td> <td> 4901.812</td> <td>    0.922</td> <td> 0.356</td> <td>-5087.301</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>          <td>-2.423e+04</td> <td> 5073.691</td> <td>   -4.775</td> <td> 0.000</td> <td>-3.42e+04</td> <td>-1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>           <td> 7497.8711</td> <td> 3584.960</td> <td>    2.091</td> <td> 0.037</td> <td>  470.802</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>           <td> 4676.5986</td> <td> 3503.161</td> <td>    1.335</td> <td> 0.182</td> <td>-2190.131</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>           <td> 1.031e+04</td> <td> 3585.786</td> <td>    2.875</td> <td> 0.004</td> <td> 3281.797</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>           <td> 1.153e+04</td> <td> 3858.893</td> <td>    2.988</td> <td> 0.003</td> <td> 3967.438</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>           <td>-1.112e+04</td> <td> 4115.238</td> <td>   -2.702</td> <td> 0.007</td> <td>-1.92e+04</td> <td>-3053.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>           <td> 1.123e+04</td> <td> 5130.808</td> <td>    2.189</td> <td> 0.029</td> <td> 1173.829</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>           <td> 1.156e+04</td> <td> 4309.943</td> <td>    2.682</td> <td> 0.007</td> <td> 3111.577</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>           <td> 5472.4704</td> <td> 3173.544</td> <td>    1.724</td> <td> 0.085</td> <td> -748.160</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>           <td> 1.062e+04</td> <td> 3838.915</td> <td>    2.766</td> <td> 0.006</td> <td> 3095.272</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>           <td> 1.211e+04</td> <td> 4304.418</td> <td>    2.814</td> <td> 0.005</td> <td> 3676.182</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>           <td> 1.628e+04</td> <td> 4109.560</td> <td>    3.962</td> <td> 0.000</td> <td> 8228.526</td> <td> 2.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>           <td> 1.164e+04</td> <td> 4260.650</td> <td>    2.732</td> <td> 0.006</td> <td> 3290.644</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>           <td>   81.8377</td> <td> 6900.673</td> <td>    0.012</td> <td> 0.991</td> <td>-1.34e+04</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>           <td> 1.186e+04</td> <td> 3968.374</td> <td>    2.988</td> <td> 0.003</td> <td> 4078.322</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>           <td> 6876.8959</td> <td> 3906.829</td> <td>    1.760</td> <td> 0.078</td> <td> -781.085</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>           <td> 1.278e+04</td> <td> 4471.768</td> <td>    2.858</td> <td> 0.004</td> <td> 4013.282</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>           <td> 4624.2742</td> <td> 3933.435</td> <td>    1.176</td> <td> 0.240</td> <td>-3085.858</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>           <td> 1.152e+04</td> <td> 4145.390</td> <td>    2.778</td> <td> 0.005</td> <td> 3390.834</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>           <td> 1.124e+04</td> <td> 4483.646</td> <td>    2.506</td> <td> 0.012</td> <td> 2447.421</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>           <td> 1.179e+04</td> <td> 3373.988</td> <td>    3.494</td> <td> 0.000</td> <td> 5173.680</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>           <td> 8504.2651</td> <td> 3427.252</td> <td>    2.481</td> <td> 0.013</td> <td> 1786.329</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>           <td> 1206.9563</td> <td> 5114.543</td> <td>    0.236</td> <td> 0.813</td> <td>-8818.328</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>           <td> 8538.3757</td> <td> 8012.486</td> <td>    1.066</td> <td> 0.287</td> <td>-7167.319</td> <td> 2.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>           <td> 1.061e+04</td> <td> 3733.791</td> <td>    2.840</td> <td> 0.005</td> <td> 3286.955</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>           <td> 1.237e+04</td> <td> 4453.015</td> <td>    2.777</td> <td> 0.005</td> <td> 3637.275</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>           <td> 1.003e+04</td> <td> 4186.629</td> <td>    2.396</td> <td> 0.017</td> <td> 1824.608</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>           <td> 2584.8064</td> <td> 3530.625</td> <td>    0.732</td> <td> 0.464</td> <td>-4335.757</td> <td> 9505.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>           <td> 7172.5703</td> <td> 3538.230</td> <td>    2.027</td> <td> 0.043</td> <td>  237.099</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>           <td> 1.209e+04</td> <td> 4213.207</td> <td>    2.869</td> <td> 0.004</td> <td> 3827.962</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>           <td> 1.163e+04</td> <td> 4219.948</td> <td>    2.756</td> <td> 0.006</td> <td> 3358.381</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>           <td>  1.16e+04</td> <td> 4153.088</td> <td>    2.793</td> <td> 0.005</td> <td> 3459.857</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>           <td> 1.155e+04</td> <td> 4402.463</td> <td>    2.623</td> <td> 0.009</td> <td> 2916.465</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>           <td> 7598.5452</td> <td> 3657.372</td> <td>    2.078</td> <td> 0.038</td> <td>  429.538</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>           <td> 1.145e+04</td> <td> 3969.600</td> <td>    2.883</td> <td> 0.004</td> <td> 3665.132</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>           <td> 1.116e+04</td> <td> 3858.424</td> <td>    2.893</td> <td> 0.004</td> <td> 3598.092</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>           <td> 8286.5642</td> <td> 5855.445</td> <td>    1.415</td> <td> 0.157</td> <td>-3191.001</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>           <td> 1.269e+04</td> <td> 4327.620</td> <td>    2.933</td> <td> 0.003</td> <td> 4208.392</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>           <td> 5732.9408</td> <td> 3915.766</td> <td>    1.464</td> <td> 0.143</td> <td>-1942.558</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>           <td> 8855.9574</td> <td> 3487.401</td> <td>    2.539</td> <td> 0.011</td> <td> 2020.120</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>           <td> 1.236e+04</td> <td> 5705.178</td> <td>    2.167</td> <td> 0.030</td> <td> 1180.548</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>           <td> 1.229e+04</td> <td> 4185.573</td> <td>    2.936</td> <td> 0.003</td> <td> 4086.024</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>           <td> 1.155e+04</td> <td> 4384.406</td> <td>    2.635</td> <td> 0.008</td> <td> 2957.935</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>           <td>-5696.3086</td> <td> 3216.442</td> <td>   -1.771</td> <td> 0.077</td> <td> -1.2e+04</td> <td>  608.408</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>           <td> 1.353e+04</td> <td> 4604.794</td> <td>    2.938</td> <td> 0.003</td> <td> 4500.806</td> <td> 2.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>           <td>-1.616e+04</td> <td> 3861.019</td> <td>   -4.185</td> <td> 0.000</td> <td>-2.37e+04</td> <td>-8591.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>           <td> 1.235e+04</td> <td> 4643.503</td> <td>    2.661</td> <td> 0.008</td> <td> 3252.897</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>           <td> 1.176e+04</td> <td> 4292.725</td> <td>    2.740</td> <td> 0.006</td> <td> 3347.536</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>           <td> 1.108e+04</td> <td> 3990.375</td> <td>    2.777</td> <td> 0.005</td> <td> 3260.672</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>           <td> 9687.5740</td> <td> 4183.001</td> <td>    2.316</td> <td> 0.021</td> <td> 1488.254</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>           <td> 9083.8144</td> <td> 3443.693</td> <td>    2.638</td> <td> 0.008</td> <td> 2333.650</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>           <td>  1.05e+04</td> <td> 4530.801</td> <td>    2.317</td> <td> 0.021</td> <td> 1615.554</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>           <td> 7561.7359</td> <td> 4168.816</td> <td>    1.814</td> <td> 0.070</td> <td> -609.779</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>           <td> 9837.8240</td> <td> 3459.183</td> <td>    2.844</td> <td> 0.004</td> <td> 3057.298</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>           <td> 9832.6262</td> <td> 3451.803</td> <td>    2.849</td> <td> 0.004</td> <td> 3066.567</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>           <td> 9440.4454</td> <td> 4191.065</td> <td>    2.253</td> <td> 0.024</td> <td> 1225.319</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>           <td> 1472.8365</td> <td> 4667.886</td> <td>    0.316</td> <td> 0.752</td> <td>-7676.931</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>           <td> 1.662e+04</td> <td> 3626.584</td> <td>    4.583</td> <td> 0.000</td> <td> 9510.311</td> <td> 2.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>           <td>  1.37e+04</td> <td> 3754.828</td> <td>    3.648</td> <td> 0.000</td> <td> 6338.237</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>           <td> 1.053e+04</td> <td> 3678.034</td> <td>    2.863</td> <td> 0.004</td> <td> 3321.390</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>           <td> 1.229e+04</td> <td> 4485.757</td> <td>    2.740</td> <td> 0.006</td> <td> 3498.969</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>           <td> 1.292e+04</td> <td> 5085.161</td> <td>    2.540</td> <td> 0.011</td> <td> 2950.954</td> <td> 2.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>           <td> 1.058e+04</td> <td> 3994.613</td> <td>    2.649</td> <td> 0.008</td> <td> 2751.203</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>           <td> 8326.5385</td> <td> 4645.531</td> <td>    1.792</td> <td> 0.073</td> <td> -779.411</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>           <td> 1.098e+04</td> <td> 3887.701</td> <td>    2.825</td> <td> 0.005</td> <td> 3362.791</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>           <td> 1.186e+04</td> <td> 4092.418</td> <td>    2.898</td> <td> 0.004</td> <td> 3839.427</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>           <td> 1.495e+04</td> <td> 4159.258</td> <td>    3.594</td> <td> 0.000</td> <td> 6793.822</td> <td> 2.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>           <td> 8249.7034</td> <td> 3588.141</td> <td>    2.299</td> <td> 0.022</td> <td> 1216.400</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>           <td> 8941.8698</td> <td> 3436.226</td> <td>    2.602</td> <td> 0.009</td> <td> 2206.343</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>           <td> 1.903e+04</td> <td> 4269.428</td> <td>    4.457</td> <td> 0.000</td> <td> 1.07e+04</td> <td> 2.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>           <td> 9413.4580</td> <td> 5148.564</td> <td>    1.828</td> <td> 0.068</td> <td> -678.513</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>           <td> 7746.3050</td> <td> 4039.279</td> <td>    1.918</td> <td> 0.055</td> <td> -171.298</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>           <td> 2.697e+04</td> <td> 3994.030</td> <td>    6.753</td> <td> 0.000</td> <td> 1.91e+04</td> <td> 3.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>           <td> 1.122e+04</td> <td> 3806.476</td> <td>    2.948</td> <td> 0.003</td> <td> 3759.564</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>           <td> 1.128e+04</td> <td> 4500.943</td> <td>    2.506</td> <td> 0.012</td> <td> 2456.332</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>           <td> -603.4573</td> <td> 5189.866</td> <td>   -0.116</td> <td> 0.907</td> <td>-1.08e+04</td> <td> 9569.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>           <td> 1.212e+04</td> <td> 4816.206</td> <td>    2.517</td> <td> 0.012</td> <td> 2679.504</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>           <td> 9727.9780</td> <td> 3461.883</td> <td>    2.810</td> <td> 0.005</td> <td> 2942.159</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>           <td> 3150.4995</td> <td> 3700.356</td> <td>    0.851</td> <td> 0.395</td> <td>-4102.762</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>           <td> 2872.0454</td> <td> 3854.823</td> <td>    0.745</td> <td> 0.456</td> <td>-4683.996</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>           <td> 1.167e+04</td> <td> 3948.587</td> <td>    2.955</td> <td> 0.003</td> <td> 3930.063</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>           <td> 1.069e+04</td> <td> 3643.036</td> <td>    2.934</td> <td> 0.003</td> <td> 3549.555</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>           <td> 7804.1713</td> <td> 3393.969</td> <td>    2.299</td> <td> 0.021</td> <td> 1151.475</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>           <td> 7570.2808</td> <td> 3556.477</td> <td>    2.129</td> <td> 0.033</td> <td>  599.044</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>           <td> 1.007e+04</td> <td> 4248.285</td> <td>    2.370</td> <td> 0.018</td> <td> 1741.721</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>           <td>   24.5930</td> <td> 4677.748</td> <td>    0.005</td> <td> 0.996</td> <td>-9144.506</td> <td> 9193.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>           <td> 1.109e+04</td> <td> 4972.750</td> <td>    2.230</td> <td> 0.026</td> <td> 1340.662</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>           <td> 1.183e+04</td> <td> 3608.116</td> <td>    3.278</td> <td> 0.001</td> <td> 4753.322</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>           <td> 1.125e+04</td> <td> 4253.619</td> <td>    2.644</td> <td> 0.008</td> <td> 2907.920</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>           <td> 9586.2867</td> <td> 3477.660</td> <td>    2.757</td> <td> 0.006</td> <td> 2769.543</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>           <td>   1.1e+04</td> <td> 4185.259</td> <td>    2.628</td> <td> 0.009</td> <td> 2793.298</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>           <td> 9412.1893</td> <td> 3497.452</td> <td>    2.691</td> <td> 0.007</td> <td> 2556.650</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>           <td>-1.845e+04</td> <td> 3426.054</td> <td>   -5.384</td> <td> 0.000</td> <td>-2.52e+04</td> <td>-1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>           <td>  1.07e+04</td> <td> 4003.464</td> <td>    2.673</td> <td> 0.008</td> <td> 2853.478</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>           <td> 7495.0260</td> <td> 4650.077</td> <td>    1.612</td> <td> 0.107</td> <td>-1619.835</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>           <td> 1.115e+04</td> <td> 4114.374</td> <td>    2.710</td> <td> 0.007</td> <td> 3084.339</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>           <td> 1.143e+04</td> <td> 4154.010</td> <td>    2.751</td> <td> 0.006</td> <td> 3285.792</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>           <td> 8500.3762</td> <td> 3369.414</td> <td>    2.523</td> <td> 0.012</td> <td> 1895.811</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>           <td> 1.233e+04</td> <td> 4279.500</td> <td>    2.882</td> <td> 0.004</td> <td> 3945.996</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>           <td> 1.085e+04</td> <td> 3860.880</td> <td>    2.809</td> <td> 0.005</td> <td> 3279.123</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>           <td> 6148.3863</td> <td> 3351.672</td> <td>    1.834</td> <td> 0.067</td> <td> -421.401</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>           <td> 1.157e+04</td> <td> 3823.181</td> <td>    3.027</td> <td> 0.002</td> <td> 4078.412</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>           <td>-2312.9745</td> <td> 3060.935</td> <td>   -0.756</td> <td> 0.450</td> <td>-8312.873</td> <td> 3686.924</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>           <td> 1.097e+04</td> <td> 3933.414</td> <td>    2.790</td> <td> 0.005</td> <td> 3264.037</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>           <td> 6918.6798</td> <td> 3337.848</td> <td>    2.073</td> <td> 0.038</td> <td>  375.988</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>           <td> 1.213e+04</td> <td> 4070.265</td> <td>    2.980</td> <td> 0.003</td> <td> 4149.874</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>           <td>  356.4399</td> <td> 4267.413</td> <td>    0.084</td> <td> 0.933</td> <td>-8008.341</td> <td> 8721.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>           <td> 9144.6351</td> <td> 3447.627</td> <td>    2.652</td> <td> 0.008</td> <td> 2386.760</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>           <td> 8358.5703</td> <td> 3491.318</td> <td>    2.394</td> <td> 0.017</td> <td> 1515.054</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>           <td> 1.463e+04</td> <td> 4333.577</td> <td>    3.376</td> <td> 0.001</td> <td> 6135.454</td> <td> 2.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>           <td> 1.169e+04</td> <td> 3733.249</td> <td>    3.133</td> <td> 0.002</td> <td> 4376.832</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>           <td> 1.057e+04</td> <td> 4161.944</td> <td>    2.540</td> <td> 0.011</td> <td> 2415.236</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>           <td> 8646.6988</td> <td> 3615.011</td> <td>    2.392</td> <td> 0.017</td> <td> 1560.727</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>           <td> 9733.8663</td> <td> 3700.483</td> <td>    2.630</td> <td> 0.009</td> <td> 2480.356</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>           <td>-1.135e+04</td> <td> 3160.766</td> <td>   -3.590</td> <td> 0.000</td> <td>-1.75e+04</td> <td>-5150.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>           <td> 9609.8260</td> <td> 4009.205</td> <td>    2.397</td> <td> 0.017</td> <td> 1751.173</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>           <td> 1.263e+04</td> <td> 4422.064</td> <td>    2.856</td> <td> 0.004</td> <td> 3962.928</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>           <td>  670.6900</td> <td> 4233.990</td> <td>    0.158</td> <td> 0.874</td> <td>-7628.576</td> <td> 8969.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>           <td> 1.106e+04</td> <td> 4258.783</td> <td>    2.597</td> <td> 0.009</td> <td> 2710.084</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>           <td> 9460.3233</td> <td> 3665.001</td> <td>    2.581</td> <td> 0.010</td> <td> 2276.363</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>           <td>-4512.5573</td> <td> 3672.896</td> <td>   -1.229</td> <td> 0.219</td> <td>-1.17e+04</td> <td> 2686.878</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>           <td>-3935.6363</td> <td> 4263.718</td> <td>   -0.923</td> <td> 0.356</td> <td>-1.23e+04</td> <td> 4421.901</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>           <td> 1.144e+04</td> <td> 3816.602</td> <td>    2.996</td> <td> 0.003</td> <td> 3954.389</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>           <td> 1.172e+04</td> <td> 4341.522</td> <td>    2.699</td> <td> 0.007</td> <td> 3209.639</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>           <td> 1.179e+04</td> <td> 3980.577</td> <td>    2.963</td> <td> 0.003</td> <td> 3990.859</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>           <td> 7885.7920</td> <td> 3619.315</td> <td>    2.179</td> <td> 0.029</td> <td>  791.382</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>           <td> 1.221e+04</td> <td> 4197.973</td> <td>    2.907</td> <td> 0.004</td> <td> 3976.711</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>           <td> 9725.3249</td> <td> 3553.388</td> <td>    2.737</td> <td> 0.006</td> <td> 2760.142</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>           <td> 4675.4601</td> <td> 5297.052</td> <td>    0.883</td> <td> 0.377</td> <td>-5707.569</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>           <td> 9134.0857</td> <td> 3514.455</td> <td>    2.599</td> <td> 0.009</td> <td> 2245.219</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>           <td> 8004.1408</td> <td> 3509.355</td> <td>    2.281</td> <td> 0.023</td> <td> 1125.269</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>           <td> 5917.2501</td> <td> 3535.404</td> <td>    1.674</td> <td> 0.094</td> <td>-1012.681</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>           <td> 1.234e+04</td> <td> 4456.469</td> <td>    2.768</td> <td> 0.006</td> <td> 3600.042</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>           <td> 7192.2790</td> <td> 3412.823</td> <td>    2.107</td> <td> 0.035</td> <td>  502.625</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>           <td>-3836.5289</td> <td> 3593.583</td> <td>   -1.068</td> <td> 0.286</td> <td>-1.09e+04</td> <td> 3207.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>           <td> 8767.0517</td> <td> 3867.407</td> <td>    2.267</td> <td> 0.023</td> <td> 1186.344</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>           <td> 9972.3254</td> <td> 3490.060</td> <td>    2.857</td> <td> 0.004</td> <td> 3131.276</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>           <td> 1.195e+04</td> <td> 3888.289</td> <td>    3.073</td> <td> 0.002</td> <td> 4327.833</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>           <td> 1.116e+04</td> <td> 4621.997</td> <td>    2.415</td> <td> 0.016</td> <td> 2100.172</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>           <td> 1.145e+04</td> <td> 3858.042</td> <td>    2.968</td> <td> 0.003</td> <td> 3889.367</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>           <td> 1.096e+04</td> <td> 3735.570</td> <td>    2.933</td> <td> 0.003</td> <td> 3633.157</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>           <td> 9127.7852</td> <td> 3485.587</td> <td>    2.619</td> <td> 0.009</td> <td> 2295.503</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>           <td> -871.8342</td> <td> 3930.179</td> <td>   -0.222</td> <td> 0.824</td> <td>-8575.584</td> <td> 6831.916</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>           <td> 1.298e+04</td> <td> 4816.729</td> <td>    2.694</td> <td> 0.007</td> <td> 3536.325</td> <td> 2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>           <td>  1.09e+04</td> <td> 3994.968</td> <td>    2.727</td> <td> 0.006</td> <td> 3064.933</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>           <td> 1.081e+04</td> <td> 4473.836</td> <td>    2.416</td> <td> 0.016</td> <td> 2039.046</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>           <td> 1.763e+04</td> <td> 4105.796</td> <td>    4.295</td> <td> 0.000</td> <td> 9585.083</td> <td> 2.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>           <td> 7532.6032</td> <td> 3354.005</td> <td>    2.246</td> <td> 0.025</td> <td>  958.242</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>           <td> 3.172e+04</td> <td> 4342.018</td> <td>    7.305</td> <td> 0.000</td> <td> 2.32e+04</td> <td> 4.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>           <td>  627.9007</td> <td> 3401.090</td> <td>    0.185</td> <td> 0.854</td> <td>-6038.754</td> <td> 7294.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>           <td> 1.199e+04</td> <td> 4342.729</td> <td>    2.761</td> <td> 0.006</td> <td> 3477.589</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>           <td> 7047.3480</td> <td> 4411.585</td> <td>    1.597</td> <td> 0.110</td> <td>-1600.032</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>           <td>  166.0404</td> <td> 5267.575</td> <td>    0.032</td> <td> 0.975</td> <td>-1.02e+04</td> <td> 1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>           <td> 1.223e+04</td> <td> 3676.982</td> <td>    3.327</td> <td> 0.001</td> <td> 5024.870</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>           <td> 1.211e+04</td> <td> 4436.509</td> <td>    2.730</td> <td> 0.006</td> <td> 3415.173</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>           <td> 1.082e+04</td> <td> 3758.601</td> <td>    2.878</td> <td> 0.004</td> <td> 3448.081</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>           <td> 4476.0239</td> <td> 3604.525</td> <td>    1.242</td> <td> 0.214</td> <td>-2589.395</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>           <td> 1.141e+04</td> <td> 5298.339</td> <td>    2.153</td> <td> 0.031</td> <td> 1023.306</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>           <td> 6118.3062</td> <td> 3257.302</td> <td>    1.878</td> <td> 0.060</td> <td> -266.503</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>           <td> 8405.6270</td> <td> 3595.904</td> <td>    2.338</td> <td> 0.019</td> <td> 1357.107</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>           <td> 8839.3495</td> <td> 3489.005</td> <td>    2.533</td> <td> 0.011</td> <td> 2000.368</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>           <td> 1.155e+04</td> <td> 3921.233</td> <td>    2.945</td> <td> 0.003</td> <td> 3861.315</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>           <td>  1.22e+04</td> <td> 4182.274</td> <td>    2.918</td> <td> 0.004</td> <td> 4006.968</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>           <td> 6244.1433</td> <td> 4396.099</td> <td>    1.420</td> <td> 0.156</td> <td>-2372.881</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>           <td> 1.061e+04</td> <td> 3657.755</td> <td>    2.902</td> <td> 0.004</td> <td> 3443.457</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>           <td> 1.286e+04</td> <td> 4613.574</td> <td>    2.787</td> <td> 0.005</td> <td> 3813.149</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>           <td> 1.138e+04</td> <td> 3885.598</td> <td>    2.930</td> <td> 0.003</td> <td> 3766.556</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>           <td> 1.174e+04</td> <td> 3947.181</td> <td>    2.974</td> <td> 0.003</td> <td> 4003.634</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>           <td> 1.188e+04</td> <td> 4431.693</td> <td>    2.680</td> <td> 0.007</td> <td> 3191.641</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>           <td> 4059.6058</td> <td> 3642.118</td> <td>    1.115</td> <td> 0.265</td> <td>-3079.500</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>           <td> 9861.0293</td> <td> 3659.118</td> <td>    2.695</td> <td> 0.007</td> <td> 2688.601</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>           <td> 8144.0343</td> <td> 3788.534</td> <td>    2.150</td> <td> 0.032</td> <td>  717.930</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>           <td>-1.264e+04</td> <td> 3291.484</td> <td>   -3.841</td> <td> 0.000</td> <td>-1.91e+04</td> <td>-6190.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>           <td> -924.1857</td> <td> 3807.903</td> <td>   -0.243</td> <td> 0.808</td> <td>-8388.256</td> <td> 6539.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>           <td> 1.229e+04</td> <td> 4462.181</td> <td>    2.754</td> <td> 0.006</td> <td> 3542.360</td> <td>  2.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>           <td> 8958.7502</td> <td> 3536.521</td> <td>    2.533</td> <td> 0.011</td> <td> 2026.629</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>           <td> 8273.3416</td> <td> 3313.001</td> <td>    2.497</td> <td> 0.013</td> <td> 1779.354</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>           <td> 1755.5094</td> <td> 3880.492</td> <td>    0.452</td> <td> 0.651</td> <td>-5850.847</td> <td> 9361.866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>           <td> 8934.2338</td> <td> 3656.665</td> <td>    2.443</td> <td> 0.015</td> <td> 1766.613</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>           <td> 6692.6708</td> <td> 3511.189</td> <td>    1.906</td> <td> 0.057</td> <td> -189.795</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>           <td> 6257.3804</td> <td> 3488.998</td> <td>    1.793</td> <td> 0.073</td> <td> -581.587</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>           <td> 1.125e+04</td> <td> 5511.232</td> <td>    2.042</td> <td> 0.041</td> <td>  451.850</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>           <td> 1.073e+04</td> <td> 3920.386</td> <td>    2.737</td> <td> 0.006</td> <td> 3047.326</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>           <td> 1.213e+04</td> <td> 4299.920</td> <td>    2.821</td> <td> 0.005</td> <td> 3702.923</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>           <td> 1.049e+04</td> <td> 3674.660</td> <td>    2.853</td> <td> 0.004</td> <td> 3282.112</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>           <td> 7285.4719</td> <td> 4336.542</td> <td>    1.680</td> <td> 0.093</td> <td>-1214.812</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>           <td> 4558.0369</td> <td> 3153.822</td> <td>    1.445</td> <td> 0.148</td> <td>-1623.935</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>           <td> 1093.0247</td> <td> 3038.271</td> <td>    0.360</td> <td> 0.719</td> <td>-4862.450</td> <td> 7048.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>           <td> 1.198e+04</td> <td> 3909.157</td> <td>    3.065</td> <td> 0.002</td> <td> 4318.137</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>           <td> 1.177e+04</td> <td> 4751.892</td> <td>    2.478</td> <td> 0.013</td> <td> 2459.741</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>           <td> 1.289e+04</td> <td> 4560.427</td> <td>    2.826</td> <td> 0.005</td> <td> 3947.042</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>           <td> 1.204e+04</td> <td> 4117.970</td> <td>    2.925</td> <td> 0.003</td> <td> 3972.425</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>           <td> 7289.7440</td> <td> 3592.474</td> <td>    2.029</td> <td> 0.042</td> <td>  247.946</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>           <td> 5144.3498</td> <td> 4655.985</td> <td>    1.105</td> <td> 0.269</td> <td>-3982.090</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>           <td> 6920.9204</td> <td> 4515.774</td> <td>    1.533</td> <td> 0.125</td> <td>-1930.685</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>           <td> 5391.0747</td> <td> 3511.619</td> <td>    1.535</td> <td> 0.125</td> <td>-1492.235</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>           <td> 1.063e+04</td> <td> 3739.240</td> <td>    2.843</td> <td> 0.004</td> <td> 3299.461</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>           <td> 1.023e+04</td> <td> 3525.223</td> <td>    2.901</td> <td> 0.004</td> <td> 3315.462</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>           <td>-1.671e+04</td> <td> 5692.134</td> <td>   -2.935</td> <td> 0.003</td> <td>-2.79e+04</td> <td>-5551.767</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>           <td>  1.14e+04</td> <td> 4667.368</td> <td>    2.443</td> <td> 0.015</td> <td> 2254.314</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>           <td> 8963.9382</td> <td> 3801.630</td> <td>    2.358</td> <td> 0.018</td> <td> 1512.163</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>           <td> 1832.5262</td> <td> 4005.949</td> <td>    0.457</td> <td> 0.647</td> <td>-6019.744</td> <td> 9684.796</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>           <td>  1.46e+04</td> <td> 3435.437</td> <td>    4.250</td> <td> 0.000</td> <td> 7865.971</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>           <td> 5007.6051</td> <td> 3235.355</td> <td>    1.548</td> <td> 0.122</td> <td>-1334.184</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>           <td>-2008.3669</td> <td> 3843.983</td> <td>   -0.522</td> <td> 0.601</td> <td>-9543.159</td> <td> 5526.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>           <td>-1.456e+04</td> <td> 4842.914</td> <td>   -3.006</td> <td> 0.003</td> <td> -2.4e+04</td> <td>-5063.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>           <td> 7563.3751</td> <td> 4295.291</td> <td>    1.761</td> <td> 0.078</td> <td> -856.049</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>           <td> 4212.3711</td> <td> 3955.432</td> <td>    1.065</td> <td> 0.287</td> <td>-3540.878</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>           <td> 3534.0283</td> <td> 3706.502</td> <td>    0.953</td> <td> 0.340</td> <td>-3731.282</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>           <td> 3671.4096</td> <td> 4352.900</td> <td>    0.843</td> <td> 0.399</td> <td>-4860.938</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>           <td> 1.156e+04</td> <td> 4595.291</td> <td>    2.516</td> <td> 0.012</td> <td> 2553.723</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>           <td> 9380.7817</td> <td> 3571.246</td> <td>    2.627</td> <td> 0.009</td> <td> 2380.594</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>           <td> 8642.2789</td> <td> 3612.505</td> <td>    2.392</td> <td> 0.017</td> <td> 1561.219</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>           <td>-6028.2181</td> <td> 5293.176</td> <td>   -1.139</td> <td> 0.255</td> <td>-1.64e+04</td> <td> 4347.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>           <td> 8520.3587</td> <td> 3445.313</td> <td>    2.473</td> <td> 0.013</td> <td> 1767.021</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>           <td> 9705.0529</td> <td> 4128.143</td> <td>    2.351</td> <td> 0.019</td> <td> 1613.262</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>           <td> 1.144e+04</td> <td> 3844.009</td> <td>    2.976</td> <td> 0.003</td> <td> 3906.063</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>           <td> 1626.6505</td> <td> 3065.684</td> <td>    0.531</td> <td> 0.596</td> <td>-4382.558</td> <td> 7635.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>           <td>  126.1225</td> <td> 3994.111</td> <td>    0.032</td> <td> 0.975</td> <td>-7702.944</td> <td> 7955.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>           <td> 1.153e+04</td> <td> 3830.821</td> <td>    3.010</td> <td> 0.003</td> <td> 4021.458</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>           <td> 3292.4690</td> <td> 3034.847</td> <td>    1.085</td> <td> 0.278</td> <td>-2656.293</td> <td> 9241.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>           <td> 1.173e+04</td> <td> 4282.121</td> <td>    2.740</td> <td> 0.006</td> <td> 3338.911</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>           <td> 9182.4470</td> <td> 3390.618</td> <td>    2.708</td> <td> 0.007</td> <td> 2536.319</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>           <td> 2332.9851</td> <td> 3127.518</td> <td>    0.746</td> <td> 0.456</td> <td>-3797.427</td> <td> 8463.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>           <td> 1.134e+04</td> <td> 3835.010</td> <td>    2.958</td> <td> 0.003</td> <td> 3827.470</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>           <td>-3.454e+04</td> <td> 3515.963</td> <td>   -9.824</td> <td> 0.000</td> <td>-4.14e+04</td> <td>-2.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>           <td> 1.263e+04</td> <td> 4527.010</td> <td>    2.790</td> <td> 0.005</td> <td> 3758.533</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>           <td> 2817.7170</td> <td> 3438.711</td> <td>    0.819</td> <td> 0.413</td> <td>-3922.682</td> <td> 9558.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>           <td> 9074.8491</td> <td> 4310.015</td> <td>    2.106</td> <td> 0.035</td> <td>  626.563</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>           <td> 9738.4652</td> <td> 3612.823</td> <td>    2.696</td> <td> 0.007</td> <td> 2656.781</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>           <td>  1.23e+04</td> <td> 4529.727</td> <td>    2.715</td> <td> 0.007</td> <td> 3421.174</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1981</th> <td>   -0.0481</td> <td>    0.071</td> <td>   -0.674</td> <td> 0.500</td> <td>   -0.188</td> <td>    0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1982</th> <td>   -0.0110</td> <td>    0.071</td> <td>   -0.155</td> <td> 0.877</td> <td>   -0.149</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1983</th> <td>   -0.0216</td> <td>    0.070</td> <td>   -0.310</td> <td> 0.757</td> <td>   -0.158</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1984</th> <td>   -0.0677</td> <td>    0.070</td> <td>   -0.970</td> <td> 0.332</td> <td>   -0.204</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1985</th> <td>   -0.1016</td> <td>    0.070</td> <td>   -1.445</td> <td> 0.148</td> <td>   -0.239</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1986</th> <td>   -0.1426</td> <td>    0.071</td> <td>   -2.003</td> <td> 0.045</td> <td>   -0.282</td> <td>   -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1987</th> <td>   -0.1461</td> <td>    0.072</td> <td>   -2.018</td> <td> 0.044</td> <td>   -0.288</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1988</th> <td>   -0.1734</td> <td>    0.073</td> <td>   -2.364</td> <td> 0.018</td> <td>   -0.317</td> <td>   -0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1989</th> <td>   -0.1766</td> <td>    0.075</td> <td>   -2.370</td> <td> 0.018</td> <td>   -0.323</td> <td>   -0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1990</th> <td>   -0.2093</td> <td>    0.076</td> <td>   -2.770</td> <td> 0.006</td> <td>   -0.357</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1991</th> <td>   -0.1902</td> <td>    0.077</td> <td>   -2.483</td> <td> 0.013</td> <td>   -0.340</td> <td>   -0.040</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1992</th> <td>   -0.1770</td> <td>    0.078</td> <td>   -2.279</td> <td> 0.023</td> <td>   -0.329</td> <td>   -0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1993</th> <td>   -0.1474</td> <td>    0.079</td> <td>   -1.860</td> <td> 0.063</td> <td>   -0.303</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1994</th> <td>   -0.1520</td> <td>    0.081</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.310</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1995</th> <td>   -0.1499</td> <td>    0.083</td> <td>   -1.812</td> <td> 0.070</td> <td>   -0.312</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1996</th> <td>   -0.1144</td> <td>    0.085</td> <td>   -1.341</td> <td> 0.180</td> <td>   -0.281</td> <td>    0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1997</th> <td>   -0.1120</td> <td>    0.088</td> <td>   -1.272</td> <td> 0.203</td> <td>   -0.284</td> <td>    0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1998</th> <td>   -0.0932</td> <td>    0.091</td> <td>   -1.028</td> <td> 0.304</td> <td>   -0.271</td> <td>    0.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1999</th> <td>    0.0086</td> <td>    0.093</td> <td>    0.093</td> <td> 0.926</td> <td>   -0.173</td> <td>    0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX2000</th> <td>   -0.0903</td> <td>    0.095</td> <td>   -0.953</td> <td> 0.341</td> <td>   -0.276</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX2001</th> <td>   -0.1343</td> <td>    0.096</td> <td>   -1.398</td> <td> 0.162</td> <td>   -0.323</td> <td>    0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1981</th> <td>    0.0015</td> <td>    0.134</td> <td>    0.011</td> <td> 0.991</td> <td>   -0.260</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1982</th> <td>   -0.0311</td> <td>    0.132</td> <td>   -0.236</td> <td> 0.813</td> <td>   -0.289</td> <td>    0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1983</th> <td>   -0.0415</td> <td>    0.129</td> <td>   -0.320</td> <td> 0.749</td> <td>   -0.295</td> <td>    0.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1984</th> <td>   -0.0741</td> <td>    0.128</td> <td>   -0.577</td> <td> 0.564</td> <td>   -0.326</td> <td>    0.178</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1985</th> <td>   -0.0813</td> <td>    0.129</td> <td>   -0.632</td> <td> 0.527</td> <td>   -0.333</td> <td>    0.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1986</th> <td>   -0.1101</td> <td>    0.129</td> <td>   -0.854</td> <td> 0.393</td> <td>   -0.363</td> <td>    0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1987</th> <td>   -0.1255</td> <td>    0.130</td> <td>   -0.967</td> <td> 0.334</td> <td>   -0.380</td> <td>    0.129</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1988</th> <td>   -0.1413</td> <td>    0.131</td> <td>   -1.082</td> <td> 0.279</td> <td>   -0.397</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1989</th> <td>   -0.1366</td> <td>    0.131</td> <td>   -1.040</td> <td> 0.299</td> <td>   -0.394</td> <td>    0.121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1990</th> <td>   -0.1397</td> <td>    0.132</td> <td>   -1.055</td> <td> 0.292</td> <td>   -0.399</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1991</th> <td>   -0.1368</td> <td>    0.134</td> <td>   -1.022</td> <td> 0.307</td> <td>   -0.399</td> <td>    0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1992</th> <td>   -0.1956</td> <td>    0.135</td> <td>   -1.450</td> <td> 0.147</td> <td>   -0.460</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1993</th> <td>   -0.2200</td> <td>    0.137</td> <td>   -1.611</td> <td> 0.107</td> <td>   -0.488</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1994</th> <td>   -0.2246</td> <td>    0.138</td> <td>   -1.626</td> <td> 0.104</td> <td>   -0.495</td> <td>    0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1995</th> <td>   -0.1987</td> <td>    0.141</td> <td>   -1.412</td> <td> 0.158</td> <td>   -0.475</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1996</th> <td>   -0.2430</td> <td>    0.144</td> <td>   -1.692</td> <td> 0.091</td> <td>   -0.525</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1997</th> <td>   -0.2197</td> <td>    0.147</td> <td>   -1.493</td> <td> 0.135</td> <td>   -0.508</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1998</th> <td>   -0.1813</td> <td>    0.151</td> <td>   -1.203</td> <td> 0.229</td> <td>   -0.477</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1999</th> <td>   -0.1905</td> <td>    0.154</td> <td>   -1.238</td> <td> 0.216</td> <td>   -0.492</td> <td>    0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX2000</th> <td>   -0.0964</td> <td>    0.157</td> <td>   -0.615</td> <td> 0.539</td> <td>   -0.404</td> <td>    0.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX2001</th> <td>   -0.2797</td> <td>    0.159</td> <td>   -1.761</td> <td> 0.078</td> <td>   -0.591</td> <td>    0.032</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21533.010</td> <th>  Durbin-Watson:     </th>   <td>   0.543</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>48401709.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>10.015</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>296.914</td>  <th>  Cond. No.          </th>   <td>2.80e+07</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.8e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.669    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.648    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      31.93    \\\\\n",
       "\\textbf{Date:}             & Wed, 16 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     02:02:58     & \\textbf{  Log-Likelihood:    } & -1.4150e+05   \\\\\n",
       "\\textbf{No. Observations:} &       13385      & \\textbf{  AIC:               } &  2.846e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       12587      & \\textbf{  BIC:               } &  2.906e+05    \\\\\n",
       "\\textbf{Df Model:}         &         797      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &   -1.381e+04  &     4167.202     &    -3.315  &         0.001        &     -2.2e+04    &    -5645.690     \\\\\n",
       "\\textbf{gspilltecIV}      &       0.1986  &        0.145     &     1.368  &         0.171        &       -0.086    &        0.483     \\\\\n",
       "\\textbf{gspillsicIV}      &       0.6659  &        0.233     &     2.852  &         0.004        &        0.208    &        1.124     \\\\\n",
       "\\textbf{pat\\_count}       &     -27.0286  &        1.911     &   -14.141  &         0.000        &      -30.775    &      -23.282     \\\\\n",
       "\\textbf{rsales}           &       0.7667  &        0.037     &    20.685  &         0.000        &        0.694    &        0.839     \\\\\n",
       "\\textbf{rppent}           &       0.6251  &        0.084     &     7.408  &         0.000        &        0.460    &        0.790     \\\\\n",
       "\\textbf{emp}              &      15.4383  &        7.160     &     2.156  &         0.031        &        1.404    &       29.473     \\\\\n",
       "\\textbf{rxrd}             &      18.8001  &        0.613     &    30.654  &         0.000        &       17.598    &       20.002     \\\\\n",
       "\\textbf{1981}             &     109.6592  &     1080.584     &     0.101  &         0.919        &    -2008.450    &     2227.768     \\\\\n",
       "\\textbf{1982}             &    -233.2389  &     1074.271     &    -0.217  &         0.828        &    -2338.975    &     1872.497     \\\\\n",
       "\\textbf{1983}             &    -128.6822  &     1065.010     &    -0.121  &         0.904        &    -2216.264    &     1958.899     \\\\\n",
       "\\textbf{1984}             &      98.8470  &     1059.772     &     0.093  &         0.926        &    -1978.467    &     2176.161     \\\\\n",
       "\\textbf{1985}             &     500.7900  &     1057.845     &     0.473  &         0.636        &    -1572.747    &     2574.327     \\\\\n",
       "\\textbf{1986}             &    1025.5479  &     1047.722     &     0.979  &         0.328        &    -1028.146    &     3079.242     \\\\\n",
       "\\textbf{1987}             &     964.1585  &     1043.238     &     0.924  &         0.355        &    -1080.748    &     3009.065     \\\\\n",
       "\\textbf{1988}             &    1227.5616  &     1042.827     &     1.177  &         0.239        &     -816.538    &     3271.662     \\\\\n",
       "\\textbf{1989}             &    1409.3119  &     1037.820     &     1.358  &         0.175        &     -624.973    &     3443.597     \\\\\n",
       "\\textbf{1990}             &    1582.7353  &     1032.683     &     1.533  &         0.125        &     -441.480    &     3606.951     \\\\\n",
       "\\textbf{1991}             &    1616.4231  &     1032.392     &     1.566  &         0.117        &     -407.222    &     3640.068     \\\\\n",
       "\\textbf{1992}             &    1589.4573  &     1031.336     &     1.541  &         0.123        &     -432.118    &     3611.033     \\\\\n",
       "\\textbf{1993}             &    1205.1245  &     1027.138     &     1.173  &         0.241        &     -808.222    &     3218.471     \\\\\n",
       "\\textbf{1994}             &    1111.0146  &     1029.798     &     1.079  &         0.281        &     -907.547    &     3129.576     \\\\\n",
       "\\textbf{1995}             &    1413.9353  &     1028.108     &     1.375  &         0.169        &     -601.312    &     3429.183     \\\\\n",
       "\\textbf{1996}             &    1164.7069  &     1030.027     &     1.131  &         0.258        &     -854.302    &     3183.716     \\\\\n",
       "\\textbf{1997}             &    1327.6436  &     1034.933     &     1.283  &         0.200        &     -700.983    &     3356.271     \\\\\n",
       "\\textbf{1998}             &     626.1367  &     1038.418     &     0.603  &         0.547        &    -1409.322    &     2661.595     \\\\\n",
       "\\textbf{1999}             &   -1672.8117  &     1048.945     &    -1.595  &         0.111        &    -3728.904    &      383.281     \\\\\n",
       "\\textbf{2000}             &      14.3278  &     1070.432     &     0.013  &         0.989        &    -2083.883    &     2112.539     \\\\\n",
       "\\textbf{2001}             &    1048.8243  &     1114.791     &     0.941  &         0.347        &    -1136.337    &     3233.986     \\\\\n",
       "\\textbf{10005.0}          &    1.075e+04  &     3748.461     &     2.868  &         0.004        &     3402.447    &     1.81e+04     \\\\\n",
       "\\textbf{10006.0}          &     1.05e+04  &     3980.492     &     2.637  &         0.008        &     2694.217    &     1.83e+04     \\\\\n",
       "\\textbf{10008.0}          &    9625.6040  &     3498.425     &     2.751  &         0.006        &     2768.158    &     1.65e+04     \\\\\n",
       "\\textbf{10016.0}          &    1.086e+04  &     3609.282     &     3.009  &         0.003        &     3784.555    &     1.79e+04     \\\\\n",
       "\\textbf{10030.0}          &    1.211e+04  &     4151.615     &     2.916  &         0.004        &     3968.672    &     2.02e+04     \\\\\n",
       "\\textbf{1004.0}           &     1.17e+04  &     3919.766     &     2.984  &         0.003        &     4013.670    &     1.94e+04     \\\\\n",
       "\\textbf{10056.0}          &    9375.3438  &     3684.022     &     2.545  &         0.011        &     2154.098    &     1.66e+04     \\\\\n",
       "\\textbf{10085.0}          &    4448.8290  &     3357.715     &     1.325  &         0.185        &    -2132.805    &      1.1e+04     \\\\\n",
       "\\textbf{10092.0}          &    1.187e+04  &     5958.377     &     1.993  &         0.046        &      193.098    &     2.36e+04     \\\\\n",
       "\\textbf{10097.0}          &    3456.7486  &     3228.208     &     1.071  &         0.284        &    -2871.032    &     9784.529     \\\\\n",
       "\\textbf{1010.0}           &    1.067e+04  &     5738.714     &     1.860  &         0.063        &     -574.920    &     2.19e+04     \\\\\n",
       "\\textbf{10109.0}          &    1.305e+04  &     4548.789     &     2.868  &         0.004        &     4130.891    &      2.2e+04     \\\\\n",
       "\\textbf{10115.0}          &    8975.8676  &     3778.522     &     2.375  &         0.018        &     1569.388    &     1.64e+04     \\\\\n",
       "\\textbf{10124.0}          &    1.317e+04  &     4577.903     &     2.877  &         0.004        &     4198.916    &     2.21e+04     \\\\\n",
       "\\textbf{1013.0}           &    5844.5024  &     3184.674     &     1.835  &         0.067        &     -397.944    &     1.21e+04     \\\\\n",
       "\\textbf{10150.0}          &    4379.0875  &     3779.417     &     1.159  &         0.247        &    -3029.145    &     1.18e+04     \\\\\n",
       "\\textbf{10159.0}          &     973.0308  &     6088.181     &     0.160  &         0.873        &     -1.1e+04    &     1.29e+04     \\\\\n",
       "\\textbf{10174.0}          &    1.223e+04  &     4299.729     &     2.844  &         0.004        &     3800.706    &     2.07e+04     \\\\\n",
       "\\textbf{10185.0}          &    9342.5104  &     4437.384     &     2.105  &         0.035        &      644.562    &      1.8e+04     \\\\\n",
       "\\textbf{10195.0}          &    1185.3886  &     4591.466     &     0.258  &         0.796        &    -7814.585    &     1.02e+04     \\\\\n",
       "\\textbf{10198.0}          &     1.21e+04  &     4116.091     &     2.940  &         0.003        &     4034.264    &     2.02e+04     \\\\\n",
       "\\textbf{10215.0}          &    1.288e+04  &     4540.119     &     2.837  &         0.005        &     3982.728    &     2.18e+04     \\\\\n",
       "\\textbf{10232.0}          &    6637.9380  &     4756.976     &     1.395  &         0.163        &    -2686.461    &      1.6e+04     \\\\\n",
       "\\textbf{10236.0}          &    1.171e+04  &     4184.951     &     2.798  &         0.005        &     3507.701    &     1.99e+04     \\\\\n",
       "\\textbf{10286.0}          &    1.045e+04  &     3617.866     &     2.889  &         0.004        &     3359.886    &     1.75e+04     \\\\\n",
       "\\textbf{10301.0}          &   -1.587e+04  &     3086.762     &    -5.140  &         0.000        &    -2.19e+04    &    -9815.445     \\\\\n",
       "\\textbf{10312.0}          &    1.152e+04  &     3852.606     &     2.990  &         0.003        &     3967.037    &     1.91e+04     \\\\\n",
       "\\textbf{10332.0}          &    6845.6324  &     4814.039     &     1.422  &         0.155        &    -2590.617    &     1.63e+04     \\\\\n",
       "\\textbf{1036.0}           &    8479.1608  &     4222.035     &     2.008  &         0.045        &      203.328    &     1.68e+04     \\\\\n",
       "\\textbf{10374.0}          &    9888.7034  &     3720.204     &     2.658  &         0.008        &     2596.536    &     1.72e+04     \\\\\n",
       "\\textbf{10386.0}          &    5773.3219  &     3175.639     &     1.818  &         0.069        &     -451.414    &      1.2e+04     \\\\\n",
       "\\textbf{10391.0}          &    2387.3954  &     3277.528     &     0.728  &         0.466        &    -4037.059    &     8811.850     \\\\\n",
       "\\textbf{10407.0}          &    5287.9053  &     3527.954     &     1.499  &         0.134        &    -1627.422    &     1.22e+04     \\\\\n",
       "\\textbf{10420.0}          &    9770.2423  &     3967.970     &     2.462  &         0.014        &     1992.416    &     1.75e+04     \\\\\n",
       "\\textbf{10422.0}          &    8393.9708  &     3804.080     &     2.207  &         0.027        &      937.394    &     1.59e+04     \\\\\n",
       "\\textbf{10426.0}          &    1.138e+04  &     4573.690     &     2.488  &         0.013        &     2416.339    &     2.03e+04     \\\\\n",
       "\\textbf{10441.0}          &    1.227e+04  &     4355.586     &     2.818  &         0.005        &     3735.499    &     2.08e+04     \\\\\n",
       "\\textbf{1045.0}           &     961.9430  &     3789.693     &     0.254  &         0.800        &    -6466.434    &     8390.320     \\\\\n",
       "\\textbf{10453.0}          &    5689.5884  &     3192.668     &     1.782  &         0.075        &     -568.527    &     1.19e+04     \\\\\n",
       "\\textbf{10482.0}          &   -1.372e+04  &     3822.485     &    -3.589  &         0.000        &    -2.12e+04    &    -6228.047     \\\\\n",
       "\\textbf{10498.0}          &    1.137e+04  &     4141.570     &     2.745  &         0.006        &     3251.953    &     1.95e+04     \\\\\n",
       "\\textbf{10499.0}          &    -212.4259  &     3309.795     &    -0.064  &         0.949        &    -6700.128    &     6275.276     \\\\\n",
       "\\textbf{10511.0}          &    1.257e+04  &     4628.344     &     2.716  &         0.007        &     3500.005    &     2.16e+04     \\\\\n",
       "\\textbf{10519.0}          &   -6240.6283  &     3068.227     &    -2.034  &         0.042        &    -1.23e+04    &     -226.435     \\\\\n",
       "\\textbf{10530.0}          &    7396.4535  &     3303.701     &     2.239  &         0.025        &      920.696    &     1.39e+04     \\\\\n",
       "\\textbf{10537.0}          &    8373.8467  &     3619.936     &     2.313  &         0.021        &     1278.221    &     1.55e+04     \\\\\n",
       "\\textbf{10540.0}          &    9487.4398  &     3460.376     &     2.742  &         0.006        &     2704.575    &     1.63e+04     \\\\\n",
       "\\textbf{10541.0}          &    1.118e+04  &     3862.112     &     2.894  &         0.004        &     3605.598    &     1.87e+04     \\\\\n",
       "\\textbf{10550.0}          &    7515.1246  &     6151.801     &     1.222  &         0.222        &    -4543.343    &     1.96e+04     \\\\\n",
       "\\textbf{10553.0}          &    5242.3296  &     3365.410     &     1.558  &         0.119        &    -1354.387    &     1.18e+04     \\\\\n",
       "\\textbf{10565.0}          &    1.274e+04  &     4159.576     &     3.062  &         0.002        &     4583.444    &     2.09e+04     \\\\\n",
       "\\textbf{10580.0}          &    1.283e+04  &     4341.856     &     2.956  &         0.003        &     4322.812    &     2.13e+04     \\\\\n",
       "\\textbf{10581.0}          &    9797.9005  &     3963.747     &     2.472  &         0.013        &     2028.352    &     1.76e+04     \\\\\n",
       "\\textbf{10588.0}          &     855.6779  &     3101.920     &     0.276  &         0.783        &    -5224.558    &     6935.914     \\\\\n",
       "\\textbf{10597.0}          &    1.105e+04  &     3914.511     &     2.823  &         0.005        &     3378.337    &     1.87e+04     \\\\\n",
       "\\textbf{10599.0}          &    1.158e+04  &     3962.497     &     2.922  &         0.003        &     3811.778    &     1.93e+04     \\\\\n",
       "\\textbf{10618.0}          &    1.051e+04  &     3872.700     &     2.715  &         0.007        &     2923.389    &     1.81e+04     \\\\\n",
       "\\textbf{10656.0}          &    1.107e+04  &     3644.052     &     3.039  &         0.002        &     3929.910    &     1.82e+04     \\\\\n",
       "\\textbf{10658.0}          &    1.097e+04  &     3625.002     &     3.027  &         0.002        &     3869.116    &     1.81e+04     \\\\\n",
       "\\textbf{10726.0}          &    1.442e+04  &     4401.151     &     3.276  &         0.001        &     5789.509    &      2.3e+04     \\\\\n",
       "\\textbf{10734.0}          &    9405.0171  &     4445.151     &     2.116  &         0.034        &      691.843    &     1.81e+04     \\\\\n",
       "\\textbf{10735.0}          &    1.206e+04  &     4437.109     &     2.718  &         0.007        &     3364.680    &     2.08e+04     \\\\\n",
       "\\textbf{10764.0}          &    1.242e+04  &     4557.772     &     2.724  &         0.006        &     3481.657    &     2.13e+04     \\\\\n",
       "\\textbf{10777.0}          &    1.093e+04  &     3624.387     &     3.015  &         0.003        &     3822.478    &      1.8e+04     \\\\\n",
       "\\textbf{1078.0}           &    2633.8775  &     3872.332     &     0.680  &         0.496        &    -4956.484    &     1.02e+04     \\\\\n",
       "\\textbf{10793.0}          &    1.041e+04  &     4157.794     &     2.504  &         0.012        &     2259.333    &     1.86e+04     \\\\\n",
       "\\textbf{10816.0}          &    9848.3369  &     3792.306     &     2.597  &         0.009        &     2414.839    &     1.73e+04     \\\\\n",
       "\\textbf{10839.0}          &    1.179e+04  &     3862.003     &     3.052  &         0.002        &     4216.664    &     1.94e+04     \\\\\n",
       "\\textbf{10857.0}          &    -845.3807  &     3281.642     &    -0.258  &         0.797        &    -7277.900    &     5587.138     \\\\\n",
       "\\textbf{10867.0}          &    6807.2531  &     4207.731     &     1.618  &         0.106        &    -1440.540    &     1.51e+04     \\\\\n",
       "\\textbf{10906.0}          &     1.09e+04  &     3926.720     &     2.775  &         0.006        &     3199.498    &     1.86e+04     \\\\\n",
       "\\textbf{10950.0}          &    1.168e+04  &     4777.897     &     2.445  &         0.014        &     2317.340    &      2.1e+04     \\\\\n",
       "\\textbf{10983.0}          &   -2.276e+04  &     3294.972     &    -6.907  &         0.000        &    -2.92e+04    &    -1.63e+04     \\\\\n",
       "\\textbf{1099.0}           &    1.067e+04  &     3750.813     &     2.845  &         0.004        &     3318.973    &      1.8e+04     \\\\\n",
       "\\textbf{10991.0}          &    7301.2367  &     4789.870     &     1.524  &         0.127        &    -2087.639    &     1.67e+04     \\\\\n",
       "\\textbf{11012.0}          &    9155.2709  &     4087.399     &     2.240  &         0.025        &     1143.346    &     1.72e+04     \\\\\n",
       "\\textbf{11038.0}          &    4372.6645  &     4278.944     &     1.022  &         0.307        &    -4014.718    &     1.28e+04     \\\\\n",
       "\\textbf{1104.0}           &    1.125e+04  &     4231.136     &     2.660  &         0.008        &     2960.063    &     1.95e+04     \\\\\n",
       "\\textbf{11060.0}          &    1.158e+04  &     4129.190     &     2.804  &         0.005        &     3485.908    &     1.97e+04     \\\\\n",
       "\\textbf{11094.0}          &     1.07e+04  &     3840.633     &     2.787  &         0.005        &     3173.778    &     1.82e+04     \\\\\n",
       "\\textbf{11096.0}          &    8640.8067  &     3421.736     &     2.525  &         0.012        &     1933.682    &     1.53e+04     \\\\\n",
       "\\textbf{11113.0}          &    1.101e+04  &     4226.667     &     2.605  &         0.009        &     2724.173    &     1.93e+04     \\\\\n",
       "\\textbf{1115.0}           &    9631.4518  &     3696.163     &     2.606  &         0.009        &     2386.409    &     1.69e+04     \\\\\n",
       "\\textbf{11161.0}          &    7721.6527  &     3683.960     &     2.096  &         0.036        &      500.530    &     1.49e+04     \\\\\n",
       "\\textbf{11225.0}          &    1.232e+04  &     4390.224     &     2.807  &         0.005        &     3719.225    &     2.09e+04     \\\\\n",
       "\\textbf{11228.0}          &    1.215e+04  &     3982.478     &     3.050  &         0.002        &     4342.074    &        2e+04     \\\\\n",
       "\\textbf{11236.0}          &    6991.3336  &     4824.075     &     1.449  &         0.147        &    -2464.589    &     1.64e+04     \\\\\n",
       "\\textbf{11288.0}          &   -2018.1984  &     4000.197     &    -0.505  &         0.614        &    -9859.194    &     5822.797     \\\\\n",
       "\\textbf{11312.0}          &     -34.4822  &     3301.898     &    -0.010  &         0.992        &    -6506.705    &     6437.741     \\\\\n",
       "\\textbf{11361.0}          &    1.062e+04  &     3541.132     &     2.998  &         0.003        &     3674.735    &     1.76e+04     \\\\\n",
       "\\textbf{11399.0}          &    2972.3845  &     3179.933     &     0.935  &         0.350        &    -3260.769    &     9205.538     \\\\\n",
       "\\textbf{114303.0}         &   -1.046e+04  &     5391.406     &    -1.941  &         0.052        &     -2.1e+04    &      104.537     \\\\\n",
       "\\textbf{11456.0}          &    5467.4855  &     3454.479     &     1.583  &         0.114        &    -1303.821    &     1.22e+04     \\\\\n",
       "\\textbf{11465.0}          &    6032.2158  &     3836.369     &     1.572  &         0.116        &    -1487.653    &     1.36e+04     \\\\\n",
       "\\textbf{11502.0}          &    1.117e+04  &     4449.872     &     2.510  &         0.012        &     2448.510    &     1.99e+04     \\\\\n",
       "\\textbf{11506.0}          &    6270.2659  &     3372.318     &     1.859  &         0.063        &     -339.991    &     1.29e+04     \\\\\n",
       "\\textbf{11537.0}          &    1.137e+04  &     3814.796     &     2.980  &         0.003        &     3891.850    &     1.88e+04     \\\\\n",
       "\\textbf{11566.0}          &    1.276e+04  &     4524.328     &     2.820  &         0.005        &     3890.205    &     2.16e+04     \\\\\n",
       "\\textbf{11573.0}          &    1.034e+04  &     3563.257     &     2.901  &         0.004        &     3353.251    &     1.73e+04     \\\\\n",
       "\\textbf{11580.0}          &    6296.8646  &     3705.782     &     1.699  &         0.089        &     -967.034    &     1.36e+04     \\\\\n",
       "\\textbf{11600.0}          &    1.189e+04  &     4332.106     &     2.745  &         0.006        &     3401.731    &     2.04e+04     \\\\\n",
       "\\textbf{11609.0}          &    1.548e+04  &     4202.441     &     3.683  &         0.000        &     7241.117    &     2.37e+04     \\\\\n",
       "\\textbf{1161.0}           &   -1188.8749  &     2976.195     &    -0.399  &         0.690        &    -7022.672    &     4644.922     \\\\\n",
       "\\textbf{11636.0}          &   -1.074e+04  &     3653.852     &    -2.939  &         0.003        &    -1.79e+04    &    -3577.916     \\\\\n",
       "\\textbf{11670.0}          &    1.238e+04  &     4581.034     &     2.702  &         0.007        &     3396.962    &     2.14e+04     \\\\\n",
       "\\textbf{11678.0}          &   -1260.8304  &     3564.728     &    -0.354  &         0.724        &    -8248.241    &     5726.581     \\\\\n",
       "\\textbf{11682.0}          &    1.073e+04  &     3611.674     &     2.970  &         0.003        &     3648.552    &     1.78e+04     \\\\\n",
       "\\textbf{11694.0}          &    1.178e+04  &     4057.704     &     2.904  &         0.004        &     3830.393    &     1.97e+04     \\\\\n",
       "\\textbf{11720.0}          &    2309.2070  &     4719.750     &     0.489  &         0.625        &    -6942.223    &     1.16e+04     \\\\\n",
       "\\textbf{11721.0}          &   -6216.7588  &     4834.181     &    -1.286  &         0.198        &    -1.57e+04    &     3258.974     \\\\\n",
       "\\textbf{11722.0}          &    1.005e+04  &     3772.084     &     2.663  &         0.008        &     2652.948    &     1.74e+04     \\\\\n",
       "\\textbf{11793.0}          &    1.032e+04  &     6325.197     &     1.632  &         0.103        &    -2074.750    &     2.27e+04     \\\\\n",
       "\\textbf{11797.0}          &    1.239e+04  &     4716.169     &     2.627  &         0.009        &     3143.742    &     2.16e+04     \\\\\n",
       "\\textbf{11914.0}          &    1.126e+04  &     4485.572     &     2.511  &         0.012        &     2470.812    &     2.01e+04     \\\\\n",
       "\\textbf{1209.0}           &    8441.1675  &     3331.506     &     2.534  &         0.011        &     1910.908    &      1.5e+04     \\\\\n",
       "\\textbf{12136.0}          &   -7389.7179  &     3617.876     &    -2.043  &         0.041        &    -1.45e+04    &     -298.129     \\\\\n",
       "\\textbf{12141.0}          &     8.96e+04  &     3470.701     &    25.815  &         0.000        &     8.28e+04    &     9.64e+04     \\\\\n",
       "\\textbf{12181.0}          &    8824.9469  &     4454.428     &     1.981  &         0.048        &       93.589    &     1.76e+04     \\\\\n",
       "\\textbf{12215.0}          &    -639.9548  &     3284.031     &    -0.195  &         0.845        &    -7077.156    &     5797.247     \\\\\n",
       "\\textbf{12216.0}          &    3684.1706  &     3584.414     &     1.028  &         0.304        &    -3341.827    &     1.07e+04     \\\\\n",
       "\\textbf{12256.0}          &    3350.1200  &     3467.654     &     0.966  &         0.334        &    -3447.011    &     1.01e+04     \\\\\n",
       "\\textbf{12262.0}          &    1.074e+04  &     4592.115     &     2.339  &         0.019        &     1739.052    &     1.97e+04     \\\\\n",
       "\\textbf{12389.0}          &    1.193e+04  &     3704.182     &     3.222  &         0.001        &     4674.066    &     1.92e+04     \\\\\n",
       "\\textbf{1239.0}           &    8045.7087  &     3516.851     &     2.288  &         0.022        &     1152.144    &     1.49e+04     \\\\\n",
       "\\textbf{12390.0}          &    9333.9268  &     4088.151     &     2.283  &         0.022        &     1320.528    &     1.73e+04     \\\\\n",
       "\\textbf{12397.0}          &    8107.0205  &     5501.313     &     1.474  &         0.141        &    -2676.391    &     1.89e+04     \\\\\n",
       "\\textbf{1243.0}           &    6447.7796  &     4080.880     &     1.580  &         0.114        &    -1551.368    &     1.44e+04     \\\\\n",
       "\\textbf{12548.0}          &    1.097e+04  &     4265.673     &     2.571  &         0.010        &     2605.463    &     1.93e+04     \\\\\n",
       "\\textbf{12570.0}          &    1.033e+04  &     4387.962     &     2.353  &         0.019        &     1724.321    &     1.89e+04     \\\\\n",
       "\\textbf{12581.0}          &    8527.2526  &     4009.702     &     2.127  &         0.033        &      667.624    &     1.64e+04     \\\\\n",
       "\\textbf{12592.0}          &    9019.3177  &     4216.569     &     2.139  &         0.032        &      754.199    &     1.73e+04     \\\\\n",
       "\\textbf{12604.0}          &    8118.8529  &     6722.925     &     1.208  &         0.227        &    -5059.104    &     2.13e+04     \\\\\n",
       "\\textbf{12656.0}          &    1.221e+04  &     4797.855     &     2.545  &         0.011        &     2806.048    &     2.16e+04     \\\\\n",
       "\\textbf{12679.0}          &   -9893.9636  &     3978.706     &    -2.487  &         0.013        &    -1.77e+04    &    -2095.094     \\\\\n",
       "\\textbf{1278.0}           &    1.164e+04  &     4567.652     &     2.549  &         0.011        &     2690.848    &     2.06e+04     \\\\\n",
       "\\textbf{12788.0}          &   -5830.7039  &     6161.565     &    -0.946  &         0.344        &    -1.79e+04    &     6246.902     \\\\\n",
       "\\textbf{1283.0}           &    1.128e+04  &     4606.609     &     2.449  &         0.014        &     2252.426    &     2.03e+04     \\\\\n",
       "\\textbf{1297.0}           &    1.082e+04  &     3929.942     &     2.753  &         0.006        &     3117.433    &     1.85e+04     \\\\\n",
       "\\textbf{12992.0}          &      1.2e+04  &     4455.981     &     2.693  &         0.007        &     3265.796    &     2.07e+04     \\\\\n",
       "\\textbf{13135.0}          &    7128.1442  &     4048.838     &     1.761  &         0.078        &     -808.196    &     1.51e+04     \\\\\n",
       "\\textbf{1327.0}           &    1639.9713  &     4435.025     &     0.370  &         0.712        &    -7053.353    &     1.03e+04     \\\\\n",
       "\\textbf{13282.0}          &    4849.3715  &     5498.022     &     0.882  &         0.378        &    -5927.589    &     1.56e+04     \\\\\n",
       "\\textbf{1334.0}           &    -511.5826  &     5831.432     &    -0.088  &         0.930        &    -1.19e+04    &     1.09e+04     \\\\\n",
       "\\textbf{13351.0}          &    6163.1783  &     4257.004     &     1.448  &         0.148        &    -2181.199    &     1.45e+04     \\\\\n",
       "\\textbf{13365.0}          &   -1.072e+04  &     4821.541     &    -2.223  &         0.026        &    -2.02e+04    &    -1268.772     \\\\\n",
       "\\textbf{13369.0}          &    8931.5001  &     3699.764     &     2.414  &         0.016        &     1679.398    &     1.62e+04     \\\\\n",
       "\\textbf{13406.0}          &    1.141e+04  &     4169.202     &     2.737  &         0.006        &     3238.275    &     1.96e+04     \\\\\n",
       "\\textbf{13407.0}          &    4068.7423  &     3712.436     &     1.096  &         0.273        &    -3208.198    &     1.13e+04     \\\\\n",
       "\\textbf{13417.0}          &    1.202e+04  &     4758.917     &     2.526  &         0.012        &     2692.416    &     2.13e+04     \\\\\n",
       "\\textbf{13525.0}          &    1258.2074  &     5299.053     &     0.237  &         0.812        &    -9128.744    &     1.16e+04     \\\\\n",
       "\\textbf{13554.0}          &    1.269e+04  &     4749.625     &     2.673  &         0.008        &     3384.617    &      2.2e+04     \\\\\n",
       "\\textbf{1359.0}           &   -3324.7685  &     5248.095     &    -0.634  &         0.526        &    -1.36e+04    &     6962.298     \\\\\n",
       "\\textbf{13623.0}          &    9061.8658  &     4120.385     &     2.199  &         0.028        &      985.282    &     1.71e+04     \\\\\n",
       "\\textbf{1372.0}           &    2057.8258  &     4237.981     &     0.486  &         0.627        &    -6249.263    &     1.04e+04     \\\\\n",
       "\\textbf{1380.0}           &    3146.3459  &     3782.620     &     0.832  &         0.406        &    -4268.166    &     1.06e+04     \\\\\n",
       "\\textbf{13923.0}          &    1.088e+04  &     4304.183     &     2.527  &         0.012        &     2439.468    &     1.93e+04     \\\\\n",
       "\\textbf{13932.0}          &     1.22e+04  &     5116.181     &     2.384  &         0.017        &     2168.201    &     2.22e+04     \\\\\n",
       "\\textbf{13941.0}          &    -596.2531  &     3355.204     &    -0.178  &         0.859        &    -7172.965    &     5980.458     \\\\\n",
       "\\textbf{1397.0}           &    9648.9416  &     3677.558     &     2.624  &         0.009        &     2440.368    &     1.69e+04     \\\\\n",
       "\\textbf{14064.0}          &    8774.2049  &     4340.256     &     2.022  &         0.043        &      266.642    &     1.73e+04     \\\\\n",
       "\\textbf{14084.0}          &    9923.8171  &     3771.402     &     2.631  &         0.009        &     2531.295    &     1.73e+04     \\\\\n",
       "\\textbf{14324.0}          &    1785.4300  &     3979.030     &     0.449  &         0.654        &    -6014.076    &     9584.936     \\\\\n",
       "\\textbf{14462.0}          &    8775.3024  &     3785.900     &     2.318  &         0.020        &     1354.361    &     1.62e+04     \\\\\n",
       "\\textbf{1447.0}           &    1.583e+04  &     5146.276     &     3.077  &         0.002        &     5747.016    &     2.59e+04     \\\\\n",
       "\\textbf{14531.0}          &    8978.9881  &     1.01e+04     &     0.886  &         0.375        &    -1.09e+04    &     2.88e+04     \\\\\n",
       "\\textbf{14593.0}          &    1.146e+04  &     4575.502     &     2.505  &         0.012        &     2491.648    &     2.04e+04     \\\\\n",
       "\\textbf{14622.0}          &    1.118e+04  &     7405.549     &     1.510  &         0.131        &    -3335.409    &     2.57e+04     \\\\\n",
       "\\textbf{1465.0}           &    1.075e+04  &     4781.058     &     2.248  &         0.025        &     1376.754    &     2.01e+04     \\\\\n",
       "\\textbf{1468.0}           &    1.188e+04  &     4462.287     &     2.663  &         0.008        &     3137.130    &     2.06e+04     \\\\\n",
       "\\textbf{14897.0}          &    9851.0172  &     5967.234     &     1.651  &         0.099        &    -1845.671    &     2.15e+04     \\\\\n",
       "\\textbf{14954.0}          &    1.089e+04  &     4473.110     &     2.434  &         0.015        &     2119.784    &     1.97e+04     \\\\\n",
       "\\textbf{1496.0}           &    1.237e+04  &     4176.488     &     2.962  &         0.003        &     4186.049    &     2.06e+04     \\\\\n",
       "\\textbf{15267.0}          &    1.032e+04  &     4202.868     &     2.456  &         0.014        &     2083.660    &     1.86e+04     \\\\\n",
       "\\textbf{15354.0}          &    5330.6304  &     3750.916     &     1.421  &         0.155        &    -2021.737    &     1.27e+04     \\\\\n",
       "\\textbf{1542.0}           &    1.025e+04  &     3687.124     &     2.780  &         0.005        &     3023.899    &     1.75e+04     \\\\\n",
       "\\textbf{15459.0}          &    7623.0653  &     3820.476     &     1.995  &         0.046        &      134.350    &     1.51e+04     \\\\\n",
       "\\textbf{1554.0}           &    1.221e+04  &     4168.390     &     2.928  &         0.003        &     4034.702    &     2.04e+04     \\\\\n",
       "\\textbf{15708.0}          &   -1.173e+04  &     4739.990     &    -2.476  &         0.013        &     -2.1e+04    &    -2442.785     \\\\\n",
       "\\textbf{15711.0}          &    9204.2718  &     4005.034     &     2.298  &         0.022        &     1353.794    &     1.71e+04     \\\\\n",
       "\\textbf{15761.0}          &    1.127e+04  &     4819.636     &     2.338  &         0.019        &     1821.834    &     2.07e+04     \\\\\n",
       "\\textbf{1581.0}           &   -2.091e+04  &     4631.919     &    -4.515  &         0.000        &       -3e+04    &    -1.18e+04     \\\\\n",
       "\\textbf{1593.0}           &    9602.8522  &     3468.687     &     2.768  &         0.006        &     2803.697    &     1.64e+04     \\\\\n",
       "\\textbf{1602.0}           &    1.722e+04  &     3483.910     &     4.944  &         0.000        &     1.04e+04    &     2.41e+04     \\\\\n",
       "\\textbf{1613.0}           &    1.144e+04  &     4184.743     &     2.733  &         0.006        &     3235.570    &     1.96e+04     \\\\\n",
       "\\textbf{16188.0}          &    7072.9575  &     4187.940     &     1.689  &         0.091        &    -1136.043    &     1.53e+04     \\\\\n",
       "\\textbf{1632.0}           &    2653.4280  &     3125.560     &     0.849  &         0.396        &    -3473.146    &     8780.002     \\\\\n",
       "\\textbf{1633.0}           &    9133.1549  &     3813.953     &     2.395  &         0.017        &     1657.226    &     1.66e+04     \\\\\n",
       "\\textbf{1635.0}           &   -5567.8350  &     4976.379     &    -1.119  &         0.263        &    -1.53e+04    &     4186.626     \\\\\n",
       "\\textbf{16401.0}          &    -662.3621  &     3678.802     &    -0.180  &         0.857        &    -7873.375    &     6548.651     \\\\\n",
       "\\textbf{16437.0}          &    4365.1379  &     4051.649     &     1.077  &         0.281        &    -3576.712    &     1.23e+04     \\\\\n",
       "\\textbf{1651.0}           &    6305.5157  &     3460.812     &     1.822  &         0.068        &     -478.204    &     1.31e+04     \\\\\n",
       "\\textbf{1655.0}           &    1.188e+04  &     4009.212     &     2.963  &         0.003        &     4021.892    &     1.97e+04     \\\\\n",
       "\\textbf{1663.0}           &    1.612e+04  &     3749.056     &     4.299  &         0.000        &     8769.535    &     2.35e+04     \\\\\n",
       "\\textbf{16710.0}          &    5981.0310  &     3906.403     &     1.531  &         0.126        &    -1676.113    &     1.36e+04     \\\\\n",
       "\\textbf{16729.0}          &    5061.6838  &     3959.387     &     1.278  &         0.201        &    -2699.318    &     1.28e+04     \\\\\n",
       "\\textbf{1690.0}           &   -1.108e+04  &     3519.723     &    -3.149  &         0.002        &     -1.8e+04    &    -4184.065     \\\\\n",
       "\\textbf{1703.0}           &    9217.5903  &     3557.851     &     2.591  &         0.010        &     2243.659    &     1.62e+04     \\\\\n",
       "\\textbf{17101.0}          &   -1.638e+04  &     1.17e+04     &    -1.405  &         0.160        &    -3.92e+04    &     6464.946     \\\\\n",
       "\\textbf{17202.0}          &    9944.8151  &     4458.719     &     2.230  &         0.026        &     1205.045    &     1.87e+04     \\\\\n",
       "\\textbf{1722.0}           &    9966.2654  &     3919.741     &     2.543  &         0.011        &     2282.975    &     1.76e+04     \\\\\n",
       "\\textbf{1728.0}           &    1.199e+04  &     4123.710     &     2.907  &         0.004        &     3904.903    &     2.01e+04     \\\\\n",
       "\\textbf{1743.0}           &     1.14e+04  &     5013.948     &     2.274  &         0.023        &     1574.373    &     2.12e+04     \\\\\n",
       "\\textbf{1754.0}           &    1.127e+04  &     4027.944     &     2.797  &         0.005        &     3371.582    &     1.92e+04     \\\\\n",
       "\\textbf{1762.0}           &    6020.1626  &     3557.041     &     1.692  &         0.091        &     -952.180    &      1.3e+04     \\\\\n",
       "\\textbf{1773.0}           &    1.144e+04  &     4568.557     &     2.504  &         0.012        &     2482.906    &     2.04e+04     \\\\\n",
       "\\textbf{1786.0}           &   -3272.3191  &     3073.973     &    -1.065  &         0.287        &    -9297.774    &     2753.136     \\\\\n",
       "\\textbf{18100.0}          &    8898.7772  &     4275.979     &     2.081  &         0.037        &      517.206    &     1.73e+04     \\\\\n",
       "\\textbf{1820.0}           &    7869.9943  &     4068.111     &     1.935  &         0.053        &     -104.124    &     1.58e+04     \\\\\n",
       "\\textbf{1848.0}           &    -542.6371  &     3639.338     &    -0.149  &         0.881        &    -7676.295    &     6591.021     \\\\\n",
       "\\textbf{18654.0}          &    1.171e+04  &     5117.475     &     2.288  &         0.022        &     1680.301    &     2.17e+04     \\\\\n",
       "\\textbf{1875.0}           &    5894.0920  &     4997.948     &     1.179  &         0.238        &    -3902.649    &     1.57e+04     \\\\\n",
       "\\textbf{1884.0}           &    1.117e+04  &     4392.864     &     2.543  &         0.011        &     2562.229    &     1.98e+04     \\\\\n",
       "\\textbf{1913.0}           &    7336.9598  &     3351.413     &     2.189  &         0.029        &      767.680    &     1.39e+04     \\\\\n",
       "\\textbf{1919.0}           &    1.059e+04  &     4046.792     &     2.616  &         0.009        &     2653.940    &     1.85e+04     \\\\\n",
       "\\textbf{1920.0}           &    7313.0108  &     3212.234     &     2.277  &         0.023        &     1016.543    &     1.36e+04     \\\\\n",
       "\\textbf{1968.0}           &    9900.3459  &     3469.706     &     2.853  &         0.004        &     3099.193    &     1.67e+04     \\\\\n",
       "\\textbf{1976.0}           &    1.247e+04  &     4073.086     &     3.062  &         0.002        &     4486.807    &     2.05e+04     \\\\\n",
       "\\textbf{1981.0}           &    1.105e+04  &     4077.664     &     2.710  &         0.007        &     3058.436    &      1.9e+04     \\\\\n",
       "\\textbf{1988.0}           &    2066.6708  &     4287.949     &     0.482  &         0.630        &    -6338.363    &     1.05e+04     \\\\\n",
       "\\textbf{1992.0}           &    1.028e+04  &     3494.046     &     2.943  &         0.003        &     3434.591    &     1.71e+04     \\\\\n",
       "\\textbf{2008.0}           &    1.017e+04  &     3530.470     &     2.880  &         0.004        &     3248.358    &     1.71e+04     \\\\\n",
       "\\textbf{2033.0}           &    1.117e+04  &     4362.901     &     2.560  &         0.010        &     2616.168    &     1.97e+04     \\\\\n",
       "\\textbf{2044.0}           &    8626.2108  &     3719.509     &     2.319  &         0.020        &     1335.406    &     1.59e+04     \\\\\n",
       "\\textbf{2049.0}           &    1.006e+04  &     3610.499     &     2.787  &         0.005        &     2984.839    &     1.71e+04     \\\\\n",
       "\\textbf{2061.0}           &    1.278e+04  &     4481.478     &     2.851  &         0.004        &     3993.798    &     2.16e+04     \\\\\n",
       "\\textbf{20779.0}          &    5.394e+04  &     3923.620     &    13.748  &         0.000        &     4.63e+04    &     6.16e+04     \\\\\n",
       "\\textbf{2085.0}           &   -5400.0504  &     4099.351     &    -1.317  &         0.188        &    -1.34e+04    &     2635.303     \\\\\n",
       "\\textbf{2086.0}           &    7832.7624  &     3450.472     &     2.270  &         0.023        &     1069.311    &     1.46e+04     \\\\\n",
       "\\textbf{2111.0}           &    8505.6464  &     3461.370     &     2.457  &         0.014        &     1720.834    &     1.53e+04     \\\\\n",
       "\\textbf{21204.0}          &    4498.8386  &     5310.844     &     0.847  &         0.397        &    -5911.226    &     1.49e+04     \\\\\n",
       "\\textbf{21238.0}          &    1.083e+04  &     4828.430     &     2.243  &         0.025        &     1365.421    &     2.03e+04     \\\\\n",
       "\\textbf{2124.0}           &    1.018e+04  &     3588.996     &     2.836  &         0.005        &     3142.326    &     1.72e+04     \\\\\n",
       "\\textbf{2146.0}           &    1.769e+04  &     5276.450     &     3.354  &         0.001        &     7352.347    &      2.8e+04     \\\\\n",
       "\\textbf{21496.0}          &    -1.28e+04  &     5019.607     &    -2.549  &         0.011        &    -2.26e+04    &    -2956.627     \\\\\n",
       "\\textbf{2154.0}           &    1.055e+04  &     3883.931     &     2.716  &         0.007        &     2936.502    &     1.82e+04     \\\\\n",
       "\\textbf{2176.0}           &    5.108e+04  &     4320.049     &    11.825  &         0.000        &     4.26e+04    &     5.96e+04     \\\\\n",
       "\\textbf{2188.0}           &    1.253e+04  &     4513.709     &     2.776  &         0.006        &     3681.859    &     2.14e+04     \\\\\n",
       "\\textbf{2189.0}           &    4436.7599  &     3438.026     &     1.290  &         0.197        &    -2302.296    &     1.12e+04     \\\\\n",
       "\\textbf{2220.0}           &    1.034e+04  &     3652.188     &     2.833  &         0.005        &     3185.997    &     1.75e+04     \\\\\n",
       "\\textbf{22205.0}          &    1.269e+04  &     4884.409     &     2.598  &         0.009        &     3114.998    &     2.23e+04     \\\\\n",
       "\\textbf{2226.0}           &    7783.8828  &     6251.292     &     1.245  &         0.213        &    -4469.603    &        2e+04     \\\\\n",
       "\\textbf{2230.0}           &    1.047e+04  &     4520.037     &     2.316  &         0.021        &     1610.401    &     1.93e+04     \\\\\n",
       "\\textbf{22325.0}          &     671.7506  &     4354.809     &     0.154  &         0.877        &    -7864.340    &     9207.841     \\\\\n",
       "\\textbf{2255.0}           &    9504.4405  &     3720.917     &     2.554  &         0.011        &     2210.876    &     1.68e+04     \\\\\n",
       "\\textbf{22619.0}          &    1.145e+04  &     4546.012     &     2.518  &         0.012        &     2536.613    &     2.04e+04     \\\\\n",
       "\\textbf{2267.0}           &     954.6225  &     3444.320     &     0.277  &         0.782        &    -5796.771    &     7706.015     \\\\\n",
       "\\textbf{22815.0}          &    5747.8505  &     3998.387     &     1.438  &         0.151        &    -2089.598    &     1.36e+04     \\\\\n",
       "\\textbf{2285.0}           &   -1.948e+04  &     3317.110     &    -5.871  &         0.000        &     -2.6e+04    &     -1.3e+04     \\\\\n",
       "\\textbf{2290.0}           &    6425.0318  &     3607.703     &     1.781  &         0.075        &     -646.616    &     1.35e+04     \\\\\n",
       "\\textbf{2295.0}           &    1.256e+04  &     5597.325     &     2.244  &         0.025        &     1586.917    &     2.35e+04     \\\\\n",
       "\\textbf{2316.0}           &    5728.1236  &     3470.724     &     1.650  &         0.099        &    -1075.024    &     1.25e+04     \\\\\n",
       "\\textbf{23220.0}          &    9924.7612  &     4113.115     &     2.413  &         0.016        &     1862.428    &      1.8e+04     \\\\\n",
       "\\textbf{23224.0}          &   -3755.6038  &     6047.146     &    -0.621  &         0.535        &    -1.56e+04    &     8097.723     \\\\\n",
       "\\textbf{2343.0}           &    -534.6446  &     6309.619     &    -0.085  &         0.932        &    -1.29e+04    &     1.18e+04     \\\\\n",
       "\\textbf{2352.0}           &    9736.7758  &     3588.280     &     2.713  &         0.007        &     2703.200    &     1.68e+04     \\\\\n",
       "\\textbf{23700.0}          &   -1938.1193  &     4766.712     &    -0.407  &         0.684        &    -1.13e+04    &     7405.363     \\\\\n",
       "\\textbf{2390.0}           &    1.238e+04  &     4216.522     &     2.936  &         0.003        &     4114.191    &     2.06e+04     \\\\\n",
       "\\textbf{2393.0}           &    7550.3051  &     3430.390     &     2.201  &         0.028        &      826.217    &     1.43e+04     \\\\\n",
       "\\textbf{2403.0}           &    1.248e+04  &     4112.791     &     3.034  &         0.002        &     4415.079    &     2.05e+04     \\\\\n",
       "\\textbf{2435.0}           &     1.42e+04  &     4525.795     &     3.138  &         0.002        &     5330.352    &     2.31e+04     \\\\\n",
       "\\textbf{2444.0}           &    7862.9049  &     3409.081     &     2.306  &         0.021        &     1180.587    &     1.45e+04     \\\\\n",
       "\\textbf{2448.0}           &    9317.7327  &     3635.420     &     2.563  &         0.010        &     2191.756    &     1.64e+04     \\\\\n",
       "\\textbf{2469.0}           &    1.172e+04  &     5182.428     &     2.262  &         0.024        &     1564.434    &     2.19e+04     \\\\\n",
       "\\textbf{24720.0}          &    1.086e+04  &     4248.909     &     2.557  &         0.011        &     2536.128    &     1.92e+04     \\\\\n",
       "\\textbf{24800.0}          &     522.9771  &     4749.139     &     0.110  &         0.912        &    -8786.059    &     9832.013     \\\\\n",
       "\\textbf{2482.0}           &    1.252e+04  &     4265.554     &     2.935  &         0.003        &     4156.786    &     2.09e+04     \\\\\n",
       "\\textbf{24969.0}          &    1.161e+04  &     5237.807     &     2.217  &         0.027        &     1346.333    &     2.19e+04     \\\\\n",
       "\\textbf{2498.0}           &    2742.2213  &     3605.284     &     0.761  &         0.447        &    -4324.684    &     9809.127     \\\\\n",
       "\\textbf{2504.0}           &   -3534.0958  &     4056.194     &    -0.871  &         0.384        &    -1.15e+04    &     4416.663     \\\\\n",
       "\\textbf{2508.0}           &     1.16e+04  &     4095.612     &     2.832  &         0.005        &     3570.062    &     1.96e+04     \\\\\n",
       "\\textbf{25124.0}          &    1.094e+04  &     4389.059     &     2.494  &         0.013        &     2341.169    &     1.95e+04     \\\\\n",
       "\\textbf{2518.0}           &     1.17e+04  &     4013.804     &     2.915  &         0.004        &     3832.399    &     1.96e+04     \\\\\n",
       "\\textbf{25224.0}          &    1.153e+04  &     7440.529     &     1.549  &         0.121        &    -3059.387    &     2.61e+04     \\\\\n",
       "\\textbf{25279.0}          &    9297.6728  &     4410.080     &     2.108  &         0.035        &      653.244    &     1.79e+04     \\\\\n",
       "\\textbf{2537.0}           &   -3686.1598  &     4295.083     &    -0.858  &         0.391        &    -1.21e+04    &     4732.858     \\\\\n",
       "\\textbf{2538.0}           &    1.152e+04  &     5113.381     &     2.252  &         0.024        &     1494.339    &     2.15e+04     \\\\\n",
       "\\textbf{25389.0}          &    1.123e+04  &     6651.487     &     1.688  &         0.091        &    -1810.098    &     2.43e+04     \\\\\n",
       "\\textbf{2547.0}           &    4060.4291  &     3620.838     &     1.121  &         0.262        &    -3036.965    &     1.12e+04     \\\\\n",
       "\\textbf{2553.0}           &    1.089e+04  &     4153.132     &     2.623  &         0.009        &     2753.170    &      1.9e+04     \\\\\n",
       "\\textbf{2574.0}           &    4823.8558  &     4406.699     &     1.095  &         0.274        &    -3813.945    &     1.35e+04     \\\\\n",
       "\\textbf{25747.0}          &    1.101e+04  &     4450.960     &     2.474  &         0.013        &     2285.080    &     1.97e+04     \\\\\n",
       "\\textbf{2577.0}           &    9384.7662  &     3451.936     &     2.719  &         0.007        &     2618.446    &     1.62e+04     \\\\\n",
       "\\textbf{2593.0}           &    9890.8041  &     3570.057     &     2.770  &         0.006        &     2892.948    &     1.69e+04     \\\\\n",
       "\\textbf{2596.0}           &    7510.0147  &     4622.486     &     1.625  &         0.104        &    -1550.762    &     1.66e+04     \\\\\n",
       "\\textbf{2663.0}           &    1.441e+04  &     3919.032     &     3.678  &         0.000        &     6732.007    &     2.21e+04     \\\\\n",
       "\\textbf{2771.0}           &    7729.5092  &     4553.521     &     1.697  &         0.090        &    -1196.087    &     1.67e+04     \\\\\n",
       "\\textbf{2787.0}           &     1.09e+04  &     3973.049     &     2.743  &         0.006        &     3111.448    &     1.87e+04     \\\\\n",
       "\\textbf{2797.0}           &   -2345.0266  &     3283.149     &    -0.714  &         0.475        &    -8780.498    &     4090.445     \\\\\n",
       "\\textbf{2802.0}           &    1.203e+04  &     4223.324     &     2.849  &         0.004        &     3753.698    &     2.03e+04     \\\\\n",
       "\\textbf{2817.0}           &       0.4896  &     3865.011     &     0.000  &         1.000        &    -7575.522    &     7576.501     \\\\\n",
       "\\textbf{28678.0}          &   -8274.1760  &     4174.195     &    -1.982  &         0.047        &    -1.65e+04    &      -92.117     \\\\\n",
       "\\textbf{28701.0}          &    9223.2361  &     3878.259     &     2.378  &         0.017        &     1621.257    &     1.68e+04     \\\\\n",
       "\\textbf{28742.0}          &   -5918.1481  &     4904.397     &    -1.207  &         0.228        &    -1.55e+04    &     3695.218     \\\\\n",
       "\\textbf{2888.0}           &    1.067e+04  &     3701.823     &     2.881  &         0.004        &     3410.014    &     1.79e+04     \\\\\n",
       "\\textbf{2897.0}           &    1.195e+04  &     4495.724     &     2.659  &         0.008        &     3141.271    &     2.08e+04     \\\\\n",
       "\\textbf{2917.0}           &    6165.4632  &     3795.123     &     1.625  &         0.104        &    -1273.556    &     1.36e+04     \\\\\n",
       "\\textbf{29392.0}          &   -3779.9320  &     3953.912     &    -0.956  &         0.339        &    -1.15e+04    &     3970.338     \\\\\n",
       "\\textbf{2950.0}           &   -1.324e+04  &     6688.460     &    -1.980  &         0.048        &    -2.64e+04    &     -133.438     \\\\\n",
       "\\textbf{2951.0}           &    1.223e+04  &     4616.162     &     2.649  &         0.008        &     3179.648    &     2.13e+04     \\\\\n",
       "\\textbf{2953.0}           &    1.061e+04  &     3801.196     &     2.792  &         0.005        &     3162.859    &     1.81e+04     \\\\\n",
       "\\textbf{2960.0}           &    9163.1119  &     4168.518     &     2.198  &         0.028        &      992.181    &     1.73e+04     \\\\\n",
       "\\textbf{2975.0}           &    4507.4799  &     3464.677     &     1.301  &         0.193        &    -2283.815    &     1.13e+04     \\\\\n",
       "\\textbf{2982.0}           &    1.035e+04  &     4073.030     &     2.540  &         0.011        &     2363.511    &     1.83e+04     \\\\\n",
       "\\textbf{2991.0}           &   -1437.3222  &     3931.504     &    -0.366  &         0.715        &    -9143.669    &     6269.025     \\\\\n",
       "\\textbf{3011.0}           &   -1447.5958  &     4429.291     &    -0.327  &         0.744        &    -1.01e+04    &     7234.490     \\\\\n",
       "\\textbf{3015.0}           &     1.26e+04  &     4378.506     &     2.878  &         0.004        &     4018.449    &     2.12e+04     \\\\\n",
       "\\textbf{3026.0}           &    9786.3293  &     3882.487     &     2.521  &         0.012        &     2176.063    &     1.74e+04     \\\\\n",
       "\\textbf{3031.0}           &   -9619.9936  &     5039.433     &    -1.909  &         0.056        &    -1.95e+04    &      258.063     \\\\\n",
       "\\textbf{3062.0}           &    1.265e+04  &     3725.803     &     3.394  &         0.001        &     5342.370    &     1.99e+04     \\\\\n",
       "\\textbf{3093.0}           &    3796.6585  &     4349.266     &     0.873  &         0.383        &    -4728.567    &     1.23e+04     \\\\\n",
       "\\textbf{3107.0}           &    1.066e+04  &     4996.454     &     2.134  &         0.033        &      867.551    &     2.05e+04     \\\\\n",
       "\\textbf{3121.0}           &    1.278e+04  &     3913.903     &     3.266  &         0.001        &     5111.474    &     2.05e+04     \\\\\n",
       "\\textbf{3126.0}           &    1.102e+04  &     3683.022     &     2.992  &         0.003        &     3800.977    &     1.82e+04     \\\\\n",
       "\\textbf{3144.0}           &    6.396e+04  &     4031.384     &    15.866  &         0.000        &     5.61e+04    &     7.19e+04     \\\\\n",
       "\\textbf{3156.0}           &    1.045e+04  &     4497.387     &     2.323  &         0.020        &     1633.650    &     1.93e+04     \\\\\n",
       "\\textbf{3157.0}           &    1.036e+04  &     3840.485     &     2.697  &         0.007        &     2830.490    &     1.79e+04     \\\\\n",
       "\\textbf{3170.0}           &    1.222e+04  &     3803.160     &     3.213  &         0.001        &     4764.830    &     1.97e+04     \\\\\n",
       "\\textbf{3178.0}           &    3978.5918  &     4778.656     &     0.833  &         0.405        &    -5388.302    &     1.33e+04     \\\\\n",
       "\\textbf{3206.0}           &    7525.3953  &     3817.285     &     1.971  &         0.049        &       42.935    &      1.5e+04     \\\\\n",
       "\\textbf{3229.0}           &    7539.4676  &     4373.199     &     1.724  &         0.085        &    -1032.668    &     1.61e+04     \\\\\n",
       "\\textbf{3235.0}           &    1.102e+04  &     3756.998     &     2.934  &         0.003        &     3657.067    &     1.84e+04     \\\\\n",
       "\\textbf{3246.0}           &    1.099e+04  &     4195.025     &     2.620  &         0.009        &     2766.893    &     1.92e+04     \\\\\n",
       "\\textbf{3248.0}           &    1.107e+04  &     3776.058     &     2.933  &         0.003        &     3671.798    &     1.85e+04     \\\\\n",
       "\\textbf{3282.0}           &   -2.004e+04  &     3435.208     &    -5.833  &         0.000        &    -2.68e+04    &    -1.33e+04     \\\\\n",
       "\\textbf{3362.0}           &    3973.2246  &     4193.822     &     0.947  &         0.343        &    -4247.305    &     1.22e+04     \\\\\n",
       "\\textbf{3372.0}           &    1.097e+04  &     4243.952     &     2.584  &         0.010        &     2648.966    &     1.93e+04     \\\\\n",
       "\\textbf{3422.0}           &    1.022e+04  &     3593.566     &     2.844  &         0.004        &     3174.659    &     1.73e+04     \\\\\n",
       "\\textbf{3497.0}           &    4718.9993  &     3142.124     &     1.502  &         0.133        &    -1440.043    &     1.09e+04     \\\\\n",
       "\\textbf{3502.0}           &    4945.7255  &     3530.657     &     1.401  &         0.161        &    -1974.900    &     1.19e+04     \\\\\n",
       "\\textbf{3504.0}           &    9538.6385  &     4075.855     &     2.340  &         0.019        &     1549.342    &     1.75e+04     \\\\\n",
       "\\textbf{3505.0}           &    8971.7211  &     3460.723     &     2.592  &         0.010        &     2188.177    &     1.58e+04     \\\\\n",
       "\\textbf{3532.0}           &    1.079e+04  &     3473.780     &     3.105  &         0.002        &     3977.426    &     1.76e+04     \\\\\n",
       "\\textbf{3574.0}           &    1.266e+04  &     5621.210     &     2.253  &         0.024        &     1645.503    &     2.37e+04     \\\\\n",
       "\\textbf{3580.0}           &    7108.9511  &     3449.114     &     2.061  &         0.039        &      348.162    &     1.39e+04     \\\\\n",
       "\\textbf{3612.0}           &    1.271e+04  &     4548.236     &     2.795  &         0.005        &     3795.224    &     2.16e+04     \\\\\n",
       "\\textbf{3619.0}           &    9717.4324  &     4083.473     &     2.380  &         0.017        &     1713.204    &     1.77e+04     \\\\\n",
       "\\textbf{3622.0}           &    1.223e+04  &     4314.302     &     2.836  &         0.005        &     3776.892    &     2.07e+04     \\\\\n",
       "\\textbf{3639.0}           &     -35.2982  &     3266.121     &    -0.011  &         0.991        &    -6437.393    &     6366.796     \\\\\n",
       "\\textbf{3650.0}           &    1972.6354  &     3363.711     &     0.586  &         0.558        &    -4620.750    &     8566.021     \\\\\n",
       "\\textbf{3662.0}           &    9054.5036  &     3495.631     &     2.590  &         0.010        &     2202.533    &     1.59e+04     \\\\\n",
       "\\textbf{3734.0}           &   -5653.8420  &     3410.927     &    -1.658  &         0.097        &    -1.23e+04    &     1032.094     \\\\\n",
       "\\textbf{3735.0}           &    9184.4944  &     3795.950     &     2.420  &         0.016        &     1743.855    &     1.66e+04     \\\\\n",
       "\\textbf{3761.0}           &    6261.7669  &     3189.760     &     1.963  &         0.050        &        9.351    &     1.25e+04     \\\\\n",
       "\\textbf{3779.0}           &   -4303.5751  &     5205.937     &    -0.827  &         0.408        &    -1.45e+04    &     5900.856     \\\\\n",
       "\\textbf{3781.0}           &    2109.6502  &     4872.186     &     0.433  &         0.665        &    -7440.577    &     1.17e+04     \\\\\n",
       "\\textbf{3782.0}           &   -2053.4891  &     3403.699     &    -0.603  &         0.546        &    -8725.259    &     4618.281     \\\\\n",
       "\\textbf{3786.0}           &    9489.0672  &     3988.705     &     2.379  &         0.017        &     1670.597    &     1.73e+04     \\\\\n",
       "\\textbf{3796.0}           &   -5002.4715  &     5240.283     &    -0.955  &         0.340        &    -1.53e+04    &     5269.283     \\\\\n",
       "\\textbf{3821.0}           &    1.161e+04  &     4287.437     &     2.708  &         0.007        &     3206.709    &        2e+04     \\\\\n",
       "\\textbf{3835.0}           &    3602.0856  &     3562.673     &     1.011  &         0.312        &    -3381.297    &     1.06e+04     \\\\\n",
       "\\textbf{3839.0}           &    7755.4892  &     4065.538     &     1.908  &         0.056        &     -213.585    &     1.57e+04     \\\\\n",
       "\\textbf{3840.0}           &    2233.2361  &     4317.199     &     0.517  &         0.605        &    -6229.132    &     1.07e+04     \\\\\n",
       "\\textbf{3895.0}           &    1.151e+04  &     4015.359     &     2.867  &         0.004        &     3640.571    &     1.94e+04     \\\\\n",
       "\\textbf{3908.0}           &    6511.3911  &     5260.917     &     1.238  &         0.216        &    -3800.809    &     1.68e+04     \\\\\n",
       "\\textbf{3911.0}           &    5754.9750  &     3276.668     &     1.756  &         0.079        &     -667.793    &     1.22e+04     \\\\\n",
       "\\textbf{3917.0}           &    1.148e+04  &     4187.146     &     2.742  &         0.006        &     3272.315    &     1.97e+04     \\\\\n",
       "\\textbf{3946.0}           &    1.227e+04  &     4348.144     &     2.822  &         0.005        &     3745.972    &     2.08e+04     \\\\\n",
       "\\textbf{3971.0}           &     1.08e+04  &     3856.276     &     2.801  &         0.005        &     3242.502    &     1.84e+04     \\\\\n",
       "\\textbf{3980.0}           &    2.076e+04  &     3714.901     &     5.588  &         0.000        &     1.35e+04    &      2.8e+04     \\\\\n",
       "\\textbf{4034.0}           &    7677.6628  &     3373.504     &     2.276  &         0.023        &     1065.082    &     1.43e+04     \\\\\n",
       "\\textbf{4036.0}           &    1.221e+04  &     4119.481     &     2.964  &         0.003        &     4134.874    &     2.03e+04     \\\\\n",
       "\\textbf{4040.0}           &    4321.4450  &     4087.621     &     1.057  &         0.290        &    -3690.916    &     1.23e+04     \\\\\n",
       "\\textbf{4058.0}           &    1.029e+04  &     3779.927     &     2.722  &         0.006        &     2881.014    &     1.77e+04     \\\\\n",
       "\\textbf{4060.0}           &   -7864.2560  &     3258.229     &    -2.414  &         0.016        &    -1.43e+04    &    -1477.631     \\\\\n",
       "\\textbf{4062.0}           &    1.447e+04  &     4511.974     &     3.207  &         0.001        &     5624.335    &     2.33e+04     \\\\\n",
       "\\textbf{4077.0}           &    9410.6027  &     4925.705     &     1.911  &         0.056        &     -244.530    &     1.91e+04     \\\\\n",
       "\\textbf{4087.0}           &   -1.904e+04  &     3566.201     &    -5.338  &         0.000        &     -2.6e+04    &     -1.2e+04     \\\\\n",
       "\\textbf{4091.0}           &    9316.1058  &     4179.475     &     2.229  &         0.026        &     1123.697    &     1.75e+04     \\\\\n",
       "\\textbf{4127.0}           &    4598.9212  &     3230.438     &     1.424  &         0.155        &    -1733.230    &     1.09e+04     \\\\\n",
       "\\textbf{4138.0}           &    1.238e+04  &     4680.272     &     2.645  &         0.008        &     3206.805    &     2.16e+04     \\\\\n",
       "\\textbf{4162.0}           &    1.048e+04  &     4305.636     &     2.433  &         0.015        &     2035.387    &     1.89e+04     \\\\\n",
       "\\textbf{4186.0}           &    1.241e+04  &     4222.066     &     2.940  &         0.003        &     4136.346    &     2.07e+04     \\\\\n",
       "\\textbf{4194.0}           &    -735.4906  &     3515.408     &    -0.209  &         0.834        &    -7626.227    &     6155.246     \\\\\n",
       "\\textbf{4199.0}           &   -3459.6812  &     3114.452     &    -1.111  &         0.267        &    -9564.483    &     2645.120     \\\\\n",
       "\\textbf{4213.0}           &    1.125e+04  &     3649.832     &     3.083  &         0.002        &     4097.360    &     1.84e+04     \\\\\n",
       "\\textbf{4222.0}           &   -2143.4353  &     3777.823     &    -0.567  &         0.570        &    -9548.544    &     5261.674     \\\\\n",
       "\\textbf{4223.0}           &    1.092e+04  &     3729.340     &     2.927  &         0.003        &     3606.865    &     1.82e+04     \\\\\n",
       "\\textbf{4251.0}           &    1.203e+04  &     4142.789     &     2.903  &         0.004        &     3906.307    &     2.01e+04     \\\\\n",
       "\\textbf{4265.0}           &    9581.9468  &     4108.600     &     2.332  &         0.020        &     1528.464    &     1.76e+04     \\\\\n",
       "\\textbf{4274.0}           &    9736.2323  &     3591.278     &     2.711  &         0.007        &     2696.779    &     1.68e+04     \\\\\n",
       "\\textbf{4321.0}           &    8285.3216  &     3383.445     &     2.449  &         0.014        &     1653.253    &     1.49e+04     \\\\\n",
       "\\textbf{4335.0}           &    9296.5716  &     4979.568     &     1.867  &         0.062        &     -464.142    &     1.91e+04     \\\\\n",
       "\\textbf{4340.0}           &    7675.5694  &     3435.786     &     2.234  &         0.026        &      940.904    &     1.44e+04     \\\\\n",
       "\\textbf{4371.0}           &    9023.4305  &     3522.492     &     2.562  &         0.010        &     2118.810    &     1.59e+04     \\\\\n",
       "\\textbf{4415.0}           &    1.091e+04  &     4160.138     &     2.624  &         0.009        &     2760.450    &     1.91e+04     \\\\\n",
       "\\textbf{4450.0}           &    9206.4725  &     4054.859     &     2.270  &         0.023        &     1258.330    &     1.72e+04     \\\\\n",
       "\\textbf{4476.0}           &     699.4465  &     3334.112     &     0.210  &         0.834        &    -5835.921    &     7234.814     \\\\\n",
       "\\textbf{4510.0}           &    4574.8460  &     3383.453     &     1.352  &         0.176        &    -2057.239    &     1.12e+04     \\\\\n",
       "\\textbf{4520.0}           &    1.052e+04  &     3525.801     &     2.985  &         0.003        &     3613.707    &     1.74e+04     \\\\\n",
       "\\textbf{4551.0}           &    9794.1435  &     7586.255     &     1.291  &         0.197        &    -5076.073    &     2.47e+04     \\\\\n",
       "\\textbf{4568.0}           &    1.119e+04  &     3823.353     &     2.926  &         0.003        &     3693.045    &     1.87e+04     \\\\\n",
       "\\textbf{4579.0}           &    1.259e+04  &     4538.028     &     2.775  &         0.006        &     3697.287    &     2.15e+04     \\\\\n",
       "\\textbf{4585.0}           &    1.212e+04  &     4254.059     &     2.849  &         0.004        &     3779.535    &     2.05e+04     \\\\\n",
       "\\textbf{4595.0}           &    8302.6110  &     3564.836     &     2.329  &         0.020        &     1314.989    &     1.53e+04     \\\\\n",
       "\\textbf{4600.0}           &    -353.5173  &     4049.300     &    -0.087  &         0.930        &    -8290.763    &     7583.728     \\\\\n",
       "\\textbf{4607.0}           &    1.197e+04  &     4164.987     &     2.874  &         0.004        &     3807.698    &     2.01e+04     \\\\\n",
       "\\textbf{4608.0}           &     570.1042  &     4185.709     &     0.136  &         0.892        &    -7634.523    &     8774.732     \\\\\n",
       "\\textbf{4622.0}           &    7234.5217  &     3341.365     &     2.165  &         0.030        &      684.936    &     1.38e+04     \\\\\n",
       "\\textbf{4623.0}           &    1.083e+04  &     4177.485     &     2.592  &         0.010        &     2639.526    &      1.9e+04     \\\\\n",
       "\\textbf{4768.0}           &    1.054e+04  &     3845.062     &     2.740  &         0.006        &     2998.522    &     1.81e+04     \\\\\n",
       "\\textbf{4771.0}           &     1.23e+04  &     4275.165     &     2.877  &         0.004        &     3920.863    &     2.07e+04     \\\\\n",
       "\\textbf{4800.0}           &    1.055e+04  &     4286.620     &     2.462  &         0.014        &     2151.440    &      1.9e+04     \\\\\n",
       "\\textbf{4802.0}           &    1.197e+04  &     4035.859     &     2.966  &         0.003        &     4059.365    &     1.99e+04     \\\\\n",
       "\\textbf{4807.0}           &    1.149e+04  &     3962.282     &     2.901  &         0.004        &     3727.073    &     1.93e+04     \\\\\n",
       "\\textbf{4839.0}           &   -1.368e+05  &     4706.996     &   -29.062  &         0.000        &    -1.46e+05    &    -1.28e+05     \\\\\n",
       "\\textbf{4843.0}           &   -3544.4727  &     5456.969     &    -0.650  &         0.516        &    -1.42e+04    &     7152.018     \\\\\n",
       "\\textbf{4881.0}           &    9402.2705  &     3445.968     &     2.728  &         0.006        &     2647.647    &     1.62e+04     \\\\\n",
       "\\textbf{4900.0}           &    9382.8163  &     4045.853     &     2.319  &         0.020        &     1452.327    &     1.73e+04     \\\\\n",
       "\\textbf{4926.0}           &    9212.0044  &     3448.176     &     2.672  &         0.008        &     2453.054    &      1.6e+04     \\\\\n",
       "\\textbf{4941.0}           &    9970.1650  &     3800.845     &     2.623  &         0.009        &     2519.929    &     1.74e+04     \\\\\n",
       "\\textbf{4961.0}           &   -6740.0949  &     3831.719     &    -1.759  &         0.079        &    -1.43e+04    &      770.659     \\\\\n",
       "\\textbf{4988.0}           &    1.766e+04  &     4298.290     &     4.110  &         0.000        &     9239.322    &     2.61e+04     \\\\\n",
       "\\textbf{4993.0}           &    1.286e+04  &     4536.766     &     2.835  &         0.005        &     3968.498    &     2.18e+04     \\\\\n",
       "\\textbf{5018.0}           &    5292.8001  &     3685.801     &     1.436  &         0.151        &    -1931.933    &     1.25e+04     \\\\\n",
       "\\textbf{5020.0}           &   -1494.8542  &     4969.243     &    -0.301  &         0.764        &    -1.12e+04    &     8245.620     \\\\\n",
       "\\textbf{5027.0}           &    6774.1739  &     3369.587     &     2.010  &         0.044        &      169.270    &     1.34e+04     \\\\\n",
       "\\textbf{5032.0}           &    1.108e+04  &     3841.583     &     2.884  &         0.004        &     3549.242    &     1.86e+04     \\\\\n",
       "\\textbf{5043.0}           &    6882.4486  &     3529.631     &     1.950  &         0.051        &      -36.166    &     1.38e+04     \\\\\n",
       "\\textbf{5046.0}           &   -2455.8570  &     3177.871     &    -0.773  &         0.440        &    -8684.969    &     3773.254     \\\\\n",
       "\\textbf{5047.0}           &    5.259e+04  &     4554.429     &    11.546  &         0.000        &     4.37e+04    &     6.15e+04     \\\\\n",
       "\\textbf{5065.0}           &    1.158e+04  &     4427.376     &     2.615  &         0.009        &     2898.115    &     2.03e+04     \\\\\n",
       "\\textbf{5071.0}           &    1.078e+04  &     4323.382     &     2.494  &         0.013        &     2310.134    &     1.93e+04     \\\\\n",
       "\\textbf{5073.0}           &    -2.02e+05  &     6401.840     &   -31.546  &         0.000        &    -2.14e+05    &    -1.89e+05     \\\\\n",
       "\\textbf{5087.0}           &    6088.8350  &     3522.160     &     1.729  &         0.084        &     -815.136    &      1.3e+04     \\\\\n",
       "\\textbf{5109.0}           &    1.217e+04  &     4304.921     &     2.826  &         0.005        &     3729.124    &     2.06e+04     \\\\\n",
       "\\textbf{5116.0}           &   -1193.1184  &     4145.651     &    -0.288  &         0.774        &    -9319.226    &     6932.989     \\\\\n",
       "\\textbf{5122.0}           &    6279.1585  &     3312.645     &     1.896  &         0.058        &     -214.130    &     1.28e+04     \\\\\n",
       "\\textbf{5134.0}           &    4252.6159  &     3634.799     &     1.170  &         0.242        &    -2872.144    &     1.14e+04     \\\\\n",
       "\\textbf{5142.0}           &    8303.3545  &     4310.795     &     1.926  &         0.054        &     -146.461    &     1.68e+04     \\\\\n",
       "\\textbf{5165.0}           &    7418.0199  &     4698.379     &     1.579  &         0.114        &    -1791.519    &     1.66e+04     \\\\\n",
       "\\textbf{5169.0}           &    2.099e+04  &     4031.196     &     5.207  &         0.000        &     1.31e+04    &     2.89e+04     \\\\\n",
       "\\textbf{5174.0}           &    8015.5017  &     3945.068     &     2.032  &         0.042        &      282.567    &     1.57e+04     \\\\\n",
       "\\textbf{5179.0}           &     1.14e+04  &     4114.840     &     2.771  &         0.006        &     3335.369    &     1.95e+04     \\\\\n",
       "\\textbf{5181.0}           &    1.224e+04  &     4311.848     &     2.838  &         0.005        &     3786.337    &     2.07e+04     \\\\\n",
       "\\textbf{5187.0}           &    1.224e+04  &     4519.231     &     2.708  &         0.007        &     3379.174    &     2.11e+04     \\\\\n",
       "\\textbf{5229.0}           &    3035.1962  &     3090.895     &     0.982  &         0.326        &    -3023.429    &     9093.822     \\\\\n",
       "\\textbf{5234.0}           &   -2691.0876  &     3571.956     &    -0.753  &         0.451        &    -9692.666    &     4310.491     \\\\\n",
       "\\textbf{5237.0}           &    1.093e+04  &     3947.487     &     2.769  &         0.006        &     3192.997    &     1.87e+04     \\\\\n",
       "\\textbf{5252.0}           &    1.027e+04  &     3622.943     &     2.835  &         0.005        &     3170.558    &     1.74e+04     \\\\\n",
       "\\textbf{5254.0}           &    1.069e+04  &     3659.795     &     2.920  &         0.004        &     3512.110    &     1.79e+04     \\\\\n",
       "\\textbf{5306.0}           &    8878.3146  &     3452.015     &     2.572  &         0.010        &     2111.840    &     1.56e+04     \\\\\n",
       "\\textbf{5338.0}           &    1.152e+04  &     3892.628     &     2.959  &         0.003        &     3886.700    &     1.91e+04     \\\\\n",
       "\\textbf{5377.0}           &    1.224e+04  &     4350.392     &     2.814  &         0.005        &     3713.543    &     2.08e+04     \\\\\n",
       "\\textbf{5439.0}           &    9178.7547  &     4056.182     &     2.263  &         0.024        &     1228.020    &     1.71e+04     \\\\\n",
       "\\textbf{5456.0}           &    1.282e+04  &     4549.255     &     2.818  &         0.005        &     3901.137    &     2.17e+04     \\\\\n",
       "\\textbf{5464.0}           &    1.027e+04  &     4237.773     &     2.423  &         0.015        &     1961.674    &     1.86e+04     \\\\\n",
       "\\textbf{5476.0}           &    1.204e+04  &     4595.709     &     2.620  &         0.009        &     3031.184    &      2.1e+04     \\\\\n",
       "\\textbf{5492.0}           &   -1.033e+04  &     3427.634     &    -3.015  &         0.003        &    -1.71e+04    &    -3615.409     \\\\\n",
       "\\textbf{5496.0}           &    1.046e+04  &     3768.357     &     2.776  &         0.006        &     3073.259    &     1.78e+04     \\\\\n",
       "\\textbf{5505.0}           &    1.179e+04  &     4246.204     &     2.777  &         0.005        &     3468.650    &     2.01e+04     \\\\\n",
       "\\textbf{5518.0}           &    9808.6268  &     4310.289     &     2.276  &         0.023        &     1359.804    &     1.83e+04     \\\\\n",
       "\\textbf{5520.0}           &    8554.6270  &     3689.535     &     2.319  &         0.020        &     1322.576    &     1.58e+04     \\\\\n",
       "\\textbf{5545.0}           &      1.2e+04  &     4089.836     &     2.933  &         0.003        &     3979.670    &        2e+04     \\\\\n",
       "\\textbf{5568.0}           &    1.447e+04  &     3861.174     &     3.747  &         0.000        &     6900.100    &      2.2e+04     \\\\\n",
       "\\textbf{5569.0}           &    1.217e+04  &     4187.067     &     2.906  &         0.004        &     3960.789    &     2.04e+04     \\\\\n",
       "\\textbf{5578.0}           &    1.152e+04  &     4034.562     &     2.856  &         0.004        &     3615.039    &     1.94e+04     \\\\\n",
       "\\textbf{5581.0}           &    1.133e+04  &     3917.795     &     2.892  &         0.004        &     3649.874    &      1.9e+04     \\\\\n",
       "\\textbf{5589.0}           &    3608.1835  &     3097.460     &     1.165  &         0.244        &    -2463.310    &     9679.677     \\\\\n",
       "\\textbf{5597.0}           &    1.356e+04  &     4297.825     &     3.156  &         0.002        &     5140.132    &      2.2e+04     \\\\\n",
       "\\textbf{5606.0}           &   -2.837e+04  &     3149.020     &    -9.010  &         0.000        &    -3.45e+04    &    -2.22e+04     \\\\\n",
       "\\textbf{5639.0}           &    1.289e+04  &     4328.275     &     2.978  &         0.003        &     4407.076    &     2.14e+04     \\\\\n",
       "\\textbf{5667.0}           &    7881.4811  &     4801.503     &     1.641  &         0.101        &    -1530.197    &     1.73e+04     \\\\\n",
       "\\textbf{5690.0}           &    1.195e+04  &     4008.021     &     2.981  &         0.003        &     4089.735    &     1.98e+04     \\\\\n",
       "\\textbf{5709.0}           &    1.123e+04  &     4100.696     &     2.739  &         0.006        &     3192.150    &     1.93e+04     \\\\\n",
       "\\textbf{5726.0}           &    1.086e+04  &     4109.937     &     2.641  &         0.008        &     2798.903    &     1.89e+04     \\\\\n",
       "\\textbf{5764.0}           &    9713.0214  &     3363.224     &     2.888  &         0.004        &     3120.589    &     1.63e+04     \\\\\n",
       "\\textbf{5772.0}           &    1.116e+04  &     3785.991     &     2.948  &         0.003        &     3740.005    &     1.86e+04     \\\\\n",
       "\\textbf{5860.0}           &   -2.048e+04  &     3302.986     &    -6.200  &         0.000        &     -2.7e+04    &     -1.4e+04     \\\\\n",
       "\\textbf{5878.0}           &    1.323e+04  &     3467.510     &     3.814  &         0.000        &     6429.968    &        2e+04     \\\\\n",
       "\\textbf{5903.0}           &    5588.3979  &     3992.481     &     1.400  &         0.162        &    -2237.473    &     1.34e+04     \\\\\n",
       "\\textbf{5905.0}           &    8895.7752  &     3610.913     &     2.464  &         0.014        &     1817.835    &      1.6e+04     \\\\\n",
       "\\textbf{5959.0}           &    6280.2646  &     3454.673     &     1.818  &         0.069        &     -491.421    &     1.31e+04     \\\\\n",
       "\\textbf{6008.0}           &    2.856e+04  &     3084.708     &     9.258  &         0.000        &     2.25e+04    &     3.46e+04     \\\\\n",
       "\\textbf{6034.0}           &    8870.7729  &     3711.642     &     2.390  &         0.017        &     1595.389    &     1.61e+04     \\\\\n",
       "\\textbf{6035.0}           &    3860.7482  &     4533.466     &     0.852  &         0.394        &    -5025.536    &     1.27e+04     \\\\\n",
       "\\textbf{6036.0}           &     981.9694  &     3092.145     &     0.318  &         0.751        &    -5079.106    &     7043.045     \\\\\n",
       "\\textbf{6039.0}           &    1.112e+04  &     3962.475     &     2.806  &         0.005        &     3350.540    &     1.89e+04     \\\\\n",
       "\\textbf{6044.0}           &    1.228e+04  &     4576.202     &     2.684  &         0.007        &     3311.930    &     2.13e+04     \\\\\n",
       "\\textbf{6066.0}           &   -2.641e+04  &     4513.111     &    -5.851  &         0.000        &    -3.53e+04    &    -1.76e+04     \\\\\n",
       "\\textbf{6078.0}           &    1.134e+04  &     3843.392     &     2.950  &         0.003        &     3805.519    &     1.89e+04     \\\\\n",
       "\\textbf{6081.0}           &   -1.015e+04  &     3315.955     &    -3.062  &         0.002        &    -1.67e+04    &    -3654.074     \\\\\n",
       "\\textbf{60893.0}          &   -6784.4217  &     6312.243     &    -1.075  &         0.282        &    -1.92e+04    &     5588.537     \\\\\n",
       "\\textbf{6097.0}           &    1.162e+04  &     4377.402     &     2.655  &         0.008        &     3043.600    &     2.02e+04     \\\\\n",
       "\\textbf{6102.0}           &    1.069e+04  &     4037.788     &     2.646  &         0.008        &     2770.560    &     1.86e+04     \\\\\n",
       "\\textbf{6104.0}           &    1028.7449  &     3549.960     &     0.290  &         0.772        &    -5929.718    &     7987.208     \\\\\n",
       "\\textbf{6109.0}           &    1854.4705  &     3067.203     &     0.605  &         0.545        &    -4157.716    &     7866.657     \\\\\n",
       "\\textbf{6127.0}           &    5567.8581  &     3841.809     &     1.449  &         0.147        &    -1962.673    &     1.31e+04     \\\\\n",
       "\\textbf{61552.0}          &   -3961.2146  &     4323.316     &    -0.916  &         0.360        &    -1.24e+04    &     4513.143     \\\\\n",
       "\\textbf{6158.0}           &    7855.3039  &     3597.807     &     2.183  &         0.029        &      803.053    &     1.49e+04     \\\\\n",
       "\\textbf{6171.0}           &    1.098e+04  &     3902.421     &     2.814  &         0.005        &     3333.242    &     1.86e+04     \\\\\n",
       "\\textbf{61780.0}          &    1.015e+04  &     5314.163     &     1.909  &         0.056        &     -270.110    &     2.06e+04     \\\\\n",
       "\\textbf{6207.0}           &    1.081e+04  &     4080.983     &     2.648  &         0.008        &     2807.745    &     1.88e+04     \\\\\n",
       "\\textbf{6214.0}           &    1.179e+04  &     3902.095     &     3.020  &         0.003        &     4136.961    &     1.94e+04     \\\\\n",
       "\\textbf{6216.0}           &    1.181e+04  &     4533.594     &     2.605  &         0.009        &     2922.193    &     2.07e+04     \\\\\n",
       "\\textbf{62221.0}          &    1.167e+04  &     5095.014     &     2.290  &         0.022        &     1679.299    &     2.17e+04     \\\\\n",
       "\\textbf{6259.0}           &    4520.9981  &     4901.812     &     0.922  &         0.356        &    -5087.301    &     1.41e+04     \\\\\n",
       "\\textbf{62599.0}          &   -2.423e+04  &     5073.691     &    -4.775  &         0.000        &    -3.42e+04    &    -1.43e+04     \\\\\n",
       "\\textbf{6266.0}           &    7497.8711  &     3584.960     &     2.091  &         0.037        &      470.802    &     1.45e+04     \\\\\n",
       "\\textbf{6268.0}           &    4676.5986  &     3503.161     &     1.335  &         0.182        &    -2190.131    &     1.15e+04     \\\\\n",
       "\\textbf{6288.0}           &    1.031e+04  &     3585.786     &     2.875  &         0.004        &     3281.797    &     1.73e+04     \\\\\n",
       "\\textbf{6297.0}           &    1.153e+04  &     3858.893     &     2.988  &         0.003        &     3967.438    &     1.91e+04     \\\\\n",
       "\\textbf{6307.0}           &   -1.112e+04  &     4115.238     &    -2.702  &         0.007        &    -1.92e+04    &    -3053.456     \\\\\n",
       "\\textbf{6313.0}           &    1.123e+04  &     5130.808     &     2.189  &         0.029        &     1173.829    &     2.13e+04     \\\\\n",
       "\\textbf{6314.0}           &    1.156e+04  &     4309.943     &     2.682  &         0.007        &     3111.577    &        2e+04     \\\\\n",
       "\\textbf{6326.0}           &    5472.4704  &     3173.544     &     1.724  &         0.085        &     -748.160    &     1.17e+04     \\\\\n",
       "\\textbf{6349.0}           &    1.062e+04  &     3838.915     &     2.766  &         0.006        &     3095.272    &     1.81e+04     \\\\\n",
       "\\textbf{6357.0}           &    1.211e+04  &     4304.418     &     2.814  &         0.005        &     3676.182    &     2.06e+04     \\\\\n",
       "\\textbf{6375.0}           &    1.628e+04  &     4109.560     &     3.962  &         0.000        &     8228.526    &     2.43e+04     \\\\\n",
       "\\textbf{6376.0}           &    1.164e+04  &     4260.650     &     2.732  &         0.006        &     3290.644    &        2e+04     \\\\\n",
       "\\textbf{6379.0}           &      81.8377  &     6900.673     &     0.012  &         0.991        &    -1.34e+04    &     1.36e+04     \\\\\n",
       "\\textbf{6386.0}           &    1.186e+04  &     3968.374     &     2.988  &         0.003        &     4078.322    &     1.96e+04     \\\\\n",
       "\\textbf{6403.0}           &    6876.8959  &     3906.829     &     1.760  &         0.078        &     -781.085    &     1.45e+04     \\\\\n",
       "\\textbf{6410.0}           &    1.278e+04  &     4471.768     &     2.858  &         0.004        &     4013.282    &     2.15e+04     \\\\\n",
       "\\textbf{6416.0}           &    4624.2742  &     3933.435     &     1.176  &         0.240        &    -3085.858    &     1.23e+04     \\\\\n",
       "\\textbf{6424.0}           &    1.152e+04  &     4145.390     &     2.778  &         0.005        &     3390.834    &     1.96e+04     \\\\\n",
       "\\textbf{6433.0}           &    1.124e+04  &     4483.646     &     2.506  &         0.012        &     2447.421    &        2e+04     \\\\\n",
       "\\textbf{6435.0}           &    1.179e+04  &     3373.988     &     3.494  &         0.000        &     5173.680    &     1.84e+04     \\\\\n",
       "\\textbf{6492.0}           &    8504.2651  &     3427.252     &     2.481  &         0.013        &     1786.329    &     1.52e+04     \\\\\n",
       "\\textbf{6497.0}           &    1206.9563  &     5114.543     &     0.236  &         0.813        &    -8818.328    &     1.12e+04     \\\\\n",
       "\\textbf{6500.0}           &    8538.3757  &     8012.486     &     1.066  &         0.287        &    -7167.319    &     2.42e+04     \\\\\n",
       "\\textbf{6509.0}           &    1.061e+04  &     3733.791     &     2.840  &         0.005        &     3286.955    &     1.79e+04     \\\\\n",
       "\\textbf{6527.0}           &    1.237e+04  &     4453.015     &     2.777  &         0.005        &     3637.275    &     2.11e+04     \\\\\n",
       "\\textbf{6528.0}           &    1.003e+04  &     4186.629     &     2.396  &         0.017        &     1824.608    &     1.82e+04     \\\\\n",
       "\\textbf{6531.0}           &    2584.8064  &     3530.625     &     0.732  &         0.464        &    -4335.757    &     9505.370     \\\\\n",
       "\\textbf{6532.0}           &    7172.5703  &     3538.230     &     2.027  &         0.043        &      237.099    &     1.41e+04     \\\\\n",
       "\\textbf{6543.0}           &    1.209e+04  &     4213.207     &     2.869  &         0.004        &     3827.962    &     2.03e+04     \\\\\n",
       "\\textbf{6548.0}           &    1.163e+04  &     4219.948     &     2.756  &         0.006        &     3358.381    &     1.99e+04     \\\\\n",
       "\\textbf{6550.0}           &     1.16e+04  &     4153.088     &     2.793  &         0.005        &     3459.857    &     1.97e+04     \\\\\n",
       "\\textbf{6552.0}           &    1.155e+04  &     4402.463     &     2.623  &         0.009        &     2916.465    &     2.02e+04     \\\\\n",
       "\\textbf{6565.0}           &    7598.5452  &     3657.372     &     2.078  &         0.038        &      429.538    &     1.48e+04     \\\\\n",
       "\\textbf{6571.0}           &    1.145e+04  &     3969.600     &     2.883  &         0.004        &     3665.132    &     1.92e+04     \\\\\n",
       "\\textbf{6573.0}           &    1.116e+04  &     3858.424     &     2.893  &         0.004        &     3598.092    &     1.87e+04     \\\\\n",
       "\\textbf{6641.0}           &    8286.5642  &     5855.445     &     1.415  &         0.157        &    -3191.001    &     1.98e+04     \\\\\n",
       "\\textbf{6649.0}           &    1.269e+04  &     4327.620     &     2.933  &         0.003        &     4208.392    &     2.12e+04     \\\\\n",
       "\\textbf{6730.0}           &    5732.9408  &     3915.766     &     1.464  &         0.143        &    -1942.558    &     1.34e+04     \\\\\n",
       "\\textbf{6731.0}           &    8855.9574  &     3487.401     &     2.539  &         0.011        &     2020.120    &     1.57e+04     \\\\\n",
       "\\textbf{6742.0}           &    1.236e+04  &     5705.178     &     2.167  &         0.030        &     1180.548    &     2.35e+04     \\\\\n",
       "\\textbf{6745.0}           &    1.229e+04  &     4185.573     &     2.936  &         0.003        &     4086.024    &     2.05e+04     \\\\\n",
       "\\textbf{6756.0}           &    1.155e+04  &     4384.406     &     2.635  &         0.008        &     2957.935    &     2.01e+04     \\\\\n",
       "\\textbf{6765.0}           &   -5696.3086  &     3216.442     &    -1.771  &         0.077        &     -1.2e+04    &      608.408     \\\\\n",
       "\\textbf{6768.0}           &    1.353e+04  &     4604.794     &     2.938  &         0.003        &     4500.806    &     2.26e+04     \\\\\n",
       "\\textbf{6774.0}           &   -1.616e+04  &     3861.019     &    -4.185  &         0.000        &    -2.37e+04    &    -8591.261     \\\\\n",
       "\\textbf{6797.0}           &    1.235e+04  &     4643.503     &     2.661  &         0.008        &     3252.897    &     2.15e+04     \\\\\n",
       "\\textbf{6803.0}           &    1.176e+04  &     4292.725     &     2.740  &         0.006        &     3347.536    &     2.02e+04     \\\\\n",
       "\\textbf{6821.0}           &    1.108e+04  &     3990.375     &     2.777  &         0.005        &     3260.672    &     1.89e+04     \\\\\n",
       "\\textbf{6830.0}           &    9687.5740  &     4183.001     &     2.316  &         0.021        &     1488.254    &     1.79e+04     \\\\\n",
       "\\textbf{6845.0}           &    9083.8144  &     3443.693     &     2.638  &         0.008        &     2333.650    &     1.58e+04     \\\\\n",
       "\\textbf{6848.0}           &     1.05e+04  &     4530.801     &     2.317  &         0.021        &     1615.554    &     1.94e+04     \\\\\n",
       "\\textbf{6873.0}           &    7561.7359  &     4168.816     &     1.814  &         0.070        &     -609.779    &     1.57e+04     \\\\\n",
       "\\textbf{6900.0}           &    9837.8240  &     3459.183     &     2.844  &         0.004        &     3057.298    &     1.66e+04     \\\\\n",
       "\\textbf{6908.0}           &    9832.6262  &     3451.803     &     2.849  &         0.004        &     3066.567    &     1.66e+04     \\\\\n",
       "\\textbf{6994.0}           &    9440.4454  &     4191.065     &     2.253  &         0.024        &     1225.319    &     1.77e+04     \\\\\n",
       "\\textbf{7045.0}           &    1472.8365  &     4667.886     &     0.316  &         0.752        &    -7676.931    &     1.06e+04     \\\\\n",
       "\\textbf{7065.0}           &    1.662e+04  &     3626.584     &     4.583  &         0.000        &     9510.311    &     2.37e+04     \\\\\n",
       "\\textbf{7085.0}           &     1.37e+04  &     3754.828     &     3.648  &         0.000        &     6338.237    &     2.11e+04     \\\\\n",
       "\\textbf{7107.0}           &    1.053e+04  &     3678.034     &     2.863  &         0.004        &     3321.390    &     1.77e+04     \\\\\n",
       "\\textbf{7116.0}           &    1.229e+04  &     4485.757     &     2.740  &         0.006        &     3498.969    &     2.11e+04     \\\\\n",
       "\\textbf{7117.0}           &    1.292e+04  &     5085.161     &     2.540  &         0.011        &     2950.954    &     2.29e+04     \\\\\n",
       "\\textbf{7121.0}           &    1.058e+04  &     3994.613     &     2.649  &         0.008        &     2751.203    &     1.84e+04     \\\\\n",
       "\\textbf{7127.0}           &    8326.5385  &     4645.531     &     1.792  &         0.073        &     -779.411    &     1.74e+04     \\\\\n",
       "\\textbf{7139.0}           &    1.098e+04  &     3887.701     &     2.825  &         0.005        &     3362.791    &     1.86e+04     \\\\\n",
       "\\textbf{7146.0}           &    1.186e+04  &     4092.418     &     2.898  &         0.004        &     3839.427    &     1.99e+04     \\\\\n",
       "\\textbf{7163.0}           &    1.495e+04  &     4159.258     &     3.594  &         0.000        &     6793.822    &     2.31e+04     \\\\\n",
       "\\textbf{7180.0}           &    8249.7034  &     3588.141     &     2.299  &         0.022        &     1216.400    &     1.53e+04     \\\\\n",
       "\\textbf{7183.0}           &    8941.8698  &     3436.226     &     2.602  &         0.009        &     2206.343    &     1.57e+04     \\\\\n",
       "\\textbf{7228.0}           &    1.903e+04  &     4269.428     &     4.457  &         0.000        &     1.07e+04    &     2.74e+04     \\\\\n",
       "\\textbf{7232.0}           &    9413.4580  &     5148.564     &     1.828  &         0.068        &     -678.513    &     1.95e+04     \\\\\n",
       "\\textbf{7250.0}           &    7746.3050  &     4039.279     &     1.918  &         0.055        &     -171.298    &     1.57e+04     \\\\\n",
       "\\textbf{7257.0}           &    2.697e+04  &     3994.030     &     6.753  &         0.000        &     1.91e+04    &     3.48e+04     \\\\\n",
       "\\textbf{7260.0}           &    1.122e+04  &     3806.476     &     2.948  &         0.003        &     3759.564    &     1.87e+04     \\\\\n",
       "\\textbf{7267.0}           &    1.128e+04  &     4500.943     &     2.506  &         0.012        &     2456.332    &     2.01e+04     \\\\\n",
       "\\textbf{7268.0}           &    -603.4573  &     5189.866     &    -0.116  &         0.907        &    -1.08e+04    &     9569.471     \\\\\n",
       "\\textbf{7281.0}           &    1.212e+04  &     4816.206     &     2.517  &         0.012        &     2679.504    &     2.16e+04     \\\\\n",
       "\\textbf{7291.0}           &    9727.9780  &     3461.883     &     2.810  &         0.005        &     2942.159    &     1.65e+04     \\\\\n",
       "\\textbf{7343.0}           &    3150.4995  &     3700.356     &     0.851  &         0.395        &    -4102.762    &     1.04e+04     \\\\\n",
       "\\textbf{7346.0}           &    2872.0454  &     3854.823     &     0.745  &         0.456        &    -4683.996    &     1.04e+04     \\\\\n",
       "\\textbf{7401.0}           &    1.167e+04  &     3948.587     &     2.955  &         0.003        &     3930.063    &     1.94e+04     \\\\\n",
       "\\textbf{7409.0}           &    1.069e+04  &     3643.036     &     2.934  &         0.003        &     3549.555    &     1.78e+04     \\\\\n",
       "\\textbf{7420.0}           &    7804.1713  &     3393.969     &     2.299  &         0.021        &     1151.475    &     1.45e+04     \\\\\n",
       "\\textbf{7435.0}           &    7570.2808  &     3556.477     &     2.129  &         0.033        &      599.044    &     1.45e+04     \\\\\n",
       "\\textbf{7466.0}           &    1.007e+04  &     4248.285     &     2.370  &         0.018        &     1741.721    &     1.84e+04     \\\\\n",
       "\\textbf{7486.0}           &      24.5930  &     4677.748     &     0.005  &         0.996        &    -9144.506    &     9193.692     \\\\\n",
       "\\textbf{7503.0}           &    1.109e+04  &     4972.750     &     2.230  &         0.026        &     1340.662    &     2.08e+04     \\\\\n",
       "\\textbf{7506.0}           &    1.183e+04  &     3608.116     &     3.278  &         0.001        &     4753.322    &     1.89e+04     \\\\\n",
       "\\textbf{7537.0}           &    1.125e+04  &     4253.619     &     2.644  &         0.008        &     2907.920    &     1.96e+04     \\\\\n",
       "\\textbf{7549.0}           &    9586.2867  &     3477.660     &     2.757  &         0.006        &     2769.543    &     1.64e+04     \\\\\n",
       "\\textbf{7554.0}           &      1.1e+04  &     4185.259     &     2.628  &         0.009        &     2793.298    &     1.92e+04     \\\\\n",
       "\\textbf{7557.0}           &    9412.1893  &     3497.452     &     2.691  &         0.007        &     2556.650    &     1.63e+04     \\\\\n",
       "\\textbf{7585.0}           &   -1.845e+04  &     3426.054     &    -5.384  &         0.000        &    -2.52e+04    &    -1.17e+04     \\\\\n",
       "\\textbf{7602.0}           &     1.07e+04  &     4003.464     &     2.673  &         0.008        &     2853.478    &     1.85e+04     \\\\\n",
       "\\textbf{7620.0}           &    7495.0260  &     4650.077     &     1.612  &         0.107        &    -1619.835    &     1.66e+04     \\\\\n",
       "\\textbf{7636.0}           &    1.115e+04  &     4114.374     &     2.710  &         0.007        &     3084.339    &     1.92e+04     \\\\\n",
       "\\textbf{7646.0}           &    1.143e+04  &     4154.010     &     2.751  &         0.006        &     3285.792    &     1.96e+04     \\\\\n",
       "\\textbf{7658.0}           &    8500.3762  &     3369.414     &     2.523  &         0.012        &     1895.811    &     1.51e+04     \\\\\n",
       "\\textbf{7683.0}           &    1.233e+04  &     4279.500     &     2.882  &         0.004        &     3945.996    &     2.07e+04     \\\\\n",
       "\\textbf{7685.0}           &    1.085e+04  &     3860.880     &     2.809  &         0.005        &     3279.123    &     1.84e+04     \\\\\n",
       "\\textbf{7692.0}           &    6148.3863  &     3351.672     &     1.834  &         0.067        &     -421.401    &     1.27e+04     \\\\\n",
       "\\textbf{7762.0}           &    1.157e+04  &     3823.181     &     3.027  &         0.002        &     4078.412    &     1.91e+04     \\\\\n",
       "\\textbf{7772.0}           &   -2312.9745  &     3060.935     &    -0.756  &         0.450        &    -8312.873    &     3686.924     \\\\\n",
       "\\textbf{7773.0}           &    1.097e+04  &     3933.414     &     2.790  &         0.005        &     3264.037    &     1.87e+04     \\\\\n",
       "\\textbf{7777.0}           &    6918.6798  &     3337.848     &     2.073  &         0.038        &      375.988    &     1.35e+04     \\\\\n",
       "\\textbf{7835.0}           &    1.213e+04  &     4070.265     &     2.980  &         0.003        &     4149.874    &     2.01e+04     \\\\\n",
       "\\textbf{7873.0}           &     356.4399  &     4267.413     &     0.084  &         0.933        &    -8008.341    &     8721.221     \\\\\n",
       "\\textbf{7883.0}           &    9144.6351  &     3447.627     &     2.652  &         0.008        &     2386.760    &     1.59e+04     \\\\\n",
       "\\textbf{7904.0}           &    8358.5703  &     3491.318     &     2.394  &         0.017        &     1515.054    &     1.52e+04     \\\\\n",
       "\\textbf{7906.0}           &    1.463e+04  &     4333.577     &     3.376  &         0.001        &     6135.454    &     2.31e+04     \\\\\n",
       "\\textbf{7921.0}           &    1.169e+04  &     3733.249     &     3.133  &         0.002        &     4376.832    &      1.9e+04     \\\\\n",
       "\\textbf{7923.0}           &    1.057e+04  &     4161.944     &     2.540  &         0.011        &     2415.236    &     1.87e+04     \\\\\n",
       "\\textbf{7935.0}           &    8646.6988  &     3615.011     &     2.392  &         0.017        &     1560.727    &     1.57e+04     \\\\\n",
       "\\textbf{7938.0}           &    9733.8663  &     3700.483     &     2.630  &         0.009        &     2480.356    &      1.7e+04     \\\\\n",
       "\\textbf{7985.0}           &   -1.135e+04  &     3160.766     &    -3.590  &         0.000        &    -1.75e+04    &    -5150.793     \\\\\n",
       "\\textbf{8014.0}           &    9609.8260  &     4009.205     &     2.397  &         0.017        &     1751.173    &     1.75e+04     \\\\\n",
       "\\textbf{8030.0}           &    1.263e+04  &     4422.064     &     2.856  &         0.004        &     3962.928    &     2.13e+04     \\\\\n",
       "\\textbf{8046.0}           &     670.6900  &     4233.990     &     0.158  &         0.874        &    -7628.576    &     8969.956     \\\\\n",
       "\\textbf{8047.0}           &    1.106e+04  &     4258.783     &     2.597  &         0.009        &     2710.084    &     1.94e+04     \\\\\n",
       "\\textbf{8062.0}           &    9460.3233  &     3665.001     &     2.581  &         0.010        &     2276.363    &     1.66e+04     \\\\\n",
       "\\textbf{8068.0}           &   -4512.5573  &     3672.896     &    -1.229  &         0.219        &    -1.17e+04    &     2686.878     \\\\\n",
       "\\textbf{8087.0}           &   -3935.6363  &     4263.718     &    -0.923  &         0.356        &    -1.23e+04    &     4421.901     \\\\\n",
       "\\textbf{8095.0}           &    1.144e+04  &     3816.602     &     2.996  &         0.003        &     3954.389    &     1.89e+04     \\\\\n",
       "\\textbf{8096.0}           &    1.172e+04  &     4341.522     &     2.699  &         0.007        &     3209.639    &     2.02e+04     \\\\\n",
       "\\textbf{8109.0}           &    1.179e+04  &     3980.577     &     2.963  &         0.003        &     3990.859    &     1.96e+04     \\\\\n",
       "\\textbf{8123.0}           &    7885.7920  &     3619.315     &     2.179  &         0.029        &      791.382    &      1.5e+04     \\\\\n",
       "\\textbf{8150.0}           &    1.221e+04  &     4197.973     &     2.907  &         0.004        &     3976.711    &     2.04e+04     \\\\\n",
       "\\textbf{8163.0}           &    9725.3249  &     3553.388     &     2.737  &         0.006        &     2760.142    &     1.67e+04     \\\\\n",
       "\\textbf{8176.0}           &    4675.4601  &     5297.052     &     0.883  &         0.377        &    -5707.569    &     1.51e+04     \\\\\n",
       "\\textbf{8202.0}           &    9134.0857  &     3514.455     &     2.599  &         0.009        &     2245.219    &      1.6e+04     \\\\\n",
       "\\textbf{8214.0}           &    8004.1408  &     3509.355     &     2.281  &         0.023        &     1125.269    &     1.49e+04     \\\\\n",
       "\\textbf{8215.0}           &    5917.2501  &     3535.404     &     1.674  &         0.094        &    -1012.681    &     1.28e+04     \\\\\n",
       "\\textbf{8219.0}           &    1.234e+04  &     4456.469     &     2.768  &         0.006        &     3600.042    &     2.11e+04     \\\\\n",
       "\\textbf{8247.0}           &    7192.2790  &     3412.823     &     2.107  &         0.035        &      502.625    &     1.39e+04     \\\\\n",
       "\\textbf{8253.0}           &   -3836.5289  &     3593.583     &    -1.068  &         0.286        &    -1.09e+04    &     3207.442     \\\\\n",
       "\\textbf{8290.0}           &    8767.0517  &     3867.407     &     2.267  &         0.023        &     1186.344    &     1.63e+04     \\\\\n",
       "\\textbf{8293.0}           &    9972.3254  &     3490.060     &     2.857  &         0.004        &     3131.276    &     1.68e+04     \\\\\n",
       "\\textbf{8304.0}           &    1.195e+04  &     3888.289     &     3.073  &         0.002        &     4327.833    &     1.96e+04     \\\\\n",
       "\\textbf{8334.0}           &    1.116e+04  &     4621.997     &     2.415  &         0.016        &     2100.172    &     2.02e+04     \\\\\n",
       "\\textbf{8348.0}           &    1.145e+04  &     3858.042     &     2.968  &         0.003        &     3889.367    &      1.9e+04     \\\\\n",
       "\\textbf{8357.0}           &    1.096e+04  &     3735.570     &     2.933  &         0.003        &     3633.157    &     1.83e+04     \\\\\n",
       "\\textbf{8358.0}           &    9127.7852  &     3485.587     &     2.619  &         0.009        &     2295.503    &      1.6e+04     \\\\\n",
       "\\textbf{8446.0}           &    -871.8342  &     3930.179     &    -0.222  &         0.824        &    -8575.584    &     6831.916     \\\\\n",
       "\\textbf{8460.0}           &    1.298e+04  &     4816.729     &     2.694  &         0.007        &     3536.325    &     2.24e+04     \\\\\n",
       "\\textbf{8463.0}           &     1.09e+04  &     3994.968     &     2.727  &         0.006        &     3064.933    &     1.87e+04     \\\\\n",
       "\\textbf{8479.0}           &    1.081e+04  &     4473.836     &     2.416  &         0.016        &     2039.046    &     1.96e+04     \\\\\n",
       "\\textbf{8530.0}           &    1.763e+04  &     4105.796     &     4.295  &         0.000        &     9585.083    &     2.57e+04     \\\\\n",
       "\\textbf{8536.0}           &    7532.6032  &     3354.005     &     2.246  &         0.025        &      958.242    &     1.41e+04     \\\\\n",
       "\\textbf{8543.0}           &    3.172e+04  &     4342.018     &     7.305  &         0.000        &     2.32e+04    &     4.02e+04     \\\\\n",
       "\\textbf{8549.0}           &     627.9007  &     3401.090     &     0.185  &         0.854        &    -6038.754    &     7294.555     \\\\\n",
       "\\textbf{8551.0}           &    1.199e+04  &     4342.729     &     2.761  &         0.006        &     3477.589    &     2.05e+04     \\\\\n",
       "\\textbf{8559.0}           &    7047.3480  &     4411.585     &     1.597  &         0.110        &    -1600.032    &     1.57e+04     \\\\\n",
       "\\textbf{8573.0}           &     166.0404  &     5267.575     &     0.032  &         0.975        &    -1.02e+04    &     1.05e+04     \\\\\n",
       "\\textbf{8606.0}           &    1.223e+04  &     3676.982     &     3.327  &         0.001        &     5024.870    &     1.94e+04     \\\\\n",
       "\\textbf{8607.0}           &    1.211e+04  &     4436.509     &     2.730  &         0.006        &     3415.173    &     2.08e+04     \\\\\n",
       "\\textbf{8648.0}           &    1.082e+04  &     3758.601     &     2.878  &         0.004        &     3448.081    &     1.82e+04     \\\\\n",
       "\\textbf{8657.0}           &    4476.0239  &     3604.525     &     1.242  &         0.214        &    -2589.395    &     1.15e+04     \\\\\n",
       "\\textbf{8675.0}           &    1.141e+04  &     5298.339     &     2.153  &         0.031        &     1023.306    &     2.18e+04     \\\\\n",
       "\\textbf{8681.0}           &    6118.3062  &     3257.302     &     1.878  &         0.060        &     -266.503    &     1.25e+04     \\\\\n",
       "\\textbf{8687.0}           &    8405.6270  &     3595.904     &     2.338  &         0.019        &     1357.107    &     1.55e+04     \\\\\n",
       "\\textbf{8692.0}           &    8839.3495  &     3489.005     &     2.533  &         0.011        &     2000.368    &     1.57e+04     \\\\\n",
       "\\textbf{8699.0}           &    1.155e+04  &     3921.233     &     2.945  &         0.003        &     3861.315    &     1.92e+04     \\\\\n",
       "\\textbf{8717.0}           &     1.22e+04  &     4182.274     &     2.918  &         0.004        &     4006.968    &     2.04e+04     \\\\\n",
       "\\textbf{8759.0}           &    6244.1433  &     4396.099     &     1.420  &         0.156        &    -2372.881    &     1.49e+04     \\\\\n",
       "\\textbf{8762.0}           &    1.061e+04  &     3657.755     &     2.902  &         0.004        &     3443.457    &     1.78e+04     \\\\\n",
       "\\textbf{8819.0}           &    1.286e+04  &     4613.574     &     2.787  &         0.005        &     3813.149    &     2.19e+04     \\\\\n",
       "\\textbf{8850.0}           &    1.138e+04  &     3885.598     &     2.930  &         0.003        &     3766.556    &      1.9e+04     \\\\\n",
       "\\textbf{8852.0}           &    1.174e+04  &     3947.181     &     2.974  &         0.003        &     4003.634    &     1.95e+04     \\\\\n",
       "\\textbf{8859.0}           &    1.188e+04  &     4431.693     &     2.680  &         0.007        &     3191.641    &     2.06e+04     \\\\\n",
       "\\textbf{8867.0}           &    4059.6058  &     3642.118     &     1.115  &         0.265        &    -3079.500    &     1.12e+04     \\\\\n",
       "\\textbf{8881.0}           &    9861.0293  &     3659.118     &     2.695  &         0.007        &     2688.601    &      1.7e+04     \\\\\n",
       "\\textbf{8958.0}           &    8144.0343  &     3788.534     &     2.150  &         0.032        &      717.930    &     1.56e+04     \\\\\n",
       "\\textbf{8972.0}           &   -1.264e+04  &     3291.484     &    -3.841  &         0.000        &    -1.91e+04    &    -6190.724     \\\\\n",
       "\\textbf{8990.0}           &    -924.1857  &     3807.903     &    -0.243  &         0.808        &    -8388.256    &     6539.885     \\\\\n",
       "\\textbf{9004.0}           &    1.229e+04  &     4462.181     &     2.754  &         0.006        &     3542.360    &      2.1e+04     \\\\\n",
       "\\textbf{9016.0}           &    8958.7502  &     3536.521     &     2.533  &         0.011        &     2026.629    &     1.59e+04     \\\\\n",
       "\\textbf{9048.0}           &    8273.3416  &     3313.001     &     2.497  &         0.013        &     1779.354    &     1.48e+04     \\\\\n",
       "\\textbf{9051.0}           &    1755.5094  &     3880.492     &     0.452  &         0.651        &    -5850.847    &     9361.866     \\\\\n",
       "\\textbf{9071.0}           &    8934.2338  &     3656.665     &     2.443  &         0.015        &     1766.613    &     1.61e+04     \\\\\n",
       "\\textbf{9112.0}           &    6692.6708  &     3511.189     &     1.906  &         0.057        &     -189.795    &     1.36e+04     \\\\\n",
       "\\textbf{9114.0}           &    6257.3804  &     3488.998     &     1.793  &         0.073        &     -581.587    &     1.31e+04     \\\\\n",
       "\\textbf{9132.0}           &    1.125e+04  &     5511.232     &     2.042  &         0.041        &      451.850    &     2.21e+04     \\\\\n",
       "\\textbf{9173.0}           &    1.073e+04  &     3920.386     &     2.737  &         0.006        &     3047.326    &     1.84e+04     \\\\\n",
       "\\textbf{9180.0}           &    1.213e+04  &     4299.920     &     2.821  &         0.005        &     3702.923    &     2.06e+04     \\\\\n",
       "\\textbf{9186.0}           &    1.049e+04  &     3674.660     &     2.853  &         0.004        &     3282.112    &     1.77e+04     \\\\\n",
       "\\textbf{9191.0}           &    7285.4719  &     4336.542     &     1.680  &         0.093        &    -1214.812    &     1.58e+04     \\\\\n",
       "\\textbf{9216.0}           &    4558.0369  &     3153.822     &     1.445  &         0.148        &    -1623.935    &     1.07e+04     \\\\\n",
       "\\textbf{9217.0}           &    1093.0247  &     3038.271     &     0.360  &         0.719        &    -4862.450    &     7048.499     \\\\\n",
       "\\textbf{9225.0}           &    1.198e+04  &     3909.157     &     3.065  &         0.002        &     4318.137    &     1.96e+04     \\\\\n",
       "\\textbf{9230.0}           &    1.177e+04  &     4751.892     &     2.478  &         0.013        &     2459.741    &     2.11e+04     \\\\\n",
       "\\textbf{9259.0}           &    1.289e+04  &     4560.427     &     2.826  &         0.005        &     3947.042    &     2.18e+04     \\\\\n",
       "\\textbf{9293.0}           &    1.204e+04  &     4117.970     &     2.925  &         0.003        &     3972.425    &     2.01e+04     \\\\\n",
       "\\textbf{9299.0}           &    7289.7440  &     3592.474     &     2.029  &         0.042        &      247.946    &     1.43e+04     \\\\\n",
       "\\textbf{9308.0}           &    5144.3498  &     4655.985     &     1.105  &         0.269        &    -3982.090    &     1.43e+04     \\\\\n",
       "\\textbf{9311.0}           &    6920.9204  &     4515.774     &     1.533  &         0.125        &    -1930.685    &     1.58e+04     \\\\\n",
       "\\textbf{9313.0}           &    5391.0747  &     3511.619     &     1.535  &         0.125        &    -1492.235    &     1.23e+04     \\\\\n",
       "\\textbf{9325.0}           &    1.063e+04  &     3739.240     &     2.843  &         0.004        &     3299.461    &      1.8e+04     \\\\\n",
       "\\textbf{9332.0}           &    1.023e+04  &     3525.223     &     2.901  &         0.004        &     3315.462    &     1.71e+04     \\\\\n",
       "\\textbf{9340.0}           &   -1.671e+04  &     5692.134     &    -2.935  &         0.003        &    -2.79e+04    &    -5551.767     \\\\\n",
       "\\textbf{9372.0}           &     1.14e+04  &     4667.368     &     2.443  &         0.015        &     2254.314    &     2.06e+04     \\\\\n",
       "\\textbf{9411.0}           &    8963.9382  &     3801.630     &     2.358  &         0.018        &     1512.163    &     1.64e+04     \\\\\n",
       "\\textbf{9459.0}           &    1832.5262  &     4005.949     &     0.457  &         0.647        &    -6019.744    &     9684.796     \\\\\n",
       "\\textbf{9465.0}           &     1.46e+04  &     3435.437     &     4.250  &         0.000        &     7865.971    &     2.13e+04     \\\\\n",
       "\\textbf{9472.0}           &    5007.6051  &     3235.355     &     1.548  &         0.122        &    -1334.184    &     1.13e+04     \\\\\n",
       "\\textbf{9483.0}           &   -2008.3669  &     3843.983     &    -0.522  &         0.601        &    -9543.159    &     5526.425     \\\\\n",
       "\\textbf{9563.0}           &   -1.456e+04  &     4842.914     &    -3.006  &         0.003        &     -2.4e+04    &    -5063.006     \\\\\n",
       "\\textbf{9590.0}           &    7563.3751  &     4295.291     &     1.761  &         0.078        &     -856.049    &      1.6e+04     \\\\\n",
       "\\textbf{9598.0}           &    4212.3711  &     3955.432     &     1.065  &         0.287        &    -3540.878    &      1.2e+04     \\\\\n",
       "\\textbf{9599.0}           &    3534.0283  &     3706.502     &     0.953  &         0.340        &    -3731.282    &     1.08e+04     \\\\\n",
       "\\textbf{9602.0}           &    3671.4096  &     4352.900     &     0.843  &         0.399        &    -4860.938    &     1.22e+04     \\\\\n",
       "\\textbf{9619.0}           &    1.156e+04  &     4595.291     &     2.516  &         0.012        &     2553.723    &     2.06e+04     \\\\\n",
       "\\textbf{9643.0}           &    9380.7817  &     3571.246     &     2.627  &         0.009        &     2380.594    &     1.64e+04     \\\\\n",
       "\\textbf{9650.0}           &    8642.2789  &     3612.505     &     2.392  &         0.017        &     1561.219    &     1.57e+04     \\\\\n",
       "\\textbf{9653.0}           &   -6028.2181  &     5293.176     &    -1.139  &         0.255        &    -1.64e+04    &     4347.214     \\\\\n",
       "\\textbf{9667.0}           &    8520.3587  &     3445.313     &     2.473  &         0.013        &     1767.021    &     1.53e+04     \\\\\n",
       "\\textbf{9698.0}           &    9705.0529  &     4128.143     &     2.351  &         0.019        &     1613.262    &     1.78e+04     \\\\\n",
       "\\textbf{9699.0}           &    1.144e+04  &     3844.009     &     2.976  &         0.003        &     3906.063    &      1.9e+04     \\\\\n",
       "\\textbf{9719.0}           &    1626.6505  &     3065.684     &     0.531  &         0.596        &    -4382.558    &     7635.859     \\\\\n",
       "\\textbf{9742.0}           &     126.1225  &     3994.111     &     0.032  &         0.975        &    -7702.944    &     7955.189     \\\\\n",
       "\\textbf{9761.0}           &    1.153e+04  &     3830.821     &     3.010  &         0.003        &     4021.458    &      1.9e+04     \\\\\n",
       "\\textbf{9771.0}           &    3292.4690  &     3034.847     &     1.085  &         0.278        &    -2656.293    &     9241.231     \\\\\n",
       "\\textbf{9772.0}           &    1.173e+04  &     4282.121     &     2.740  &         0.006        &     3338.911    &     2.01e+04     \\\\\n",
       "\\textbf{9778.0}           &    9182.4470  &     3390.618     &     2.708  &         0.007        &     2536.319    &     1.58e+04     \\\\\n",
       "\\textbf{9799.0}           &    2332.9851  &     3127.518     &     0.746  &         0.456        &    -3797.427    &     8463.398     \\\\\n",
       "\\textbf{9815.0}           &    1.134e+04  &     3835.010     &     2.958  &         0.003        &     3827.470    &     1.89e+04     \\\\\n",
       "\\textbf{9818.0}           &   -3.454e+04  &     3515.963     &    -9.824  &         0.000        &    -4.14e+04    &    -2.76e+04     \\\\\n",
       "\\textbf{9837.0}           &    1.263e+04  &     4527.010     &     2.790  &         0.005        &     3758.533    &     2.15e+04     \\\\\n",
       "\\textbf{9922.0}           &    2817.7170  &     3438.711     &     0.819  &         0.413        &    -3922.682    &     9558.116     \\\\\n",
       "\\textbf{9954.0}           &    9074.8491  &     4310.015     &     2.106  &         0.035        &      626.563    &     1.75e+04     \\\\\n",
       "\\textbf{9963.0}           &    9738.4652  &     3612.823     &     2.696  &         0.007        &     2656.781    &     1.68e+04     \\\\\n",
       "\\textbf{9988.0}           &     1.23e+04  &     4529.727     &     2.715  &         0.007        &     3421.174    &     2.12e+04     \\\\\n",
       "\\textbf{gspilltecIVX1981} &      -0.0481  &        0.071     &    -0.674  &         0.500        &       -0.188    &        0.092     \\\\\n",
       "\\textbf{gspilltecIVX1982} &      -0.0110  &        0.071     &    -0.155  &         0.877        &       -0.149    &        0.127     \\\\\n",
       "\\textbf{gspilltecIVX1983} &      -0.0216  &        0.070     &    -0.310  &         0.757        &       -0.158    &        0.115     \\\\\n",
       "\\textbf{gspilltecIVX1984} &      -0.0677  &        0.070     &    -0.970  &         0.332        &       -0.204    &        0.069     \\\\\n",
       "\\textbf{gspilltecIVX1985} &      -0.1016  &        0.070     &    -1.445  &         0.148        &       -0.239    &        0.036     \\\\\n",
       "\\textbf{gspilltecIVX1986} &      -0.1426  &        0.071     &    -2.003  &         0.045        &       -0.282    &       -0.003     \\\\\n",
       "\\textbf{gspilltecIVX1987} &      -0.1461  &        0.072     &    -2.018  &         0.044        &       -0.288    &       -0.004     \\\\\n",
       "\\textbf{gspilltecIVX1988} &      -0.1734  &        0.073     &    -2.364  &         0.018        &       -0.317    &       -0.030     \\\\\n",
       "\\textbf{gspilltecIVX1989} &      -0.1766  &        0.075     &    -2.370  &         0.018        &       -0.323    &       -0.031     \\\\\n",
       "\\textbf{gspilltecIVX1990} &      -0.2093  &        0.076     &    -2.770  &         0.006        &       -0.357    &       -0.061     \\\\\n",
       "\\textbf{gspilltecIVX1991} &      -0.1902  &        0.077     &    -2.483  &         0.013        &       -0.340    &       -0.040     \\\\\n",
       "\\textbf{gspilltecIVX1992} &      -0.1770  &        0.078     &    -2.279  &         0.023        &       -0.329    &       -0.025     \\\\\n",
       "\\textbf{gspilltecIVX1993} &      -0.1474  &        0.079     &    -1.860  &         0.063        &       -0.303    &        0.008     \\\\\n",
       "\\textbf{gspilltecIVX1994} &      -0.1520  &        0.081     &    -1.884  &         0.060        &       -0.310    &        0.006     \\\\\n",
       "\\textbf{gspilltecIVX1995} &      -0.1499  &        0.083     &    -1.812  &         0.070        &       -0.312    &        0.012     \\\\\n",
       "\\textbf{gspilltecIVX1996} &      -0.1144  &        0.085     &    -1.341  &         0.180        &       -0.281    &        0.053     \\\\\n",
       "\\textbf{gspilltecIVX1997} &      -0.1120  &        0.088     &    -1.272  &         0.203        &       -0.284    &        0.061     \\\\\n",
       "\\textbf{gspilltecIVX1998} &      -0.0932  &        0.091     &    -1.028  &         0.304        &       -0.271    &        0.084     \\\\\n",
       "\\textbf{gspilltecIVX1999} &       0.0086  &        0.093     &     0.093  &         0.926        &       -0.173    &        0.191     \\\\\n",
       "\\textbf{gspilltecIVX2000} &      -0.0903  &        0.095     &    -0.953  &         0.341        &       -0.276    &        0.095     \\\\\n",
       "\\textbf{gspilltecIVX2001} &      -0.1343  &        0.096     &    -1.398  &         0.162        &       -0.323    &        0.054     \\\\\n",
       "\\textbf{gspillsicIVX1981} &       0.0015  &        0.134     &     0.011  &         0.991        &       -0.260    &        0.264     \\\\\n",
       "\\textbf{gspillsicIVX1982} &      -0.0311  &        0.132     &    -0.236  &         0.813        &       -0.289    &        0.227     \\\\\n",
       "\\textbf{gspillsicIVX1983} &      -0.0415  &        0.129     &    -0.320  &         0.749        &       -0.295    &        0.212     \\\\\n",
       "\\textbf{gspillsicIVX1984} &      -0.0741  &        0.128     &    -0.577  &         0.564        &       -0.326    &        0.178     \\\\\n",
       "\\textbf{gspillsicIVX1985} &      -0.0813  &        0.129     &    -0.632  &         0.527        &       -0.333    &        0.171     \\\\\n",
       "\\textbf{gspillsicIVX1986} &      -0.1101  &        0.129     &    -0.854  &         0.393        &       -0.363    &        0.143     \\\\\n",
       "\\textbf{gspillsicIVX1987} &      -0.1255  &        0.130     &    -0.967  &         0.334        &       -0.380    &        0.129     \\\\\n",
       "\\textbf{gspillsicIVX1988} &      -0.1413  &        0.131     &    -1.082  &         0.279        &       -0.397    &        0.115     \\\\\n",
       "\\textbf{gspillsicIVX1989} &      -0.1366  &        0.131     &    -1.040  &         0.299        &       -0.394    &        0.121     \\\\\n",
       "\\textbf{gspillsicIVX1990} &      -0.1397  &        0.132     &    -1.055  &         0.292        &       -0.399    &        0.120     \\\\\n",
       "\\textbf{gspillsicIVX1991} &      -0.1368  &        0.134     &    -1.022  &         0.307        &       -0.399    &        0.125     \\\\\n",
       "\\textbf{gspillsicIVX1992} &      -0.1956  &        0.135     &    -1.450  &         0.147        &       -0.460    &        0.069     \\\\\n",
       "\\textbf{gspillsicIVX1993} &      -0.2200  &        0.137     &    -1.611  &         0.107        &       -0.488    &        0.048     \\\\\n",
       "\\textbf{gspillsicIVX1994} &      -0.2246  &        0.138     &    -1.626  &         0.104        &       -0.495    &        0.046     \\\\\n",
       "\\textbf{gspillsicIVX1995} &      -0.1987  &        0.141     &    -1.412  &         0.158        &       -0.475    &        0.077     \\\\\n",
       "\\textbf{gspillsicIVX1996} &      -0.2430  &        0.144     &    -1.692  &         0.091        &       -0.525    &        0.039     \\\\\n",
       "\\textbf{gspillsicIVX1997} &      -0.2197  &        0.147     &    -1.493  &         0.135        &       -0.508    &        0.069     \\\\\n",
       "\\textbf{gspillsicIVX1998} &      -0.1813  &        0.151     &    -1.203  &         0.229        &       -0.477    &        0.114     \\\\\n",
       "\\textbf{gspillsicIVX1999} &      -0.1905  &        0.154     &    -1.238  &         0.216        &       -0.492    &        0.111     \\\\\n",
       "\\textbf{gspillsicIVX2000} &      -0.0964  &        0.157     &    -0.615  &         0.539        &       -0.404    &        0.211     \\\\\n",
       "\\textbf{gspillsicIVX2001} &      -0.2797  &        0.159     &    -1.761  &         0.078        &       -0.591    &        0.032     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21533.010 & \\textbf{  Durbin-Watson:     } &      0.543    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 48401709.884  \\\\\n",
       "\\textbf{Skew:}          &   10.015  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  296.914  & \\textbf{  Cond. No.          } &   2.80e+07    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.8e+07. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.669\n",
       "Model:                            OLS   Adj. R-squared:                  0.648\n",
       "Method:                 Least Squares   F-statistic:                     31.93\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        02:02:58   Log-Likelihood:            -1.4150e+05\n",
       "No. Observations:               13385   AIC:                         2.846e+05\n",
       "Df Residuals:                   12587   BIC:                         2.906e+05\n",
       "Df Model:                         797                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -1.381e+04   4167.202     -3.315      0.001    -2.2e+04   -5645.690\n",
       "gspilltecIV          0.1986      0.145      1.368      0.171      -0.086       0.483\n",
       "gspillsicIV          0.6659      0.233      2.852      0.004       0.208       1.124\n",
       "pat_count          -27.0286      1.911    -14.141      0.000     -30.775     -23.282\n",
       "rsales               0.7667      0.037     20.685      0.000       0.694       0.839\n",
       "rppent               0.6251      0.084      7.408      0.000       0.460       0.790\n",
       "emp                 15.4383      7.160      2.156      0.031       1.404      29.473\n",
       "rxrd                18.8001      0.613     30.654      0.000      17.598      20.002\n",
       "1981               109.6592   1080.584      0.101      0.919   -2008.450    2227.768\n",
       "1982              -233.2389   1074.271     -0.217      0.828   -2338.975    1872.497\n",
       "1983              -128.6822   1065.010     -0.121      0.904   -2216.264    1958.899\n",
       "1984                98.8470   1059.772      0.093      0.926   -1978.467    2176.161\n",
       "1985               500.7900   1057.845      0.473      0.636   -1572.747    2574.327\n",
       "1986              1025.5479   1047.722      0.979      0.328   -1028.146    3079.242\n",
       "1987               964.1585   1043.238      0.924      0.355   -1080.748    3009.065\n",
       "1988              1227.5616   1042.827      1.177      0.239    -816.538    3271.662\n",
       "1989              1409.3119   1037.820      1.358      0.175    -624.973    3443.597\n",
       "1990              1582.7353   1032.683      1.533      0.125    -441.480    3606.951\n",
       "1991              1616.4231   1032.392      1.566      0.117    -407.222    3640.068\n",
       "1992              1589.4573   1031.336      1.541      0.123    -432.118    3611.033\n",
       "1993              1205.1245   1027.138      1.173      0.241    -808.222    3218.471\n",
       "1994              1111.0146   1029.798      1.079      0.281    -907.547    3129.576\n",
       "1995              1413.9353   1028.108      1.375      0.169    -601.312    3429.183\n",
       "1996              1164.7069   1030.027      1.131      0.258    -854.302    3183.716\n",
       "1997              1327.6436   1034.933      1.283      0.200    -700.983    3356.271\n",
       "1998               626.1367   1038.418      0.603      0.547   -1409.322    2661.595\n",
       "1999             -1672.8117   1048.945     -1.595      0.111   -3728.904     383.281\n",
       "2000                14.3278   1070.432      0.013      0.989   -2083.883    2112.539\n",
       "2001              1048.8243   1114.791      0.941      0.347   -1136.337    3233.986\n",
       "10005.0           1.075e+04   3748.461      2.868      0.004    3402.447    1.81e+04\n",
       "10006.0            1.05e+04   3980.492      2.637      0.008    2694.217    1.83e+04\n",
       "10008.0           9625.6040   3498.425      2.751      0.006    2768.158    1.65e+04\n",
       "10016.0           1.086e+04   3609.282      3.009      0.003    3784.555    1.79e+04\n",
       "10030.0           1.211e+04   4151.615      2.916      0.004    3968.672    2.02e+04\n",
       "1004.0             1.17e+04   3919.766      2.984      0.003    4013.670    1.94e+04\n",
       "10056.0           9375.3438   3684.022      2.545      0.011    2154.098    1.66e+04\n",
       "10085.0           4448.8290   3357.715      1.325      0.185   -2132.805     1.1e+04\n",
       "10092.0           1.187e+04   5958.377      1.993      0.046     193.098    2.36e+04\n",
       "10097.0           3456.7486   3228.208      1.071      0.284   -2871.032    9784.529\n",
       "1010.0            1.067e+04   5738.714      1.860      0.063    -574.920    2.19e+04\n",
       "10109.0           1.305e+04   4548.789      2.868      0.004    4130.891     2.2e+04\n",
       "10115.0           8975.8676   3778.522      2.375      0.018    1569.388    1.64e+04\n",
       "10124.0           1.317e+04   4577.903      2.877      0.004    4198.916    2.21e+04\n",
       "1013.0            5844.5024   3184.674      1.835      0.067    -397.944    1.21e+04\n",
       "10150.0           4379.0875   3779.417      1.159      0.247   -3029.145    1.18e+04\n",
       "10159.0            973.0308   6088.181      0.160      0.873    -1.1e+04    1.29e+04\n",
       "10174.0           1.223e+04   4299.729      2.844      0.004    3800.706    2.07e+04\n",
       "10185.0           9342.5104   4437.384      2.105      0.035     644.562     1.8e+04\n",
       "10195.0           1185.3886   4591.466      0.258      0.796   -7814.585    1.02e+04\n",
       "10198.0            1.21e+04   4116.091      2.940      0.003    4034.264    2.02e+04\n",
       "10215.0           1.288e+04   4540.119      2.837      0.005    3982.728    2.18e+04\n",
       "10232.0           6637.9380   4756.976      1.395      0.163   -2686.461     1.6e+04\n",
       "10236.0           1.171e+04   4184.951      2.798      0.005    3507.701    1.99e+04\n",
       "10286.0           1.045e+04   3617.866      2.889      0.004    3359.886    1.75e+04\n",
       "10301.0          -1.587e+04   3086.762     -5.140      0.000   -2.19e+04   -9815.445\n",
       "10312.0           1.152e+04   3852.606      2.990      0.003    3967.037    1.91e+04\n",
       "10332.0           6845.6324   4814.039      1.422      0.155   -2590.617    1.63e+04\n",
       "1036.0            8479.1608   4222.035      2.008      0.045     203.328    1.68e+04\n",
       "10374.0           9888.7034   3720.204      2.658      0.008    2596.536    1.72e+04\n",
       "10386.0           5773.3219   3175.639      1.818      0.069    -451.414     1.2e+04\n",
       "10391.0           2387.3954   3277.528      0.728      0.466   -4037.059    8811.850\n",
       "10407.0           5287.9053   3527.954      1.499      0.134   -1627.422    1.22e+04\n",
       "10420.0           9770.2423   3967.970      2.462      0.014    1992.416    1.75e+04\n",
       "10422.0           8393.9708   3804.080      2.207      0.027     937.394    1.59e+04\n",
       "10426.0           1.138e+04   4573.690      2.488      0.013    2416.339    2.03e+04\n",
       "10441.0           1.227e+04   4355.586      2.818      0.005    3735.499    2.08e+04\n",
       "1045.0             961.9430   3789.693      0.254      0.800   -6466.434    8390.320\n",
       "10453.0           5689.5884   3192.668      1.782      0.075    -568.527    1.19e+04\n",
       "10482.0          -1.372e+04   3822.485     -3.589      0.000   -2.12e+04   -6228.047\n",
       "10498.0           1.137e+04   4141.570      2.745      0.006    3251.953    1.95e+04\n",
       "10499.0           -212.4259   3309.795     -0.064      0.949   -6700.128    6275.276\n",
       "10511.0           1.257e+04   4628.344      2.716      0.007    3500.005    2.16e+04\n",
       "10519.0          -6240.6283   3068.227     -2.034      0.042   -1.23e+04    -226.435\n",
       "10530.0           7396.4535   3303.701      2.239      0.025     920.696    1.39e+04\n",
       "10537.0           8373.8467   3619.936      2.313      0.021    1278.221    1.55e+04\n",
       "10540.0           9487.4398   3460.376      2.742      0.006    2704.575    1.63e+04\n",
       "10541.0           1.118e+04   3862.112      2.894      0.004    3605.598    1.87e+04\n",
       "10550.0           7515.1246   6151.801      1.222      0.222   -4543.343    1.96e+04\n",
       "10553.0           5242.3296   3365.410      1.558      0.119   -1354.387    1.18e+04\n",
       "10565.0           1.274e+04   4159.576      3.062      0.002    4583.444    2.09e+04\n",
       "10580.0           1.283e+04   4341.856      2.956      0.003    4322.812    2.13e+04\n",
       "10581.0           9797.9005   3963.747      2.472      0.013    2028.352    1.76e+04\n",
       "10588.0            855.6779   3101.920      0.276      0.783   -5224.558    6935.914\n",
       "10597.0           1.105e+04   3914.511      2.823      0.005    3378.337    1.87e+04\n",
       "10599.0           1.158e+04   3962.497      2.922      0.003    3811.778    1.93e+04\n",
       "10618.0           1.051e+04   3872.700      2.715      0.007    2923.389    1.81e+04\n",
       "10656.0           1.107e+04   3644.052      3.039      0.002    3929.910    1.82e+04\n",
       "10658.0           1.097e+04   3625.002      3.027      0.002    3869.116    1.81e+04\n",
       "10726.0           1.442e+04   4401.151      3.276      0.001    5789.509     2.3e+04\n",
       "10734.0           9405.0171   4445.151      2.116      0.034     691.843    1.81e+04\n",
       "10735.0           1.206e+04   4437.109      2.718      0.007    3364.680    2.08e+04\n",
       "10764.0           1.242e+04   4557.772      2.724      0.006    3481.657    2.13e+04\n",
       "10777.0           1.093e+04   3624.387      3.015      0.003    3822.478     1.8e+04\n",
       "1078.0            2633.8775   3872.332      0.680      0.496   -4956.484    1.02e+04\n",
       "10793.0           1.041e+04   4157.794      2.504      0.012    2259.333    1.86e+04\n",
       "10816.0           9848.3369   3792.306      2.597      0.009    2414.839    1.73e+04\n",
       "10839.0           1.179e+04   3862.003      3.052      0.002    4216.664    1.94e+04\n",
       "10857.0           -845.3807   3281.642     -0.258      0.797   -7277.900    5587.138\n",
       "10867.0           6807.2531   4207.731      1.618      0.106   -1440.540    1.51e+04\n",
       "10906.0            1.09e+04   3926.720      2.775      0.006    3199.498    1.86e+04\n",
       "10950.0           1.168e+04   4777.897      2.445      0.014    2317.340     2.1e+04\n",
       "10983.0          -2.276e+04   3294.972     -6.907      0.000   -2.92e+04   -1.63e+04\n",
       "1099.0            1.067e+04   3750.813      2.845      0.004    3318.973     1.8e+04\n",
       "10991.0           7301.2367   4789.870      1.524      0.127   -2087.639    1.67e+04\n",
       "11012.0           9155.2709   4087.399      2.240      0.025    1143.346    1.72e+04\n",
       "11038.0           4372.6645   4278.944      1.022      0.307   -4014.718    1.28e+04\n",
       "1104.0            1.125e+04   4231.136      2.660      0.008    2960.063    1.95e+04\n",
       "11060.0           1.158e+04   4129.190      2.804      0.005    3485.908    1.97e+04\n",
       "11094.0            1.07e+04   3840.633      2.787      0.005    3173.778    1.82e+04\n",
       "11096.0           8640.8067   3421.736      2.525      0.012    1933.682    1.53e+04\n",
       "11113.0           1.101e+04   4226.667      2.605      0.009    2724.173    1.93e+04\n",
       "1115.0            9631.4518   3696.163      2.606      0.009    2386.409    1.69e+04\n",
       "11161.0           7721.6527   3683.960      2.096      0.036     500.530    1.49e+04\n",
       "11225.0           1.232e+04   4390.224      2.807      0.005    3719.225    2.09e+04\n",
       "11228.0           1.215e+04   3982.478      3.050      0.002    4342.074       2e+04\n",
       "11236.0           6991.3336   4824.075      1.449      0.147   -2464.589    1.64e+04\n",
       "11288.0          -2018.1984   4000.197     -0.505      0.614   -9859.194    5822.797\n",
       "11312.0            -34.4822   3301.898     -0.010      0.992   -6506.705    6437.741\n",
       "11361.0           1.062e+04   3541.132      2.998      0.003    3674.735    1.76e+04\n",
       "11399.0           2972.3845   3179.933      0.935      0.350   -3260.769    9205.538\n",
       "114303.0         -1.046e+04   5391.406     -1.941      0.052    -2.1e+04     104.537\n",
       "11456.0           5467.4855   3454.479      1.583      0.114   -1303.821    1.22e+04\n",
       "11465.0           6032.2158   3836.369      1.572      0.116   -1487.653    1.36e+04\n",
       "11502.0           1.117e+04   4449.872      2.510      0.012    2448.510    1.99e+04\n",
       "11506.0           6270.2659   3372.318      1.859      0.063    -339.991    1.29e+04\n",
       "11537.0           1.137e+04   3814.796      2.980      0.003    3891.850    1.88e+04\n",
       "11566.0           1.276e+04   4524.328      2.820      0.005    3890.205    2.16e+04\n",
       "11573.0           1.034e+04   3563.257      2.901      0.004    3353.251    1.73e+04\n",
       "11580.0           6296.8646   3705.782      1.699      0.089    -967.034    1.36e+04\n",
       "11600.0           1.189e+04   4332.106      2.745      0.006    3401.731    2.04e+04\n",
       "11609.0           1.548e+04   4202.441      3.683      0.000    7241.117    2.37e+04\n",
       "1161.0           -1188.8749   2976.195     -0.399      0.690   -7022.672    4644.922\n",
       "11636.0          -1.074e+04   3653.852     -2.939      0.003   -1.79e+04   -3577.916\n",
       "11670.0           1.238e+04   4581.034      2.702      0.007    3396.962    2.14e+04\n",
       "11678.0          -1260.8304   3564.728     -0.354      0.724   -8248.241    5726.581\n",
       "11682.0           1.073e+04   3611.674      2.970      0.003    3648.552    1.78e+04\n",
       "11694.0           1.178e+04   4057.704      2.904      0.004    3830.393    1.97e+04\n",
       "11720.0           2309.2070   4719.750      0.489      0.625   -6942.223    1.16e+04\n",
       "11721.0          -6216.7588   4834.181     -1.286      0.198   -1.57e+04    3258.974\n",
       "11722.0           1.005e+04   3772.084      2.663      0.008    2652.948    1.74e+04\n",
       "11793.0           1.032e+04   6325.197      1.632      0.103   -2074.750    2.27e+04\n",
       "11797.0           1.239e+04   4716.169      2.627      0.009    3143.742    2.16e+04\n",
       "11914.0           1.126e+04   4485.572      2.511      0.012    2470.812    2.01e+04\n",
       "1209.0            8441.1675   3331.506      2.534      0.011    1910.908     1.5e+04\n",
       "12136.0          -7389.7179   3617.876     -2.043      0.041   -1.45e+04    -298.129\n",
       "12141.0            8.96e+04   3470.701     25.815      0.000    8.28e+04    9.64e+04\n",
       "12181.0           8824.9469   4454.428      1.981      0.048      93.589    1.76e+04\n",
       "12215.0           -639.9548   3284.031     -0.195      0.845   -7077.156    5797.247\n",
       "12216.0           3684.1706   3584.414      1.028      0.304   -3341.827    1.07e+04\n",
       "12256.0           3350.1200   3467.654      0.966      0.334   -3447.011    1.01e+04\n",
       "12262.0           1.074e+04   4592.115      2.339      0.019    1739.052    1.97e+04\n",
       "12389.0           1.193e+04   3704.182      3.222      0.001    4674.066    1.92e+04\n",
       "1239.0            8045.7087   3516.851      2.288      0.022    1152.144    1.49e+04\n",
       "12390.0           9333.9268   4088.151      2.283      0.022    1320.528    1.73e+04\n",
       "12397.0           8107.0205   5501.313      1.474      0.141   -2676.391    1.89e+04\n",
       "1243.0            6447.7796   4080.880      1.580      0.114   -1551.368    1.44e+04\n",
       "12548.0           1.097e+04   4265.673      2.571      0.010    2605.463    1.93e+04\n",
       "12570.0           1.033e+04   4387.962      2.353      0.019    1724.321    1.89e+04\n",
       "12581.0           8527.2526   4009.702      2.127      0.033     667.624    1.64e+04\n",
       "12592.0           9019.3177   4216.569      2.139      0.032     754.199    1.73e+04\n",
       "12604.0           8118.8529   6722.925      1.208      0.227   -5059.104    2.13e+04\n",
       "12656.0           1.221e+04   4797.855      2.545      0.011    2806.048    2.16e+04\n",
       "12679.0          -9893.9636   3978.706     -2.487      0.013   -1.77e+04   -2095.094\n",
       "1278.0            1.164e+04   4567.652      2.549      0.011    2690.848    2.06e+04\n",
       "12788.0          -5830.7039   6161.565     -0.946      0.344   -1.79e+04    6246.902\n",
       "1283.0            1.128e+04   4606.609      2.449      0.014    2252.426    2.03e+04\n",
       "1297.0            1.082e+04   3929.942      2.753      0.006    3117.433    1.85e+04\n",
       "12992.0             1.2e+04   4455.981      2.693      0.007    3265.796    2.07e+04\n",
       "13135.0           7128.1442   4048.838      1.761      0.078    -808.196    1.51e+04\n",
       "1327.0            1639.9713   4435.025      0.370      0.712   -7053.353    1.03e+04\n",
       "13282.0           4849.3715   5498.022      0.882      0.378   -5927.589    1.56e+04\n",
       "1334.0            -511.5826   5831.432     -0.088      0.930   -1.19e+04    1.09e+04\n",
       "13351.0           6163.1783   4257.004      1.448      0.148   -2181.199    1.45e+04\n",
       "13365.0          -1.072e+04   4821.541     -2.223      0.026   -2.02e+04   -1268.772\n",
       "13369.0           8931.5001   3699.764      2.414      0.016    1679.398    1.62e+04\n",
       "13406.0           1.141e+04   4169.202      2.737      0.006    3238.275    1.96e+04\n",
       "13407.0           4068.7423   3712.436      1.096      0.273   -3208.198    1.13e+04\n",
       "13417.0           1.202e+04   4758.917      2.526      0.012    2692.416    2.13e+04\n",
       "13525.0           1258.2074   5299.053      0.237      0.812   -9128.744    1.16e+04\n",
       "13554.0           1.269e+04   4749.625      2.673      0.008    3384.617     2.2e+04\n",
       "1359.0           -3324.7685   5248.095     -0.634      0.526   -1.36e+04    6962.298\n",
       "13623.0           9061.8658   4120.385      2.199      0.028     985.282    1.71e+04\n",
       "1372.0            2057.8258   4237.981      0.486      0.627   -6249.263    1.04e+04\n",
       "1380.0            3146.3459   3782.620      0.832      0.406   -4268.166    1.06e+04\n",
       "13923.0           1.088e+04   4304.183      2.527      0.012    2439.468    1.93e+04\n",
       "13932.0            1.22e+04   5116.181      2.384      0.017    2168.201    2.22e+04\n",
       "13941.0           -596.2531   3355.204     -0.178      0.859   -7172.965    5980.458\n",
       "1397.0            9648.9416   3677.558      2.624      0.009    2440.368    1.69e+04\n",
       "14064.0           8774.2049   4340.256      2.022      0.043     266.642    1.73e+04\n",
       "14084.0           9923.8171   3771.402      2.631      0.009    2531.295    1.73e+04\n",
       "14324.0           1785.4300   3979.030      0.449      0.654   -6014.076    9584.936\n",
       "14462.0           8775.3024   3785.900      2.318      0.020    1354.361    1.62e+04\n",
       "1447.0            1.583e+04   5146.276      3.077      0.002    5747.016    2.59e+04\n",
       "14531.0           8978.9881   1.01e+04      0.886      0.375   -1.09e+04    2.88e+04\n",
       "14593.0           1.146e+04   4575.502      2.505      0.012    2491.648    2.04e+04\n",
       "14622.0           1.118e+04   7405.549      1.510      0.131   -3335.409    2.57e+04\n",
       "1465.0            1.075e+04   4781.058      2.248      0.025    1376.754    2.01e+04\n",
       "1468.0            1.188e+04   4462.287      2.663      0.008    3137.130    2.06e+04\n",
       "14897.0           9851.0172   5967.234      1.651      0.099   -1845.671    2.15e+04\n",
       "14954.0           1.089e+04   4473.110      2.434      0.015    2119.784    1.97e+04\n",
       "1496.0            1.237e+04   4176.488      2.962      0.003    4186.049    2.06e+04\n",
       "15267.0           1.032e+04   4202.868      2.456      0.014    2083.660    1.86e+04\n",
       "15354.0           5330.6304   3750.916      1.421      0.155   -2021.737    1.27e+04\n",
       "1542.0            1.025e+04   3687.124      2.780      0.005    3023.899    1.75e+04\n",
       "15459.0           7623.0653   3820.476      1.995      0.046     134.350    1.51e+04\n",
       "1554.0            1.221e+04   4168.390      2.928      0.003    4034.702    2.04e+04\n",
       "15708.0          -1.173e+04   4739.990     -2.476      0.013    -2.1e+04   -2442.785\n",
       "15711.0           9204.2718   4005.034      2.298      0.022    1353.794    1.71e+04\n",
       "15761.0           1.127e+04   4819.636      2.338      0.019    1821.834    2.07e+04\n",
       "1581.0           -2.091e+04   4631.919     -4.515      0.000      -3e+04   -1.18e+04\n",
       "1593.0            9602.8522   3468.687      2.768      0.006    2803.697    1.64e+04\n",
       "1602.0            1.722e+04   3483.910      4.944      0.000    1.04e+04    2.41e+04\n",
       "1613.0            1.144e+04   4184.743      2.733      0.006    3235.570    1.96e+04\n",
       "16188.0           7072.9575   4187.940      1.689      0.091   -1136.043    1.53e+04\n",
       "1632.0            2653.4280   3125.560      0.849      0.396   -3473.146    8780.002\n",
       "1633.0            9133.1549   3813.953      2.395      0.017    1657.226    1.66e+04\n",
       "1635.0           -5567.8350   4976.379     -1.119      0.263   -1.53e+04    4186.626\n",
       "16401.0           -662.3621   3678.802     -0.180      0.857   -7873.375    6548.651\n",
       "16437.0           4365.1379   4051.649      1.077      0.281   -3576.712    1.23e+04\n",
       "1651.0            6305.5157   3460.812      1.822      0.068    -478.204    1.31e+04\n",
       "1655.0            1.188e+04   4009.212      2.963      0.003    4021.892    1.97e+04\n",
       "1663.0            1.612e+04   3749.056      4.299      0.000    8769.535    2.35e+04\n",
       "16710.0           5981.0310   3906.403      1.531      0.126   -1676.113    1.36e+04\n",
       "16729.0           5061.6838   3959.387      1.278      0.201   -2699.318    1.28e+04\n",
       "1690.0           -1.108e+04   3519.723     -3.149      0.002    -1.8e+04   -4184.065\n",
       "1703.0            9217.5903   3557.851      2.591      0.010    2243.659    1.62e+04\n",
       "17101.0          -1.638e+04   1.17e+04     -1.405      0.160   -3.92e+04    6464.946\n",
       "17202.0           9944.8151   4458.719      2.230      0.026    1205.045    1.87e+04\n",
       "1722.0            9966.2654   3919.741      2.543      0.011    2282.975    1.76e+04\n",
       "1728.0            1.199e+04   4123.710      2.907      0.004    3904.903    2.01e+04\n",
       "1743.0             1.14e+04   5013.948      2.274      0.023    1574.373    2.12e+04\n",
       "1754.0            1.127e+04   4027.944      2.797      0.005    3371.582    1.92e+04\n",
       "1762.0            6020.1626   3557.041      1.692      0.091    -952.180     1.3e+04\n",
       "1773.0            1.144e+04   4568.557      2.504      0.012    2482.906    2.04e+04\n",
       "1786.0           -3272.3191   3073.973     -1.065      0.287   -9297.774    2753.136\n",
       "18100.0           8898.7772   4275.979      2.081      0.037     517.206    1.73e+04\n",
       "1820.0            7869.9943   4068.111      1.935      0.053    -104.124    1.58e+04\n",
       "1848.0            -542.6371   3639.338     -0.149      0.881   -7676.295    6591.021\n",
       "18654.0           1.171e+04   5117.475      2.288      0.022    1680.301    2.17e+04\n",
       "1875.0            5894.0920   4997.948      1.179      0.238   -3902.649    1.57e+04\n",
       "1884.0            1.117e+04   4392.864      2.543      0.011    2562.229    1.98e+04\n",
       "1913.0            7336.9598   3351.413      2.189      0.029     767.680    1.39e+04\n",
       "1919.0            1.059e+04   4046.792      2.616      0.009    2653.940    1.85e+04\n",
       "1920.0            7313.0108   3212.234      2.277      0.023    1016.543    1.36e+04\n",
       "1968.0            9900.3459   3469.706      2.853      0.004    3099.193    1.67e+04\n",
       "1976.0            1.247e+04   4073.086      3.062      0.002    4486.807    2.05e+04\n",
       "1981.0            1.105e+04   4077.664      2.710      0.007    3058.436     1.9e+04\n",
       "1988.0            2066.6708   4287.949      0.482      0.630   -6338.363    1.05e+04\n",
       "1992.0            1.028e+04   3494.046      2.943      0.003    3434.591    1.71e+04\n",
       "2008.0            1.017e+04   3530.470      2.880      0.004    3248.358    1.71e+04\n",
       "2033.0            1.117e+04   4362.901      2.560      0.010    2616.168    1.97e+04\n",
       "2044.0            8626.2108   3719.509      2.319      0.020    1335.406    1.59e+04\n",
       "2049.0            1.006e+04   3610.499      2.787      0.005    2984.839    1.71e+04\n",
       "2061.0            1.278e+04   4481.478      2.851      0.004    3993.798    2.16e+04\n",
       "20779.0           5.394e+04   3923.620     13.748      0.000    4.63e+04    6.16e+04\n",
       "2085.0           -5400.0504   4099.351     -1.317      0.188   -1.34e+04    2635.303\n",
       "2086.0            7832.7624   3450.472      2.270      0.023    1069.311    1.46e+04\n",
       "2111.0            8505.6464   3461.370      2.457      0.014    1720.834    1.53e+04\n",
       "21204.0           4498.8386   5310.844      0.847      0.397   -5911.226    1.49e+04\n",
       "21238.0           1.083e+04   4828.430      2.243      0.025    1365.421    2.03e+04\n",
       "2124.0            1.018e+04   3588.996      2.836      0.005    3142.326    1.72e+04\n",
       "2146.0            1.769e+04   5276.450      3.354      0.001    7352.347     2.8e+04\n",
       "21496.0           -1.28e+04   5019.607     -2.549      0.011   -2.26e+04   -2956.627\n",
       "2154.0            1.055e+04   3883.931      2.716      0.007    2936.502    1.82e+04\n",
       "2176.0            5.108e+04   4320.049     11.825      0.000    4.26e+04    5.96e+04\n",
       "2188.0            1.253e+04   4513.709      2.776      0.006    3681.859    2.14e+04\n",
       "2189.0            4436.7599   3438.026      1.290      0.197   -2302.296    1.12e+04\n",
       "2220.0            1.034e+04   3652.188      2.833      0.005    3185.997    1.75e+04\n",
       "22205.0           1.269e+04   4884.409      2.598      0.009    3114.998    2.23e+04\n",
       "2226.0            7783.8828   6251.292      1.245      0.213   -4469.603       2e+04\n",
       "2230.0            1.047e+04   4520.037      2.316      0.021    1610.401    1.93e+04\n",
       "22325.0            671.7506   4354.809      0.154      0.877   -7864.340    9207.841\n",
       "2255.0            9504.4405   3720.917      2.554      0.011    2210.876    1.68e+04\n",
       "22619.0           1.145e+04   4546.012      2.518      0.012    2536.613    2.04e+04\n",
       "2267.0             954.6225   3444.320      0.277      0.782   -5796.771    7706.015\n",
       "22815.0           5747.8505   3998.387      1.438      0.151   -2089.598    1.36e+04\n",
       "2285.0           -1.948e+04   3317.110     -5.871      0.000    -2.6e+04    -1.3e+04\n",
       "2290.0            6425.0318   3607.703      1.781      0.075    -646.616    1.35e+04\n",
       "2295.0            1.256e+04   5597.325      2.244      0.025    1586.917    2.35e+04\n",
       "2316.0            5728.1236   3470.724      1.650      0.099   -1075.024    1.25e+04\n",
       "23220.0           9924.7612   4113.115      2.413      0.016    1862.428     1.8e+04\n",
       "23224.0          -3755.6038   6047.146     -0.621      0.535   -1.56e+04    8097.723\n",
       "2343.0            -534.6446   6309.619     -0.085      0.932   -1.29e+04    1.18e+04\n",
       "2352.0            9736.7758   3588.280      2.713      0.007    2703.200    1.68e+04\n",
       "23700.0          -1938.1193   4766.712     -0.407      0.684   -1.13e+04    7405.363\n",
       "2390.0            1.238e+04   4216.522      2.936      0.003    4114.191    2.06e+04\n",
       "2393.0            7550.3051   3430.390      2.201      0.028     826.217    1.43e+04\n",
       "2403.0            1.248e+04   4112.791      3.034      0.002    4415.079    2.05e+04\n",
       "2435.0             1.42e+04   4525.795      3.138      0.002    5330.352    2.31e+04\n",
       "2444.0            7862.9049   3409.081      2.306      0.021    1180.587    1.45e+04\n",
       "2448.0            9317.7327   3635.420      2.563      0.010    2191.756    1.64e+04\n",
       "2469.0            1.172e+04   5182.428      2.262      0.024    1564.434    2.19e+04\n",
       "24720.0           1.086e+04   4248.909      2.557      0.011    2536.128    1.92e+04\n",
       "24800.0            522.9771   4749.139      0.110      0.912   -8786.059    9832.013\n",
       "2482.0            1.252e+04   4265.554      2.935      0.003    4156.786    2.09e+04\n",
       "24969.0           1.161e+04   5237.807      2.217      0.027    1346.333    2.19e+04\n",
       "2498.0            2742.2213   3605.284      0.761      0.447   -4324.684    9809.127\n",
       "2504.0           -3534.0958   4056.194     -0.871      0.384   -1.15e+04    4416.663\n",
       "2508.0             1.16e+04   4095.612      2.832      0.005    3570.062    1.96e+04\n",
       "25124.0           1.094e+04   4389.059      2.494      0.013    2341.169    1.95e+04\n",
       "2518.0             1.17e+04   4013.804      2.915      0.004    3832.399    1.96e+04\n",
       "25224.0           1.153e+04   7440.529      1.549      0.121   -3059.387    2.61e+04\n",
       "25279.0           9297.6728   4410.080      2.108      0.035     653.244    1.79e+04\n",
       "2537.0           -3686.1598   4295.083     -0.858      0.391   -1.21e+04    4732.858\n",
       "2538.0            1.152e+04   5113.381      2.252      0.024    1494.339    2.15e+04\n",
       "25389.0           1.123e+04   6651.487      1.688      0.091   -1810.098    2.43e+04\n",
       "2547.0            4060.4291   3620.838      1.121      0.262   -3036.965    1.12e+04\n",
       "2553.0            1.089e+04   4153.132      2.623      0.009    2753.170     1.9e+04\n",
       "2574.0            4823.8558   4406.699      1.095      0.274   -3813.945    1.35e+04\n",
       "25747.0           1.101e+04   4450.960      2.474      0.013    2285.080    1.97e+04\n",
       "2577.0            9384.7662   3451.936      2.719      0.007    2618.446    1.62e+04\n",
       "2593.0            9890.8041   3570.057      2.770      0.006    2892.948    1.69e+04\n",
       "2596.0            7510.0147   4622.486      1.625      0.104   -1550.762    1.66e+04\n",
       "2663.0            1.441e+04   3919.032      3.678      0.000    6732.007    2.21e+04\n",
       "2771.0            7729.5092   4553.521      1.697      0.090   -1196.087    1.67e+04\n",
       "2787.0             1.09e+04   3973.049      2.743      0.006    3111.448    1.87e+04\n",
       "2797.0           -2345.0266   3283.149     -0.714      0.475   -8780.498    4090.445\n",
       "2802.0            1.203e+04   4223.324      2.849      0.004    3753.698    2.03e+04\n",
       "2817.0               0.4896   3865.011      0.000      1.000   -7575.522    7576.501\n",
       "28678.0          -8274.1760   4174.195     -1.982      0.047   -1.65e+04     -92.117\n",
       "28701.0           9223.2361   3878.259      2.378      0.017    1621.257    1.68e+04\n",
       "28742.0          -5918.1481   4904.397     -1.207      0.228   -1.55e+04    3695.218\n",
       "2888.0            1.067e+04   3701.823      2.881      0.004    3410.014    1.79e+04\n",
       "2897.0            1.195e+04   4495.724      2.659      0.008    3141.271    2.08e+04\n",
       "2917.0            6165.4632   3795.123      1.625      0.104   -1273.556    1.36e+04\n",
       "29392.0          -3779.9320   3953.912     -0.956      0.339   -1.15e+04    3970.338\n",
       "2950.0           -1.324e+04   6688.460     -1.980      0.048   -2.64e+04    -133.438\n",
       "2951.0            1.223e+04   4616.162      2.649      0.008    3179.648    2.13e+04\n",
       "2953.0            1.061e+04   3801.196      2.792      0.005    3162.859    1.81e+04\n",
       "2960.0            9163.1119   4168.518      2.198      0.028     992.181    1.73e+04\n",
       "2975.0            4507.4799   3464.677      1.301      0.193   -2283.815    1.13e+04\n",
       "2982.0            1.035e+04   4073.030      2.540      0.011    2363.511    1.83e+04\n",
       "2991.0           -1437.3222   3931.504     -0.366      0.715   -9143.669    6269.025\n",
       "3011.0           -1447.5958   4429.291     -0.327      0.744   -1.01e+04    7234.490\n",
       "3015.0             1.26e+04   4378.506      2.878      0.004    4018.449    2.12e+04\n",
       "3026.0            9786.3293   3882.487      2.521      0.012    2176.063    1.74e+04\n",
       "3031.0           -9619.9936   5039.433     -1.909      0.056   -1.95e+04     258.063\n",
       "3062.0            1.265e+04   3725.803      3.394      0.001    5342.370    1.99e+04\n",
       "3093.0            3796.6585   4349.266      0.873      0.383   -4728.567    1.23e+04\n",
       "3107.0            1.066e+04   4996.454      2.134      0.033     867.551    2.05e+04\n",
       "3121.0            1.278e+04   3913.903      3.266      0.001    5111.474    2.05e+04\n",
       "3126.0            1.102e+04   3683.022      2.992      0.003    3800.977    1.82e+04\n",
       "3144.0            6.396e+04   4031.384     15.866      0.000    5.61e+04    7.19e+04\n",
       "3156.0            1.045e+04   4497.387      2.323      0.020    1633.650    1.93e+04\n",
       "3157.0            1.036e+04   3840.485      2.697      0.007    2830.490    1.79e+04\n",
       "3170.0            1.222e+04   3803.160      3.213      0.001    4764.830    1.97e+04\n",
       "3178.0            3978.5918   4778.656      0.833      0.405   -5388.302    1.33e+04\n",
       "3206.0            7525.3953   3817.285      1.971      0.049      42.935     1.5e+04\n",
       "3229.0            7539.4676   4373.199      1.724      0.085   -1032.668    1.61e+04\n",
       "3235.0            1.102e+04   3756.998      2.934      0.003    3657.067    1.84e+04\n",
       "3246.0            1.099e+04   4195.025      2.620      0.009    2766.893    1.92e+04\n",
       "3248.0            1.107e+04   3776.058      2.933      0.003    3671.798    1.85e+04\n",
       "3282.0           -2.004e+04   3435.208     -5.833      0.000   -2.68e+04   -1.33e+04\n",
       "3362.0            3973.2246   4193.822      0.947      0.343   -4247.305    1.22e+04\n",
       "3372.0            1.097e+04   4243.952      2.584      0.010    2648.966    1.93e+04\n",
       "3422.0            1.022e+04   3593.566      2.844      0.004    3174.659    1.73e+04\n",
       "3497.0            4718.9993   3142.124      1.502      0.133   -1440.043    1.09e+04\n",
       "3502.0            4945.7255   3530.657      1.401      0.161   -1974.900    1.19e+04\n",
       "3504.0            9538.6385   4075.855      2.340      0.019    1549.342    1.75e+04\n",
       "3505.0            8971.7211   3460.723      2.592      0.010    2188.177    1.58e+04\n",
       "3532.0            1.079e+04   3473.780      3.105      0.002    3977.426    1.76e+04\n",
       "3574.0            1.266e+04   5621.210      2.253      0.024    1645.503    2.37e+04\n",
       "3580.0            7108.9511   3449.114      2.061      0.039     348.162    1.39e+04\n",
       "3612.0            1.271e+04   4548.236      2.795      0.005    3795.224    2.16e+04\n",
       "3619.0            9717.4324   4083.473      2.380      0.017    1713.204    1.77e+04\n",
       "3622.0            1.223e+04   4314.302      2.836      0.005    3776.892    2.07e+04\n",
       "3639.0             -35.2982   3266.121     -0.011      0.991   -6437.393    6366.796\n",
       "3650.0            1972.6354   3363.711      0.586      0.558   -4620.750    8566.021\n",
       "3662.0            9054.5036   3495.631      2.590      0.010    2202.533    1.59e+04\n",
       "3734.0           -5653.8420   3410.927     -1.658      0.097   -1.23e+04    1032.094\n",
       "3735.0            9184.4944   3795.950      2.420      0.016    1743.855    1.66e+04\n",
       "3761.0            6261.7669   3189.760      1.963      0.050       9.351    1.25e+04\n",
       "3779.0           -4303.5751   5205.937     -0.827      0.408   -1.45e+04    5900.856\n",
       "3781.0            2109.6502   4872.186      0.433      0.665   -7440.577    1.17e+04\n",
       "3782.0           -2053.4891   3403.699     -0.603      0.546   -8725.259    4618.281\n",
       "3786.0            9489.0672   3988.705      2.379      0.017    1670.597    1.73e+04\n",
       "3796.0           -5002.4715   5240.283     -0.955      0.340   -1.53e+04    5269.283\n",
       "3821.0            1.161e+04   4287.437      2.708      0.007    3206.709       2e+04\n",
       "3835.0            3602.0856   3562.673      1.011      0.312   -3381.297    1.06e+04\n",
       "3839.0            7755.4892   4065.538      1.908      0.056    -213.585    1.57e+04\n",
       "3840.0            2233.2361   4317.199      0.517      0.605   -6229.132    1.07e+04\n",
       "3895.0            1.151e+04   4015.359      2.867      0.004    3640.571    1.94e+04\n",
       "3908.0            6511.3911   5260.917      1.238      0.216   -3800.809    1.68e+04\n",
       "3911.0            5754.9750   3276.668      1.756      0.079    -667.793    1.22e+04\n",
       "3917.0            1.148e+04   4187.146      2.742      0.006    3272.315    1.97e+04\n",
       "3946.0            1.227e+04   4348.144      2.822      0.005    3745.972    2.08e+04\n",
       "3971.0             1.08e+04   3856.276      2.801      0.005    3242.502    1.84e+04\n",
       "3980.0            2.076e+04   3714.901      5.588      0.000    1.35e+04     2.8e+04\n",
       "4034.0            7677.6628   3373.504      2.276      0.023    1065.082    1.43e+04\n",
       "4036.0            1.221e+04   4119.481      2.964      0.003    4134.874    2.03e+04\n",
       "4040.0            4321.4450   4087.621      1.057      0.290   -3690.916    1.23e+04\n",
       "4058.0            1.029e+04   3779.927      2.722      0.006    2881.014    1.77e+04\n",
       "4060.0           -7864.2560   3258.229     -2.414      0.016   -1.43e+04   -1477.631\n",
       "4062.0            1.447e+04   4511.974      3.207      0.001    5624.335    2.33e+04\n",
       "4077.0            9410.6027   4925.705      1.911      0.056    -244.530    1.91e+04\n",
       "4087.0           -1.904e+04   3566.201     -5.338      0.000    -2.6e+04    -1.2e+04\n",
       "4091.0            9316.1058   4179.475      2.229      0.026    1123.697    1.75e+04\n",
       "4127.0            4598.9212   3230.438      1.424      0.155   -1733.230    1.09e+04\n",
       "4138.0            1.238e+04   4680.272      2.645      0.008    3206.805    2.16e+04\n",
       "4162.0            1.048e+04   4305.636      2.433      0.015    2035.387    1.89e+04\n",
       "4186.0            1.241e+04   4222.066      2.940      0.003    4136.346    2.07e+04\n",
       "4194.0            -735.4906   3515.408     -0.209      0.834   -7626.227    6155.246\n",
       "4199.0           -3459.6812   3114.452     -1.111      0.267   -9564.483    2645.120\n",
       "4213.0            1.125e+04   3649.832      3.083      0.002    4097.360    1.84e+04\n",
       "4222.0           -2143.4353   3777.823     -0.567      0.570   -9548.544    5261.674\n",
       "4223.0            1.092e+04   3729.340      2.927      0.003    3606.865    1.82e+04\n",
       "4251.0            1.203e+04   4142.789      2.903      0.004    3906.307    2.01e+04\n",
       "4265.0            9581.9468   4108.600      2.332      0.020    1528.464    1.76e+04\n",
       "4274.0            9736.2323   3591.278      2.711      0.007    2696.779    1.68e+04\n",
       "4321.0            8285.3216   3383.445      2.449      0.014    1653.253    1.49e+04\n",
       "4335.0            9296.5716   4979.568      1.867      0.062    -464.142    1.91e+04\n",
       "4340.0            7675.5694   3435.786      2.234      0.026     940.904    1.44e+04\n",
       "4371.0            9023.4305   3522.492      2.562      0.010    2118.810    1.59e+04\n",
       "4415.0            1.091e+04   4160.138      2.624      0.009    2760.450    1.91e+04\n",
       "4450.0            9206.4725   4054.859      2.270      0.023    1258.330    1.72e+04\n",
       "4476.0             699.4465   3334.112      0.210      0.834   -5835.921    7234.814\n",
       "4510.0            4574.8460   3383.453      1.352      0.176   -2057.239    1.12e+04\n",
       "4520.0            1.052e+04   3525.801      2.985      0.003    3613.707    1.74e+04\n",
       "4551.0            9794.1435   7586.255      1.291      0.197   -5076.073    2.47e+04\n",
       "4568.0            1.119e+04   3823.353      2.926      0.003    3693.045    1.87e+04\n",
       "4579.0            1.259e+04   4538.028      2.775      0.006    3697.287    2.15e+04\n",
       "4585.0            1.212e+04   4254.059      2.849      0.004    3779.535    2.05e+04\n",
       "4595.0            8302.6110   3564.836      2.329      0.020    1314.989    1.53e+04\n",
       "4600.0            -353.5173   4049.300     -0.087      0.930   -8290.763    7583.728\n",
       "4607.0            1.197e+04   4164.987      2.874      0.004    3807.698    2.01e+04\n",
       "4608.0             570.1042   4185.709      0.136      0.892   -7634.523    8774.732\n",
       "4622.0            7234.5217   3341.365      2.165      0.030     684.936    1.38e+04\n",
       "4623.0            1.083e+04   4177.485      2.592      0.010    2639.526     1.9e+04\n",
       "4768.0            1.054e+04   3845.062      2.740      0.006    2998.522    1.81e+04\n",
       "4771.0             1.23e+04   4275.165      2.877      0.004    3920.863    2.07e+04\n",
       "4800.0            1.055e+04   4286.620      2.462      0.014    2151.440     1.9e+04\n",
       "4802.0            1.197e+04   4035.859      2.966      0.003    4059.365    1.99e+04\n",
       "4807.0            1.149e+04   3962.282      2.901      0.004    3727.073    1.93e+04\n",
       "4839.0           -1.368e+05   4706.996    -29.062      0.000   -1.46e+05   -1.28e+05\n",
       "4843.0           -3544.4727   5456.969     -0.650      0.516   -1.42e+04    7152.018\n",
       "4881.0            9402.2705   3445.968      2.728      0.006    2647.647    1.62e+04\n",
       "4900.0            9382.8163   4045.853      2.319      0.020    1452.327    1.73e+04\n",
       "4926.0            9212.0044   3448.176      2.672      0.008    2453.054     1.6e+04\n",
       "4941.0            9970.1650   3800.845      2.623      0.009    2519.929    1.74e+04\n",
       "4961.0           -6740.0949   3831.719     -1.759      0.079   -1.43e+04     770.659\n",
       "4988.0            1.766e+04   4298.290      4.110      0.000    9239.322    2.61e+04\n",
       "4993.0            1.286e+04   4536.766      2.835      0.005    3968.498    2.18e+04\n",
       "5018.0            5292.8001   3685.801      1.436      0.151   -1931.933    1.25e+04\n",
       "5020.0           -1494.8542   4969.243     -0.301      0.764   -1.12e+04    8245.620\n",
       "5027.0            6774.1739   3369.587      2.010      0.044     169.270    1.34e+04\n",
       "5032.0            1.108e+04   3841.583      2.884      0.004    3549.242    1.86e+04\n",
       "5043.0            6882.4486   3529.631      1.950      0.051     -36.166    1.38e+04\n",
       "5046.0           -2455.8570   3177.871     -0.773      0.440   -8684.969    3773.254\n",
       "5047.0            5.259e+04   4554.429     11.546      0.000    4.37e+04    6.15e+04\n",
       "5065.0            1.158e+04   4427.376      2.615      0.009    2898.115    2.03e+04\n",
       "5071.0            1.078e+04   4323.382      2.494      0.013    2310.134    1.93e+04\n",
       "5073.0            -2.02e+05   6401.840    -31.546      0.000   -2.14e+05   -1.89e+05\n",
       "5087.0            6088.8350   3522.160      1.729      0.084    -815.136     1.3e+04\n",
       "5109.0            1.217e+04   4304.921      2.826      0.005    3729.124    2.06e+04\n",
       "5116.0           -1193.1184   4145.651     -0.288      0.774   -9319.226    6932.989\n",
       "5122.0            6279.1585   3312.645      1.896      0.058    -214.130    1.28e+04\n",
       "5134.0            4252.6159   3634.799      1.170      0.242   -2872.144    1.14e+04\n",
       "5142.0            8303.3545   4310.795      1.926      0.054    -146.461    1.68e+04\n",
       "5165.0            7418.0199   4698.379      1.579      0.114   -1791.519    1.66e+04\n",
       "5169.0            2.099e+04   4031.196      5.207      0.000    1.31e+04    2.89e+04\n",
       "5174.0            8015.5017   3945.068      2.032      0.042     282.567    1.57e+04\n",
       "5179.0             1.14e+04   4114.840      2.771      0.006    3335.369    1.95e+04\n",
       "5181.0            1.224e+04   4311.848      2.838      0.005    3786.337    2.07e+04\n",
       "5187.0            1.224e+04   4519.231      2.708      0.007    3379.174    2.11e+04\n",
       "5229.0            3035.1962   3090.895      0.982      0.326   -3023.429    9093.822\n",
       "5234.0           -2691.0876   3571.956     -0.753      0.451   -9692.666    4310.491\n",
       "5237.0            1.093e+04   3947.487      2.769      0.006    3192.997    1.87e+04\n",
       "5252.0            1.027e+04   3622.943      2.835      0.005    3170.558    1.74e+04\n",
       "5254.0            1.069e+04   3659.795      2.920      0.004    3512.110    1.79e+04\n",
       "5306.0            8878.3146   3452.015      2.572      0.010    2111.840    1.56e+04\n",
       "5338.0            1.152e+04   3892.628      2.959      0.003    3886.700    1.91e+04\n",
       "5377.0            1.224e+04   4350.392      2.814      0.005    3713.543    2.08e+04\n",
       "5439.0            9178.7547   4056.182      2.263      0.024    1228.020    1.71e+04\n",
       "5456.0            1.282e+04   4549.255      2.818      0.005    3901.137    2.17e+04\n",
       "5464.0            1.027e+04   4237.773      2.423      0.015    1961.674    1.86e+04\n",
       "5476.0            1.204e+04   4595.709      2.620      0.009    3031.184     2.1e+04\n",
       "5492.0           -1.033e+04   3427.634     -3.015      0.003   -1.71e+04   -3615.409\n",
       "5496.0            1.046e+04   3768.357      2.776      0.006    3073.259    1.78e+04\n",
       "5505.0            1.179e+04   4246.204      2.777      0.005    3468.650    2.01e+04\n",
       "5518.0            9808.6268   4310.289      2.276      0.023    1359.804    1.83e+04\n",
       "5520.0            8554.6270   3689.535      2.319      0.020    1322.576    1.58e+04\n",
       "5545.0              1.2e+04   4089.836      2.933      0.003    3979.670       2e+04\n",
       "5568.0            1.447e+04   3861.174      3.747      0.000    6900.100     2.2e+04\n",
       "5569.0            1.217e+04   4187.067      2.906      0.004    3960.789    2.04e+04\n",
       "5578.0            1.152e+04   4034.562      2.856      0.004    3615.039    1.94e+04\n",
       "5581.0            1.133e+04   3917.795      2.892      0.004    3649.874     1.9e+04\n",
       "5589.0            3608.1835   3097.460      1.165      0.244   -2463.310    9679.677\n",
       "5597.0            1.356e+04   4297.825      3.156      0.002    5140.132     2.2e+04\n",
       "5606.0           -2.837e+04   3149.020     -9.010      0.000   -3.45e+04   -2.22e+04\n",
       "5639.0            1.289e+04   4328.275      2.978      0.003    4407.076    2.14e+04\n",
       "5667.0            7881.4811   4801.503      1.641      0.101   -1530.197    1.73e+04\n",
       "5690.0            1.195e+04   4008.021      2.981      0.003    4089.735    1.98e+04\n",
       "5709.0            1.123e+04   4100.696      2.739      0.006    3192.150    1.93e+04\n",
       "5726.0            1.086e+04   4109.937      2.641      0.008    2798.903    1.89e+04\n",
       "5764.0            9713.0214   3363.224      2.888      0.004    3120.589    1.63e+04\n",
       "5772.0            1.116e+04   3785.991      2.948      0.003    3740.005    1.86e+04\n",
       "5860.0           -2.048e+04   3302.986     -6.200      0.000    -2.7e+04    -1.4e+04\n",
       "5878.0            1.323e+04   3467.510      3.814      0.000    6429.968       2e+04\n",
       "5903.0            5588.3979   3992.481      1.400      0.162   -2237.473    1.34e+04\n",
       "5905.0            8895.7752   3610.913      2.464      0.014    1817.835     1.6e+04\n",
       "5959.0            6280.2646   3454.673      1.818      0.069    -491.421    1.31e+04\n",
       "6008.0            2.856e+04   3084.708      9.258      0.000    2.25e+04    3.46e+04\n",
       "6034.0            8870.7729   3711.642      2.390      0.017    1595.389    1.61e+04\n",
       "6035.0            3860.7482   4533.466      0.852      0.394   -5025.536    1.27e+04\n",
       "6036.0             981.9694   3092.145      0.318      0.751   -5079.106    7043.045\n",
       "6039.0            1.112e+04   3962.475      2.806      0.005    3350.540    1.89e+04\n",
       "6044.0            1.228e+04   4576.202      2.684      0.007    3311.930    2.13e+04\n",
       "6066.0           -2.641e+04   4513.111     -5.851      0.000   -3.53e+04   -1.76e+04\n",
       "6078.0            1.134e+04   3843.392      2.950      0.003    3805.519    1.89e+04\n",
       "6081.0           -1.015e+04   3315.955     -3.062      0.002   -1.67e+04   -3654.074\n",
       "60893.0          -6784.4217   6312.243     -1.075      0.282   -1.92e+04    5588.537\n",
       "6097.0            1.162e+04   4377.402      2.655      0.008    3043.600    2.02e+04\n",
       "6102.0            1.069e+04   4037.788      2.646      0.008    2770.560    1.86e+04\n",
       "6104.0            1028.7449   3549.960      0.290      0.772   -5929.718    7987.208\n",
       "6109.0            1854.4705   3067.203      0.605      0.545   -4157.716    7866.657\n",
       "6127.0            5567.8581   3841.809      1.449      0.147   -1962.673    1.31e+04\n",
       "61552.0          -3961.2146   4323.316     -0.916      0.360   -1.24e+04    4513.143\n",
       "6158.0            7855.3039   3597.807      2.183      0.029     803.053    1.49e+04\n",
       "6171.0            1.098e+04   3902.421      2.814      0.005    3333.242    1.86e+04\n",
       "61780.0           1.015e+04   5314.163      1.909      0.056    -270.110    2.06e+04\n",
       "6207.0            1.081e+04   4080.983      2.648      0.008    2807.745    1.88e+04\n",
       "6214.0            1.179e+04   3902.095      3.020      0.003    4136.961    1.94e+04\n",
       "6216.0            1.181e+04   4533.594      2.605      0.009    2922.193    2.07e+04\n",
       "62221.0           1.167e+04   5095.014      2.290      0.022    1679.299    2.17e+04\n",
       "6259.0            4520.9981   4901.812      0.922      0.356   -5087.301    1.41e+04\n",
       "62599.0          -2.423e+04   5073.691     -4.775      0.000   -3.42e+04   -1.43e+04\n",
       "6266.0            7497.8711   3584.960      2.091      0.037     470.802    1.45e+04\n",
       "6268.0            4676.5986   3503.161      1.335      0.182   -2190.131    1.15e+04\n",
       "6288.0            1.031e+04   3585.786      2.875      0.004    3281.797    1.73e+04\n",
       "6297.0            1.153e+04   3858.893      2.988      0.003    3967.438    1.91e+04\n",
       "6307.0           -1.112e+04   4115.238     -2.702      0.007   -1.92e+04   -3053.456\n",
       "6313.0            1.123e+04   5130.808      2.189      0.029    1173.829    2.13e+04\n",
       "6314.0            1.156e+04   4309.943      2.682      0.007    3111.577       2e+04\n",
       "6326.0            5472.4704   3173.544      1.724      0.085    -748.160    1.17e+04\n",
       "6349.0            1.062e+04   3838.915      2.766      0.006    3095.272    1.81e+04\n",
       "6357.0            1.211e+04   4304.418      2.814      0.005    3676.182    2.06e+04\n",
       "6375.0            1.628e+04   4109.560      3.962      0.000    8228.526    2.43e+04\n",
       "6376.0            1.164e+04   4260.650      2.732      0.006    3290.644       2e+04\n",
       "6379.0              81.8377   6900.673      0.012      0.991   -1.34e+04    1.36e+04\n",
       "6386.0            1.186e+04   3968.374      2.988      0.003    4078.322    1.96e+04\n",
       "6403.0            6876.8959   3906.829      1.760      0.078    -781.085    1.45e+04\n",
       "6410.0            1.278e+04   4471.768      2.858      0.004    4013.282    2.15e+04\n",
       "6416.0            4624.2742   3933.435      1.176      0.240   -3085.858    1.23e+04\n",
       "6424.0            1.152e+04   4145.390      2.778      0.005    3390.834    1.96e+04\n",
       "6433.0            1.124e+04   4483.646      2.506      0.012    2447.421       2e+04\n",
       "6435.0            1.179e+04   3373.988      3.494      0.000    5173.680    1.84e+04\n",
       "6492.0            8504.2651   3427.252      2.481      0.013    1786.329    1.52e+04\n",
       "6497.0            1206.9563   5114.543      0.236      0.813   -8818.328    1.12e+04\n",
       "6500.0            8538.3757   8012.486      1.066      0.287   -7167.319    2.42e+04\n",
       "6509.0            1.061e+04   3733.791      2.840      0.005    3286.955    1.79e+04\n",
       "6527.0            1.237e+04   4453.015      2.777      0.005    3637.275    2.11e+04\n",
       "6528.0            1.003e+04   4186.629      2.396      0.017    1824.608    1.82e+04\n",
       "6531.0            2584.8064   3530.625      0.732      0.464   -4335.757    9505.370\n",
       "6532.0            7172.5703   3538.230      2.027      0.043     237.099    1.41e+04\n",
       "6543.0            1.209e+04   4213.207      2.869      0.004    3827.962    2.03e+04\n",
       "6548.0            1.163e+04   4219.948      2.756      0.006    3358.381    1.99e+04\n",
       "6550.0             1.16e+04   4153.088      2.793      0.005    3459.857    1.97e+04\n",
       "6552.0            1.155e+04   4402.463      2.623      0.009    2916.465    2.02e+04\n",
       "6565.0            7598.5452   3657.372      2.078      0.038     429.538    1.48e+04\n",
       "6571.0            1.145e+04   3969.600      2.883      0.004    3665.132    1.92e+04\n",
       "6573.0            1.116e+04   3858.424      2.893      0.004    3598.092    1.87e+04\n",
       "6641.0            8286.5642   5855.445      1.415      0.157   -3191.001    1.98e+04\n",
       "6649.0            1.269e+04   4327.620      2.933      0.003    4208.392    2.12e+04\n",
       "6730.0            5732.9408   3915.766      1.464      0.143   -1942.558    1.34e+04\n",
       "6731.0            8855.9574   3487.401      2.539      0.011    2020.120    1.57e+04\n",
       "6742.0            1.236e+04   5705.178      2.167      0.030    1180.548    2.35e+04\n",
       "6745.0            1.229e+04   4185.573      2.936      0.003    4086.024    2.05e+04\n",
       "6756.0            1.155e+04   4384.406      2.635      0.008    2957.935    2.01e+04\n",
       "6765.0           -5696.3086   3216.442     -1.771      0.077    -1.2e+04     608.408\n",
       "6768.0            1.353e+04   4604.794      2.938      0.003    4500.806    2.26e+04\n",
       "6774.0           -1.616e+04   3861.019     -4.185      0.000   -2.37e+04   -8591.261\n",
       "6797.0            1.235e+04   4643.503      2.661      0.008    3252.897    2.15e+04\n",
       "6803.0            1.176e+04   4292.725      2.740      0.006    3347.536    2.02e+04\n",
       "6821.0            1.108e+04   3990.375      2.777      0.005    3260.672    1.89e+04\n",
       "6830.0            9687.5740   4183.001      2.316      0.021    1488.254    1.79e+04\n",
       "6845.0            9083.8144   3443.693      2.638      0.008    2333.650    1.58e+04\n",
       "6848.0             1.05e+04   4530.801      2.317      0.021    1615.554    1.94e+04\n",
       "6873.0            7561.7359   4168.816      1.814      0.070    -609.779    1.57e+04\n",
       "6900.0            9837.8240   3459.183      2.844      0.004    3057.298    1.66e+04\n",
       "6908.0            9832.6262   3451.803      2.849      0.004    3066.567    1.66e+04\n",
       "6994.0            9440.4454   4191.065      2.253      0.024    1225.319    1.77e+04\n",
       "7045.0            1472.8365   4667.886      0.316      0.752   -7676.931    1.06e+04\n",
       "7065.0            1.662e+04   3626.584      4.583      0.000    9510.311    2.37e+04\n",
       "7085.0             1.37e+04   3754.828      3.648      0.000    6338.237    2.11e+04\n",
       "7107.0            1.053e+04   3678.034      2.863      0.004    3321.390    1.77e+04\n",
       "7116.0            1.229e+04   4485.757      2.740      0.006    3498.969    2.11e+04\n",
       "7117.0            1.292e+04   5085.161      2.540      0.011    2950.954    2.29e+04\n",
       "7121.0            1.058e+04   3994.613      2.649      0.008    2751.203    1.84e+04\n",
       "7127.0            8326.5385   4645.531      1.792      0.073    -779.411    1.74e+04\n",
       "7139.0            1.098e+04   3887.701      2.825      0.005    3362.791    1.86e+04\n",
       "7146.0            1.186e+04   4092.418      2.898      0.004    3839.427    1.99e+04\n",
       "7163.0            1.495e+04   4159.258      3.594      0.000    6793.822    2.31e+04\n",
       "7180.0            8249.7034   3588.141      2.299      0.022    1216.400    1.53e+04\n",
       "7183.0            8941.8698   3436.226      2.602      0.009    2206.343    1.57e+04\n",
       "7228.0            1.903e+04   4269.428      4.457      0.000    1.07e+04    2.74e+04\n",
       "7232.0            9413.4580   5148.564      1.828      0.068    -678.513    1.95e+04\n",
       "7250.0            7746.3050   4039.279      1.918      0.055    -171.298    1.57e+04\n",
       "7257.0            2.697e+04   3994.030      6.753      0.000    1.91e+04    3.48e+04\n",
       "7260.0            1.122e+04   3806.476      2.948      0.003    3759.564    1.87e+04\n",
       "7267.0            1.128e+04   4500.943      2.506      0.012    2456.332    2.01e+04\n",
       "7268.0            -603.4573   5189.866     -0.116      0.907   -1.08e+04    9569.471\n",
       "7281.0            1.212e+04   4816.206      2.517      0.012    2679.504    2.16e+04\n",
       "7291.0            9727.9780   3461.883      2.810      0.005    2942.159    1.65e+04\n",
       "7343.0            3150.4995   3700.356      0.851      0.395   -4102.762    1.04e+04\n",
       "7346.0            2872.0454   3854.823      0.745      0.456   -4683.996    1.04e+04\n",
       "7401.0            1.167e+04   3948.587      2.955      0.003    3930.063    1.94e+04\n",
       "7409.0            1.069e+04   3643.036      2.934      0.003    3549.555    1.78e+04\n",
       "7420.0            7804.1713   3393.969      2.299      0.021    1151.475    1.45e+04\n",
       "7435.0            7570.2808   3556.477      2.129      0.033     599.044    1.45e+04\n",
       "7466.0            1.007e+04   4248.285      2.370      0.018    1741.721    1.84e+04\n",
       "7486.0              24.5930   4677.748      0.005      0.996   -9144.506    9193.692\n",
       "7503.0            1.109e+04   4972.750      2.230      0.026    1340.662    2.08e+04\n",
       "7506.0            1.183e+04   3608.116      3.278      0.001    4753.322    1.89e+04\n",
       "7537.0            1.125e+04   4253.619      2.644      0.008    2907.920    1.96e+04\n",
       "7549.0            9586.2867   3477.660      2.757      0.006    2769.543    1.64e+04\n",
       "7554.0              1.1e+04   4185.259      2.628      0.009    2793.298    1.92e+04\n",
       "7557.0            9412.1893   3497.452      2.691      0.007    2556.650    1.63e+04\n",
       "7585.0           -1.845e+04   3426.054     -5.384      0.000   -2.52e+04   -1.17e+04\n",
       "7602.0             1.07e+04   4003.464      2.673      0.008    2853.478    1.85e+04\n",
       "7620.0            7495.0260   4650.077      1.612      0.107   -1619.835    1.66e+04\n",
       "7636.0            1.115e+04   4114.374      2.710      0.007    3084.339    1.92e+04\n",
       "7646.0            1.143e+04   4154.010      2.751      0.006    3285.792    1.96e+04\n",
       "7658.0            8500.3762   3369.414      2.523      0.012    1895.811    1.51e+04\n",
       "7683.0            1.233e+04   4279.500      2.882      0.004    3945.996    2.07e+04\n",
       "7685.0            1.085e+04   3860.880      2.809      0.005    3279.123    1.84e+04\n",
       "7692.0            6148.3863   3351.672      1.834      0.067    -421.401    1.27e+04\n",
       "7762.0            1.157e+04   3823.181      3.027      0.002    4078.412    1.91e+04\n",
       "7772.0           -2312.9745   3060.935     -0.756      0.450   -8312.873    3686.924\n",
       "7773.0            1.097e+04   3933.414      2.790      0.005    3264.037    1.87e+04\n",
       "7777.0            6918.6798   3337.848      2.073      0.038     375.988    1.35e+04\n",
       "7835.0            1.213e+04   4070.265      2.980      0.003    4149.874    2.01e+04\n",
       "7873.0             356.4399   4267.413      0.084      0.933   -8008.341    8721.221\n",
       "7883.0            9144.6351   3447.627      2.652      0.008    2386.760    1.59e+04\n",
       "7904.0            8358.5703   3491.318      2.394      0.017    1515.054    1.52e+04\n",
       "7906.0            1.463e+04   4333.577      3.376      0.001    6135.454    2.31e+04\n",
       "7921.0            1.169e+04   3733.249      3.133      0.002    4376.832     1.9e+04\n",
       "7923.0            1.057e+04   4161.944      2.540      0.011    2415.236    1.87e+04\n",
       "7935.0            8646.6988   3615.011      2.392      0.017    1560.727    1.57e+04\n",
       "7938.0            9733.8663   3700.483      2.630      0.009    2480.356     1.7e+04\n",
       "7985.0           -1.135e+04   3160.766     -3.590      0.000   -1.75e+04   -5150.793\n",
       "8014.0            9609.8260   4009.205      2.397      0.017    1751.173    1.75e+04\n",
       "8030.0            1.263e+04   4422.064      2.856      0.004    3962.928    2.13e+04\n",
       "8046.0             670.6900   4233.990      0.158      0.874   -7628.576    8969.956\n",
       "8047.0            1.106e+04   4258.783      2.597      0.009    2710.084    1.94e+04\n",
       "8062.0            9460.3233   3665.001      2.581      0.010    2276.363    1.66e+04\n",
       "8068.0           -4512.5573   3672.896     -1.229      0.219   -1.17e+04    2686.878\n",
       "8087.0           -3935.6363   4263.718     -0.923      0.356   -1.23e+04    4421.901\n",
       "8095.0            1.144e+04   3816.602      2.996      0.003    3954.389    1.89e+04\n",
       "8096.0            1.172e+04   4341.522      2.699      0.007    3209.639    2.02e+04\n",
       "8109.0            1.179e+04   3980.577      2.963      0.003    3990.859    1.96e+04\n",
       "8123.0            7885.7920   3619.315      2.179      0.029     791.382     1.5e+04\n",
       "8150.0            1.221e+04   4197.973      2.907      0.004    3976.711    2.04e+04\n",
       "8163.0            9725.3249   3553.388      2.737      0.006    2760.142    1.67e+04\n",
       "8176.0            4675.4601   5297.052      0.883      0.377   -5707.569    1.51e+04\n",
       "8202.0            9134.0857   3514.455      2.599      0.009    2245.219     1.6e+04\n",
       "8214.0            8004.1408   3509.355      2.281      0.023    1125.269    1.49e+04\n",
       "8215.0            5917.2501   3535.404      1.674      0.094   -1012.681    1.28e+04\n",
       "8219.0            1.234e+04   4456.469      2.768      0.006    3600.042    2.11e+04\n",
       "8247.0            7192.2790   3412.823      2.107      0.035     502.625    1.39e+04\n",
       "8253.0           -3836.5289   3593.583     -1.068      0.286   -1.09e+04    3207.442\n",
       "8290.0            8767.0517   3867.407      2.267      0.023    1186.344    1.63e+04\n",
       "8293.0            9972.3254   3490.060      2.857      0.004    3131.276    1.68e+04\n",
       "8304.0            1.195e+04   3888.289      3.073      0.002    4327.833    1.96e+04\n",
       "8334.0            1.116e+04   4621.997      2.415      0.016    2100.172    2.02e+04\n",
       "8348.0            1.145e+04   3858.042      2.968      0.003    3889.367     1.9e+04\n",
       "8357.0            1.096e+04   3735.570      2.933      0.003    3633.157    1.83e+04\n",
       "8358.0            9127.7852   3485.587      2.619      0.009    2295.503     1.6e+04\n",
       "8446.0            -871.8342   3930.179     -0.222      0.824   -8575.584    6831.916\n",
       "8460.0            1.298e+04   4816.729      2.694      0.007    3536.325    2.24e+04\n",
       "8463.0             1.09e+04   3994.968      2.727      0.006    3064.933    1.87e+04\n",
       "8479.0            1.081e+04   4473.836      2.416      0.016    2039.046    1.96e+04\n",
       "8530.0            1.763e+04   4105.796      4.295      0.000    9585.083    2.57e+04\n",
       "8536.0            7532.6032   3354.005      2.246      0.025     958.242    1.41e+04\n",
       "8543.0            3.172e+04   4342.018      7.305      0.000    2.32e+04    4.02e+04\n",
       "8549.0             627.9007   3401.090      0.185      0.854   -6038.754    7294.555\n",
       "8551.0            1.199e+04   4342.729      2.761      0.006    3477.589    2.05e+04\n",
       "8559.0            7047.3480   4411.585      1.597      0.110   -1600.032    1.57e+04\n",
       "8573.0             166.0404   5267.575      0.032      0.975   -1.02e+04    1.05e+04\n",
       "8606.0            1.223e+04   3676.982      3.327      0.001    5024.870    1.94e+04\n",
       "8607.0            1.211e+04   4436.509      2.730      0.006    3415.173    2.08e+04\n",
       "8648.0            1.082e+04   3758.601      2.878      0.004    3448.081    1.82e+04\n",
       "8657.0            4476.0239   3604.525      1.242      0.214   -2589.395    1.15e+04\n",
       "8675.0            1.141e+04   5298.339      2.153      0.031    1023.306    2.18e+04\n",
       "8681.0            6118.3062   3257.302      1.878      0.060    -266.503    1.25e+04\n",
       "8687.0            8405.6270   3595.904      2.338      0.019    1357.107    1.55e+04\n",
       "8692.0            8839.3495   3489.005      2.533      0.011    2000.368    1.57e+04\n",
       "8699.0            1.155e+04   3921.233      2.945      0.003    3861.315    1.92e+04\n",
       "8717.0             1.22e+04   4182.274      2.918      0.004    4006.968    2.04e+04\n",
       "8759.0            6244.1433   4396.099      1.420      0.156   -2372.881    1.49e+04\n",
       "8762.0            1.061e+04   3657.755      2.902      0.004    3443.457    1.78e+04\n",
       "8819.0            1.286e+04   4613.574      2.787      0.005    3813.149    2.19e+04\n",
       "8850.0            1.138e+04   3885.598      2.930      0.003    3766.556     1.9e+04\n",
       "8852.0            1.174e+04   3947.181      2.974      0.003    4003.634    1.95e+04\n",
       "8859.0            1.188e+04   4431.693      2.680      0.007    3191.641    2.06e+04\n",
       "8867.0            4059.6058   3642.118      1.115      0.265   -3079.500    1.12e+04\n",
       "8881.0            9861.0293   3659.118      2.695      0.007    2688.601     1.7e+04\n",
       "8958.0            8144.0343   3788.534      2.150      0.032     717.930    1.56e+04\n",
       "8972.0           -1.264e+04   3291.484     -3.841      0.000   -1.91e+04   -6190.724\n",
       "8990.0            -924.1857   3807.903     -0.243      0.808   -8388.256    6539.885\n",
       "9004.0            1.229e+04   4462.181      2.754      0.006    3542.360     2.1e+04\n",
       "9016.0            8958.7502   3536.521      2.533      0.011    2026.629    1.59e+04\n",
       "9048.0            8273.3416   3313.001      2.497      0.013    1779.354    1.48e+04\n",
       "9051.0            1755.5094   3880.492      0.452      0.651   -5850.847    9361.866\n",
       "9071.0            8934.2338   3656.665      2.443      0.015    1766.613    1.61e+04\n",
       "9112.0            6692.6708   3511.189      1.906      0.057    -189.795    1.36e+04\n",
       "9114.0            6257.3804   3488.998      1.793      0.073    -581.587    1.31e+04\n",
       "9132.0            1.125e+04   5511.232      2.042      0.041     451.850    2.21e+04\n",
       "9173.0            1.073e+04   3920.386      2.737      0.006    3047.326    1.84e+04\n",
       "9180.0            1.213e+04   4299.920      2.821      0.005    3702.923    2.06e+04\n",
       "9186.0            1.049e+04   3674.660      2.853      0.004    3282.112    1.77e+04\n",
       "9191.0            7285.4719   4336.542      1.680      0.093   -1214.812    1.58e+04\n",
       "9216.0            4558.0369   3153.822      1.445      0.148   -1623.935    1.07e+04\n",
       "9217.0            1093.0247   3038.271      0.360      0.719   -4862.450    7048.499\n",
       "9225.0            1.198e+04   3909.157      3.065      0.002    4318.137    1.96e+04\n",
       "9230.0            1.177e+04   4751.892      2.478      0.013    2459.741    2.11e+04\n",
       "9259.0            1.289e+04   4560.427      2.826      0.005    3947.042    2.18e+04\n",
       "9293.0            1.204e+04   4117.970      2.925      0.003    3972.425    2.01e+04\n",
       "9299.0            7289.7440   3592.474      2.029      0.042     247.946    1.43e+04\n",
       "9308.0            5144.3498   4655.985      1.105      0.269   -3982.090    1.43e+04\n",
       "9311.0            6920.9204   4515.774      1.533      0.125   -1930.685    1.58e+04\n",
       "9313.0            5391.0747   3511.619      1.535      0.125   -1492.235    1.23e+04\n",
       "9325.0            1.063e+04   3739.240      2.843      0.004    3299.461     1.8e+04\n",
       "9332.0            1.023e+04   3525.223      2.901      0.004    3315.462    1.71e+04\n",
       "9340.0           -1.671e+04   5692.134     -2.935      0.003   -2.79e+04   -5551.767\n",
       "9372.0             1.14e+04   4667.368      2.443      0.015    2254.314    2.06e+04\n",
       "9411.0            8963.9382   3801.630      2.358      0.018    1512.163    1.64e+04\n",
       "9459.0            1832.5262   4005.949      0.457      0.647   -6019.744    9684.796\n",
       "9465.0             1.46e+04   3435.437      4.250      0.000    7865.971    2.13e+04\n",
       "9472.0            5007.6051   3235.355      1.548      0.122   -1334.184    1.13e+04\n",
       "9483.0           -2008.3669   3843.983     -0.522      0.601   -9543.159    5526.425\n",
       "9563.0           -1.456e+04   4842.914     -3.006      0.003    -2.4e+04   -5063.006\n",
       "9590.0            7563.3751   4295.291      1.761      0.078    -856.049     1.6e+04\n",
       "9598.0            4212.3711   3955.432      1.065      0.287   -3540.878     1.2e+04\n",
       "9599.0            3534.0283   3706.502      0.953      0.340   -3731.282    1.08e+04\n",
       "9602.0            3671.4096   4352.900      0.843      0.399   -4860.938    1.22e+04\n",
       "9619.0            1.156e+04   4595.291      2.516      0.012    2553.723    2.06e+04\n",
       "9643.0            9380.7817   3571.246      2.627      0.009    2380.594    1.64e+04\n",
       "9650.0            8642.2789   3612.505      2.392      0.017    1561.219    1.57e+04\n",
       "9653.0           -6028.2181   5293.176     -1.139      0.255   -1.64e+04    4347.214\n",
       "9667.0            8520.3587   3445.313      2.473      0.013    1767.021    1.53e+04\n",
       "9698.0            9705.0529   4128.143      2.351      0.019    1613.262    1.78e+04\n",
       "9699.0            1.144e+04   3844.009      2.976      0.003    3906.063     1.9e+04\n",
       "9719.0            1626.6505   3065.684      0.531      0.596   -4382.558    7635.859\n",
       "9742.0             126.1225   3994.111      0.032      0.975   -7702.944    7955.189\n",
       "9761.0            1.153e+04   3830.821      3.010      0.003    4021.458     1.9e+04\n",
       "9771.0            3292.4690   3034.847      1.085      0.278   -2656.293    9241.231\n",
       "9772.0            1.173e+04   4282.121      2.740      0.006    3338.911    2.01e+04\n",
       "9778.0            9182.4470   3390.618      2.708      0.007    2536.319    1.58e+04\n",
       "9799.0            2332.9851   3127.518      0.746      0.456   -3797.427    8463.398\n",
       "9815.0            1.134e+04   3835.010      2.958      0.003    3827.470    1.89e+04\n",
       "9818.0           -3.454e+04   3515.963     -9.824      0.000   -4.14e+04   -2.76e+04\n",
       "9837.0            1.263e+04   4527.010      2.790      0.005    3758.533    2.15e+04\n",
       "9922.0            2817.7170   3438.711      0.819      0.413   -3922.682    9558.116\n",
       "9954.0            9074.8491   4310.015      2.106      0.035     626.563    1.75e+04\n",
       "9963.0            9738.4652   3612.823      2.696      0.007    2656.781    1.68e+04\n",
       "9988.0             1.23e+04   4529.727      2.715      0.007    3421.174    2.12e+04\n",
       "gspilltecIVX1981    -0.0481      0.071     -0.674      0.500      -0.188       0.092\n",
       "gspilltecIVX1982    -0.0110      0.071     -0.155      0.877      -0.149       0.127\n",
       "gspilltecIVX1983    -0.0216      0.070     -0.310      0.757      -0.158       0.115\n",
       "gspilltecIVX1984    -0.0677      0.070     -0.970      0.332      -0.204       0.069\n",
       "gspilltecIVX1985    -0.1016      0.070     -1.445      0.148      -0.239       0.036\n",
       "gspilltecIVX1986    -0.1426      0.071     -2.003      0.045      -0.282      -0.003\n",
       "gspilltecIVX1987    -0.1461      0.072     -2.018      0.044      -0.288      -0.004\n",
       "gspilltecIVX1988    -0.1734      0.073     -2.364      0.018      -0.317      -0.030\n",
       "gspilltecIVX1989    -0.1766      0.075     -2.370      0.018      -0.323      -0.031\n",
       "gspilltecIVX1990    -0.2093      0.076     -2.770      0.006      -0.357      -0.061\n",
       "gspilltecIVX1991    -0.1902      0.077     -2.483      0.013      -0.340      -0.040\n",
       "gspilltecIVX1992    -0.1770      0.078     -2.279      0.023      -0.329      -0.025\n",
       "gspilltecIVX1993    -0.1474      0.079     -1.860      0.063      -0.303       0.008\n",
       "gspilltecIVX1994    -0.1520      0.081     -1.884      0.060      -0.310       0.006\n",
       "gspilltecIVX1995    -0.1499      0.083     -1.812      0.070      -0.312       0.012\n",
       "gspilltecIVX1996    -0.1144      0.085     -1.341      0.180      -0.281       0.053\n",
       "gspilltecIVX1997    -0.1120      0.088     -1.272      0.203      -0.284       0.061\n",
       "gspilltecIVX1998    -0.0932      0.091     -1.028      0.304      -0.271       0.084\n",
       "gspilltecIVX1999     0.0086      0.093      0.093      0.926      -0.173       0.191\n",
       "gspilltecIVX2000    -0.0903      0.095     -0.953      0.341      -0.276       0.095\n",
       "gspilltecIVX2001    -0.1343      0.096     -1.398      0.162      -0.323       0.054\n",
       "gspillsicIVX1981     0.0015      0.134      0.011      0.991      -0.260       0.264\n",
       "gspillsicIVX1982    -0.0311      0.132     -0.236      0.813      -0.289       0.227\n",
       "gspillsicIVX1983    -0.0415      0.129     -0.320      0.749      -0.295       0.212\n",
       "gspillsicIVX1984    -0.0741      0.128     -0.577      0.564      -0.326       0.178\n",
       "gspillsicIVX1985    -0.0813      0.129     -0.632      0.527      -0.333       0.171\n",
       "gspillsicIVX1986    -0.1101      0.129     -0.854      0.393      -0.363       0.143\n",
       "gspillsicIVX1987    -0.1255      0.130     -0.967      0.334      -0.380       0.129\n",
       "gspillsicIVX1988    -0.1413      0.131     -1.082      0.279      -0.397       0.115\n",
       "gspillsicIVX1989    -0.1366      0.131     -1.040      0.299      -0.394       0.121\n",
       "gspillsicIVX1990    -0.1397      0.132     -1.055      0.292      -0.399       0.120\n",
       "gspillsicIVX1991    -0.1368      0.134     -1.022      0.307      -0.399       0.125\n",
       "gspillsicIVX1992    -0.1956      0.135     -1.450      0.147      -0.460       0.069\n",
       "gspillsicIVX1993    -0.2200      0.137     -1.611      0.107      -0.488       0.048\n",
       "gspillsicIVX1994    -0.2246      0.138     -1.626      0.104      -0.495       0.046\n",
       "gspillsicIVX1995    -0.1987      0.141     -1.412      0.158      -0.475       0.077\n",
       "gspillsicIVX1996    -0.2430      0.144     -1.692      0.091      -0.525       0.039\n",
       "gspillsicIVX1997    -0.2197      0.147     -1.493      0.135      -0.508       0.069\n",
       "gspillsicIVX1998    -0.1813      0.151     -1.203      0.229      -0.477       0.114\n",
       "gspillsicIVX1999    -0.1905      0.154     -1.238      0.216      -0.492       0.111\n",
       "gspillsicIVX2000    -0.0964      0.157     -0.615      0.539      -0.404       0.211\n",
       "gspillsicIVX2001    -0.2797      0.159     -1.761      0.078      -0.591       0.032\n",
       "==============================================================================\n",
       "Omnibus:                    21533.010   Durbin-Watson:                   0.543\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         48401709.884\n",
       "Skew:                          10.015   Prob(JB):                         0.00\n",
       "Kurtosis:                     296.914   Cond. No.                     2.80e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.8e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate effect of spillovers on firm value, OLS w/o firm FE's. Estimate spillovers separately, then together.\n",
    "\n",
    "# Full model\n",
    "year_model1 = sm.OLS(y_var,x_vars).fit()\n",
    "year_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "650d2db3-e371-4728-b906-7120bba05230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.358</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.357</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   275.7</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:02:59</td>     <th>  Log-Likelihood:    </th> <td>-1.4593e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13385</td>      <th>  AIC:               </th>  <td>2.919e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13357</td>      <th>  BIC:               </th>  <td>2.921e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    27</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td> 1170.0198</td> <td>  203.818</td> <td>    5.741</td> <td> 0.000</td> <td>  770.508</td> <td> 1569.532</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th>      <td>   -0.0529</td> <td>    0.039</td> <td>   -1.368</td> <td> 0.171</td> <td>   -0.129</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>   -9.5652</td> <td>    1.825</td> <td>   -5.241</td> <td> 0.000</td> <td>  -13.142</td> <td>   -5.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.4633</td> <td>    0.034</td> <td>   13.628</td> <td> 0.000</td> <td>    0.397</td> <td>    0.530</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.4524</td> <td>    0.048</td> <td>    9.398</td> <td> 0.000</td> <td>    0.358</td> <td>    0.547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>  -37.6440</td> <td>    3.501</td> <td>  -10.753</td> <td> 0.000</td> <td>  -44.506</td> <td>  -30.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>   12.1159</td> <td>    0.463</td> <td>   26.146</td> <td> 0.000</td> <td>   11.208</td> <td>   13.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1981</th> <td>   -0.0200</td> <td>    0.050</td> <td>   -0.399</td> <td> 0.690</td> <td>   -0.118</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1982</th> <td>   -0.0029</td> <td>    0.049</td> <td>   -0.060</td> <td> 0.953</td> <td>   -0.100</td> <td>    0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1983</th> <td>    0.0100</td> <td>    0.048</td> <td>    0.209</td> <td> 0.835</td> <td>   -0.084</td> <td>    0.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1984</th> <td>   -0.0064</td> <td>    0.047</td> <td>   -0.136</td> <td> 0.892</td> <td>   -0.098</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1985</th> <td>   -0.0011</td> <td>    0.046</td> <td>   -0.024</td> <td> 0.981</td> <td>   -0.091</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1986</th> <td>    0.0009</td> <td>    0.045</td> <td>    0.021</td> <td> 0.983</td> <td>   -0.087</td> <td>    0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1987</th> <td>    0.0010</td> <td>    0.044</td> <td>    0.023</td> <td> 0.981</td> <td>   -0.086</td> <td>    0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1988</th> <td>   -0.0061</td> <td>    0.044</td> <td>   -0.138</td> <td> 0.890</td> <td>   -0.092</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1989</th> <td>    0.0085</td> <td>    0.043</td> <td>    0.196</td> <td> 0.845</td> <td>   -0.076</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1990</th> <td>   -0.0048</td> <td>    0.043</td> <td>   -0.113</td> <td> 0.910</td> <td>   -0.089</td> <td>    0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1991</th> <td>    0.0184</td> <td>    0.042</td> <td>    0.433</td> <td> 0.665</td> <td>   -0.065</td> <td>    0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1992</th> <td>    0.0194</td> <td>    0.042</td> <td>    0.461</td> <td> 0.645</td> <td>   -0.063</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1993</th> <td>    0.0334</td> <td>    0.042</td> <td>    0.801</td> <td> 0.423</td> <td>   -0.048</td> <td>    0.115</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1994</th> <td>    0.0308</td> <td>    0.042</td> <td>    0.742</td> <td> 0.458</td> <td>   -0.051</td> <td>    0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1995</th> <td>    0.0607</td> <td>    0.041</td> <td>    1.472</td> <td> 0.141</td> <td>   -0.020</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1996</th> <td>    0.0883</td> <td>    0.041</td> <td>    2.160</td> <td> 0.031</td> <td>    0.008</td> <td>    0.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1997</th> <td>    0.1267</td> <td>    0.041</td> <td>    3.120</td> <td> 0.002</td> <td>    0.047</td> <td>    0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1998</th> <td>    0.1688</td> <td>    0.040</td> <td>    4.177</td> <td> 0.000</td> <td>    0.090</td> <td>    0.248</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1999</th> <td>    0.2330</td> <td>    0.040</td> <td>    5.779</td> <td> 0.000</td> <td>    0.154</td> <td>    0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX2000</th> <td>    0.2164</td> <td>    0.040</td> <td>    5.354</td> <td> 0.000</td> <td>    0.137</td> <td>    0.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX2001</th> <td>    0.1534</td> <td>    0.041</td> <td>    3.778</td> <td> 0.000</td> <td>    0.074</td> <td>    0.233</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>23791.941</td> <th>  Durbin-Watson:     </th>   <td>   0.353</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>58097440.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>12.502</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>324.786</td>  <th>  Cond. No.          </th>   <td>5.02e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.02e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.358    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.357    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      275.7    \\\\\n",
       "\\textbf{Date:}             & Wed, 16 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     02:02:59     & \\textbf{  Log-Likelihood:    } & -1.4593e+05   \\\\\n",
       "\\textbf{No. Observations:} &       13385      & \\textbf{  AIC:               } &  2.919e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       13357      & \\textbf{  BIC:               } &  2.921e+05    \\\\\n",
       "\\textbf{Df Model:}         &          27      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &    1170.0198  &      203.818     &     5.741  &         0.000        &      770.508    &     1569.532     \\\\\n",
       "\\textbf{gspilltecIV}      &      -0.0529  &        0.039     &    -1.368  &         0.171        &       -0.129    &        0.023     \\\\\n",
       "\\textbf{pat\\_count}       &      -9.5652  &        1.825     &    -5.241  &         0.000        &      -13.142    &       -5.988     \\\\\n",
       "\\textbf{rsales}           &       0.4633  &        0.034     &    13.628  &         0.000        &        0.397    &        0.530     \\\\\n",
       "\\textbf{rppent}           &       0.4524  &        0.048     &     9.398  &         0.000        &        0.358    &        0.547     \\\\\n",
       "\\textbf{emp}              &     -37.6440  &        3.501     &   -10.753  &         0.000        &      -44.506    &      -30.782     \\\\\n",
       "\\textbf{rxrd}             &      12.1159  &        0.463     &    26.146  &         0.000        &       11.208    &       13.024     \\\\\n",
       "\\textbf{gspilltecIVX1981} &      -0.0200  &        0.050     &    -0.399  &         0.690        &       -0.118    &        0.078     \\\\\n",
       "\\textbf{gspilltecIVX1982} &      -0.0029  &        0.049     &    -0.060  &         0.953        &       -0.100    &        0.094     \\\\\n",
       "\\textbf{gspilltecIVX1983} &       0.0100  &        0.048     &     0.209  &         0.835        &       -0.084    &        0.104     \\\\\n",
       "\\textbf{gspilltecIVX1984} &      -0.0064  &        0.047     &    -0.136  &         0.892        &       -0.098    &        0.086     \\\\\n",
       "\\textbf{gspilltecIVX1985} &      -0.0011  &        0.046     &    -0.024  &         0.981        &       -0.091    &        0.089     \\\\\n",
       "\\textbf{gspilltecIVX1986} &       0.0009  &        0.045     &     0.021  &         0.983        &       -0.087    &        0.089     \\\\\n",
       "\\textbf{gspilltecIVX1987} &       0.0010  &        0.044     &     0.023  &         0.981        &       -0.086    &        0.088     \\\\\n",
       "\\textbf{gspilltecIVX1988} &      -0.0061  &        0.044     &    -0.138  &         0.890        &       -0.092    &        0.080     \\\\\n",
       "\\textbf{gspilltecIVX1989} &       0.0085  &        0.043     &     0.196  &         0.845        &       -0.076    &        0.093     \\\\\n",
       "\\textbf{gspilltecIVX1990} &      -0.0048  &        0.043     &    -0.113  &         0.910        &       -0.089    &        0.079     \\\\\n",
       "\\textbf{gspilltecIVX1991} &       0.0184  &        0.042     &     0.433  &         0.665        &       -0.065    &        0.101     \\\\\n",
       "\\textbf{gspilltecIVX1992} &       0.0194  &        0.042     &     0.461  &         0.645        &       -0.063    &        0.102     \\\\\n",
       "\\textbf{gspilltecIVX1993} &       0.0334  &        0.042     &     0.801  &         0.423        &       -0.048    &        0.115     \\\\\n",
       "\\textbf{gspilltecIVX1994} &       0.0308  &        0.042     &     0.742  &         0.458        &       -0.051    &        0.112     \\\\\n",
       "\\textbf{gspilltecIVX1995} &       0.0607  &        0.041     &     1.472  &         0.141        &       -0.020    &        0.141     \\\\\n",
       "\\textbf{gspilltecIVX1996} &       0.0883  &        0.041     &     2.160  &         0.031        &        0.008    &        0.168     \\\\\n",
       "\\textbf{gspilltecIVX1997} &       0.1267  &        0.041     &     3.120  &         0.002        &        0.047    &        0.206     \\\\\n",
       "\\textbf{gspilltecIVX1998} &       0.1688  &        0.040     &     4.177  &         0.000        &        0.090    &        0.248     \\\\\n",
       "\\textbf{gspilltecIVX1999} &       0.2330  &        0.040     &     5.779  &         0.000        &        0.154    &        0.312     \\\\\n",
       "\\textbf{gspilltecIVX2000} &       0.2164  &        0.040     &     5.354  &         0.000        &        0.137    &        0.296     \\\\\n",
       "\\textbf{gspilltecIVX2001} &       0.1534  &        0.041     &     3.778  &         0.000        &        0.074    &        0.233     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 23791.941 & \\textbf{  Durbin-Watson:     } &      0.353    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 58097440.358  \\\\\n",
       "\\textbf{Skew:}          &   12.502  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  324.786  & \\textbf{  Cond. No.          } &   5.02e+04    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 5.02e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.358\n",
       "Model:                            OLS   Adj. R-squared:                  0.357\n",
       "Method:                 Least Squares   F-statistic:                     275.7\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        02:02:59   Log-Likelihood:            -1.4593e+05\n",
       "No. Observations:               13385   AIC:                         2.919e+05\n",
       "Df Residuals:                   13357   BIC:                         2.921e+05\n",
       "Df Model:                          27                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const             1170.0198    203.818      5.741      0.000     770.508    1569.532\n",
       "gspilltecIV         -0.0529      0.039     -1.368      0.171      -0.129       0.023\n",
       "pat_count           -9.5652      1.825     -5.241      0.000     -13.142      -5.988\n",
       "rsales               0.4633      0.034     13.628      0.000       0.397       0.530\n",
       "rppent               0.4524      0.048      9.398      0.000       0.358       0.547\n",
       "emp                -37.6440      3.501    -10.753      0.000     -44.506     -30.782\n",
       "rxrd                12.1159      0.463     26.146      0.000      11.208      13.024\n",
       "gspilltecIVX1981    -0.0200      0.050     -0.399      0.690      -0.118       0.078\n",
       "gspilltecIVX1982    -0.0029      0.049     -0.060      0.953      -0.100       0.094\n",
       "gspilltecIVX1983     0.0100      0.048      0.209      0.835      -0.084       0.104\n",
       "gspilltecIVX1984    -0.0064      0.047     -0.136      0.892      -0.098       0.086\n",
       "gspilltecIVX1985    -0.0011      0.046     -0.024      0.981      -0.091       0.089\n",
       "gspilltecIVX1986     0.0009      0.045      0.021      0.983      -0.087       0.089\n",
       "gspilltecIVX1987     0.0010      0.044      0.023      0.981      -0.086       0.088\n",
       "gspilltecIVX1988    -0.0061      0.044     -0.138      0.890      -0.092       0.080\n",
       "gspilltecIVX1989     0.0085      0.043      0.196      0.845      -0.076       0.093\n",
       "gspilltecIVX1990    -0.0048      0.043     -0.113      0.910      -0.089       0.079\n",
       "gspilltecIVX1991     0.0184      0.042      0.433      0.665      -0.065       0.101\n",
       "gspilltecIVX1992     0.0194      0.042      0.461      0.645      -0.063       0.102\n",
       "gspilltecIVX1993     0.0334      0.042      0.801      0.423      -0.048       0.115\n",
       "gspilltecIVX1994     0.0308      0.042      0.742      0.458      -0.051       0.112\n",
       "gspilltecIVX1995     0.0607      0.041      1.472      0.141      -0.020       0.141\n",
       "gspilltecIVX1996     0.0883      0.041      2.160      0.031       0.008       0.168\n",
       "gspilltecIVX1997     0.1267      0.041      3.120      0.002       0.047       0.206\n",
       "gspilltecIVX1998     0.1688      0.040      4.177      0.000       0.090       0.248\n",
       "gspilltecIVX1999     0.2330      0.040      5.779      0.000       0.154       0.312\n",
       "gspilltecIVX2000     0.2164      0.040      5.354      0.000       0.137       0.296\n",
       "gspilltecIVX2001     0.1534      0.041      3.778      0.000       0.074       0.233\n",
       "==============================================================================\n",
       "Omnibus:                    23791.941   Durbin-Watson:                   0.353\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         58097440.358\n",
       "Skew:                          12.502   Prob(JB):                         0.00\n",
       "Kurtosis:                     324.786   Cond. No.                     5.02e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.02e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tech spillovers model, no firm FE's\n",
    "drop_columns = [col for col in x_vars.columns if 'gspillsicIV' in col]\n",
    "x_vars_nofe = x_vars.drop(columns=fixed_effects)\n",
    "x_vars_nofe = x_vars_nofe.drop(columns=drop_columns)\n",
    "\n",
    "year_model2 = sm.OLS(y_var,x_vars_nofe).fit()\n",
    "year_model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de709391-2187-4eb9-b53d-163e32e529f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.667</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.647</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   32.62</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:03:00</td>     <th>  Log-Likelihood:    </th> <td>-1.4153e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13385</td>      <th>  AIC:               </th>  <td>2.846e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 12609</td>      <th>  BIC:               </th>  <td>2.904e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   775</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-1.404e+04</td> <td> 4166.434</td> <td>   -3.371</td> <td> 0.001</td> <td>-2.22e+04</td> <td>-5877.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIV</th>      <td>    0.4517</td> <td>    0.123</td> <td>    3.677</td> <td> 0.000</td> <td>    0.211</td> <td>    0.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -27.4800</td> <td>    1.908</td> <td>  -14.400</td> <td> 0.000</td> <td>  -31.221</td> <td>  -23.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.7669</td> <td>    0.037</td> <td>   20.684</td> <td> 0.000</td> <td>    0.694</td> <td>    0.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.5705</td> <td>    0.084</td> <td>    6.780</td> <td> 0.000</td> <td>    0.406</td> <td>    0.735</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>   12.9965</td> <td>    7.160</td> <td>    1.815</td> <td> 0.070</td> <td>   -1.038</td> <td>   27.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>   19.7263</td> <td>    0.598</td> <td>   32.993</td> <td> 0.000</td> <td>   18.554</td> <td>   20.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981</th>             <td>  110.3657</td> <td> 1080.039</td> <td>    0.102</td> <td> 0.919</td> <td>-2006.675</td> <td> 2227.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>             <td> -243.4640</td> <td> 1072.952</td> <td>   -0.227</td> <td> 0.820</td> <td>-2346.613</td> <td> 1859.685</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>             <td> -114.3389</td> <td> 1063.325</td> <td>   -0.108</td> <td> 0.914</td> <td>-2198.618</td> <td> 1969.940</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>             <td>  115.2448</td> <td> 1057.289</td> <td>    0.109</td> <td> 0.913</td> <td>-1957.203</td> <td> 2187.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>             <td>  542.6004</td> <td> 1055.171</td> <td>    0.514</td> <td> 0.607</td> <td>-1525.695</td> <td> 2610.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>             <td> 1069.3608</td> <td> 1045.395</td> <td>    1.023</td> <td> 0.306</td> <td> -979.773</td> <td> 3118.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>             <td> 1025.4005</td> <td> 1040.967</td> <td>    0.985</td> <td> 0.325</td> <td>-1015.054</td> <td> 3065.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>             <td> 1291.3045</td> <td> 1041.029</td> <td>    1.240</td> <td> 0.215</td> <td> -749.270</td> <td> 3331.879</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>             <td> 1514.1450</td> <td> 1036.323</td> <td>    1.461</td> <td> 0.144</td> <td> -517.206</td> <td> 3545.496</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>             <td> 1689.8009</td> <td> 1031.752</td> <td>    1.638</td> <td> 0.101</td> <td> -332.589</td> <td> 3712.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>             <td> 1757.6723</td> <td> 1031.035</td> <td>    1.705</td> <td> 0.088</td> <td> -263.313</td> <td> 3778.657</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>             <td> 1663.9210</td> <td> 1030.301</td> <td>    1.615</td> <td> 0.106</td> <td> -355.627</td> <td> 3683.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>             <td> 1275.2181</td> <td> 1026.193</td> <td>    1.243</td> <td> 0.214</td> <td> -736.277</td> <td> 3286.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>             <td> 1193.1166</td> <td> 1028.610</td> <td>    1.160</td> <td> 0.246</td> <td> -823.115</td> <td> 3209.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>             <td> 1591.0463</td> <td> 1026.610</td> <td>    1.550</td> <td> 0.121</td> <td> -421.265</td> <td> 3603.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>             <td> 1293.6561</td> <td> 1028.769</td> <td>    1.257</td> <td> 0.209</td> <td> -722.888</td> <td> 3310.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>             <td> 1571.4228</td> <td> 1033.392</td> <td>    1.521</td> <td> 0.128</td> <td> -454.183</td> <td> 3597.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>             <td> 1015.9035</td> <td> 1036.611</td> <td>    0.980</td> <td> 0.327</td> <td>-1016.012</td> <td> 3047.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>             <td>-1264.8357</td> <td> 1047.178</td> <td>   -1.208</td> <td> 0.227</td> <td>-3317.464</td> <td>  787.792</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2000</th>             <td>  683.4387</td> <td> 1067.982</td> <td>    0.640</td> <td> 0.522</td> <td>-1409.969</td> <td> 2776.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2001</th>             <td> 1290.2571</td> <td> 1113.123</td> <td>    1.159</td> <td> 0.246</td> <td> -891.633</td> <td> 3472.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>          <td> 8336.4171</td> <td> 3699.630</td> <td>    2.253</td> <td> 0.024</td> <td> 1084.579</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>          <td> 7128.2051</td> <td> 3884.307</td> <td>    1.835</td> <td> 0.067</td> <td> -485.627</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>          <td> 6433.2934</td> <td> 3402.253</td> <td>    1.891</td> <td> 0.059</td> <td> -235.640</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>          <td> 7164.4693</td> <td> 3479.856</td> <td>    2.059</td> <td> 0.040</td> <td>  343.423</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>          <td> 1.086e+04</td> <td> 4142.866</td> <td>    2.621</td> <td> 0.009</td> <td> 2739.409</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>           <td> 9386.8209</td> <td> 3877.670</td> <td>    2.421</td> <td> 0.016</td> <td> 1785.997</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>          <td> 7896.2518</td> <td> 3668.891</td> <td>    2.152</td> <td> 0.031</td> <td>  704.667</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>          <td> 5990.2646</td> <td> 3332.521</td> <td>    1.798</td> <td> 0.072</td> <td> -541.984</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>          <td> 1.098e+04</td> <td> 5959.766</td> <td>    1.842</td> <td> 0.066</td> <td> -705.369</td> <td> 2.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>          <td> 1809.1528</td> <td> 3207.129</td> <td>    0.564</td> <td> 0.573</td> <td>-4477.308</td> <td> 8095.614</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>           <td> 7643.0741</td> <td> 5674.555</td> <td>    1.347</td> <td> 0.178</td> <td>-3479.917</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>          <td> 1.308e+04</td> <td> 4550.294</td> <td>    2.875</td> <td> 0.004</td> <td> 4161.111</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>          <td> 1.006e+04</td> <td> 3768.736</td> <td>    2.670</td> <td> 0.008</td> <td> 2676.196</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>          <td> 1.321e+04</td> <td> 4579.363</td> <td>    2.886</td> <td> 0.004</td> <td> 4238.105</td> <td> 2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>           <td> 4740.3993</td> <td> 3172.561</td> <td>    1.494</td> <td> 0.135</td> <td>-1478.302</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>          <td> 4494.4694</td> <td> 3784.080</td> <td>    1.188</td> <td> 0.235</td> <td>-2922.902</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>          <td> 1.165e+04</td> <td> 5152.687</td> <td>    2.260</td> <td> 0.024</td> <td> 1546.547</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>          <td> 1.061e+04</td> <td> 4285.422</td> <td>    2.477</td> <td> 0.013</td> <td> 2214.472</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>          <td> 1.119e+04</td> <td> 4410.048</td> <td>    2.538</td> <td> 0.011</td> <td> 2547.094</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>          <td> 9918.8258</td> <td> 3937.320</td> <td>    2.519</td> <td> 0.012</td> <td> 2201.079</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>          <td> 1.066e+04</td> <td> 4103.271</td> <td>    2.598</td> <td> 0.009</td> <td> 2617.745</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>          <td> 1.282e+04</td> <td> 4541.798</td> <td>    2.823</td> <td> 0.005</td> <td> 3919.267</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>          <td> 1.259e+04</td> <td> 4488.212</td> <td>    2.804</td> <td> 0.005</td> <td> 3787.767</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>          <td> 1.113e+04</td> <td> 4185.336</td> <td>    2.659</td> <td> 0.008</td> <td> 2926.278</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>          <td> 6944.5078</td> <td> 3499.189</td> <td>    1.985</td> <td> 0.047</td> <td>   85.565</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>          <td>-1.406e+04</td> <td> 3043.938</td> <td>   -4.618</td> <td> 0.000</td> <td>   -2e+04</td> <td>-8091.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>          <td> 8882.7969</td> <td> 3795.864</td> <td>    2.340</td> <td> 0.019</td> <td> 1442.327</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>          <td>-4179.0338</td> <td> 3944.302</td> <td>   -1.060</td> <td> 0.289</td> <td>-1.19e+04</td> <td> 3552.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>           <td> 9705.2346</td> <td> 4210.687</td> <td>    2.305</td> <td> 0.021</td> <td> 1451.648</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>          <td> 8442.5866</td> <td> 3705.690</td> <td>    2.278</td> <td> 0.023</td> <td> 1178.869</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>          <td> 1560.4253</td> <td> 2977.524</td> <td>    0.524</td> <td> 0.600</td> <td>-4275.976</td> <td> 7396.826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>          <td>-2882.9016</td> <td> 2983.643</td> <td>   -0.966</td> <td> 0.334</td> <td>-8731.297</td> <td> 2965.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>          <td> 7373.1369</td> <td> 3485.250</td> <td>    2.116</td> <td> 0.034</td> <td>  541.517</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>          <td> 1.144e+04</td> <td> 3948.088</td> <td>    2.898</td> <td> 0.004</td> <td> 3703.384</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>          <td> 7742.9911</td> <td> 3805.122</td> <td>    2.035</td> <td> 0.042</td> <td>  284.373</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>          <td> 1.184e+04</td> <td> 4573.963</td> <td>    2.588</td> <td> 0.010</td> <td> 2873.904</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>          <td> 1.192e+04</td> <td> 4357.147</td> <td>    2.735</td> <td> 0.006</td> <td> 3375.210</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>           <td>-2088.3861</td> <td> 3668.933</td> <td>   -0.569</td> <td> 0.569</td> <td>-9280.053</td> <td> 5103.280</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>          <td> 1226.7046</td> <td> 2981.021</td> <td>    0.412</td> <td> 0.681</td> <td>-4616.550</td> <td> 7069.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>          <td>-1.245e+04</td> <td> 3822.048</td> <td>   -3.258</td> <td> 0.001</td> <td>-1.99e+04</td> <td>-4961.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>          <td> 1.049e+04</td> <td> 4138.590</td> <td>    2.535</td> <td> 0.011</td> <td> 2377.788</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>          <td>-3707.8662</td> <td> 3195.067</td> <td>   -1.160</td> <td> 0.246</td> <td>-9970.684</td> <td> 2554.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>          <td> 1.272e+04</td> <td> 4629.963</td> <td>    2.747</td> <td> 0.006</td> <td> 3643.423</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>          <td>-3690.5599</td> <td> 2990.879</td> <td>   -1.234</td> <td> 0.217</td> <td>-9553.138</td> <td> 2172.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>          <td> 2107.4873</td> <td> 3002.129</td> <td>    0.702</td> <td> 0.483</td> <td>-3777.142</td> <td> 7992.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>          <td> 4731.3617</td> <td> 3496.655</td> <td>    1.353</td> <td> 0.176</td> <td>-2122.614</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>          <td> 6573.0163</td> <td> 3380.260</td> <td>    1.945</td> <td> 0.052</td> <td>  -52.807</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>          <td> 8401.1498</td> <td> 3790.382</td> <td>    2.216</td> <td> 0.027</td> <td>  971.424</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>          <td> 5503.3473</td> <td> 6140.198</td> <td>    0.896</td> <td> 0.370</td> <td>-6532.375</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>          <td> 2834.0974</td> <td> 3318.240</td> <td>    0.854</td> <td> 0.393</td> <td>-3670.158</td> <td> 9338.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>          <td>  1.14e+04</td> <td> 4148.160</td> <td>    2.747</td> <td> 0.006</td> <td> 3265.326</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>          <td>  1.27e+04</td> <td> 4343.865</td> <td>    2.925</td> <td> 0.003</td> <td> 4189.164</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>          <td> 8236.5095</td> <td> 3945.775</td> <td>    2.087</td> <td> 0.037</td> <td>  502.189</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>          <td> 3146.3468</td> <td> 3044.889</td> <td>    1.033</td> <td> 0.301</td> <td>-2822.100</td> <td> 9114.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>          <td> 9207.2402</td> <td> 3889.421</td> <td>    2.367</td> <td> 0.018</td> <td> 1583.382</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>          <td> 9329.2321</td> <td> 3923.775</td> <td>    2.378</td> <td> 0.017</td> <td> 1638.037</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>          <td> 8960.0187</td> <td> 3856.497</td> <td>    2.323</td> <td> 0.020</td> <td> 1400.697</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>          <td> 7268.4839</td> <td> 3511.216</td> <td>    2.070</td> <td> 0.038</td> <td>  385.967</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>          <td> 7005.8260</td> <td> 3479.247</td> <td>    2.014</td> <td> 0.044</td> <td>  185.972</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>          <td> 1.437e+04</td> <td> 4403.463</td> <td>    3.264</td> <td> 0.001</td> <td> 5743.323</td> <td>  2.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>          <td> 9878.1281</td> <td> 4446.542</td> <td>    2.222</td> <td> 0.026</td> <td> 1162.229</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>          <td> 1.202e+04</td> <td> 4438.906</td> <td>    2.708</td> <td> 0.007</td> <td> 3320.678</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>          <td> 1.234e+04</td> <td> 4559.888</td> <td>    2.707</td> <td> 0.007</td> <td> 3406.665</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>          <td> 6970.8867</td> <td> 3477.112</td> <td>    2.005</td> <td> 0.045</td> <td>  155.219</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>           <td> 1.116e+04</td> <td> 3066.828</td> <td>    3.638</td> <td> 0.000</td> <td> 5146.780</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>          <td> 9218.8031</td> <td> 4147.854</td> <td>    2.223</td> <td> 0.026</td> <td> 1088.377</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>          <td> 7007.4075</td> <td> 3721.310</td> <td>    1.883</td> <td> 0.060</td> <td> -286.926</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>          <td> 9251.2564</td> <td> 3809.506</td> <td>    2.428</td> <td> 0.015</td> <td> 1784.046</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>          <td>-5584.6220</td> <td> 3018.547</td> <td>   -1.850</td> <td> 0.064</td> <td>-1.15e+04</td> <td>  332.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>          <td> 6266.9034</td> <td> 4195.822</td> <td>    1.494</td> <td> 0.135</td> <td>-1957.547</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>          <td> 9612.5911</td> <td> 3916.798</td> <td>    2.454</td> <td> 0.014</td> <td> 1935.071</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>          <td> 9574.1163</td> <td> 4743.479</td> <td>    2.018</td> <td> 0.044</td> <td>  276.177</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>          <td>-2.613e+04</td> <td> 3208.797</td> <td>   -8.143</td> <td> 0.000</td> <td>-3.24e+04</td> <td>-1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>           <td> 8222.4417</td> <td> 3701.273</td> <td>    2.222</td> <td> 0.026</td> <td>  967.384</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>          <td> 1.145e+04</td> <td> 4668.265</td> <td>    2.453</td> <td> 0.014</td> <td> 2301.167</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>          <td> 1.018e+04</td> <td> 4079.198</td> <td>    2.496</td> <td> 0.013</td> <td> 2186.232</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>          <td> 6908.5529</td> <td> 4237.646</td> <td>    1.630</td> <td> 0.103</td> <td>-1397.878</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>           <td> 1.139e+04</td> <td> 4232.982</td> <td>    2.691</td> <td> 0.007</td> <td> 3091.881</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>          <td> 1.027e+04</td> <td> 4117.218</td> <td>    2.494</td> <td> 0.013</td> <td> 2198.570</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>          <td> 8968.6893</td> <td> 3818.676</td> <td>    2.349</td> <td> 0.019</td> <td> 1483.504</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>          <td> 5736.6141</td> <td> 3342.590</td> <td>    1.716</td> <td> 0.086</td> <td> -815.370</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>          <td> 1.022e+04</td> <td> 4226.040</td> <td>    2.417</td> <td> 0.016</td> <td> 1932.709</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>           <td> 8098.0254</td> <td> 3678.632</td> <td>    2.201</td> <td> 0.028</td> <td>  887.347</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>          <td> 8338.9787</td> <td> 3682.770</td> <td>    2.264</td> <td> 0.024</td> <td> 1120.190</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>          <td> 1.202e+04</td> <td> 4392.061</td> <td>    2.736</td> <td> 0.006</td> <td> 3406.259</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>          <td> 1.056e+04</td> <td> 3964.185</td> <td>    2.663</td> <td> 0.008</td> <td> 2785.075</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>          <td>  432.3561</td> <td> 4531.433</td> <td>    0.095</td> <td> 0.924</td> <td>-8449.942</td> <td> 9314.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>          <td> 6685.4946</td> <td> 3144.248</td> <td>    2.126</td> <td> 0.034</td> <td>  522.291</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>          <td> 3341.2076</td> <td> 3149.887</td> <td>    1.061</td> <td> 0.289</td> <td>-2833.050</td> <td> 9515.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>          <td> 5797.5155</td> <td> 3313.162</td> <td>    1.750</td> <td> 0.080</td> <td> -696.786</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>          <td> 1791.0186</td> <td> 3171.702</td> <td>    0.565</td> <td> 0.572</td> <td>-4426.001</td> <td> 8008.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>         <td>-8554.7210</td> <td> 5395.081</td> <td>   -1.586</td> <td> 0.113</td> <td>-1.91e+04</td> <td> 2020.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>          <td>  655.3210</td> <td> 3182.529</td> <td>    0.206</td> <td> 0.837</td> <td>-5582.920</td> <td> 6893.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>          <td> 4678.7849</td> <td> 3824.265</td> <td>    1.223</td> <td> 0.221</td> <td>-2817.356</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>          <td> 1.186e+04</td> <td> 4447.890</td> <td>    2.667</td> <td> 0.008</td> <td> 3143.313</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>          <td> 1427.5921</td> <td> 3092.088</td> <td>    0.462</td> <td> 0.644</td> <td>-4633.370</td> <td> 7488.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>          <td> 8750.9564</td> <td> 3758.034</td> <td>    2.329</td> <td> 0.020</td> <td> 1384.639</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>          <td> 1.275e+04</td> <td> 4525.949</td> <td>    2.818</td> <td> 0.005</td> <td> 3883.438</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>          <td> 6824.1547</td> <td> 3447.624</td> <td>    1.979</td> <td> 0.048</td> <td>   66.287</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>          <td>  133.1965</td> <td> 3351.986</td> <td>    0.040</td> <td> 0.968</td> <td>-6437.205</td> <td> 6703.598</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>          <td> 1.196e+04</td> <td> 4334.154</td> <td>    2.760</td> <td> 0.006</td> <td> 3464.914</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>          <td> 1.459e+04</td> <td> 4199.329</td> <td>    3.475</td> <td> 0.001</td> <td> 6359.859</td> <td> 2.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>           <td>-2819.4272</td> <td> 2954.270</td> <td>   -0.954</td> <td> 0.340</td> <td>-8610.245</td> <td> 2971.391</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>          <td>-9598.0267</td> <td> 3635.579</td> <td>   -2.640</td> <td> 0.008</td> <td>-1.67e+04</td> <td>-2471.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>          <td>  1.29e+04</td> <td> 4579.826</td> <td>    2.816</td> <td> 0.005</td> <td> 3919.660</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>          <td> 3653.3124</td> <td> 3268.448</td> <td>    1.118</td> <td> 0.264</td> <td>-2753.342</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>          <td> 5697.0009</td> <td> 3338.579</td> <td>    1.706</td> <td> 0.088</td> <td> -847.122</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>          <td> 9646.8768</td> <td> 4026.329</td> <td>    2.396</td> <td> 0.017</td> <td> 1754.660</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>          <td> 8111.0790</td> <td> 4501.821</td> <td>    1.802</td> <td> 0.072</td> <td> -713.174</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>          <td> 6037.3698</td> <td> 3627.836</td> <td>    1.664</td> <td> 0.096</td> <td>-1073.741</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>          <td> 7005.1179</td> <td> 3697.572</td> <td>    1.895</td> <td> 0.058</td> <td> -242.685</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>          <td> 3172.6538</td> <td> 6017.047</td> <td>    0.527</td> <td> 0.598</td> <td>-8621.674</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>          <td> 1.205e+04</td> <td> 4718.873</td> <td>    2.553</td> <td> 0.011</td> <td> 2797.242</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>          <td> 9708.1373</td> <td> 4476.264</td> <td>    2.169</td> <td> 0.030</td> <td>  933.979</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>           <td> 5758.0179</td> <td> 3254.162</td> <td>    1.769</td> <td> 0.077</td> <td> -620.634</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>          <td>-1345.1719</td> <td> 3228.391</td> <td>   -0.417</td> <td> 0.677</td> <td>-7673.310</td> <td> 4982.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>          <td> 9.132e+04</td> <td> 3418.316</td> <td>   26.715</td> <td> 0.000</td> <td> 8.46e+04</td> <td>  9.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>          <td> 5925.9690</td> <td> 4393.720</td> <td>    1.349</td> <td> 0.177</td> <td>-2686.390</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>          <td> 1497.9987</td> <td> 3243.616</td> <td>    0.462</td> <td> 0.644</td> <td>-4859.983</td> <td> 7855.980</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>          <td> 7768.8755</td> <td> 3429.726</td> <td>    2.265</td> <td> 0.024</td> <td> 1046.090</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>          <td> 4432.5743</td> <td> 3460.578</td> <td>    1.281</td> <td> 0.200</td> <td>-2350.685</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>          <td> 1.193e+04</td> <td> 4581.767</td> <td>    2.603</td> <td> 0.009</td> <td> 2947.382</td> <td> 2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>          <td> 5632.2076</td> <td> 3333.542</td> <td>    1.690</td> <td> 0.091</td> <td> -902.041</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>           <td> 7169.2164</td> <td> 3514.315</td> <td>    2.040</td> <td> 0.041</td> <td>  280.625</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>          <td> 7378.0172</td> <td> 4065.874</td> <td>    1.815</td> <td> 0.070</td> <td> -591.714</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>          <td> 6407.6851</td> <td> 5492.646</td> <td>    1.167</td> <td> 0.243</td> <td>-4358.737</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>           <td> 5287.9136</td> <td> 4067.600</td> <td>    1.300</td> <td> 0.194</td> <td>-2685.201</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>          <td> 8984.0388</td> <td> 4241.517</td> <td>    2.118</td> <td> 0.034</td> <td>  670.020</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>          <td> 1.033e+04</td> <td> 4391.485</td> <td>    2.351</td> <td> 0.019</td> <td> 1717.135</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>          <td> 4742.3562</td> <td> 3893.599</td> <td>    1.218</td> <td> 0.223</td> <td>-2889.690</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>          <td> 9733.1829</td> <td> 4214.514</td> <td>    2.309</td> <td> 0.021</td> <td> 1472.094</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>          <td> 9261.3599</td> <td> 6722.913</td> <td>    1.378</td> <td> 0.168</td> <td>-3916.572</td> <td> 2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>          <td> 1.275e+04</td> <td> 4797.150</td> <td>    2.658</td> <td> 0.008</td> <td> 3345.860</td> <td> 2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>          <td>-1467.2846</td> <td> 3335.617</td> <td>   -0.440</td> <td> 0.660</td> <td>-8005.602</td> <td> 5071.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>           <td> 1.187e+04</td> <td> 4568.977</td> <td>    2.598</td> <td> 0.009</td> <td> 2912.156</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>          <td> 1.027e+04</td> <td> 4533.225</td> <td>    2.266</td> <td> 0.023</td> <td> 1385.836</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>           <td> 1.308e+04</td> <td> 4581.350</td> <td>    2.855</td> <td> 0.004</td> <td> 4101.178</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>           <td> 9383.3666</td> <td> 3915.787</td> <td>    2.396</td> <td> 0.017</td> <td> 1707.829</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>          <td> 1.093e+04</td> <td> 4453.163</td> <td>    2.454</td> <td> 0.014</td> <td> 2199.830</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>          <td> 7207.4649</td> <td> 4052.732</td> <td>    1.778</td> <td> 0.075</td> <td> -736.507</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>           <td> 9423.6690</td> <td> 3915.403</td> <td>    2.407</td> <td> 0.016</td> <td> 1748.884</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>          <td> 3509.7671</td> <td> 5497.179</td> <td>    0.638</td> <td> 0.523</td> <td>-7265.539</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>           <td> 1.289e+04</td> <td> 4607.074</td> <td>    2.799</td> <td> 0.005</td> <td> 3862.866</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>          <td> 1135.7044</td> <td> 4077.886</td> <td>    0.279</td> <td> 0.781</td> <td>-6857.572</td> <td> 9128.981</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>          <td> 2862.8486</td> <td> 3342.703</td> <td>    0.856</td> <td> 0.392</td> <td>-3689.358</td> <td> 9415.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>          <td> 3079.9892</td> <td> 3388.253</td> <td>    0.909</td> <td> 0.363</td> <td>-3561.502</td> <td> 9721.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>          <td> 9632.3318</td> <td> 4151.138</td> <td>    2.320</td> <td> 0.020</td> <td> 1495.471</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>          <td> 6001.9300</td> <td> 3683.087</td> <td>    1.630</td> <td> 0.103</td> <td>-1217.480</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>          <td> 1.207e+04</td> <td> 4762.098</td> <td>    2.535</td> <td> 0.011</td> <td> 2736.437</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>          <td> 1.146e+04</td> <td> 4599.907</td> <td>    2.492</td> <td> 0.013</td> <td> 2448.079</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>          <td>  1.27e+04</td> <td> 4752.213</td> <td>    2.673</td> <td> 0.008</td> <td> 3385.257</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>           <td> 1.051e+04</td> <td> 3699.533</td> <td>    2.840</td> <td> 0.005</td> <td> 3254.467</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>          <td> 9234.8077</td> <td> 4123.463</td> <td>    2.240</td> <td> 0.025</td> <td> 1152.194</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>           <td> 8934.5926</td> <td> 3806.329</td> <td>    2.347</td> <td> 0.019</td> <td> 1473.610</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>           <td> 5440.6218</td> <td> 3743.636</td> <td>    1.453</td> <td> 0.146</td> <td>-1897.474</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>          <td> 8736.2570</td> <td> 4274.096</td> <td>    2.044</td> <td> 0.041</td> <td>  358.378</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>          <td> 1.118e+04</td> <td> 5116.978</td> <td>    2.185</td> <td> 0.029</td> <td> 1148.847</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>          <td> 1019.1180</td> <td> 3337.043</td> <td>    0.305</td> <td> 0.760</td> <td>-5521.994</td> <td> 7560.230</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>           <td> 6617.9837</td> <td> 3587.349</td> <td>    1.845</td> <td> 0.065</td> <td> -413.766</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>          <td> 1.049e+04</td> <td> 4317.313</td> <td>    2.431</td> <td> 0.015</td> <td> 2032.134</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>          <td> 4235.9708</td> <td> 3485.117</td> <td>    1.215</td> <td> 0.224</td> <td>-2595.388</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>          <td> 7822.4027</td> <td> 3672.860</td> <td>    2.130</td> <td> 0.033</td> <td>  623.039</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>          <td> 2250.0033</td> <td> 3404.219</td> <td>    0.661</td> <td> 0.509</td> <td>-4422.784</td> <td> 8922.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>           <td> 1.349e+04</td> <td> 5104.500</td> <td>    2.643</td> <td> 0.008</td> <td> 3483.368</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14531.0</th>          <td> 3181.5998</td> <td>    1e+04</td> <td>    0.318</td> <td> 0.751</td> <td>-1.64e+04</td> <td> 2.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>          <td> 1.076e+04</td> <td> 4576.649</td> <td>    2.352</td> <td> 0.019</td> <td> 1793.113</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>          <td> 7006.0702</td> <td> 7326.553</td> <td>    0.956</td> <td> 0.339</td> <td>-7355.088</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>           <td> 1.192e+04</td> <td> 4771.898</td> <td>    2.498</td> <td> 0.012</td> <td> 2568.838</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>           <td> 1.027e+04</td> <td> 4449.925</td> <td>    2.309</td> <td> 0.021</td> <td> 1552.365</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>          <td> 1.042e+04</td> <td> 5971.789</td> <td>    1.745</td> <td> 0.081</td> <td>-1282.470</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>          <td>  1.06e+04</td> <td> 4476.738</td> <td>    2.368</td> <td> 0.018</td> <td> 1825.159</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>           <td> 1.104e+04</td> <td> 4166.134</td> <td>    2.651</td> <td> 0.008</td> <td> 2878.667</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>          <td> 9090.7110</td> <td> 4197.561</td> <td>    2.166</td> <td> 0.030</td> <td>  862.852</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>          <td> 3967.3799</td> <td> 3742.828</td> <td>    1.060</td> <td> 0.289</td> <td>-3369.133</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>           <td> 7923.6795</td> <td> 3639.706</td> <td>    2.177</td> <td> 0.029</td> <td>  789.302</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>          <td> 1006.0868</td> <td> 3431.298</td> <td>    0.293</td> <td> 0.769</td> <td>-5719.779</td> <td> 7731.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>           <td> 1.105e+04</td> <td> 4161.285</td> <td>    2.655</td> <td> 0.008</td> <td> 2891.119</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>          <td> 1345.3597</td> <td> 3415.184</td> <td>    0.394</td> <td> 0.694</td> <td>-5348.921</td> <td> 8039.640</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>          <td> 6104.6576</td> <td> 3935.001</td> <td>    1.551</td> <td> 0.121</td> <td>-1608.543</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>          <td> 9451.7981</td> <td> 4802.983</td> <td>    1.968</td> <td> 0.049</td> <td>   37.220</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>           <td>-2.976e+04</td> <td> 4038.352</td> <td>   -7.369</td> <td> 0.000</td> <td>-3.77e+04</td> <td>-2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>           <td> 5515.0972</td> <td> 3304.175</td> <td>    1.669</td> <td> 0.095</td> <td> -961.588</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>           <td> 1.395e+04</td> <td> 3396.118</td> <td>    4.109</td> <td> 0.000</td> <td> 7296.484</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>           <td> 1.101e+04</td> <td> 4186.090</td> <td>    2.629</td> <td> 0.009</td> <td> 2799.971</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>          <td> 8669.1429</td> <td> 4170.880</td> <td>    2.078</td> <td> 0.038</td> <td>  493.583</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>           <td> 4256.0786</td> <td> 3098.060</td> <td>    1.374</td> <td> 0.170</td> <td>-1816.591</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>           <td> 8908.1974</td> <td> 3817.027</td> <td>    2.334</td> <td> 0.020</td> <td> 1426.244</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>           <td> 7450.2837</td> <td> 3526.786</td> <td>    2.112</td> <td> 0.035</td> <td>  537.246</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>          <td>-4061.5505</td> <td> 3576.531</td> <td>   -1.136</td> <td> 0.256</td> <td>-1.11e+04</td> <td> 2948.994</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>          <td> 3430.9607</td> <td> 4052.273</td> <td>    0.847</td> <td> 0.397</td> <td>-4512.110</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>           <td> 7112.8693</td> <td> 3456.619</td> <td>    2.058</td> <td> 0.040</td> <td>  337.371</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>           <td> 9935.4841</td> <td> 3981.206</td> <td>    2.496</td> <td> 0.013</td> <td> 2131.715</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>           <td> 1.363e+04</td> <td> 3679.861</td> <td>    3.704</td> <td> 0.000</td> <td> 6415.407</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>          <td> 5871.3980</td> <td> 3911.464</td> <td>    1.501</td> <td> 0.133</td> <td>-1795.666</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>          <td> 6628.1263</td> <td> 3941.492</td> <td>    1.682</td> <td> 0.093</td> <td>-1097.799</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>           <td>-5003.5820</td> <td> 3105.268</td> <td>   -1.611</td> <td> 0.107</td> <td>-1.11e+04</td> <td> 1083.216</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>           <td> 6095.9361</td> <td> 3462.476</td> <td>    1.761</td> <td> 0.078</td> <td> -691.044</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17101.0</th>          <td> 4955.5819</td> <td>    1e+04</td> <td>    0.495</td> <td> 0.621</td> <td>-1.47e+04</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>          <td> 1.037e+04</td> <td> 4459.996</td> <td>    2.325</td> <td> 0.020</td> <td> 1629.279</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>           <td> 7914.1858</td> <td> 3882.992</td> <td>    2.038</td> <td> 0.042</td> <td>  302.930</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>           <td> 1.051e+04</td> <td> 4109.935</td> <td>    2.558</td> <td> 0.011</td> <td> 2457.721</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>           <td> 1.061e+04</td> <td> 5016.063</td> <td>    2.116</td> <td> 0.034</td> <td>  782.085</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>           <td> 9871.2919</td> <td> 4014.380</td> <td>    2.459</td> <td> 0.014</td> <td> 2002.497</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>           <td>-1360.6791</td> <td> 2978.263</td> <td>   -0.457</td> <td> 0.648</td> <td>-7198.529</td> <td> 4477.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>           <td> 1.168e+04</td> <td> 4569.107</td> <td>    2.556</td> <td> 0.011</td> <td> 2720.433</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>           <td>-1787.4250</td> <td> 3044.853</td> <td>   -0.587</td> <td> 0.557</td> <td>-7755.799</td> <td> 4180.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>          <td> 9369.5941</td> <td> 4277.263</td> <td>    2.191</td> <td> 0.029</td> <td>  985.507</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>           <td> 1.025e+04</td> <td> 4017.719</td> <td>    2.552</td> <td> 0.011</td> <td> 2376.254</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>           <td> -174.7376</td> <td> 3641.414</td> <td>   -0.048</td> <td> 0.962</td> <td>-7312.463</td> <td> 6962.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>          <td> 9895.0330</td> <td> 5108.443</td> <td>    1.937</td> <td> 0.053</td> <td> -118.292</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>           <td> 8101.0355</td> <td> 4958.513</td> <td>    1.634</td> <td> 0.102</td> <td>-1618.405</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>           <td> 1.173e+04</td> <td> 4391.837</td> <td>    2.672</td> <td> 0.008</td> <td> 3124.748</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>           <td> 1494.6254</td> <td> 2975.110</td> <td>    0.502</td> <td> 0.615</td> <td>-4337.042</td> <td> 7326.293</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>           <td> 8651.6988</td> <td> 4021.054</td> <td>    2.152</td> <td> 0.031</td> <td>  769.821</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>           <td> 4718.3260</td> <td> 3144.201</td> <td>    1.501</td> <td> 0.133</td> <td>-1444.787</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>           <td> 5476.8899</td> <td> 3274.829</td> <td>    1.672</td> <td> 0.094</td> <td> -942.272</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>           <td> 1.191e+04</td> <td> 4073.536</td> <td>    2.923</td> <td> 0.003</td> <td> 3923.878</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>           <td> 1.044e+04</td> <td> 4077.941</td> <td>    2.559</td> <td> 0.011</td> <td> 2441.799</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>           <td>-5069.7376</td> <td> 3891.247</td> <td>   -1.303</td> <td> 0.193</td> <td>-1.27e+04</td> <td> 2557.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>           <td> 5039.6675</td> <td> 3216.455</td> <td>    1.567</td> <td> 0.117</td> <td>-1265.073</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>           <td> 7226.1408</td> <td> 3451.123</td> <td>    2.094</td> <td> 0.036</td> <td>  461.414</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>           <td> 9390.2572</td> <td> 4347.121</td> <td>    2.160</td> <td> 0.031</td> <td>  869.239</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>           <td> 8829.3117</td> <td> 3722.148</td> <td>    2.372</td> <td> 0.018</td> <td> 1533.335</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>           <td> 7251.7676</td> <td> 3540.296</td> <td>    2.048</td> <td> 0.041</td> <td>  312.249</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>           <td> 1.251e+04</td> <td> 4483.089</td> <td>    2.790</td> <td> 0.005</td> <td> 3718.981</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>          <td> 5.344e+04</td> <td> 3925.737</td> <td>   13.614</td> <td> 0.000</td> <td> 4.57e+04</td> <td> 6.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>           <td> 4409.4735</td> <td> 3154.407</td> <td>    1.398</td> <td> 0.162</td> <td>-1773.645</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>           <td> 5974.6876</td> <td> 3423.325</td> <td>    1.745</td> <td> 0.081</td> <td> -735.550</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>           <td> 7052.1036</td> <td> 3445.832</td> <td>    2.047</td> <td> 0.041</td> <td>  297.749</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>          <td> 1.256e+04</td> <td> 4928.367</td> <td>    2.548</td> <td> 0.011</td> <td> 2898.240</td> <td> 2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>          <td> 1.239e+04</td> <td> 4811.460</td> <td>    2.575</td> <td> 0.010</td> <td> 2959.156</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>           <td> 5697.0681</td> <td> 3402.162</td> <td>    1.675</td> <td> 0.094</td> <td> -971.688</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>           <td> 2.541e+04</td> <td> 4992.641</td> <td>    5.089</td> <td> 0.000</td> <td> 1.56e+04</td> <td> 3.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>          <td> 1455.5099</td> <td> 3547.770</td> <td>    0.410</td> <td> 0.682</td> <td>-5498.658</td> <td> 8409.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>           <td> 9090.5296</td> <td> 3869.063</td> <td>    2.350</td> <td> 0.019</td> <td> 1506.577</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>           <td> 5.038e+04</td> <td> 4320.920</td> <td>   11.660</td> <td> 0.000</td> <td> 4.19e+04</td> <td> 5.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>           <td> 1.238e+04</td> <td> 4515.888</td> <td>    2.740</td> <td> 0.006</td> <td> 3523.399</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>           <td> -562.1249</td> <td> 3162.012</td> <td>   -0.178</td> <td> 0.859</td> <td>-6760.150</td> <td> 5635.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>           <td> 7170.2235</td> <td> 3562.804</td> <td>    2.013</td> <td> 0.044</td> <td>  186.585</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>          <td>  1.25e+04</td> <td> 4887.973</td> <td>    2.557</td> <td> 0.011</td> <td> 2915.532</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>           <td> 6694.5745</td> <td> 6256.551</td> <td>    1.070</td> <td> 0.285</td> <td>-5569.217</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>           <td> 1.276e+04</td> <td> 4478.745</td> <td>    2.850</td> <td> 0.004</td> <td> 3983.822</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>          <td> 9261.0607</td> <td> 3787.917</td> <td>    2.445</td> <td> 0.015</td> <td> 1836.167</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>           <td> 6563.5332</td> <td> 3644.695</td> <td>    1.801</td> <td> 0.072</td> <td> -580.623</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>          <td> 9490.2470</td> <td> 4525.918</td> <td>    2.097</td> <td> 0.036</td> <td>  618.759</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>           <td> 4985.7416</td> <td> 3253.204</td> <td>    1.533</td> <td> 0.125</td> <td>-1391.034</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>          <td> 6901.8595</td> <td> 3991.510</td> <td>    1.729</td> <td> 0.084</td> <td> -922.108</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>           <td>-2.214e+04</td> <td> 3272.038</td> <td>   -6.766</td> <td> 0.000</td> <td>-2.86e+04</td> <td>-1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>           <td> 3654.5615</td> <td> 3530.734</td> <td>    1.035</td> <td> 0.301</td> <td>-3266.215</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>           <td>   1.3e+04</td> <td> 5600.714</td> <td>    2.322</td> <td> 0.020</td> <td> 2026.157</td> <td>  2.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>           <td> 3158.2020</td> <td> 3401.547</td> <td>    0.928</td> <td> 0.353</td> <td>-3509.348</td> <td> 9825.752</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>          <td> 6844.3205</td> <td> 4048.020</td> <td>    1.691</td> <td> 0.091</td> <td>-1090.414</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>          <td> 1.102e+04</td> <td> 4772.809</td> <td>    2.308</td> <td> 0.021</td> <td> 1662.511</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>           <td> 8651.7177</td> <td> 5862.373</td> <td>    1.476</td> <td> 0.140</td> <td>-2839.425</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>           <td> 3701.1087</td> <td> 3232.290</td> <td>    1.145</td> <td> 0.252</td> <td>-2634.671</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>          <td>-2730.5076</td> <td> 4771.481</td> <td>   -0.572</td> <td> 0.567</td> <td>-1.21e+04</td> <td> 6622.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>           <td> 1.128e+04</td> <td> 4210.584</td> <td>    2.680</td> <td> 0.007</td> <td> 3029.690</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>           <td>  927.7677</td> <td> 2948.534</td> <td>    0.315</td> <td> 0.753</td> <td>-4851.808</td> <td> 6707.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>           <td> 2.172e+04</td> <td> 3176.339</td> <td>    6.839</td> <td> 0.000</td> <td> 1.55e+04</td> <td> 2.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>           <td> 1.411e+04</td> <td> 4527.637</td> <td>    3.116</td> <td> 0.002</td> <td> 5235.229</td> <td>  2.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>           <td> 2387.4015</td> <td> 3090.909</td> <td>    0.772</td> <td> 0.440</td> <td>-3671.251</td> <td> 8446.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>           <td> 7993.4498</td> <td> 3623.624</td> <td>    2.206</td> <td> 0.027</td> <td>  890.595</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>           <td> 1.061e+04</td> <td> 5183.112</td> <td>    2.046</td> <td> 0.041</td> <td>  445.737</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>          <td> 7113.8480</td> <td> 4153.805</td> <td>    1.713</td> <td> 0.087</td> <td>-1028.242</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>          <td>  1.15e+04</td> <td> 3897.050</td> <td>    2.951</td> <td> 0.003</td> <td> 3860.020</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>           <td> 1.153e+04</td> <td> 4261.214</td> <td>    2.705</td> <td> 0.007</td> <td> 3173.514</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>          <td>  1.11e+04</td> <td> 5242.485</td> <td>    2.117</td> <td> 0.034</td> <td>  821.772</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>           <td> 5950.1850</td> <td> 3496.509</td> <td>    1.702</td> <td> 0.089</td> <td> -903.504</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>           <td>-1.316e+04</td> <td> 3179.409</td> <td>   -4.140</td> <td> 0.000</td> <td>-1.94e+04</td> <td>-6930.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>           <td> 9444.1082</td> <td> 4064.529</td> <td>    2.324</td> <td> 0.020</td> <td> 1477.012</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>          <td> 8152.1151</td> <td> 4336.426</td> <td>    1.880</td> <td> 0.060</td> <td> -347.939</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>           <td> 9780.3988</td> <td> 3987.017</td> <td>    2.453</td> <td> 0.014</td> <td> 1965.239</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>          <td> 7547.9150</td> <td> 7367.374</td> <td>    1.025</td> <td> 0.306</td> <td>-6893.258</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>          <td> 1.073e+04</td> <td> 4396.741</td> <td>    2.441</td> <td> 0.015</td> <td> 2115.387</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>           <td> 6190.5805</td> <td> 3390.712</td> <td>    1.826</td> <td> 0.068</td> <td> -455.730</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>           <td> 1.165e+04</td> <td> 5117.482</td> <td>    2.276</td> <td> 0.023</td> <td> 1614.429</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>          <td> 1.075e+04</td> <td> 6659.968</td> <td>    1.614</td> <td> 0.107</td> <td>-2304.315</td> <td> 2.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>           <td>  -72.1448</td> <td> 3461.233</td> <td>   -0.021</td> <td> 0.983</td> <td>-6856.688</td> <td> 6712.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>           <td>  1.08e+04</td> <td> 4155.736</td> <td>    2.600</td> <td> 0.009</td> <td> 2657.280</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>           <td> 3872.3138</td> <td> 4394.763</td> <td>    0.881</td> <td> 0.378</td> <td>-4742.090</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>          <td> 8533.8385</td> <td> 4417.126</td> <td>    1.932</td> <td> 0.053</td> <td> -124.400</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>           <td> 5974.7236</td> <td> 3339.669</td> <td>    1.789</td> <td> 0.074</td> <td> -571.536</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>           <td> 6166.0952</td> <td> 3437.442</td> <td>    1.794</td> <td> 0.073</td> <td> -571.814</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>           <td> 1.235e+04</td> <td> 4438.456</td> <td>    2.782</td> <td> 0.005</td> <td> 3646.159</td> <td>  2.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>           <td> 1.216e+04</td> <td> 3876.313</td> <td>    3.138</td> <td> 0.002</td> <td> 4564.624</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>           <td> 1.193e+04</td> <td> 4407.974</td> <td>    2.706</td> <td> 0.007</td> <td> 3287.864</td> <td> 2.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>           <td> 9469.7837</td> <td> 3959.582</td> <td>    2.392</td> <td> 0.017</td> <td> 1708.400</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>           <td> 1966.6171</td> <td> 3049.874</td> <td>    0.645</td> <td> 0.519</td> <td>-4011.601</td> <td> 7944.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>           <td> 1.127e+04</td> <td> 4221.976</td> <td>    2.670</td> <td> 0.008</td> <td> 2996.835</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>           <td>-8930.8400</td> <td> 3046.906</td> <td>   -2.931</td> <td> 0.003</td> <td>-1.49e+04</td> <td>-2958.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>          <td>-1537.9801</td> <td> 3864.197</td> <td>   -0.398</td> <td> 0.691</td> <td>-9112.395</td> <td> 6036.434</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>          <td> 8387.4495</td> <td> 3874.922</td> <td>    2.165</td> <td> 0.030</td> <td>  792.013</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>          <td> 4282.1391</td> <td> 4229.061</td> <td>    1.013</td> <td> 0.311</td> <td>-4007.463</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>           <td> 6760.4694</td> <td> 3567.651</td> <td>    1.895</td> <td> 0.058</td> <td> -232.670</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>           <td> 9862.4574</td> <td> 4460.678</td> <td>    2.211</td> <td> 0.027</td> <td> 1118.850</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>           <td> 4638.9279</td> <td> 3766.582</td> <td>    1.232</td> <td> 0.218</td> <td>-2744.147</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>          <td>-6393.4066</td> <td> 3894.751</td> <td>   -1.642</td> <td> 0.101</td> <td> -1.4e+04</td> <td> 1240.897</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>           <td> 7545.1302</td> <td> 4190.581</td> <td>    1.800</td> <td> 0.072</td> <td> -669.046</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>           <td>  1.16e+04</td> <td> 4618.097</td> <td>    2.513</td> <td> 0.012</td> <td> 2552.014</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>           <td> 8985.2452</td> <td> 3782.157</td> <td>    2.376</td> <td> 0.018</td> <td> 1571.641</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>           <td> 5222.5971</td> <td> 4059.135</td> <td>    1.287</td> <td> 0.198</td> <td>-2733.925</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>           <td> 6256.0595</td> <td> 3431.701</td> <td>    1.823</td> <td> 0.068</td> <td> -470.597</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>           <td> 1.023e+04</td> <td> 4075.609</td> <td>    2.510</td> <td> 0.012</td> <td> 2239.386</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>           <td>  501.0336</td> <td> 3925.088</td> <td>    0.128</td> <td> 0.898</td> <td>-7192.737</td> <td> 8194.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>           <td> 7688.4059</td> <td> 3704.565</td> <td>    2.075</td> <td> 0.038</td> <td>  426.894</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>           <td> 1.268e+04</td> <td> 4380.448</td> <td>    2.894</td> <td> 0.004</td> <td> 4091.619</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>           <td> 9521.4095</td> <td> 3885.510</td> <td>    2.450</td> <td> 0.014</td> <td> 1905.218</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>           <td> 3433.1702</td> <td> 3896.852</td> <td>    0.881</td> <td> 0.378</td> <td>-4205.253</td> <td> 1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>           <td> 8372.4668</td> <td> 3560.764</td> <td>    2.351</td> <td> 0.019</td> <td> 1392.827</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>           <td> 9456.1844</td> <td> 4013.928</td> <td>    2.356</td> <td> 0.018</td> <td> 1588.274</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>           <td> 6693.9808</td> <td> 4904.836</td> <td>    1.365</td> <td> 0.172</td> <td>-2920.245</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>           <td> 1.116e+04</td> <td> 3895.822</td> <td>    2.866</td> <td> 0.004</td> <td> 3527.714</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>           <td> 7440.8391</td> <td> 3566.309</td> <td>    2.086</td> <td> 0.037</td> <td>  450.331</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>           <td>  6.25e+04</td> <td> 4012.827</td> <td>   15.576</td> <td> 0.000</td> <td> 5.46e+04</td> <td> 7.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>           <td> 1.129e+04</td> <td> 4493.279</td> <td>    2.513</td> <td> 0.012</td> <td> 2482.732</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>           <td> 8993.0023</td> <td> 3828.880</td> <td>    2.349</td> <td> 0.019</td> <td> 1487.815</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>           <td> 1.304e+04</td> <td> 3799.293</td> <td>    3.433</td> <td> 0.001</td> <td> 5597.506</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>           <td> 1.137e+04</td> <td> 4347.953</td> <td>    2.616</td> <td> 0.009</td> <td> 2851.078</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>           <td> 4493.9310</td> <td> 3713.576</td> <td>    1.210</td> <td> 0.226</td> <td>-2785.242</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>           <td> 1.114e+04</td> <td> 4280.634</td> <td>    2.602</td> <td> 0.009</td> <td> 2748.862</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>           <td> 7123.2217</td> <td> 3595.742</td> <td>    1.981</td> <td> 0.048</td> <td>   75.020</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>           <td> 1.031e+04</td> <td> 4194.329</td> <td>    2.458</td> <td> 0.014</td> <td> 2087.307</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>           <td> 8084.0530</td> <td> 3694.773</td> <td>    2.188</td> <td> 0.029</td> <td>  841.736</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>           <td> -1.52e+04</td> <td> 3154.401</td> <td>   -4.819</td> <td> 0.000</td> <td>-2.14e+04</td> <td>-9018.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>           <td> 2024.3697</td> <td> 4165.120</td> <td>    0.486</td> <td> 0.627</td> <td>-6139.899</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>           <td> 9456.8896</td> <td> 4228.253</td> <td>    2.237</td> <td> 0.025</td> <td> 1168.871</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>           <td> 7340.0009</td> <td> 3512.525</td> <td>    2.090</td> <td> 0.037</td> <td>  454.918</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>           <td> 1221.2387</td> <td> 3007.116</td> <td>    0.406</td> <td> 0.685</td> <td>-4673.166</td> <td> 7115.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>           <td> 6705.2576</td> <td> 3499.979</td> <td>    1.916</td> <td> 0.055</td> <td> -155.233</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>           <td> 5886.5006</td> <td> 3980.714</td> <td>    1.479</td> <td> 0.139</td> <td>-1916.304</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>           <td> 3746.5464</td> <td> 3176.698</td> <td>    1.179</td> <td> 0.238</td> <td>-2480.265</td> <td> 9973.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>           <td> 9082.7373</td> <td> 3451.767</td> <td>    2.631</td> <td> 0.009</td> <td> 2316.749</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>           <td> 1.135e+04</td> <td> 5613.448</td> <td>    2.022</td> <td> 0.043</td> <td>  347.319</td> <td> 2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>           <td> 5970.7232</td> <td> 3441.810</td> <td>    1.735</td> <td> 0.083</td> <td> -775.749</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>           <td>  1.31e+04</td> <td> 4548.023</td> <td>    2.880</td> <td> 0.004</td> <td> 4181.503</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>           <td> 8659.8696</td> <td> 4076.577</td> <td>    2.124</td> <td> 0.034</td> <td>  669.159</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>           <td> 1.137e+04</td> <td> 4311.284</td> <td>    2.637</td> <td> 0.008</td> <td> 2917.477</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>           <td> 3712.0637</td> <td> 3103.375</td> <td>    1.196</td> <td> 0.232</td> <td>-2371.024</td> <td> 9795.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>           <td>-4218.2473</td> <td> 2945.040</td> <td>   -1.432</td> <td> 0.152</td> <td>-9990.974</td> <td> 1554.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>           <td> 2096.9656</td> <td> 2973.505</td> <td>    0.705</td> <td> 0.481</td> <td>-3731.557</td> <td> 7925.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>           <td> -618.7346</td> <td> 3131.546</td> <td>   -0.198</td> <td> 0.843</td> <td>-6757.041</td> <td> 5519.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>           <td> 1714.5441</td> <td> 3273.573</td> <td>    0.524</td> <td> 0.600</td> <td>-4702.157</td> <td> 8131.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>           <td> 2948.2889</td> <td> 3076.541</td> <td>    0.958</td> <td> 0.338</td> <td>-3082.201</td> <td> 8978.778</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>           <td> 8398.0005</td> <td> 3769.925</td> <td>    2.228</td> <td> 0.026</td> <td> 1008.373</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>           <td> 9848.7736</td> <td> 4341.478</td> <td>    2.269</td> <td> 0.023</td> <td> 1338.816</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>           <td> 2652.2226</td> <td> 3142.421</td> <td>    0.844</td> <td> 0.399</td> <td>-3507.401</td> <td> 8811.846</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>           <td> 9975.5723</td> <td> 3988.454</td> <td>    2.501</td> <td> 0.012</td> <td> 2157.595</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>           <td> 8412.7977</td> <td> 3868.706</td> <td>    2.175</td> <td> 0.030</td> <td>  829.545</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>           <td> 1.076e+04</td> <td> 4284.698</td> <td>    2.510</td> <td> 0.012</td> <td> 2356.627</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>           <td>  325.3667</td> <td> 3469.095</td> <td>    0.094</td> <td> 0.925</td> <td>-6474.587</td> <td> 7125.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>           <td> 1627.1748</td> <td> 3742.626</td> <td>    0.435</td> <td> 0.664</td> <td>-5708.942</td> <td> 8963.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>           <td> 9349.7452</td> <td> 3871.757</td> <td>    2.415</td> <td> 0.016</td> <td> 1760.512</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>           <td> 1.011e+04</td> <td> 4002.931</td> <td>    2.526</td> <td> 0.012</td> <td> 2266.773</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>           <td> 1.179e+04</td> <td> 5017.974</td> <td>    2.350</td> <td> 0.019</td> <td> 1956.045</td> <td> 2.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>           <td> 1074.0551</td> <td> 3033.956</td> <td>    0.354</td> <td> 0.723</td> <td>-4872.960</td> <td> 7021.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>           <td> 1.084e+04</td> <td> 4187.474</td> <td>    2.589</td> <td> 0.010</td> <td> 2634.498</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>           <td> 1.182e+04</td> <td> 4349.230</td> <td>    2.717</td> <td> 0.007</td> <td> 3293.217</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>           <td> 8785.2598</td> <td> 3826.746</td> <td>    2.296</td> <td> 0.022</td> <td> 1284.255</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>           <td> 2.001e+04</td> <td> 3707.464</td> <td>    5.396</td> <td> 0.000</td> <td> 1.27e+04</td> <td> 2.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>           <td> 3089.9753</td> <td> 3148.117</td> <td>    0.982</td> <td> 0.326</td> <td>-3080.813</td> <td> 9260.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>           <td> 1.078e+04</td> <td> 4106.787</td> <td>    2.626</td> <td> 0.009</td> <td> 2734.110</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>           <td> 9522.3439</td> <td> 3858.347</td> <td>    2.468</td> <td> 0.014</td> <td> 1959.397</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>           <td> 9224.0234</td> <td> 3774.366</td> <td>    2.444</td> <td> 0.015</td> <td> 1825.692</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>           <td>-1.239e+04</td> <td> 3046.591</td> <td>   -4.068</td> <td> 0.000</td> <td>-1.84e+04</td> <td>-6422.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>           <td> 1.461e+04</td> <td> 4513.229</td> <td>    3.238</td> <td> 0.001</td> <td> 5765.432</td> <td> 2.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>           <td> 1.029e+04</td> <td> 4923.332</td> <td>    2.090</td> <td> 0.037</td> <td>  639.909</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>           <td>-2.318e+04</td> <td> 3393.341</td> <td>   -6.830</td> <td> 0.000</td> <td>-2.98e+04</td> <td>-1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>           <td> 8767.6889</td> <td> 4182.215</td> <td>    2.096</td> <td> 0.036</td> <td>  569.911</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>           <td> 5139.3960</td> <td> 3231.497</td> <td>    1.590</td> <td> 0.112</td> <td>-1194.830</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>           <td> 1.158e+04</td> <td> 4678.133</td> <td>    2.475</td> <td> 0.013</td> <td> 2406.218</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>           <td> 9400.3071</td> <td> 4301.461</td> <td>    2.185</td> <td> 0.029</td> <td>  968.789</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>           <td>  1.14e+04</td> <td> 4217.295</td> <td>    2.702</td> <td> 0.007</td> <td> 3130.468</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>           <td>-1943.8093</td> <td> 3510.323</td> <td>   -0.554</td> <td> 0.580</td> <td>-8824.576</td> <td> 4936.957</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>           <td>-5041.3250</td> <td> 3091.922</td> <td>   -1.630</td> <td> 0.103</td> <td>-1.11e+04</td> <td> 1019.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>           <td> 8197.8902</td> <td> 3566.600</td> <td>    2.299</td> <td> 0.022</td> <td> 1206.811</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>           <td> 5080.5991</td> <td> 3227.534</td> <td>    1.574</td> <td> 0.115</td> <td>-1245.859</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>           <td> 8439.7735</td> <td> 3678.170</td> <td>    2.295</td> <td> 0.022</td> <td> 1230.000</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>           <td> 1.088e+04</td> <td> 4135.671</td> <td>    2.630</td> <td> 0.009</td> <td> 2769.630</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>           <td> 1.008e+04</td> <td> 4108.597</td> <td>    2.453</td> <td> 0.014</td> <td> 2024.501</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>           <td> 5602.3767</td> <td> 3433.578</td> <td>    1.632</td> <td> 0.103</td> <td>-1127.959</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>           <td> 2518.2649</td> <td> 3032.594</td> <td>    0.830</td> <td> 0.406</td> <td>-3426.080</td> <td> 8462.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>           <td> 6984.8365</td> <td> 4954.407</td> <td>    1.410</td> <td> 0.159</td> <td>-2726.555</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>           <td> 6389.0278</td> <td> 3424.452</td> <td>    1.866</td> <td> 0.062</td> <td> -323.419</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>           <td> 4744.8951</td> <td> 3344.844</td> <td>    1.419</td> <td> 0.156</td> <td>-1811.509</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>           <td> 1.048e+04</td> <td> 4162.124</td> <td>    2.519</td> <td> 0.012</td> <td> 2326.029</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>           <td> 1.042e+04</td> <td> 4042.227</td> <td>    2.579</td> <td> 0.010</td> <td> 2501.120</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>           <td> 3409.8999</td> <td> 3264.845</td> <td>    1.044</td> <td> 0.296</td> <td>-2989.694</td> <td> 9809.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>           <td>-1483.8606</td> <td> 2986.044</td> <td>   -0.497</td> <td> 0.619</td> <td>-7336.961</td> <td> 4369.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>           <td> 5466.5214</td> <td> 3271.905</td> <td>    1.671</td> <td> 0.095</td> <td> -946.911</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>           <td> 9684.9935</td> <td> 7598.552</td> <td>    1.275</td> <td> 0.202</td> <td>-5209.325</td> <td> 2.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>           <td> 7745.7072</td> <td> 3724.777</td> <td>    2.080</td> <td> 0.038</td> <td>  444.577</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>           <td> 1.293e+04</td> <td> 4538.177</td> <td>    2.850</td> <td> 0.004</td> <td> 4036.981</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>           <td> 1.123e+04</td> <td> 4250.879</td> <td>    2.642</td> <td> 0.008</td> <td> 2899.048</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>           <td> 7537.0673</td> <td> 3563.928</td> <td>    2.115</td> <td> 0.034</td> <td>  551.227</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>           <td> 6442.9841</td> <td> 3535.061</td> <td>    1.823</td> <td> 0.068</td> <td> -486.273</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>           <td> 1.098e+04</td> <td> 4160.760</td> <td>    2.639</td> <td> 0.008</td> <td> 2823.984</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>           <td> 8172.4690</td> <td> 3636.861</td> <td>    2.247</td> <td> 0.025</td> <td> 1043.669</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>           <td> 1427.8834</td> <td> 2973.671</td> <td>    0.480</td> <td> 0.631</td> <td>-4400.964</td> <td> 7256.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>           <td> 1.065e+04</td> <td> 4180.459</td> <td>    2.547</td> <td> 0.011</td> <td> 2453.751</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>           <td> 7792.0053</td> <td> 3781.774</td> <td>    2.060</td> <td> 0.039</td> <td>  379.153</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>           <td> 1.153e+04</td> <td> 4273.534</td> <td>    2.697</td> <td> 0.007</td> <td> 3149.114</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>           <td> 1.009e+04</td> <td> 4286.046</td> <td>    2.354</td> <td> 0.019</td> <td> 1689.827</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>           <td>  1.02e+04</td> <td> 4013.159</td> <td>    2.541</td> <td> 0.011</td> <td> 2331.904</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>           <td> 9213.6890</td> <td> 3923.273</td> <td>    2.348</td> <td> 0.019</td> <td> 1523.476</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>           <td>-1.455e+05</td> <td> 4319.284</td> <td>  -33.685</td> <td> 0.000</td> <td>-1.54e+05</td> <td>-1.37e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>           <td> 1.067e+04</td> <td> 3887.642</td> <td>    2.745</td> <td> 0.006</td> <td> 3051.533</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>           <td> 5934.0337</td> <td> 3330.016</td> <td>    1.782</td> <td> 0.075</td> <td> -593.304</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>           <td> 1.034e+04</td> <td> 4039.223</td> <td>    2.559</td> <td> 0.011</td> <td> 2419.087</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>           <td> 4935.9631</td> <td> 3265.920</td> <td>    1.511</td> <td> 0.131</td> <td>-1465.736</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>           <td> 7424.2795</td> <td> 3741.649</td> <td>    1.984</td> <td> 0.047</td> <td>   90.079</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>           <td>-5213.7396</td> <td> 3833.242</td> <td>   -1.360</td> <td> 0.174</td> <td>-1.27e+04</td> <td> 2299.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>           <td> 1.739e+04</td> <td> 4299.529</td> <td>    4.045</td> <td> 0.000</td> <td> 8963.828</td> <td> 2.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>           <td> 1.283e+04</td> <td> 4538.407</td> <td>    2.828</td> <td> 0.005</td> <td> 3938.733</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>           <td>-2996.3265</td> <td> 2963.717</td> <td>   -1.011</td> <td> 0.312</td> <td>-8805.664</td> <td> 2813.011</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>           <td> 1.124e+04</td> <td> 3584.056</td> <td>    3.137</td> <td> 0.002</td> <td> 4218.320</td> <td> 1.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>           <td> 5890.7880</td> <td> 3367.220</td> <td>    1.749</td> <td> 0.080</td> <td> -709.475</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>           <td> 8682.1785</td> <td> 3795.147</td> <td>    2.288</td> <td> 0.022</td> <td> 1243.113</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>           <td> 6755.2178</td> <td> 3532.992</td> <td>    1.912</td> <td> 0.056</td> <td> -169.984</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>           <td>-6585.9934</td> <td> 2994.407</td> <td>   -2.199</td> <td> 0.028</td> <td>-1.25e+04</td> <td> -716.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>           <td> 4.461e+04</td> <td> 4003.739</td> <td>   11.142</td> <td> 0.000</td> <td> 3.68e+04</td> <td> 5.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>           <td> 1.074e+04</td> <td> 4425.978</td> <td>    2.427</td> <td> 0.015</td> <td> 2067.109</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>           <td> 9434.0249</td> <td> 4306.851</td> <td>    2.190</td> <td> 0.029</td> <td>  991.941</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>           <td>-2.062e+05</td> <td> 6352.355</td> <td>  -32.459</td> <td> 0.000</td> <td>-2.19e+05</td> <td>-1.94e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>           <td> 5704.1523</td> <td> 3525.290</td> <td>    1.618</td> <td> 0.106</td> <td>-1205.953</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>           <td> 1.138e+04</td> <td> 4303.060</td> <td>    2.645</td> <td> 0.008</td> <td> 2944.822</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>           <td> 6984.6176</td> <td> 3536.143</td> <td>    1.975</td> <td> 0.048</td> <td>   53.239</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>           <td> 5068.8879</td> <td> 3304.120</td> <td>    1.534</td> <td> 0.125</td> <td>-1407.690</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>           <td>   65.7087</td> <td> 3427.998</td> <td>    0.019</td> <td> 0.985</td> <td>-6653.689</td> <td> 6785.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>           <td> 7326.5923</td> <td> 4310.388</td> <td>    1.700</td> <td> 0.089</td> <td>-1122.423</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>           <td> 1.177e+04</td> <td> 4553.903</td> <td>    2.584</td> <td> 0.010</td> <td> 2840.692</td> <td> 2.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>           <td>  2.07e+04</td> <td> 4033.456</td> <td>    5.132</td> <td> 0.000</td> <td> 1.28e+04</td> <td> 2.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>           <td> 8533.9796</td> <td> 3945.822</td> <td>    2.163</td> <td> 0.031</td> <td>  799.568</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>           <td> 1.093e+04</td> <td> 4116.086</td> <td>    2.656</td> <td> 0.008</td> <td> 2862.704</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>           <td> 1.125e+04</td> <td> 4306.955</td> <td>    2.613</td> <td> 0.009</td> <td> 2811.335</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>           <td> 1.166e+04</td> <td> 4521.494</td> <td>    2.579</td> <td> 0.010</td> <td> 2797.023</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>           <td>  -63.7561</td> <td> 2986.566</td> <td>   -0.021</td> <td> 0.983</td> <td>-5917.879</td> <td> 5790.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>           <td>-7623.9666</td> <td> 3323.390</td> <td>   -2.294</td> <td> 0.022</td> <td>-1.41e+04</td> <td>-1109.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>           <td> 9923.3867</td> <td> 3942.736</td> <td>    2.517</td> <td> 0.012</td> <td> 2195.024</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>           <td> 7558.9349</td> <td> 3558.107</td> <td>    2.124</td> <td> 0.034</td> <td>  584.503</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>           <td> 7539.1361</td> <td> 3570.188</td> <td>    2.112</td> <td> 0.035</td> <td>  541.024</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>           <td> 2084.0427</td> <td> 2954.633</td> <td>    0.705</td> <td> 0.481</td> <td>-3707.487</td> <td> 7875.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>           <td> 9445.9791</td> <td> 3859.542</td> <td>    2.447</td> <td> 0.014</td> <td> 1880.689</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>           <td> 1.154e+04</td> <td> 4349.504</td> <td>    2.653</td> <td> 0.008</td> <td> 3013.763</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>           <td> 8864.7143</td> <td> 4057.848</td> <td>    2.185</td> <td> 0.029</td> <td>  910.715</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>           <td> 1.274e+04</td> <td> 4551.099</td> <td>    2.800</td> <td> 0.005</td> <td> 3822.865</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>           <td> 7275.3968</td> <td> 4173.456</td> <td>    1.743</td> <td> 0.081</td> <td> -905.213</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>           <td> 1.339e+04</td> <td> 4581.284</td> <td>    2.922</td> <td> 0.003</td> <td> 4407.281</td> <td> 2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>           <td>-4549.3336</td> <td> 3052.005</td> <td>   -1.491</td> <td> 0.136</td> <td>-1.05e+04</td> <td> 1433.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>           <td> 8244.9060</td> <td> 3727.831</td> <td>    2.212</td> <td> 0.027</td> <td>  937.791</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>           <td> 1.081e+04</td> <td> 4241.696</td> <td>    2.548</td> <td> 0.011</td> <td> 2494.547</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>           <td> 8692.2261</td> <td> 4307.097</td> <td>    2.018</td> <td> 0.044</td> <td>  249.661</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>           <td> 8145.9612</td> <td> 3692.089</td> <td>    2.206</td> <td> 0.027</td> <td>  908.905</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>           <td>  1.01e+04</td> <td> 4060.744</td> <td>    2.487</td> <td> 0.013</td> <td> 2138.140</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>           <td> 1.208e+04</td> <td> 3810.286</td> <td>    3.172</td> <td> 0.002</td> <td> 4615.917</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>           <td>  1.09e+04</td> <td> 4177.942</td> <td>    2.609</td> <td> 0.009</td> <td> 2708.894</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>           <td> 1.051e+04</td> <td> 4029.722</td> <td>    2.609</td> <td> 0.009</td> <td> 2612.725</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>           <td>  1.02e+04</td> <td> 3910.445</td> <td>    2.609</td> <td> 0.009</td> <td> 2536.038</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>           <td>  -78.8477</td> <td> 2944.356</td> <td>   -0.027</td> <td> 0.979</td> <td>-5850.234</td> <td> 5692.538</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>           <td> 1.083e+04</td> <td> 4249.786</td> <td>    2.549</td> <td> 0.011</td> <td> 2504.453</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>           <td>-2.861e+04</td> <td> 3146.602</td> <td>   -9.092</td> <td> 0.000</td> <td>-3.48e+04</td> <td>-2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>           <td> 1.236e+04</td> <td> 4328.856</td> <td>    2.856</td> <td> 0.004</td> <td> 3878.499</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>           <td> 1.291e+04</td> <td> 4602.692</td> <td>    2.805</td> <td> 0.005</td> <td> 3890.075</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>           <td> 1.004e+04</td> <td> 3981.493</td> <td>    2.522</td> <td> 0.012</td> <td> 2238.699</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>           <td> 9814.3704</td> <td> 4087.748</td> <td>    2.401</td> <td> 0.016</td> <td> 1801.762</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>           <td> 1.092e+04</td> <td> 4112.433</td> <td>    2.655</td> <td> 0.008</td> <td> 2856.723</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>           <td> 5926.0852</td> <td> 3214.035</td> <td>    1.844</td> <td> 0.065</td> <td> -373.911</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>           <td> 8435.6372</td> <td> 3722.716</td> <td>    2.266</td> <td> 0.023</td> <td> 1138.547</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>           <td>-1.639e+04</td> <td> 3104.532</td> <td>   -5.278</td> <td> 0.000</td> <td>-2.25e+04</td> <td>-1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>           <td> 9767.1273</td> <td> 3351.289</td> <td>    2.914</td> <td> 0.004</td> <td> 3198.091</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>           <td> 8484.1600</td> <td> 3929.427</td> <td>    2.159</td> <td> 0.031</td> <td>  781.885</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>           <td> 5019.2603</td> <td> 3455.387</td> <td>    1.453</td> <td> 0.146</td> <td>-1753.823</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>           <td> 1950.1034</td> <td> 3270.756</td> <td>    0.596</td> <td> 0.551</td> <td>-4461.076</td> <td> 8361.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>           <td> 2.543e+04</td> <td> 3008.929</td> <td>    8.450</td> <td> 0.000</td> <td> 1.95e+04</td> <td> 3.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>           <td>  835.4076</td> <td> 3061.073</td> <td>    0.273</td> <td> 0.785</td> <td>-5164.761</td> <td> 6835.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>           <td> 9294.2228</td> <td> 4264.637</td> <td>    2.179</td> <td> 0.029</td> <td>  934.885</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>           <td> 1789.1188</td> <td> 3089.325</td> <td>    0.579</td> <td> 0.563</td> <td>-4266.429</td> <td> 7844.667</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>           <td> 9810.8574</td> <td> 3952.369</td> <td>    2.482</td> <td> 0.013</td> <td> 2063.612</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>           <td> 1.186e+04</td> <td> 4578.145</td> <td>    2.590</td> <td> 0.010</td> <td> 2881.663</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>           <td>-3.183e+04</td> <td> 4377.392</td> <td>   -7.272</td> <td> 0.000</td> <td>-4.04e+04</td> <td>-2.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>           <td>  1.12e+04</td> <td> 3846.718</td> <td>    2.911</td> <td> 0.004</td> <td> 3656.829</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>           <td>-4720.5814</td> <td> 2959.879</td> <td>   -1.595</td> <td> 0.111</td> <td>-1.05e+04</td> <td> 1081.232</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>          <td> 9199.3006</td> <td> 5076.118</td> <td>    1.812</td> <td> 0.070</td> <td> -750.663</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>           <td> 1.263e+04</td> <td> 4369.572</td> <td>    2.890</td> <td> 0.004</td> <td> 4064.557</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>           <td> 9052.2338</td> <td> 4019.358</td> <td>    2.252</td> <td> 0.024</td> <td> 1173.680</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>           <td>-1187.3521</td> <td> 3482.045</td> <td>   -0.341</td> <td> 0.733</td> <td>-8012.690</td> <td> 5637.986</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>           <td> 3213.2067</td> <td> 3050.268</td> <td>    1.053</td> <td> 0.292</td> <td>-2765.782</td> <td> 9192.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>           <td> 2228.4004</td> <td> 3709.238</td> <td>    0.601</td> <td> 0.548</td> <td>-5042.271</td> <td> 9499.072</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>          <td>-1975.3039</td> <td> 4309.096</td> <td>   -0.458</td> <td> 0.647</td> <td>-1.04e+04</td> <td> 6471.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>           <td> 6941.3184</td> <td> 3594.907</td> <td>    1.931</td> <td> 0.054</td> <td> -105.246</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>           <td> 9725.1938</td> <td> 3892.892</td> <td>    2.498</td> <td> 0.012</td> <td> 2094.532</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>          <td> 6319.1379</td> <td> 5239.208</td> <td>    1.206</td> <td> 0.228</td> <td>-3950.508</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>           <td> 1.056e+04</td> <td> 4083.381</td> <td>    2.587</td> <td> 0.010</td> <td> 2558.543</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>           <td> 9359.9377</td> <td> 3855.444</td> <td>    2.428</td> <td> 0.015</td> <td> 1802.682</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>           <td> 1.232e+04</td> <td> 4532.307</td> <td>    2.719</td> <td> 0.007</td> <td> 3437.128</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>          <td> 9709.7805</td> <td> 5083.029</td> <td>    1.910</td> <td> 0.056</td> <td> -253.730</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>           <td> 1.154e+04</td> <td> 4450.023</td> <td>    2.593</td> <td> 0.010</td> <td> 2818.012</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>          <td>-2.896e+04</td> <td> 5015.920</td> <td>   -5.773</td> <td> 0.000</td> <td>-3.88e+04</td> <td>-1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>           <td>  1.33e+04</td> <td> 3145.921</td> <td>    4.229</td> <td> 0.000</td> <td> 7137.443</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>           <td>-1276.6700</td> <td> 3136.449</td> <td>   -0.407</td> <td> 0.684</td> <td>-7424.587</td> <td> 4871.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>           <td> 7204.7187</td> <td> 3492.523</td> <td>    2.063</td> <td> 0.039</td> <td>  358.843</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>           <td> 8422.0184</td> <td> 3780.383</td> <td>    2.228</td> <td> 0.026</td> <td> 1011.893</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>           <td>-1.498e+04</td> <td> 3955.696</td> <td>   -3.788</td> <td> 0.000</td> <td>-2.27e+04</td> <td>-7231.134</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>           <td>  1.04e+04</td> <td> 5133.926</td> <td>    2.025</td> <td> 0.043</td> <td>  332.897</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>           <td> 1.172e+04</td> <td> 4311.513</td> <td>    2.718</td> <td> 0.007</td> <td> 3269.269</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>           <td> 1065.9030</td> <td> 2959.552</td> <td>    0.360</td> <td> 0.719</td> <td>-4735.269</td> <td> 6867.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>           <td> 9045.9152</td> <td> 3821.348</td> <td>    2.367</td> <td> 0.018</td> <td> 1555.492</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>           <td> 1.133e+04</td> <td> 4303.700</td> <td>    2.632</td> <td> 0.009</td> <td> 2889.397</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>           <td> 1.517e+04</td> <td> 4101.820</td> <td>    3.699</td> <td> 0.000</td> <td> 7133.811</td> <td> 2.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>           <td> 1.082e+04</td> <td> 4258.242</td> <td>    2.541</td> <td> 0.011</td> <td> 2474.895</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>           <td>  942.3993</td> <td> 6905.998</td> <td>    0.136</td> <td> 0.891</td> <td>-1.26e+04</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>           <td> 9756.4149</td> <td> 3934.848</td> <td>    2.479</td> <td> 0.013</td> <td> 2043.513</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>           <td> 7896.0643</td> <td> 3900.575</td> <td>    2.024</td> <td> 0.043</td> <td>  250.343</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>           <td> 1.246e+04</td> <td> 4473.422</td> <td>    2.786</td> <td> 0.005</td> <td> 3695.738</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>           <td> 8278.6855</td> <td> 3814.267</td> <td>    2.170</td> <td> 0.030</td> <td>  802.142</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>           <td>  1.07e+04</td> <td> 4143.301</td> <td>    2.581</td> <td> 0.010</td> <td> 2574.325</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>           <td> 1.238e+04</td> <td> 4473.315</td> <td>    2.767</td> <td> 0.006</td> <td> 3611.317</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>           <td> 7739.4594</td> <td> 3195.857</td> <td>    2.422</td> <td> 0.015</td> <td> 1475.094</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>           <td> 3058.4908</td> <td> 3102.949</td> <td>    0.986</td> <td> 0.324</td> <td>-3023.761</td> <td> 9140.743</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>           <td>  1.16e+04</td> <td> 4271.985</td> <td>    2.714</td> <td> 0.007</td> <td> 3222.039</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>           <td> 1.238e+04</td> <td> 7941.667</td> <td>    1.559</td> <td> 0.119</td> <td>-3184.200</td> <td> 2.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>           <td> 8036.7520</td> <td> 3678.547</td> <td>    2.185</td> <td> 0.029</td> <td>  826.241</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>           <td> 1.133e+04</td> <td> 4450.159</td> <td>    2.546</td> <td> 0.011</td> <td> 2605.843</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>           <td> 1.032e+04</td> <td> 4188.627</td> <td>    2.463</td> <td> 0.014</td> <td> 2107.083</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>           <td>-4200.0683</td> <td> 3016.386</td> <td>   -1.392</td> <td> 0.164</td> <td>-1.01e+04</td> <td> 1712.508</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>           <td> 6540.4623</td> <td> 3539.456</td> <td>    1.848</td> <td> 0.065</td> <td> -397.410</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>           <td> 1.105e+04</td> <td> 4207.876</td> <td>    2.626</td> <td> 0.009</td> <td> 2801.752</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>           <td> 1.103e+04</td> <td> 4220.182</td> <td>    2.613</td> <td> 0.009</td> <td> 2754.830</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>           <td> 9640.8987</td> <td> 4124.209</td> <td>    2.338</td> <td> 0.019</td> <td> 1556.821</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>           <td> 1.098e+04</td> <td> 4402.914</td> <td>    2.494</td> <td> 0.013</td> <td> 2350.328</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>           <td>  106.4800</td> <td> 3104.123</td> <td>    0.034</td> <td> 0.973</td> <td>-5978.073</td> <td> 6191.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>           <td> 9703.4230</td> <td> 3948.030</td> <td>    2.458</td> <td> 0.014</td> <td> 1964.683</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>           <td> 9345.6370</td> <td> 3833.374</td> <td>    2.438</td> <td> 0.015</td> <td> 1831.641</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>           <td> 9731.3147</td> <td> 5847.278</td> <td>    1.664</td> <td> 0.096</td> <td>-1730.239</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>           <td> 1.201e+04</td> <td> 4326.600</td> <td>    2.775</td> <td> 0.006</td> <td> 3526.284</td> <td> 2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>           <td> 1.398e+04</td> <td> 3155.542</td> <td>    4.431</td> <td> 0.000</td> <td> 7797.210</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>           <td> 3159.2103</td> <td> 3140.716</td> <td>    1.006</td> <td> 0.314</td> <td>-2997.071</td> <td> 9315.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>           <td> 1.325e+04</td> <td> 5704.105</td> <td>    2.324</td> <td> 0.020</td> <td> 2073.387</td> <td> 2.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>           <td> 1.087e+04</td> <td> 4172.152</td> <td>    2.606</td> <td> 0.009</td> <td> 2694.673</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>           <td> 1.218e+04</td> <td> 4382.072</td> <td>    2.779</td> <td> 0.005</td> <td> 3587.033</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>           <td>-1623.7257</td> <td> 3020.376</td> <td>   -0.538</td> <td> 0.591</td> <td>-7544.123</td> <td> 4296.672</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>           <td> 1.376e+04</td> <td> 4605.552</td> <td>    2.987</td> <td> 0.003</td> <td> 4729.602</td> <td> 2.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>           <td>-1.068e+04</td> <td> 3459.448</td> <td>   -3.087</td> <td> 0.002</td> <td>-1.75e+04</td> <td>-3899.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>           <td> 1.167e+04</td> <td> 4644.903</td> <td>    2.511</td> <td> 0.012</td> <td> 2560.590</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>           <td>  1.15e+04</td> <td> 4294.555</td> <td>    2.677</td> <td> 0.007</td> <td> 3077.736</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>           <td> 9550.7894</td> <td> 3972.501</td> <td>    2.404</td> <td> 0.016</td> <td> 1764.084</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>           <td> 1.067e+04</td> <td> 4175.203</td> <td>    2.555</td> <td> 0.011</td> <td> 2483.547</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>           <td> 2794.6874</td> <td> 3024.603</td> <td>    0.924</td> <td> 0.356</td> <td>-3133.995</td> <td> 8723.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>           <td> 1.194e+04</td> <td> 4512.653</td> <td>    2.647</td> <td> 0.008</td> <td> 3099.269</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>           <td> 4102.7272</td> <td> 4083.423</td> <td>    1.005</td> <td> 0.315</td> <td>-3901.402</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>           <td> 4912.8458</td> <td> 3212.206</td> <td>    1.529</td> <td> 0.126</td> <td>-1383.567</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>           <td> 5531.9600</td> <td> 3268.028</td> <td>    1.693</td> <td> 0.091</td> <td> -873.873</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>           <td> 1.113e+04</td> <td> 4166.476</td> <td>    2.672</td> <td> 0.008</td> <td> 2967.783</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>           <td> 7685.9806</td> <td> 4363.589</td> <td>    1.761</td> <td> 0.078</td> <td> -867.317</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>           <td> 1.265e+04</td> <td> 3474.516</td> <td>    3.642</td> <td> 0.000</td> <td> 5842.833</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>           <td> 1.076e+04</td> <td> 3676.094</td> <td>    2.926</td> <td> 0.003</td> <td> 3549.393</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>           <td> 5981.2294</td> <td> 3491.103</td> <td>    1.713</td> <td> 0.087</td> <td> -861.863</td> <td> 1.28e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>           <td> 1.242e+04</td> <td> 4487.009</td> <td>    2.768</td> <td> 0.006</td> <td> 3626.309</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>           <td> 1.293e+04</td> <td> 5088.069</td> <td>    2.541</td> <td> 0.011</td> <td> 2954.216</td> <td> 2.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>           <td>     1e+04</td> <td> 3995.575</td> <td>    2.504</td> <td> 0.012</td> <td> 2172.638</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>           <td> 1.031e+04</td> <td> 4620.297</td> <td>    2.231</td> <td> 0.026</td> <td> 1252.392</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>           <td> 8759.6584</td> <td> 3847.940</td> <td>    2.276</td> <td> 0.023</td> <td> 1217.111</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>           <td> 1.058e+04</td> <td> 4082.559</td> <td>    2.591</td> <td> 0.010</td> <td> 2576.987</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>           <td> 1.392e+04</td> <td> 4153.892</td> <td>    3.351</td> <td> 0.001</td> <td> 5778.922</td> <td> 2.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>           <td> 5301.0622</td> <td> 3502.912</td> <td>    1.513</td> <td> 0.130</td> <td>-1565.178</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>           <td> 4120.5527</td> <td> 3188.548</td> <td>    1.292</td> <td> 0.196</td> <td>-2129.486</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>           <td> 1.969e+04</td> <td> 4266.255</td> <td>    4.614</td> <td> 0.000</td> <td> 1.13e+04</td> <td>  2.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>           <td> 1.219e+04</td> <td> 5085.387</td> <td>    2.396</td> <td> 0.017</td> <td> 2217.046</td> <td> 2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>           <td> 8856.4192</td> <td> 4031.036</td> <td>    2.197</td> <td> 0.028</td> <td>  954.976</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>           <td> 3.532e+04</td> <td> 3220.846</td> <td>   10.966</td> <td> 0.000</td> <td>  2.9e+04</td> <td> 4.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>           <td> 9192.0479</td> <td> 3774.724</td> <td>    2.435</td> <td> 0.015</td> <td> 1793.015</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>           <td> 1.111e+04</td> <td> 4501.757</td> <td>    2.469</td> <td> 0.014</td> <td> 2288.679</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>           <td> 1.087e+04</td> <td> 4196.367</td> <td>    2.591</td> <td> 0.010</td> <td> 2649.108</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>           <td> 1.135e+04</td> <td> 4815.400</td> <td>    2.358</td> <td> 0.018</td> <td> 1915.397</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>           <td> 5098.6077</td> <td> 3246.491</td> <td>    1.570</td> <td> 0.116</td> <td>-1265.010</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>           <td> 7842.5035</td> <td> 3534.507</td> <td>    2.219</td> <td> 0.027</td> <td>  914.333</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>           <td> 7723.8849</td> <td> 3633.104</td> <td>    2.126</td> <td> 0.034</td> <td>  602.448</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>           <td> 9328.3374</td> <td> 3905.660</td> <td>    2.388</td> <td> 0.017</td> <td> 1672.650</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>           <td> 7397.1967</td> <td> 3546.545</td> <td>    2.086</td> <td> 0.037</td> <td>  445.428</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>           <td> 5897.1210</td> <td> 3362.452</td> <td>    1.754</td> <td> 0.079</td> <td> -693.797</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>           <td> 1078.3592</td> <td> 3147.847</td> <td>    0.343</td> <td> 0.732</td> <td>-5091.901</td> <td> 7248.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>           <td>  1.04e+04</td> <td> 4250.324</td> <td>    2.446</td> <td> 0.014</td> <td> 2063.728</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>           <td> 9520.0452</td> <td> 3911.185</td> <td>    2.434</td> <td> 0.015</td> <td> 1853.528</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>           <td> 8453.4334</td> <td> 4930.048</td> <td>    1.715</td> <td> 0.086</td> <td>-1210.210</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>           <td> 8867.4155</td> <td> 3529.618</td> <td>    2.512</td> <td> 0.012</td> <td> 1948.827</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>           <td> 1.144e+04</td> <td> 4255.597</td> <td>    2.689</td> <td> 0.007</td> <td> 3102.766</td> <td> 1.98e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>           <td> 5151.4501</td> <td> 3281.945</td> <td>    1.570</td> <td> 0.117</td> <td>-1281.661</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>           <td> 1.029e+04</td> <td> 4183.920</td> <td>    2.459</td> <td> 0.014</td> <td> 2089.086</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>           <td> 4008.4871</td> <td> 3177.140</td> <td>    1.262</td> <td> 0.207</td> <td>-2219.191</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>           <td>-1.479e+04</td> <td> 3233.253</td> <td>   -4.575</td> <td> 0.000</td> <td>-2.11e+04</td> <td>-8454.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>           <td> 1.019e+04</td> <td> 4004.646</td> <td>    2.544</td> <td> 0.011</td> <td> 2339.317</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>           <td> 1.137e+04</td> <td> 4519.517</td> <td>    2.516</td> <td> 0.012</td> <td> 2510.606</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>           <td> 1.076e+04</td> <td> 4116.216</td> <td>    2.614</td> <td> 0.009</td> <td> 2690.138</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>           <td> 1.076e+04</td> <td> 4153.781</td> <td>    2.591</td> <td> 0.010</td> <td> 2620.102</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>           <td> 3359.7419</td> <td> 3088.730</td> <td>    1.088</td> <td> 0.277</td> <td>-2694.638</td> <td> 9414.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>           <td>   1.1e+04</td> <td> 4267.354</td> <td>    2.579</td> <td> 0.010</td> <td> 2640.333</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>           <td> 8204.8310</td> <td> 3806.973</td> <td>    2.155</td> <td> 0.031</td> <td>  742.585</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>           <td> 5703.7807</td> <td> 3354.251</td> <td>    1.700</td> <td> 0.089</td> <td> -871.062</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>           <td> 9143.7308</td> <td> 3775.228</td> <td>    2.422</td> <td> 0.015</td> <td> 1743.710</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>           <td>-4709.9246</td> <td> 3006.136</td> <td>   -1.567</td> <td> 0.117</td> <td>-1.06e+04</td> <td> 1182.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>           <td> 9293.3462</td> <td> 3912.452</td> <td>    2.375</td> <td> 0.018</td> <td> 1624.345</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>           <td> 5454.0280</td> <td> 3322.397</td> <td>    1.642</td> <td> 0.101</td> <td>-1058.375</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>           <td> 1.042e+04</td> <td> 4050.637</td> <td>    2.572</td> <td> 0.010</td> <td> 2480.063</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>           <td> 8293.3585</td> <td> 3686.192</td> <td>    2.250</td> <td> 0.024</td> <td> 1067.862</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>           <td> 2715.7040</td> <td> 3007.334</td> <td>    0.903</td> <td> 0.367</td> <td>-3179.127</td> <td> 8610.535</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>           <td> 1919.1878</td> <td> 3040.146</td> <td>    0.631</td> <td> 0.528</td> <td>-4039.961</td> <td> 7878.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>           <td> 1.391e+04</td> <td> 4332.521</td> <td>    3.210</td> <td> 0.001</td> <td> 5416.523</td> <td> 2.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>           <td> 8796.3042</td> <td> 3660.684</td> <td>    2.403</td> <td> 0.016</td> <td> 1620.807</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>           <td> 9824.0520</td> <td> 4152.989</td> <td>    2.366</td> <td> 0.018</td> <td> 1683.561</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>           <td> 7306.5846</td> <td> 3602.685</td> <td>    2.028</td> <td> 0.043</td> <td>  244.774</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>           <td> 6665.8266</td> <td> 3616.131</td> <td>    1.843</td> <td> 0.065</td> <td> -422.340</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>           <td>-9857.0510</td> <td> 3135.543</td> <td>   -3.144</td> <td> 0.002</td> <td> -1.6e+04</td> <td>-3710.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>           <td> 9728.9685</td> <td> 4012.019</td> <td>    2.425</td> <td> 0.015</td> <td> 1864.801</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>           <td> 1.285e+04</td> <td> 4423.355</td> <td>    2.904</td> <td> 0.004</td> <td> 4175.418</td> <td> 2.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>           <td> 8260.1941</td> <td> 3659.637</td> <td>    2.257</td> <td> 0.024</td> <td> 1086.748</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>           <td> 8521.1702</td> <td> 4219.749</td> <td>    2.019</td> <td> 0.043</td> <td>  249.820</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>           <td> 7568.8138</td> <td> 3635.126</td> <td>    2.082</td> <td> 0.037</td> <td>  443.413</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>           <td>-5940.6066</td> <td> 3633.910</td> <td>   -1.635</td> <td> 0.102</td> <td>-1.31e+04</td> <td> 1182.410</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>           <td> 5782.3278</td> <td> 3379.903</td> <td>    1.711</td> <td> 0.087</td> <td> -842.797</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>           <td> 8573.5924</td> <td> 3747.465</td> <td>    2.288</td> <td> 0.022</td> <td> 1227.991</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>           <td> 1.191e+04</td> <td> 4342.888</td> <td>    2.743</td> <td> 0.006</td> <td> 3399.602</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>           <td> 9871.9424</td> <td> 3953.432</td> <td>    2.497</td> <td> 0.013</td> <td> 2122.614</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>           <td>  -63.1503</td> <td> 2943.464</td> <td>   -0.021</td> <td> 0.983</td> <td>-5832.788</td> <td> 5706.488</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>           <td> 1.101e+04</td> <td> 4189.962</td> <td>    2.627</td> <td> 0.009</td> <td> 2795.062</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>           <td> 3268.2742</td> <td> 3100.134</td> <td>    1.054</td> <td> 0.292</td> <td>-2808.460</td> <td> 9345.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>           <td> 1.265e+04</td> <td> 4772.087</td> <td>    2.651</td> <td> 0.008</td> <td> 3294.603</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>           <td> 5157.2125</td> <td> 3345.753</td> <td>    1.541</td> <td> 0.123</td> <td>-1400.972</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>           <td> 3394.2616</td> <td> 3294.579</td> <td>    1.030</td> <td> 0.303</td> <td>-3063.615</td> <td> 9852.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>           <td>  -54.6148</td> <td> 3158.648</td> <td>   -0.017</td> <td> 0.986</td> <td>-6246.045</td> <td> 6136.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>           <td> 1.203e+04</td> <td> 4458.083</td> <td>    2.699</td> <td> 0.007</td> <td> 3294.795</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>           <td> 2375.9926</td> <td> 3170.269</td> <td>    0.749</td> <td> 0.454</td> <td>-3838.218</td> <td> 8590.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>           <td> 2666.7976</td> <td> 3125.567</td> <td>    0.853</td> <td> 0.394</td> <td>-3459.789</td> <td> 8793.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>           <td> 6743.8027</td> <td> 3815.948</td> <td>    1.767</td> <td> 0.077</td> <td> -736.037</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>           <td> 3811.2373</td> <td> 3095.166</td> <td>    1.231</td> <td> 0.218</td> <td>-2255.759</td> <td> 9878.234</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>           <td> 1.042e+04</td> <td> 3872.462</td> <td>    2.692</td> <td> 0.007</td> <td> 2833.174</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>           <td> 1.273e+04</td> <td> 4602.405</td> <td>    2.766</td> <td> 0.006</td> <td> 3710.745</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>           <td> 8859.6583</td> <td> 3798.952</td> <td>    2.332</td> <td> 0.020</td> <td> 1413.135</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>           <td> 8358.1549</td> <td> 3677.448</td> <td>    2.273</td> <td> 0.023</td> <td> 1149.797</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>           <td> 4807.9611</td> <td> 3297.333</td> <td>    1.458</td> <td> 0.145</td> <td>-1655.312</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>           <td>-3937.5812</td> <td> 3829.797</td> <td>   -1.028</td> <td> 0.304</td> <td>-1.14e+04</td> <td> 3569.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>           <td> 1.303e+04</td> <td> 4819.099</td> <td>    2.705</td> <td> 0.007</td> <td> 3587.830</td> <td> 2.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>           <td> 9571.6362</td> <td> 3984.079</td> <td>    2.402</td> <td> 0.016</td> <td> 1762.235</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>           <td> 9664.9227</td> <td> 4452.608</td> <td>    2.171</td> <td> 0.030</td> <td>  937.133</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>           <td> 2.662e+04</td> <td> 3200.718</td> <td>    8.316</td> <td> 0.000</td> <td> 2.03e+04</td> <td> 3.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>           <td> 2847.7871</td> <td> 3111.934</td> <td>    0.915</td> <td> 0.360</td> <td>-3252.076</td> <td> 8947.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>           <td> 3.097e+04</td> <td> 4334.476</td> <td>    7.144</td> <td> 0.000</td> <td> 2.25e+04</td> <td> 3.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>           <td> 1064.3040</td> <td> 3405.646</td> <td>    0.313</td> <td> 0.755</td> <td>-5611.280</td> <td> 7739.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>           <td> 1.143e+04</td> <td> 4342.977</td> <td>    2.632</td> <td> 0.009</td> <td> 2917.232</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>           <td> 1.097e+04</td> <td> 4260.535</td> <td>    2.576</td> <td> 0.010</td> <td> 2622.868</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>           <td> 1.106e+04</td> <td> 4277.534</td> <td>    2.587</td> <td> 0.010</td> <td> 2679.397</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>           <td> 8852.5352</td> <td> 3572.763</td> <td>    2.478</td> <td> 0.013</td> <td> 1849.375</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>           <td> 1.211e+04</td> <td> 4438.488</td> <td>    2.727</td> <td> 0.006</td> <td> 3405.146</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>           <td> 8539.2383</td> <td> 3716.683</td> <td>    2.298</td> <td> 0.022</td> <td> 1253.975</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>           <td> 5903.3861</td> <td> 3583.915</td> <td>    1.647</td> <td> 0.100</td> <td>-1121.633</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>           <td> 1.141e+04</td> <td> 5303.064</td> <td>    2.152</td> <td> 0.031</td> <td> 1018.708</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>           <td> 5266.6759</td> <td> 3251.575</td> <td>    1.620</td> <td> 0.105</td> <td>-1106.905</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>           <td> 6525.4736</td> <td> 3566.937</td> <td>    1.829</td> <td> 0.067</td> <td> -466.265</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>           <td> 5590.8047</td> <td> 3384.259</td> <td>    1.652</td> <td> 0.099</td> <td>-1042.858</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>           <td> 9524.4686</td> <td> 3890.257</td> <td>    2.448</td> <td> 0.014</td> <td> 1898.974</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>           <td> 1.113e+04</td> <td> 4176.221</td> <td>    2.664</td> <td> 0.008</td> <td> 2939.597</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>           <td> 1.096e+04</td> <td> 4208.368</td> <td>    2.605</td> <td> 0.009</td> <td> 2713.368</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>           <td>  1.12e+04</td> <td> 3656.399</td> <td>    3.062</td> <td> 0.002</td> <td> 4028.997</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>           <td> 1.298e+04</td> <td> 4615.110</td> <td>    2.813</td> <td> 0.005</td> <td> 3937.940</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>           <td> 9181.1749</td> <td> 3848.090</td> <td>    2.386</td> <td> 0.017</td> <td> 1638.333</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>           <td> 9627.9364</td> <td> 3911.676</td> <td>    2.461</td> <td> 0.014</td> <td> 1960.456</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>           <td> 1.208e+04</td> <td> 4432.894</td> <td>    2.725</td> <td> 0.006</td> <td> 3390.509</td> <td> 2.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>           <td> 3661.8316</td> <td> 3646.213</td> <td>    1.004</td> <td> 0.315</td> <td>-3485.301</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>           <td> 7769.9386</td> <td> 3624.646</td> <td>    2.144</td> <td> 0.032</td> <td>  665.082</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>           <td> 8951.7960</td> <td> 3784.276</td> <td>    2.366</td> <td> 0.018</td> <td> 1534.039</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>           <td>-8091.6358</td> <td> 3040.544</td> <td>   -2.661</td> <td> 0.008</td> <td>-1.41e+04</td> <td>-2131.707</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>           <td> 5547.1038</td> <td> 3395.854</td> <td>    1.633</td> <td> 0.102</td> <td>-1109.288</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>           <td> 1.154e+04</td> <td> 4462.107</td> <td>    2.587</td> <td> 0.010</td> <td> 2797.496</td> <td> 2.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>           <td> 7357.7328</td> <td> 3517.351</td> <td>    2.092</td> <td> 0.036</td> <td>  463.190</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>           <td> 4707.7775</td> <td> 3184.670</td> <td>    1.478</td> <td> 0.139</td> <td>-1534.661</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>           <td> 6169.5902</td> <td> 3713.425</td> <td>    1.661</td> <td> 0.097</td> <td>-1109.287</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>           <td>  877.5078</td> <td> 2975.713</td> <td>    0.295</td> <td> 0.768</td> <td>-4955.343</td> <td> 6710.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>           <td> 6934.4051</td> <td> 3514.220</td> <td>    1.973</td> <td> 0.048</td> <td>   45.999</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>           <td> 1029.6249</td> <td> 3180.276</td> <td>    0.324</td> <td> 0.746</td> <td>-5204.201</td> <td> 7263.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>           <td> 1.297e+04</td> <td> 5496.010</td> <td>    2.359</td> <td> 0.018</td> <td> 2192.891</td> <td> 2.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>           <td> 7935.3986</td> <td> 3848.198</td> <td>    2.062</td> <td> 0.039</td> <td>  392.346</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>           <td> 1.117e+04</td> <td> 4295.561</td> <td>    2.599</td> <td> 0.009</td> <td> 2745.212</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>           <td> 7783.4450</td> <td> 3611.743</td> <td>    2.155</td> <td> 0.031</td> <td>  703.880</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>           <td> 2427.7345</td> <td> 4179.686</td> <td>    0.581</td> <td> 0.561</td> <td>-5765.085</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>           <td>  161.8933</td> <td> 2939.543</td> <td>    0.055</td> <td> 0.956</td> <td>-5600.057</td> <td> 5923.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>           <td>-1862.4721</td> <td> 2944.618</td> <td>   -0.633</td> <td> 0.527</td> <td>-7634.372</td> <td> 3909.427</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>           <td> 9577.3064</td> <td> 3862.968</td> <td>    2.479</td> <td> 0.013</td> <td> 2005.301</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>           <td> 1.207e+04</td> <td> 4753.482</td> <td>    2.540</td> <td> 0.011</td> <td> 2756.935</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>           <td> 1.294e+04</td> <td> 4561.875</td> <td>    2.837</td> <td> 0.005</td> <td> 4000.503</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>           <td> 1.064e+04</td> <td> 4105.021</td> <td>    2.592</td> <td> 0.010</td> <td> 2592.742</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>           <td> 3233.5415</td> <td> 3419.949</td> <td>    0.945</td> <td> 0.344</td> <td>-3470.080</td> <td> 9937.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>           <td>  1.15e+04</td> <td> 4307.956</td> <td>    2.670</td> <td> 0.008</td> <td> 3059.867</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>           <td> 8250.1694</td> <td> 4502.595</td> <td>    1.832</td> <td> 0.067</td> <td> -575.601</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>           <td> 5615.0259</td> <td> 3514.373</td> <td>    1.598</td> <td> 0.110</td> <td>-1273.679</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>           <td> 8184.6337</td> <td> 3690.329</td> <td>    2.218</td> <td> 0.027</td> <td>  951.026</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>           <td> 5773.7851</td> <td> 3330.914</td> <td>    1.733</td> <td> 0.083</td> <td> -755.313</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>           <td>  379.0420</td> <td> 3897.387</td> <td>    0.097</td> <td> 0.923</td> <td>-7260.429</td> <td> 8018.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>           <td> 1.291e+04</td> <td> 4648.350</td> <td>    2.777</td> <td> 0.005</td> <td> 3798.287</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>           <td> 6573.8418</td> <td> 3745.839</td> <td>    1.755</td> <td> 0.079</td> <td> -768.573</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>           <td> 1.104e+04</td> <td> 3107.094</td> <td>    3.554</td> <td> 0.000</td> <td> 4953.235</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>           <td> 1.003e+04</td> <td> 3232.899</td> <td>    3.103</td> <td> 0.002</td> <td> 3693.095</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>           <td> 5266.1813</td> <td> 3238.912</td> <td>    1.626</td> <td> 0.104</td> <td>-1082.580</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>           <td> 5490.3001</td> <td> 3265.877</td> <td>    1.681</td> <td> 0.093</td> <td> -911.315</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>           <td>-1.465e+04</td> <td> 4841.183</td> <td>   -3.026</td> <td> 0.002</td> <td>-2.41e+04</td> <td>-5157.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>           <td> 1.113e+04</td> <td> 4184.411</td> <td>    2.660</td> <td> 0.008</td> <td> 2930.217</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>           <td> 5799.6371</td> <td> 3924.945</td> <td>    1.478</td> <td> 0.140</td> <td>-1893.852</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>           <td> 7653.2768</td> <td> 3540.545</td> <td>    2.162</td> <td> 0.031</td> <td>  713.270</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>           <td> 5222.4602</td> <td> 4344.338</td> <td>    1.202</td> <td> 0.229</td> <td>-3293.102</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>           <td> 1.294e+04</td> <td> 4579.938</td> <td>    2.826</td> <td> 0.005</td> <td> 3967.053</td> <td> 2.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>           <td> 2786.9648</td> <td> 3094.998</td> <td>    0.900</td> <td> 0.368</td> <td>-3279.702</td> <td> 8853.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>           <td>  696.8935</td> <td> 2942.311</td> <td>    0.237</td> <td> 0.813</td> <td>-5070.484</td> <td> 6464.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>           <td>-6797.1413</td> <td> 5279.218</td> <td>   -1.288</td> <td> 0.198</td> <td>-1.71e+04</td> <td> 3550.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>           <td> 2075.9287</td> <td> 2998.392</td> <td>    0.692</td> <td> 0.489</td> <td>-3801.376</td> <td> 7953.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>           <td> 1.078e+04</td> <td> 4118.761</td> <td>    2.617</td> <td> 0.009</td> <td> 2705.388</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>           <td>  1.07e+04</td> <td> 3842.617</td> <td>    2.784</td> <td> 0.005</td> <td> 3164.148</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>           <td> 3103.5465</td> <td> 3044.700</td> <td>    1.019</td> <td> 0.308</td> <td>-2864.530</td> <td> 9071.623</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>           <td> 6948.3637</td> <td> 3504.437</td> <td>    1.983</td> <td> 0.047</td> <td>   79.134</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>           <td> 8658.0344</td> <td> 3760.543</td> <td>    2.302</td> <td> 0.021</td> <td> 1286.797</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>           <td> 1994.4442</td> <td> 3022.632</td> <td>    0.660</td> <td> 0.509</td> <td>-3930.375</td> <td> 7919.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>           <td> 1.174e+04</td> <td> 4284.250</td> <td>    2.741</td> <td> 0.006</td> <td> 3344.095</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>           <td> 5796.5913</td> <td> 3278.106</td> <td>    1.768</td> <td> 0.077</td> <td> -628.996</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>           <td>  722.1063</td> <td> 3107.287</td> <td>    0.232</td> <td> 0.816</td> <td>-5368.648</td> <td> 6812.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>           <td> 8538.2025</td> <td> 3759.120</td> <td>    2.271</td> <td> 0.023</td> <td> 1169.755</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>           <td>-3.283e+04</td> <td> 3444.849</td> <td>   -9.531</td> <td> 0.000</td> <td>-3.96e+04</td> <td>-2.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>           <td> 1.252e+04</td> <td> 4529.032</td> <td>    2.765</td> <td> 0.006</td> <td> 3644.228</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>           <td> 5934.5649</td> <td> 3333.050</td> <td>    1.781</td> <td> 0.075</td> <td> -598.720</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>           <td>  775.4874</td> <td> 3649.467</td> <td>    0.212</td> <td> 0.832</td> <td>-6378.023</td> <td> 7928.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>           <td> 5664.6187</td> <td> 3452.978</td> <td>    1.641</td> <td> 0.101</td> <td>-1103.743</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>           <td> 1.221e+04</td> <td> 4531.381</td> <td>    2.694</td> <td> 0.007</td> <td> 3327.463</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1981</th> <td>   -0.0528</td> <td>    0.065</td> <td>   -0.807</td> <td> 0.419</td> <td>   -0.181</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1982</th> <td>   -0.0262</td> <td>    0.065</td> <td>   -0.406</td> <td> 0.685</td> <td>   -0.153</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1983</th> <td>   -0.0435</td> <td>    0.064</td> <td>   -0.683</td> <td> 0.494</td> <td>   -0.168</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1984</th> <td>   -0.1019</td> <td>    0.063</td> <td>   -1.606</td> <td> 0.108</td> <td>   -0.226</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1985</th> <td>   -0.1423</td> <td>    0.064</td> <td>   -2.236</td> <td> 0.025</td> <td>   -0.267</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1986</th> <td>   -0.1942</td> <td>    0.064</td> <td>   -3.030</td> <td> 0.002</td> <td>   -0.320</td> <td>   -0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1987</th> <td>   -0.2053</td> <td>    0.065</td> <td>   -3.167</td> <td> 0.002</td> <td>   -0.332</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1988</th> <td>   -0.2385</td> <td>    0.065</td> <td>   -3.643</td> <td> 0.000</td> <td>   -0.367</td> <td>   -0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1989</th> <td>   -0.2438</td> <td>    0.066</td> <td>   -3.672</td> <td> 0.000</td> <td>   -0.374</td> <td>   -0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1990</th> <td>   -0.2782</td> <td>    0.067</td> <td>   -4.138</td> <td> 0.000</td> <td>   -0.410</td> <td>   -0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1991</th> <td>   -0.2604</td> <td>    0.068</td> <td>   -3.830</td> <td> 0.000</td> <td>   -0.394</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1992</th> <td>   -0.2623</td> <td>    0.069</td> <td>   -3.810</td> <td> 0.000</td> <td>   -0.397</td> <td>   -0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1993</th> <td>   -0.2406</td> <td>    0.070</td> <td>   -3.430</td> <td> 0.001</td> <td>   -0.378</td> <td>   -0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1994</th> <td>   -0.2479</td> <td>    0.071</td> <td>   -3.482</td> <td> 0.000</td> <td>   -0.387</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1995</th> <td>   -0.2435</td> <td>    0.073</td> <td>   -3.348</td> <td> 0.001</td> <td>   -0.386</td> <td>   -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1996</th> <td>   -0.2201</td> <td>    0.075</td> <td>   -2.945</td> <td> 0.003</td> <td>   -0.367</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1997</th> <td>   -0.2168</td> <td>    0.077</td> <td>   -2.820</td> <td> 0.005</td> <td>   -0.368</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1998</th> <td>   -0.1931</td> <td>    0.079</td> <td>   -2.446</td> <td> 0.014</td> <td>   -0.348</td> <td>   -0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX1999</th> <td>   -0.0952</td> <td>    0.081</td> <td>   -1.180</td> <td> 0.238</td> <td>   -0.253</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX2000</th> <td>   -0.1748</td> <td>    0.082</td> <td>   -2.127</td> <td> 0.033</td> <td>   -0.336</td> <td>   -0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspilltecIVX2001</th> <td>   -0.2609</td> <td>    0.083</td> <td>   -3.134</td> <td> 0.002</td> <td>   -0.424</td> <td>   -0.098</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21422.439</td> <th>  Durbin-Watson:     </th>   <td>   0.544</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>46586321.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 9.916</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>291.337</td>  <th>  Cond. No.          </th>   <td>2.69e+07</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.69e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.667    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.647    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      32.62    \\\\\n",
       "\\textbf{Date:}             & Wed, 16 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     02:03:00     & \\textbf{  Log-Likelihood:    } & -1.4153e+05   \\\\\n",
       "\\textbf{No. Observations:} &       13385      & \\textbf{  AIC:               } &  2.846e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       12609      & \\textbf{  BIC:               } &  2.904e+05    \\\\\n",
       "\\textbf{Df Model:}         &         775      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &   -1.404e+04  &     4166.434     &    -3.371  &         0.001        &    -2.22e+04    &    -5877.761     \\\\\n",
       "\\textbf{gspilltecIV}      &       0.4517  &        0.123     &     3.677  &         0.000        &        0.211    &        0.693     \\\\\n",
       "\\textbf{pat\\_count}       &     -27.4800  &        1.908     &   -14.400  &         0.000        &      -31.221    &      -23.739     \\\\\n",
       "\\textbf{rsales}           &       0.7669  &        0.037     &    20.684  &         0.000        &        0.694    &        0.840     \\\\\n",
       "\\textbf{rppent}           &       0.5705  &        0.084     &     6.780  &         0.000        &        0.406    &        0.735     \\\\\n",
       "\\textbf{emp}              &      12.9965  &        7.160     &     1.815  &         0.070        &       -1.038    &       27.031     \\\\\n",
       "\\textbf{rxrd}             &      19.7263  &        0.598     &    32.993  &         0.000        &       18.554    &       20.898     \\\\\n",
       "\\textbf{1981}             &     110.3657  &     1080.039     &     0.102  &         0.919        &    -2006.675    &     2227.406     \\\\\n",
       "\\textbf{1982}             &    -243.4640  &     1072.952     &    -0.227  &         0.820        &    -2346.613    &     1859.685     \\\\\n",
       "\\textbf{1983}             &    -114.3389  &     1063.325     &    -0.108  &         0.914        &    -2198.618    &     1969.940     \\\\\n",
       "\\textbf{1984}             &     115.2448  &     1057.289     &     0.109  &         0.913        &    -1957.203    &     2187.692     \\\\\n",
       "\\textbf{1985}             &     542.6004  &     1055.171     &     0.514  &         0.607        &    -1525.695    &     2610.895     \\\\\n",
       "\\textbf{1986}             &    1069.3608  &     1045.395     &     1.023  &         0.306        &     -979.773    &     3118.495     \\\\\n",
       "\\textbf{1987}             &    1025.4005  &     1040.967     &     0.985  &         0.325        &    -1015.054    &     3065.855     \\\\\n",
       "\\textbf{1988}             &    1291.3045  &     1041.029     &     1.240  &         0.215        &     -749.270    &     3331.879     \\\\\n",
       "\\textbf{1989}             &    1514.1450  &     1036.323     &     1.461  &         0.144        &     -517.206    &     3545.496     \\\\\n",
       "\\textbf{1990}             &    1689.8009  &     1031.752     &     1.638  &         0.101        &     -332.589    &     3712.191     \\\\\n",
       "\\textbf{1991}             &    1757.6723  &     1031.035     &     1.705  &         0.088        &     -263.313    &     3778.657     \\\\\n",
       "\\textbf{1992}             &    1663.9210  &     1030.301     &     1.615  &         0.106        &     -355.627    &     3683.469     \\\\\n",
       "\\textbf{1993}             &    1275.2181  &     1026.193     &     1.243  &         0.214        &     -736.277    &     3286.713     \\\\\n",
       "\\textbf{1994}             &    1193.1166  &     1028.610     &     1.160  &         0.246        &     -823.115    &     3209.348     \\\\\n",
       "\\textbf{1995}             &    1591.0463  &     1026.610     &     1.550  &         0.121        &     -421.265    &     3603.357     \\\\\n",
       "\\textbf{1996}             &    1293.6561  &     1028.769     &     1.257  &         0.209        &     -722.888    &     3310.200     \\\\\n",
       "\\textbf{1997}             &    1571.4228  &     1033.392     &     1.521  &         0.128        &     -454.183    &     3597.029     \\\\\n",
       "\\textbf{1998}             &    1015.9035  &     1036.611     &     0.980  &         0.327        &    -1016.012    &     3047.819     \\\\\n",
       "\\textbf{1999}             &   -1264.8357  &     1047.178     &    -1.208  &         0.227        &    -3317.464    &      787.792     \\\\\n",
       "\\textbf{2000}             &     683.4387  &     1067.982     &     0.640  &         0.522        &    -1409.969    &     2776.846     \\\\\n",
       "\\textbf{2001}             &    1290.2571  &     1113.123     &     1.159  &         0.246        &     -891.633    &     3472.147     \\\\\n",
       "\\textbf{10005.0}          &    8336.4171  &     3699.630     &     2.253  &         0.024        &     1084.579    &     1.56e+04     \\\\\n",
       "\\textbf{10006.0}          &    7128.2051  &     3884.307     &     1.835  &         0.067        &     -485.627    &     1.47e+04     \\\\\n",
       "\\textbf{10008.0}          &    6433.2934  &     3402.253     &     1.891  &         0.059        &     -235.640    &     1.31e+04     \\\\\n",
       "\\textbf{10016.0}          &    7164.4693  &     3479.856     &     2.059  &         0.040        &      343.423    &      1.4e+04     \\\\\n",
       "\\textbf{10030.0}          &    1.086e+04  &     4142.866     &     2.621  &         0.009        &     2739.409    &      1.9e+04     \\\\\n",
       "\\textbf{1004.0}           &    9386.8209  &     3877.670     &     2.421  &         0.016        &     1785.997    &      1.7e+04     \\\\\n",
       "\\textbf{10056.0}          &    7896.2518  &     3668.891     &     2.152  &         0.031        &      704.667    &     1.51e+04     \\\\\n",
       "\\textbf{10085.0}          &    5990.2646  &     3332.521     &     1.798  &         0.072        &     -541.984    &     1.25e+04     \\\\\n",
       "\\textbf{10092.0}          &    1.098e+04  &     5959.766     &     1.842  &         0.066        &     -705.369    &     2.27e+04     \\\\\n",
       "\\textbf{10097.0}          &    1809.1528  &     3207.129     &     0.564  &         0.573        &    -4477.308    &     8095.614     \\\\\n",
       "\\textbf{1010.0}           &    7643.0741  &     5674.555     &     1.347  &         0.178        &    -3479.917    &     1.88e+04     \\\\\n",
       "\\textbf{10109.0}          &    1.308e+04  &     4550.294     &     2.875  &         0.004        &     4161.111    &      2.2e+04     \\\\\n",
       "\\textbf{10115.0}          &    1.006e+04  &     3768.736     &     2.670  &         0.008        &     2676.196    &     1.75e+04     \\\\\n",
       "\\textbf{10124.0}          &    1.321e+04  &     4579.363     &     2.886  &         0.004        &     4238.105    &     2.22e+04     \\\\\n",
       "\\textbf{1013.0}           &    4740.3993  &     3172.561     &     1.494  &         0.135        &    -1478.302    &      1.1e+04     \\\\\n",
       "\\textbf{10150.0}          &    4494.4694  &     3784.080     &     1.188  &         0.235        &    -2922.902    &     1.19e+04     \\\\\n",
       "\\textbf{10159.0}          &    1.165e+04  &     5152.687     &     2.260  &         0.024        &     1546.547    &     2.17e+04     \\\\\n",
       "\\textbf{10174.0}          &    1.061e+04  &     4285.422     &     2.477  &         0.013        &     2214.472    &      1.9e+04     \\\\\n",
       "\\textbf{10185.0}          &    1.119e+04  &     4410.048     &     2.538  &         0.011        &     2547.094    &     1.98e+04     \\\\\n",
       "\\textbf{10195.0}          &    9918.8258  &     3937.320     &     2.519  &         0.012        &     2201.079    &     1.76e+04     \\\\\n",
       "\\textbf{10198.0}          &    1.066e+04  &     4103.271     &     2.598  &         0.009        &     2617.745    &     1.87e+04     \\\\\n",
       "\\textbf{10215.0}          &    1.282e+04  &     4541.798     &     2.823  &         0.005        &     3919.267    &     2.17e+04     \\\\\n",
       "\\textbf{10232.0}          &    1.259e+04  &     4488.212     &     2.804  &         0.005        &     3787.767    &     2.14e+04     \\\\\n",
       "\\textbf{10236.0}          &    1.113e+04  &     4185.336     &     2.659  &         0.008        &     2926.278    &     1.93e+04     \\\\\n",
       "\\textbf{10286.0}          &    6944.5078  &     3499.189     &     1.985  &         0.047        &       85.565    &     1.38e+04     \\\\\n",
       "\\textbf{10301.0}          &   -1.406e+04  &     3043.938     &    -4.618  &         0.000        &       -2e+04    &    -8091.002     \\\\\n",
       "\\textbf{10312.0}          &    8882.7969  &     3795.864     &     2.340  &         0.019        &     1442.327    &     1.63e+04     \\\\\n",
       "\\textbf{10332.0}          &   -4179.0338  &     3944.302     &    -1.060  &         0.289        &    -1.19e+04    &     3552.399     \\\\\n",
       "\\textbf{1036.0}           &    9705.2346  &     4210.687     &     2.305  &         0.021        &     1451.648    &      1.8e+04     \\\\\n",
       "\\textbf{10374.0}          &    8442.5866  &     3705.690     &     2.278  &         0.023        &     1178.869    &     1.57e+04     \\\\\n",
       "\\textbf{10386.0}          &    1560.4253  &     2977.524     &     0.524  &         0.600        &    -4275.976    &     7396.826     \\\\\n",
       "\\textbf{10391.0}          &   -2882.9016  &     2983.643     &    -0.966  &         0.334        &    -8731.297    &     2965.493     \\\\\n",
       "\\textbf{10407.0}          &    7373.1369  &     3485.250     &     2.116  &         0.034        &      541.517    &     1.42e+04     \\\\\n",
       "\\textbf{10420.0}          &    1.144e+04  &     3948.088     &     2.898  &         0.004        &     3703.384    &     1.92e+04     \\\\\n",
       "\\textbf{10422.0}          &    7742.9911  &     3805.122     &     2.035  &         0.042        &      284.373    &     1.52e+04     \\\\\n",
       "\\textbf{10426.0}          &    1.184e+04  &     4573.963     &     2.588  &         0.010        &     2873.904    &     2.08e+04     \\\\\n",
       "\\textbf{10441.0}          &    1.192e+04  &     4357.147     &     2.735  &         0.006        &     3375.210    &     2.05e+04     \\\\\n",
       "\\textbf{1045.0}           &   -2088.3861  &     3668.933     &    -0.569  &         0.569        &    -9280.053    &     5103.280     \\\\\n",
       "\\textbf{10453.0}          &    1226.7046  &     2981.021     &     0.412  &         0.681        &    -4616.550    &     7069.959     \\\\\n",
       "\\textbf{10482.0}          &   -1.245e+04  &     3822.048     &    -3.258  &         0.001        &    -1.99e+04    &    -4961.414     \\\\\n",
       "\\textbf{10498.0}          &    1.049e+04  &     4138.590     &     2.535  &         0.011        &     2377.788    &     1.86e+04     \\\\\n",
       "\\textbf{10499.0}          &   -3707.8662  &     3195.067     &    -1.160  &         0.246        &    -9970.684    &     2554.952     \\\\\n",
       "\\textbf{10511.0}          &    1.272e+04  &     4629.963     &     2.747  &         0.006        &     3643.423    &     2.18e+04     \\\\\n",
       "\\textbf{10519.0}          &   -3690.5599  &     2990.879     &    -1.234  &         0.217        &    -9553.138    &     2172.018     \\\\\n",
       "\\textbf{10530.0}          &    2107.4873  &     3002.129     &     0.702  &         0.483        &    -3777.142    &     7992.116     \\\\\n",
       "\\textbf{10537.0}          &    4731.3617  &     3496.655     &     1.353  &         0.176        &    -2122.614    &     1.16e+04     \\\\\n",
       "\\textbf{10540.0}          &    6573.0163  &     3380.260     &     1.945  &         0.052        &      -52.807    &     1.32e+04     \\\\\n",
       "\\textbf{10541.0}          &    8401.1498  &     3790.382     &     2.216  &         0.027        &      971.424    &     1.58e+04     \\\\\n",
       "\\textbf{10550.0}          &    5503.3473  &     6140.198     &     0.896  &         0.370        &    -6532.375    &     1.75e+04     \\\\\n",
       "\\textbf{10553.0}          &    2834.0974  &     3318.240     &     0.854  &         0.393        &    -3670.158    &     9338.352     \\\\\n",
       "\\textbf{10565.0}          &     1.14e+04  &     4148.160     &     2.747  &         0.006        &     3265.326    &     1.95e+04     \\\\\n",
       "\\textbf{10580.0}          &     1.27e+04  &     4343.865     &     2.925  &         0.003        &     4189.164    &     2.12e+04     \\\\\n",
       "\\textbf{10581.0}          &    8236.5095  &     3945.775     &     2.087  &         0.037        &      502.189    &      1.6e+04     \\\\\n",
       "\\textbf{10588.0}          &    3146.3468  &     3044.889     &     1.033  &         0.301        &    -2822.100    &     9114.793     \\\\\n",
       "\\textbf{10597.0}          &    9207.2402  &     3889.421     &     2.367  &         0.018        &     1583.382    &     1.68e+04     \\\\\n",
       "\\textbf{10599.0}          &    9329.2321  &     3923.775     &     2.378  &         0.017        &     1638.037    &      1.7e+04     \\\\\n",
       "\\textbf{10618.0}          &    8960.0187  &     3856.497     &     2.323  &         0.020        &     1400.697    &     1.65e+04     \\\\\n",
       "\\textbf{10656.0}          &    7268.4839  &     3511.216     &     2.070  &         0.038        &      385.967    &     1.42e+04     \\\\\n",
       "\\textbf{10658.0}          &    7005.8260  &     3479.247     &     2.014  &         0.044        &      185.972    &     1.38e+04     \\\\\n",
       "\\textbf{10726.0}          &    1.437e+04  &     4403.463     &     3.264  &         0.001        &     5743.323    &      2.3e+04     \\\\\n",
       "\\textbf{10734.0}          &    9878.1281  &     4446.542     &     2.222  &         0.026        &     1162.229    &     1.86e+04     \\\\\n",
       "\\textbf{10735.0}          &    1.202e+04  &     4438.906     &     2.708  &         0.007        &     3320.678    &     2.07e+04     \\\\\n",
       "\\textbf{10764.0}          &    1.234e+04  &     4559.888     &     2.707  &         0.007        &     3406.665    &     2.13e+04     \\\\\n",
       "\\textbf{10777.0}          &    6970.8867  &     3477.112     &     2.005  &         0.045        &      155.219    &     1.38e+04     \\\\\n",
       "\\textbf{1078.0}           &    1.116e+04  &     3066.828     &     3.638  &         0.000        &     5146.780    &     1.72e+04     \\\\\n",
       "\\textbf{10793.0}          &    9218.8031  &     4147.854     &     2.223  &         0.026        &     1088.377    &     1.73e+04     \\\\\n",
       "\\textbf{10816.0}          &    7007.4075  &     3721.310     &     1.883  &         0.060        &     -286.926    &     1.43e+04     \\\\\n",
       "\\textbf{10839.0}          &    9251.2564  &     3809.506     &     2.428  &         0.015        &     1784.046    &     1.67e+04     \\\\\n",
       "\\textbf{10857.0}          &   -5584.6220  &     3018.547     &    -1.850  &         0.064        &    -1.15e+04    &      332.189     \\\\\n",
       "\\textbf{10867.0}          &    6266.9034  &     4195.822     &     1.494  &         0.135        &    -1957.547    &     1.45e+04     \\\\\n",
       "\\textbf{10906.0}          &    9612.5911  &     3916.798     &     2.454  &         0.014        &     1935.071    &     1.73e+04     \\\\\n",
       "\\textbf{10950.0}          &    9574.1163  &     4743.479     &     2.018  &         0.044        &      276.177    &     1.89e+04     \\\\\n",
       "\\textbf{10983.0}          &   -2.613e+04  &     3208.797     &    -8.143  &         0.000        &    -3.24e+04    &    -1.98e+04     \\\\\n",
       "\\textbf{1099.0}           &    8222.4417  &     3701.273     &     2.222  &         0.026        &      967.384    &     1.55e+04     \\\\\n",
       "\\textbf{10991.0}          &    1.145e+04  &     4668.265     &     2.453  &         0.014        &     2301.167    &     2.06e+04     \\\\\n",
       "\\textbf{11012.0}          &    1.018e+04  &     4079.198     &     2.496  &         0.013        &     2186.232    &     1.82e+04     \\\\\n",
       "\\textbf{11038.0}          &    6908.5529  &     4237.646     &     1.630  &         0.103        &    -1397.878    &     1.52e+04     \\\\\n",
       "\\textbf{1104.0}           &    1.139e+04  &     4232.982     &     2.691  &         0.007        &     3091.881    &     1.97e+04     \\\\\n",
       "\\textbf{11060.0}          &    1.027e+04  &     4117.218     &     2.494  &         0.013        &     2198.570    &     1.83e+04     \\\\\n",
       "\\textbf{11094.0}          &    8968.6893  &     3818.676     &     2.349  &         0.019        &     1483.504    &     1.65e+04     \\\\\n",
       "\\textbf{11096.0}          &    5736.6141  &     3342.590     &     1.716  &         0.086        &     -815.370    &     1.23e+04     \\\\\n",
       "\\textbf{11113.0}          &    1.022e+04  &     4226.040     &     2.417  &         0.016        &     1932.709    &     1.85e+04     \\\\\n",
       "\\textbf{1115.0}           &    8098.0254  &     3678.632     &     2.201  &         0.028        &      887.347    &     1.53e+04     \\\\\n",
       "\\textbf{11161.0}          &    8338.9787  &     3682.770     &     2.264  &         0.024        &     1120.190    &     1.56e+04     \\\\\n",
       "\\textbf{11225.0}          &    1.202e+04  &     4392.061     &     2.736  &         0.006        &     3406.259    &     2.06e+04     \\\\\n",
       "\\textbf{11228.0}          &    1.056e+04  &     3964.185     &     2.663  &         0.008        &     2785.075    &     1.83e+04     \\\\\n",
       "\\textbf{11236.0}          &     432.3561  &     4531.433     &     0.095  &         0.924        &    -8449.942    &     9314.654     \\\\\n",
       "\\textbf{11288.0}          &    6685.4946  &     3144.248     &     2.126  &         0.034        &      522.291    &     1.28e+04     \\\\\n",
       "\\textbf{11312.0}          &    3341.2076  &     3149.887     &     1.061  &         0.289        &    -2833.050    &     9515.465     \\\\\n",
       "\\textbf{11361.0}          &    5797.5155  &     3313.162     &     1.750  &         0.080        &     -696.786    &     1.23e+04     \\\\\n",
       "\\textbf{11399.0}          &    1791.0186  &     3171.702     &     0.565  &         0.572        &    -4426.001    &     8008.038     \\\\\n",
       "\\textbf{114303.0}         &   -8554.7210  &     5395.081     &    -1.586  &         0.113        &    -1.91e+04    &     2020.459     \\\\\n",
       "\\textbf{11456.0}          &     655.3210  &     3182.529     &     0.206  &         0.837        &    -5582.920    &     6893.562     \\\\\n",
       "\\textbf{11465.0}          &    4678.7849  &     3824.265     &     1.223  &         0.221        &    -2817.356    &     1.22e+04     \\\\\n",
       "\\textbf{11502.0}          &    1.186e+04  &     4447.890     &     2.667  &         0.008        &     3143.313    &     2.06e+04     \\\\\n",
       "\\textbf{11506.0}          &    1427.5921  &     3092.088     &     0.462  &         0.644        &    -4633.370    &     7488.555     \\\\\n",
       "\\textbf{11537.0}          &    8750.9564  &     3758.034     &     2.329  &         0.020        &     1384.639    &     1.61e+04     \\\\\n",
       "\\textbf{11566.0}          &    1.275e+04  &     4525.949     &     2.818  &         0.005        &     3883.438    &     2.16e+04     \\\\\n",
       "\\textbf{11573.0}          &    6824.1547  &     3447.624     &     1.979  &         0.048        &       66.287    &     1.36e+04     \\\\\n",
       "\\textbf{11580.0}          &     133.1965  &     3351.986     &     0.040  &         0.968        &    -6437.205    &     6703.598     \\\\\n",
       "\\textbf{11600.0}          &    1.196e+04  &     4334.154     &     2.760  &         0.006        &     3464.914    &     2.05e+04     \\\\\n",
       "\\textbf{11609.0}          &    1.459e+04  &     4199.329     &     3.475  &         0.001        &     6359.859    &     2.28e+04     \\\\\n",
       "\\textbf{1161.0}           &   -2819.4272  &     2954.270     &    -0.954  &         0.340        &    -8610.245    &     2971.391     \\\\\n",
       "\\textbf{11636.0}          &   -9598.0267  &     3635.579     &    -2.640  &         0.008        &    -1.67e+04    &    -2471.739     \\\\\n",
       "\\textbf{11670.0}          &     1.29e+04  &     4579.826     &     2.816  &         0.005        &     3919.660    &     2.19e+04     \\\\\n",
       "\\textbf{11678.0}          &    3653.3124  &     3268.448     &     1.118  &         0.264        &    -2753.342    &     1.01e+04     \\\\\n",
       "\\textbf{11682.0}          &    5697.0009  &     3338.579     &     1.706  &         0.088        &     -847.122    &     1.22e+04     \\\\\n",
       "\\textbf{11694.0}          &    9646.8768  &     4026.329     &     2.396  &         0.017        &     1754.660    &     1.75e+04     \\\\\n",
       "\\textbf{11720.0}          &    8111.0790  &     4501.821     &     1.802  &         0.072        &     -713.174    &     1.69e+04     \\\\\n",
       "\\textbf{11721.0}          &    6037.3698  &     3627.836     &     1.664  &         0.096        &    -1073.741    &     1.31e+04     \\\\\n",
       "\\textbf{11722.0}          &    7005.1179  &     3697.572     &     1.895  &         0.058        &     -242.685    &     1.43e+04     \\\\\n",
       "\\textbf{11793.0}          &    3172.6538  &     6017.047     &     0.527  &         0.598        &    -8621.674    &      1.5e+04     \\\\\n",
       "\\textbf{11797.0}          &    1.205e+04  &     4718.873     &     2.553  &         0.011        &     2797.242    &     2.13e+04     \\\\\n",
       "\\textbf{11914.0}          &    9708.1373  &     4476.264     &     2.169  &         0.030        &      933.979    &     1.85e+04     \\\\\n",
       "\\textbf{1209.0}           &    5758.0179  &     3254.162     &     1.769  &         0.077        &     -620.634    &     1.21e+04     \\\\\n",
       "\\textbf{12136.0}          &   -1345.1719  &     3228.391     &    -0.417  &         0.677        &    -7673.310    &     4982.966     \\\\\n",
       "\\textbf{12141.0}          &    9.132e+04  &     3418.316     &    26.715  &         0.000        &     8.46e+04    &      9.8e+04     \\\\\n",
       "\\textbf{12181.0}          &    5925.9690  &     4393.720     &     1.349  &         0.177        &    -2686.390    &     1.45e+04     \\\\\n",
       "\\textbf{12215.0}          &    1497.9987  &     3243.616     &     0.462  &         0.644        &    -4859.983    &     7855.980     \\\\\n",
       "\\textbf{12216.0}          &    7768.8755  &     3429.726     &     2.265  &         0.024        &     1046.090    &     1.45e+04     \\\\\n",
       "\\textbf{12256.0}          &    4432.5743  &     3460.578     &     1.281  &         0.200        &    -2350.685    &     1.12e+04     \\\\\n",
       "\\textbf{12262.0}          &    1.193e+04  &     4581.767     &     2.603  &         0.009        &     2947.382    &     2.09e+04     \\\\\n",
       "\\textbf{12389.0}          &    5632.2076  &     3333.542     &     1.690  &         0.091        &     -902.041    &     1.22e+04     \\\\\n",
       "\\textbf{1239.0}           &    7169.2164  &     3514.315     &     2.040  &         0.041        &      280.625    &     1.41e+04     \\\\\n",
       "\\textbf{12390.0}          &    7378.0172  &     4065.874     &     1.815  &         0.070        &     -591.714    &     1.53e+04     \\\\\n",
       "\\textbf{12397.0}          &    6407.6851  &     5492.646     &     1.167  &         0.243        &    -4358.737    &     1.72e+04     \\\\\n",
       "\\textbf{1243.0}           &    5287.9136  &     4067.600     &     1.300  &         0.194        &    -2685.201    &     1.33e+04     \\\\\n",
       "\\textbf{12548.0}          &    8984.0388  &     4241.517     &     2.118  &         0.034        &      670.020    &     1.73e+04     \\\\\n",
       "\\textbf{12570.0}          &    1.033e+04  &     4391.485     &     2.351  &         0.019        &     1717.135    &     1.89e+04     \\\\\n",
       "\\textbf{12581.0}          &    4742.3562  &     3893.599     &     1.218  &         0.223        &    -2889.690    &     1.24e+04     \\\\\n",
       "\\textbf{12592.0}          &    9733.1829  &     4214.514     &     2.309  &         0.021        &     1472.094    &      1.8e+04     \\\\\n",
       "\\textbf{12604.0}          &    9261.3599  &     6722.913     &     1.378  &         0.168        &    -3916.572    &     2.24e+04     \\\\\n",
       "\\textbf{12656.0}          &    1.275e+04  &     4797.150     &     2.658  &         0.008        &     3345.860    &     2.22e+04     \\\\\n",
       "\\textbf{12679.0}          &   -1467.2846  &     3335.617     &    -0.440  &         0.660        &    -8005.602    &     5071.033     \\\\\n",
       "\\textbf{1278.0}           &    1.187e+04  &     4568.977     &     2.598  &         0.009        &     2912.156    &     2.08e+04     \\\\\n",
       "\\textbf{12788.0}          &    1.027e+04  &     4533.225     &     2.266  &         0.023        &     1385.836    &     1.92e+04     \\\\\n",
       "\\textbf{1283.0}           &    1.308e+04  &     4581.350     &     2.855  &         0.004        &     4101.178    &     2.21e+04     \\\\\n",
       "\\textbf{1297.0}           &    9383.3666  &     3915.787     &     2.396  &         0.017        &     1707.829    &     1.71e+04     \\\\\n",
       "\\textbf{12992.0}          &    1.093e+04  &     4453.163     &     2.454  &         0.014        &     2199.830    &     1.97e+04     \\\\\n",
       "\\textbf{13135.0}          &    7207.4649  &     4052.732     &     1.778  &         0.075        &     -736.507    &     1.52e+04     \\\\\n",
       "\\textbf{1327.0}           &    9423.6690  &     3915.403     &     2.407  &         0.016        &     1748.884    &     1.71e+04     \\\\\n",
       "\\textbf{13282.0}          &    3509.7671  &     5497.179     &     0.638  &         0.523        &    -7265.539    &     1.43e+04     \\\\\n",
       "\\textbf{1334.0}           &    1.289e+04  &     4607.074     &     2.799  &         0.005        &     3862.866    &     2.19e+04     \\\\\n",
       "\\textbf{13351.0}          &    1135.7044  &     4077.886     &     0.279  &         0.781        &    -6857.572    &     9128.981     \\\\\n",
       "\\textbf{13365.0}          &    2862.8486  &     3342.703     &     0.856  &         0.392        &    -3689.358    &     9415.056     \\\\\n",
       "\\textbf{13369.0}          &    3079.9892  &     3388.253     &     0.909  &         0.363        &    -3561.502    &     9721.481     \\\\\n",
       "\\textbf{13406.0}          &    9632.3318  &     4151.138     &     2.320  &         0.020        &     1495.471    &     1.78e+04     \\\\\n",
       "\\textbf{13407.0}          &    6001.9300  &     3683.087     &     1.630  &         0.103        &    -1217.480    &     1.32e+04     \\\\\n",
       "\\textbf{13417.0}          &    1.207e+04  &     4762.098     &     2.535  &         0.011        &     2736.437    &     2.14e+04     \\\\\n",
       "\\textbf{13525.0}          &    1.146e+04  &     4599.907     &     2.492  &         0.013        &     2448.079    &     2.05e+04     \\\\\n",
       "\\textbf{13554.0}          &     1.27e+04  &     4752.213     &     2.673  &         0.008        &     3385.257    &      2.2e+04     \\\\\n",
       "\\textbf{1359.0}           &    1.051e+04  &     3699.533     &     2.840  &         0.005        &     3254.467    &     1.78e+04     \\\\\n",
       "\\textbf{13623.0}          &    9234.8077  &     4123.463     &     2.240  &         0.025        &     1152.194    &     1.73e+04     \\\\\n",
       "\\textbf{1372.0}           &    8934.5926  &     3806.329     &     2.347  &         0.019        &     1473.610    &     1.64e+04     \\\\\n",
       "\\textbf{1380.0}           &    5440.6218  &     3743.636     &     1.453  &         0.146        &    -1897.474    &     1.28e+04     \\\\\n",
       "\\textbf{13923.0}          &    8736.2570  &     4274.096     &     2.044  &         0.041        &      358.378    &     1.71e+04     \\\\\n",
       "\\textbf{13932.0}          &    1.118e+04  &     5116.978     &     2.185  &         0.029        &     1148.847    &     2.12e+04     \\\\\n",
       "\\textbf{13941.0}          &    1019.1180  &     3337.043     &     0.305  &         0.760        &    -5521.994    &     7560.230     \\\\\n",
       "\\textbf{1397.0}           &    6617.9837  &     3587.349     &     1.845  &         0.065        &     -413.766    &     1.36e+04     \\\\\n",
       "\\textbf{14064.0}          &    1.049e+04  &     4317.313     &     2.431  &         0.015        &     2032.134    &      1.9e+04     \\\\\n",
       "\\textbf{14084.0}          &    4235.9708  &     3485.117     &     1.215  &         0.224        &    -2595.388    &     1.11e+04     \\\\\n",
       "\\textbf{14324.0}          &    7822.4027  &     3672.860     &     2.130  &         0.033        &      623.039    &      1.5e+04     \\\\\n",
       "\\textbf{14462.0}          &    2250.0033  &     3404.219     &     0.661  &         0.509        &    -4422.784    &     8922.791     \\\\\n",
       "\\textbf{1447.0}           &    1.349e+04  &     5104.500     &     2.643  &         0.008        &     3483.368    &     2.35e+04     \\\\\n",
       "\\textbf{14531.0}          &    3181.5998  &        1e+04     &     0.318  &         0.751        &    -1.64e+04    &     2.28e+04     \\\\\n",
       "\\textbf{14593.0}          &    1.076e+04  &     4576.649     &     2.352  &         0.019        &     1793.113    &     1.97e+04     \\\\\n",
       "\\textbf{14622.0}          &    7006.0702  &     7326.553     &     0.956  &         0.339        &    -7355.088    &     2.14e+04     \\\\\n",
       "\\textbf{1465.0}           &    1.192e+04  &     4771.898     &     2.498  &         0.012        &     2568.838    &     2.13e+04     \\\\\n",
       "\\textbf{1468.0}           &    1.027e+04  &     4449.925     &     2.309  &         0.021        &     1552.365    &      1.9e+04     \\\\\n",
       "\\textbf{14897.0}          &    1.042e+04  &     5971.789     &     1.745  &         0.081        &    -1282.470    &     2.21e+04     \\\\\n",
       "\\textbf{14954.0}          &     1.06e+04  &     4476.738     &     2.368  &         0.018        &     1825.159    &     1.94e+04     \\\\\n",
       "\\textbf{1496.0}           &    1.104e+04  &     4166.134     &     2.651  &         0.008        &     2878.667    &     1.92e+04     \\\\\n",
       "\\textbf{15267.0}          &    9090.7110  &     4197.561     &     2.166  &         0.030        &      862.852    &     1.73e+04     \\\\\n",
       "\\textbf{15354.0}          &    3967.3799  &     3742.828     &     1.060  &         0.289        &    -3369.133    &     1.13e+04     \\\\\n",
       "\\textbf{1542.0}           &    7923.6795  &     3639.706     &     2.177  &         0.029        &      789.302    &     1.51e+04     \\\\\n",
       "\\textbf{15459.0}          &    1006.0868  &     3431.298     &     0.293  &         0.769        &    -5719.779    &     7731.953     \\\\\n",
       "\\textbf{1554.0}           &    1.105e+04  &     4161.285     &     2.655  &         0.008        &     2891.119    &     1.92e+04     \\\\\n",
       "\\textbf{15708.0}          &    1345.3597  &     3415.184     &     0.394  &         0.694        &    -5348.921    &     8039.640     \\\\\n",
       "\\textbf{15711.0}          &    6104.6576  &     3935.001     &     1.551  &         0.121        &    -1608.543    &     1.38e+04     \\\\\n",
       "\\textbf{15761.0}          &    9451.7981  &     4802.983     &     1.968  &         0.049        &       37.220    &     1.89e+04     \\\\\n",
       "\\textbf{1581.0}           &   -2.976e+04  &     4038.352     &    -7.369  &         0.000        &    -3.77e+04    &    -2.18e+04     \\\\\n",
       "\\textbf{1593.0}           &    5515.0972  &     3304.175     &     1.669  &         0.095        &     -961.588    &      1.2e+04     \\\\\n",
       "\\textbf{1602.0}           &    1.395e+04  &     3396.118     &     4.109  &         0.000        &     7296.484    &     2.06e+04     \\\\\n",
       "\\textbf{1613.0}           &    1.101e+04  &     4186.090     &     2.629  &         0.009        &     2799.971    &     1.92e+04     \\\\\n",
       "\\textbf{16188.0}          &    8669.1429  &     4170.880     &     2.078  &         0.038        &      493.583    &     1.68e+04     \\\\\n",
       "\\textbf{1632.0}           &    4256.0786  &     3098.060     &     1.374  &         0.170        &    -1816.591    &     1.03e+04     \\\\\n",
       "\\textbf{1633.0}           &    8908.1974  &     3817.027     &     2.334  &         0.020        &     1426.244    &     1.64e+04     \\\\\n",
       "\\textbf{1635.0}           &    7450.2837  &     3526.786     &     2.112  &         0.035        &      537.246    &     1.44e+04     \\\\\n",
       "\\textbf{16401.0}          &   -4061.5505  &     3576.531     &    -1.136  &         0.256        &    -1.11e+04    &     2948.994     \\\\\n",
       "\\textbf{16437.0}          &    3430.9607  &     4052.273     &     0.847  &         0.397        &    -4512.110    &     1.14e+04     \\\\\n",
       "\\textbf{1651.0}           &    7112.8693  &     3456.619     &     2.058  &         0.040        &      337.371    &     1.39e+04     \\\\\n",
       "\\textbf{1655.0}           &    9935.4841  &     3981.206     &     2.496  &         0.013        &     2131.715    &     1.77e+04     \\\\\n",
       "\\textbf{1663.0}           &    1.363e+04  &     3679.861     &     3.704  &         0.000        &     6415.407    &     2.08e+04     \\\\\n",
       "\\textbf{16710.0}          &    5871.3980  &     3911.464     &     1.501  &         0.133        &    -1795.666    &     1.35e+04     \\\\\n",
       "\\textbf{16729.0}          &    6628.1263  &     3941.492     &     1.682  &         0.093        &    -1097.799    &     1.44e+04     \\\\\n",
       "\\textbf{1690.0}           &   -5003.5820  &     3105.268     &    -1.611  &         0.107        &    -1.11e+04    &     1083.216     \\\\\n",
       "\\textbf{1703.0}           &    6095.9361  &     3462.476     &     1.761  &         0.078        &     -691.044    &     1.29e+04     \\\\\n",
       "\\textbf{17101.0}          &    4955.5819  &        1e+04     &     0.495  &         0.621        &    -1.47e+04    &     2.46e+04     \\\\\n",
       "\\textbf{17202.0}          &    1.037e+04  &     4459.996     &     2.325  &         0.020        &     1629.279    &     1.91e+04     \\\\\n",
       "\\textbf{1722.0}           &    7914.1858  &     3882.992     &     2.038  &         0.042        &      302.930    &     1.55e+04     \\\\\n",
       "\\textbf{1728.0}           &    1.051e+04  &     4109.935     &     2.558  &         0.011        &     2457.721    &     1.86e+04     \\\\\n",
       "\\textbf{1743.0}           &    1.061e+04  &     5016.063     &     2.116  &         0.034        &      782.085    &     2.04e+04     \\\\\n",
       "\\textbf{1754.0}           &    9871.2919  &     4014.380     &     2.459  &         0.014        &     2002.497    &     1.77e+04     \\\\\n",
       "\\textbf{1762.0}           &   -1360.6791  &     2978.263     &    -0.457  &         0.648        &    -7198.529    &     4477.170     \\\\\n",
       "\\textbf{1773.0}           &    1.168e+04  &     4569.107     &     2.556  &         0.011        &     2720.433    &     2.06e+04     \\\\\n",
       "\\textbf{1786.0}           &   -1787.4250  &     3044.853     &    -0.587  &         0.557        &    -7755.799    &     4180.950     \\\\\n",
       "\\textbf{18100.0}          &    9369.5941  &     4277.263     &     2.191  &         0.029        &      985.507    &     1.78e+04     \\\\\n",
       "\\textbf{1820.0}           &    1.025e+04  &     4017.719     &     2.552  &         0.011        &     2376.254    &     1.81e+04     \\\\\n",
       "\\textbf{1848.0}           &    -174.7376  &     3641.414     &    -0.048  &         0.962        &    -7312.463    &     6962.988     \\\\\n",
       "\\textbf{18654.0}          &    9895.0330  &     5108.443     &     1.937  &         0.053        &     -118.292    &     1.99e+04     \\\\\n",
       "\\textbf{1875.0}           &    8101.0355  &     4958.513     &     1.634  &         0.102        &    -1618.405    &     1.78e+04     \\\\\n",
       "\\textbf{1884.0}           &    1.173e+04  &     4391.837     &     2.672  &         0.008        &     3124.748    &     2.03e+04     \\\\\n",
       "\\textbf{1913.0}           &    1494.6254  &     2975.110     &     0.502  &         0.615        &    -4337.042    &     7326.293     \\\\\n",
       "\\textbf{1919.0}           &    8651.6988  &     4021.054     &     2.152  &         0.031        &      769.821    &     1.65e+04     \\\\\n",
       "\\textbf{1920.0}           &    4718.3260  &     3144.201     &     1.501  &         0.133        &    -1444.787    &     1.09e+04     \\\\\n",
       "\\textbf{1968.0}           &    5476.8899  &     3274.829     &     1.672  &         0.094        &     -942.272    &     1.19e+04     \\\\\n",
       "\\textbf{1976.0}           &    1.191e+04  &     4073.536     &     2.923  &         0.003        &     3923.878    &     1.99e+04     \\\\\n",
       "\\textbf{1981.0}           &    1.044e+04  &     4077.941     &     2.559  &         0.011        &     2441.799    &     1.84e+04     \\\\\n",
       "\\textbf{1988.0}           &   -5069.7376  &     3891.247     &    -1.303  &         0.193        &    -1.27e+04    &     2557.699     \\\\\n",
       "\\textbf{1992.0}           &    5039.6675  &     3216.455     &     1.567  &         0.117        &    -1265.073    &     1.13e+04     \\\\\n",
       "\\textbf{2008.0}           &    7226.1408  &     3451.123     &     2.094  &         0.036        &      461.414    &      1.4e+04     \\\\\n",
       "\\textbf{2033.0}           &    9390.2572  &     4347.121     &     2.160  &         0.031        &      869.239    &     1.79e+04     \\\\\n",
       "\\textbf{2044.0}           &    8829.3117  &     3722.148     &     2.372  &         0.018        &     1533.335    &     1.61e+04     \\\\\n",
       "\\textbf{2049.0}           &    7251.7676  &     3540.296     &     2.048  &         0.041        &      312.249    &     1.42e+04     \\\\\n",
       "\\textbf{2061.0}           &    1.251e+04  &     4483.089     &     2.790  &         0.005        &     3718.981    &     2.13e+04     \\\\\n",
       "\\textbf{20779.0}          &    5.344e+04  &     3925.737     &    13.614  &         0.000        &     4.57e+04    &     6.11e+04     \\\\\n",
       "\\textbf{2085.0}           &    4409.4735  &     3154.407     &     1.398  &         0.162        &    -1773.645    &     1.06e+04     \\\\\n",
       "\\textbf{2086.0}           &    5974.6876  &     3423.325     &     1.745  &         0.081        &     -735.550    &     1.27e+04     \\\\\n",
       "\\textbf{2111.0}           &    7052.1036  &     3445.832     &     2.047  &         0.041        &      297.749    &     1.38e+04     \\\\\n",
       "\\textbf{21204.0}          &    1.256e+04  &     4928.367     &     2.548  &         0.011        &     2898.240    &     2.22e+04     \\\\\n",
       "\\textbf{21238.0}          &    1.239e+04  &     4811.460     &     2.575  &         0.010        &     2959.156    &     2.18e+04     \\\\\n",
       "\\textbf{2124.0}           &    5697.0681  &     3402.162     &     1.675  &         0.094        &     -971.688    &     1.24e+04     \\\\\n",
       "\\textbf{2146.0}           &    2.541e+04  &     4992.641     &     5.089  &         0.000        &     1.56e+04    &     3.52e+04     \\\\\n",
       "\\textbf{21496.0}          &    1455.5099  &     3547.770     &     0.410  &         0.682        &    -5498.658    &     8409.678     \\\\\n",
       "\\textbf{2154.0}           &    9090.5296  &     3869.063     &     2.350  &         0.019        &     1506.577    &     1.67e+04     \\\\\n",
       "\\textbf{2176.0}           &    5.038e+04  &     4320.920     &    11.660  &         0.000        &     4.19e+04    &     5.89e+04     \\\\\n",
       "\\textbf{2188.0}           &    1.238e+04  &     4515.888     &     2.740  &         0.006        &     3523.399    &     2.12e+04     \\\\\n",
       "\\textbf{2189.0}           &    -562.1249  &     3162.012     &    -0.178  &         0.859        &    -6760.150    &     5635.900     \\\\\n",
       "\\textbf{2220.0}           &    7170.2235  &     3562.804     &     2.013  &         0.044        &      186.585    &     1.42e+04     \\\\\n",
       "\\textbf{22205.0}          &     1.25e+04  &     4887.973     &     2.557  &         0.011        &     2915.532    &     2.21e+04     \\\\\n",
       "\\textbf{2226.0}           &    6694.5745  &     6256.551     &     1.070  &         0.285        &    -5569.217    &      1.9e+04     \\\\\n",
       "\\textbf{2230.0}           &    1.276e+04  &     4478.745     &     2.850  &         0.004        &     3983.822    &     2.15e+04     \\\\\n",
       "\\textbf{22325.0}          &    9261.0607  &     3787.917     &     2.445  &         0.015        &     1836.167    &     1.67e+04     \\\\\n",
       "\\textbf{2255.0}           &    6563.5332  &     3644.695     &     1.801  &         0.072        &     -580.623    &     1.37e+04     \\\\\n",
       "\\textbf{22619.0}          &    9490.2470  &     4525.918     &     2.097  &         0.036        &      618.759    &     1.84e+04     \\\\\n",
       "\\textbf{2267.0}           &    4985.7416  &     3253.204     &     1.533  &         0.125        &    -1391.034    &     1.14e+04     \\\\\n",
       "\\textbf{22815.0}          &    6901.8595  &     3991.510     &     1.729  &         0.084        &     -922.108    &     1.47e+04     \\\\\n",
       "\\textbf{2285.0}           &   -2.214e+04  &     3272.038     &    -6.766  &         0.000        &    -2.86e+04    &    -1.57e+04     \\\\\n",
       "\\textbf{2290.0}           &    3654.5615  &     3530.734     &     1.035  &         0.301        &    -3266.215    &     1.06e+04     \\\\\n",
       "\\textbf{2295.0}           &      1.3e+04  &     5600.714     &     2.322  &         0.020        &     2026.157    &      2.4e+04     \\\\\n",
       "\\textbf{2316.0}           &    3158.2020  &     3401.547     &     0.928  &         0.353        &    -3509.348    &     9825.752     \\\\\n",
       "\\textbf{23220.0}          &    6844.3205  &     4048.020     &     1.691  &         0.091        &    -1090.414    &     1.48e+04     \\\\\n",
       "\\textbf{23224.0}          &    1.102e+04  &     4772.809     &     2.308  &         0.021        &     1662.511    &     2.04e+04     \\\\\n",
       "\\textbf{2343.0}           &    8651.7177  &     5862.373     &     1.476  &         0.140        &    -2839.425    &     2.01e+04     \\\\\n",
       "\\textbf{2352.0}           &    3701.1087  &     3232.290     &     1.145  &         0.252        &    -2634.671    &        1e+04     \\\\\n",
       "\\textbf{23700.0}          &   -2730.5076  &     4771.481     &    -0.572  &         0.567        &    -1.21e+04    &     6622.322     \\\\\n",
       "\\textbf{2390.0}           &    1.128e+04  &     4210.584     &     2.680  &         0.007        &     3029.690    &     1.95e+04     \\\\\n",
       "\\textbf{2393.0}           &     927.7677  &     2948.534     &     0.315  &         0.753        &    -4851.808    &     6707.343     \\\\\n",
       "\\textbf{2403.0}           &    2.172e+04  &     3176.339     &     6.839  &         0.000        &     1.55e+04    &     2.79e+04     \\\\\n",
       "\\textbf{2435.0}           &    1.411e+04  &     4527.637     &     3.116  &         0.002        &     5235.229    &      2.3e+04     \\\\\n",
       "\\textbf{2444.0}           &    2387.4015  &     3090.909     &     0.772  &         0.440        &    -3671.251    &     8446.054     \\\\\n",
       "\\textbf{2448.0}           &    7993.4498  &     3623.624     &     2.206  &         0.027        &      890.595    &     1.51e+04     \\\\\n",
       "\\textbf{2469.0}           &    1.061e+04  &     5183.112     &     2.046  &         0.041        &      445.737    &     2.08e+04     \\\\\n",
       "\\textbf{24720.0}          &    7113.8480  &     4153.805     &     1.713  &         0.087        &    -1028.242    &     1.53e+04     \\\\\n",
       "\\textbf{24800.0}          &     1.15e+04  &     3897.050     &     2.951  &         0.003        &     3860.020    &     1.91e+04     \\\\\n",
       "\\textbf{2482.0}           &    1.153e+04  &     4261.214     &     2.705  &         0.007        &     3173.514    &     1.99e+04     \\\\\n",
       "\\textbf{24969.0}          &     1.11e+04  &     5242.485     &     2.117  &         0.034        &      821.772    &     2.14e+04     \\\\\n",
       "\\textbf{2498.0}           &    5950.1850  &     3496.509     &     1.702  &         0.089        &     -903.504    &     1.28e+04     \\\\\n",
       "\\textbf{2504.0}           &   -1.316e+04  &     3179.409     &    -4.140  &         0.000        &    -1.94e+04    &    -6930.258     \\\\\n",
       "\\textbf{2508.0}           &    9444.1082  &     4064.529     &     2.324  &         0.020        &     1477.012    &     1.74e+04     \\\\\n",
       "\\textbf{25124.0}          &    8152.1151  &     4336.426     &     1.880  &         0.060        &     -347.939    &     1.67e+04     \\\\\n",
       "\\textbf{2518.0}           &    9780.3988  &     3987.017     &     2.453  &         0.014        &     1965.239    &     1.76e+04     \\\\\n",
       "\\textbf{25224.0}          &    7547.9150  &     7367.374     &     1.025  &         0.306        &    -6893.258    &      2.2e+04     \\\\\n",
       "\\textbf{25279.0}          &    1.073e+04  &     4396.741     &     2.441  &         0.015        &     2115.387    &     1.94e+04     \\\\\n",
       "\\textbf{2537.0}           &    6190.5805  &     3390.712     &     1.826  &         0.068        &     -455.730    &     1.28e+04     \\\\\n",
       "\\textbf{2538.0}           &    1.165e+04  &     5117.482     &     2.276  &         0.023        &     1614.429    &     2.17e+04     \\\\\n",
       "\\textbf{25389.0}          &    1.075e+04  &     6659.968     &     1.614  &         0.107        &    -2304.315    &     2.38e+04     \\\\\n",
       "\\textbf{2547.0}           &     -72.1448  &     3461.233     &    -0.021  &         0.983        &    -6856.688    &     6712.398     \\\\\n",
       "\\textbf{2553.0}           &     1.08e+04  &     4155.736     &     2.600  &         0.009        &     2657.280    &     1.89e+04     \\\\\n",
       "\\textbf{2574.0}           &    3872.3138  &     4394.763     &     0.881  &         0.378        &    -4742.090    &     1.25e+04     \\\\\n",
       "\\textbf{25747.0}          &    8533.8385  &     4417.126     &     1.932  &         0.053        &     -124.400    &     1.72e+04     \\\\\n",
       "\\textbf{2577.0}           &    5974.7236  &     3339.669     &     1.789  &         0.074        &     -571.536    &     1.25e+04     \\\\\n",
       "\\textbf{2593.0}           &    6166.0952  &     3437.442     &     1.794  &         0.073        &     -571.814    &     1.29e+04     \\\\\n",
       "\\textbf{2596.0}           &    1.235e+04  &     4438.456     &     2.782  &         0.005        &     3646.159    &      2.1e+04     \\\\\n",
       "\\textbf{2663.0}           &    1.216e+04  &     3876.313     &     3.138  &         0.002        &     4564.624    &     1.98e+04     \\\\\n",
       "\\textbf{2771.0}           &    1.193e+04  &     4407.974     &     2.706  &         0.007        &     3287.864    &     2.06e+04     \\\\\n",
       "\\textbf{2787.0}           &    9469.7837  &     3959.582     &     2.392  &         0.017        &     1708.400    &     1.72e+04     \\\\\n",
       "\\textbf{2797.0}           &    1966.6171  &     3049.874     &     0.645  &         0.519        &    -4011.601    &     7944.835     \\\\\n",
       "\\textbf{2802.0}           &    1.127e+04  &     4221.976     &     2.670  &         0.008        &     2996.835    &     1.95e+04     \\\\\n",
       "\\textbf{2817.0}           &   -8930.8400  &     3046.906     &    -2.931  &         0.003        &    -1.49e+04    &    -2958.440     \\\\\n",
       "\\textbf{28678.0}          &   -1537.9801  &     3864.197     &    -0.398  &         0.691        &    -9112.395    &     6036.434     \\\\\n",
       "\\textbf{28701.0}          &    8387.4495  &     3874.922     &     2.165  &         0.030        &      792.013    &      1.6e+04     \\\\\n",
       "\\textbf{28742.0}          &    4282.1391  &     4229.061     &     1.013  &         0.311        &    -4007.463    &     1.26e+04     \\\\\n",
       "\\textbf{2888.0}           &    6760.4694  &     3567.651     &     1.895  &         0.058        &     -232.670    &     1.38e+04     \\\\\n",
       "\\textbf{2897.0}           &    9862.4574  &     4460.678     &     2.211  &         0.027        &     1118.850    &     1.86e+04     \\\\\n",
       "\\textbf{2917.0}           &    4638.9279  &     3766.582     &     1.232  &         0.218        &    -2744.147    &      1.2e+04     \\\\\n",
       "\\textbf{29392.0}          &   -6393.4066  &     3894.751     &    -1.642  &         0.101        &     -1.4e+04    &     1240.897     \\\\\n",
       "\\textbf{2950.0}           &    7545.1302  &     4190.581     &     1.800  &         0.072        &     -669.046    &     1.58e+04     \\\\\n",
       "\\textbf{2951.0}           &     1.16e+04  &     4618.097     &     2.513  &         0.012        &     2552.014    &     2.07e+04     \\\\\n",
       "\\textbf{2953.0}           &    8985.2452  &     3782.157     &     2.376  &         0.018        &     1571.641    &     1.64e+04     \\\\\n",
       "\\textbf{2960.0}           &    5222.5971  &     4059.135     &     1.287  &         0.198        &    -2733.925    &     1.32e+04     \\\\\n",
       "\\textbf{2975.0}           &    6256.0595  &     3431.701     &     1.823  &         0.068        &     -470.597    &      1.3e+04     \\\\\n",
       "\\textbf{2982.0}           &    1.023e+04  &     4075.609     &     2.510  &         0.012        &     2239.386    &     1.82e+04     \\\\\n",
       "\\textbf{2991.0}           &     501.0336  &     3925.088     &     0.128  &         0.898        &    -7192.737    &     8194.804     \\\\\n",
       "\\textbf{3011.0}           &    7688.4059  &     3704.565     &     2.075  &         0.038        &      426.894    &     1.49e+04     \\\\\n",
       "\\textbf{3015.0}           &    1.268e+04  &     4380.448     &     2.894  &         0.004        &     4091.619    &     2.13e+04     \\\\\n",
       "\\textbf{3026.0}           &    9521.4095  &     3885.510     &     2.450  &         0.014        &     1905.218    &     1.71e+04     \\\\\n",
       "\\textbf{3031.0}           &    3433.1702  &     3896.852     &     0.881  &         0.378        &    -4205.253    &     1.11e+04     \\\\\n",
       "\\textbf{3062.0}           &    8372.4668  &     3560.764     &     2.351  &         0.019        &     1392.827    &     1.54e+04     \\\\\n",
       "\\textbf{3093.0}           &    9456.1844  &     4013.928     &     2.356  &         0.018        &     1588.274    &     1.73e+04     \\\\\n",
       "\\textbf{3107.0}           &    6693.9808  &     4904.836     &     1.365  &         0.172        &    -2920.245    &     1.63e+04     \\\\\n",
       "\\textbf{3121.0}           &    1.116e+04  &     3895.822     &     2.866  &         0.004        &     3527.714    &     1.88e+04     \\\\\n",
       "\\textbf{3126.0}           &    7440.8391  &     3566.309     &     2.086  &         0.037        &      450.331    &     1.44e+04     \\\\\n",
       "\\textbf{3144.0}           &     6.25e+04  &     4012.827     &    15.576  &         0.000        &     5.46e+04    &     7.04e+04     \\\\\n",
       "\\textbf{3156.0}           &    1.129e+04  &     4493.279     &     2.513  &         0.012        &     2482.732    &     2.01e+04     \\\\\n",
       "\\textbf{3157.0}           &    8993.0023  &     3828.880     &     2.349  &         0.019        &     1487.815    &     1.65e+04     \\\\\n",
       "\\textbf{3170.0}           &    1.304e+04  &     3799.293     &     3.433  &         0.001        &     5597.506    &     2.05e+04     \\\\\n",
       "\\textbf{3178.0}           &    1.137e+04  &     4347.953     &     2.616  &         0.009        &     2851.078    &     1.99e+04     \\\\\n",
       "\\textbf{3206.0}           &    4493.9310  &     3713.576     &     1.210  &         0.226        &    -2785.242    &     1.18e+04     \\\\\n",
       "\\textbf{3229.0}           &    1.114e+04  &     4280.634     &     2.602  &         0.009        &     2748.862    &     1.95e+04     \\\\\n",
       "\\textbf{3235.0}           &    7123.2217  &     3595.742     &     1.981  &         0.048        &       75.020    &     1.42e+04     \\\\\n",
       "\\textbf{3246.0}           &    1.031e+04  &     4194.329     &     2.458  &         0.014        &     2087.307    &     1.85e+04     \\\\\n",
       "\\textbf{3248.0}           &    8084.0530  &     3694.773     &     2.188  &         0.029        &      841.736    &     1.53e+04     \\\\\n",
       "\\textbf{3282.0}           &    -1.52e+04  &     3154.401     &    -4.819  &         0.000        &    -2.14e+04    &    -9018.369     \\\\\n",
       "\\textbf{3362.0}           &    2024.3697  &     4165.120     &     0.486  &         0.627        &    -6139.899    &     1.02e+04     \\\\\n",
       "\\textbf{3372.0}           &    9456.8896  &     4228.253     &     2.237  &         0.025        &     1168.871    &     1.77e+04     \\\\\n",
       "\\textbf{3422.0}           &    7340.0009  &     3512.525     &     2.090  &         0.037        &      454.918    &     1.42e+04     \\\\\n",
       "\\textbf{3497.0}           &    1221.2387  &     3007.116     &     0.406  &         0.685        &    -4673.166    &     7115.643     \\\\\n",
       "\\textbf{3502.0}           &    6705.2576  &     3499.979     &     1.916  &         0.055        &     -155.233    &     1.36e+04     \\\\\n",
       "\\textbf{3504.0}           &    5886.5006  &     3980.714     &     1.479  &         0.139        &    -1916.304    &     1.37e+04     \\\\\n",
       "\\textbf{3505.0}           &    3746.5464  &     3176.698     &     1.179  &         0.238        &    -2480.265    &     9973.358     \\\\\n",
       "\\textbf{3532.0}           &    9082.7373  &     3451.767     &     2.631  &         0.009        &     2316.749    &     1.58e+04     \\\\\n",
       "\\textbf{3574.0}           &    1.135e+04  &     5613.448     &     2.022  &         0.043        &      347.319    &     2.24e+04     \\\\\n",
       "\\textbf{3580.0}           &    5970.7232  &     3441.810     &     1.735  &         0.083        &     -775.749    &     1.27e+04     \\\\\n",
       "\\textbf{3612.0}           &     1.31e+04  &     4548.023     &     2.880  &         0.004        &     4181.503    &      2.2e+04     \\\\\n",
       "\\textbf{3619.0}           &    8659.8696  &     4076.577     &     2.124  &         0.034        &      669.159    &     1.67e+04     \\\\\n",
       "\\textbf{3622.0}           &    1.137e+04  &     4311.284     &     2.637  &         0.008        &     2917.477    &     1.98e+04     \\\\\n",
       "\\textbf{3639.0}           &    3712.0637  &     3103.375     &     1.196  &         0.232        &    -2371.024    &     9795.151     \\\\\n",
       "\\textbf{3650.0}           &   -4218.2473  &     2945.040     &    -1.432  &         0.152        &    -9990.974    &     1554.479     \\\\\n",
       "\\textbf{3662.0}           &    2096.9656  &     2973.505     &     0.705  &         0.481        &    -3731.557    &     7925.488     \\\\\n",
       "\\textbf{3734.0}           &    -618.7346  &     3131.546     &    -0.198  &         0.843        &    -6757.041    &     5519.571     \\\\\n",
       "\\textbf{3735.0}           &    1714.5441  &     3273.573     &     0.524  &         0.600        &    -4702.157    &     8131.245     \\\\\n",
       "\\textbf{3761.0}           &    2948.2889  &     3076.541     &     0.958  &         0.338        &    -3082.201    &     8978.778     \\\\\n",
       "\\textbf{3779.0}           &    8398.0005  &     3769.925     &     2.228  &         0.026        &     1008.373    &     1.58e+04     \\\\\n",
       "\\textbf{3781.0}           &    9848.7736  &     4341.478     &     2.269  &         0.023        &     1338.816    &     1.84e+04     \\\\\n",
       "\\textbf{3782.0}           &    2652.2226  &     3142.421     &     0.844  &         0.399        &    -3507.401    &     8811.846     \\\\\n",
       "\\textbf{3786.0}           &    9975.5723  &     3988.454     &     2.501  &         0.012        &     2157.595    &     1.78e+04     \\\\\n",
       "\\textbf{3796.0}           &    8412.7977  &     3868.706     &     2.175  &         0.030        &      829.545    &      1.6e+04     \\\\\n",
       "\\textbf{3821.0}           &    1.076e+04  &     4284.698     &     2.510  &         0.012        &     2356.627    &     1.92e+04     \\\\\n",
       "\\textbf{3835.0}           &     325.3667  &     3469.095     &     0.094  &         0.925        &    -6474.587    &     7125.321     \\\\\n",
       "\\textbf{3839.0}           &    1627.1748  &     3742.626     &     0.435  &         0.664        &    -5708.942    &     8963.291     \\\\\n",
       "\\textbf{3840.0}           &    9349.7452  &     3871.757     &     2.415  &         0.016        &     1760.512    &     1.69e+04     \\\\\n",
       "\\textbf{3895.0}           &    1.011e+04  &     4002.931     &     2.526  &         0.012        &     2266.773    &      1.8e+04     \\\\\n",
       "\\textbf{3908.0}           &    1.179e+04  &     5017.974     &     2.350  &         0.019        &     1956.045    &     2.16e+04     \\\\\n",
       "\\textbf{3911.0}           &    1074.0551  &     3033.956     &     0.354  &         0.723        &    -4872.960    &     7021.070     \\\\\n",
       "\\textbf{3917.0}           &    1.084e+04  &     4187.474     &     2.589  &         0.010        &     2634.498    &     1.91e+04     \\\\\n",
       "\\textbf{3946.0}           &    1.182e+04  &     4349.230     &     2.717  &         0.007        &     3293.217    &     2.03e+04     \\\\\n",
       "\\textbf{3971.0}           &    8785.2598  &     3826.746     &     2.296  &         0.022        &     1284.255    &     1.63e+04     \\\\\n",
       "\\textbf{3980.0}           &    2.001e+04  &     3707.464     &     5.396  &         0.000        &     1.27e+04    &     2.73e+04     \\\\\n",
       "\\textbf{4034.0}           &    3089.9753  &     3148.117     &     0.982  &         0.326        &    -3080.813    &     9260.764     \\\\\n",
       "\\textbf{4036.0}           &    1.078e+04  &     4106.787     &     2.626  &         0.009        &     2734.110    &     1.88e+04     \\\\\n",
       "\\textbf{4040.0}           &    9522.3439  &     3858.347     &     2.468  &         0.014        &     1959.397    &     1.71e+04     \\\\\n",
       "\\textbf{4058.0}           &    9224.0234  &     3774.366     &     2.444  &         0.015        &     1825.692    &     1.66e+04     \\\\\n",
       "\\textbf{4060.0}           &   -1.239e+04  &     3046.591     &    -4.068  &         0.000        &    -1.84e+04    &    -6422.563     \\\\\n",
       "\\textbf{4062.0}           &    1.461e+04  &     4513.229     &     3.238  &         0.001        &     5765.432    &     2.35e+04     \\\\\n",
       "\\textbf{4077.0}           &    1.029e+04  &     4923.332     &     2.090  &         0.037        &      639.909    &     1.99e+04     \\\\\n",
       "\\textbf{4087.0}           &   -2.318e+04  &     3393.341     &    -6.830  &         0.000        &    -2.98e+04    &    -1.65e+04     \\\\\n",
       "\\textbf{4091.0}           &    8767.6889  &     4182.215     &     2.096  &         0.036        &      569.911    &      1.7e+04     \\\\\n",
       "\\textbf{4127.0}           &    5139.3960  &     3231.497     &     1.590  &         0.112        &    -1194.830    &     1.15e+04     \\\\\n",
       "\\textbf{4138.0}           &    1.158e+04  &     4678.133     &     2.475  &         0.013        &     2406.218    &     2.07e+04     \\\\\n",
       "\\textbf{4162.0}           &    9400.3071  &     4301.461     &     2.185  &         0.029        &      968.789    &     1.78e+04     \\\\\n",
       "\\textbf{4186.0}           &     1.14e+04  &     4217.295     &     2.702  &         0.007        &     3130.468    &     1.97e+04     \\\\\n",
       "\\textbf{4194.0}           &   -1943.8093  &     3510.323     &    -0.554  &         0.580        &    -8824.576    &     4936.957     \\\\\n",
       "\\textbf{4199.0}           &   -5041.3250  &     3091.922     &    -1.630  &         0.103        &    -1.11e+04    &     1019.313     \\\\\n",
       "\\textbf{4213.0}           &    8197.8902  &     3566.600     &     2.299  &         0.022        &     1206.811    &     1.52e+04     \\\\\n",
       "\\textbf{4222.0}           &    5080.5991  &     3227.534     &     1.574  &         0.115        &    -1245.859    &     1.14e+04     \\\\\n",
       "\\textbf{4223.0}           &    8439.7735  &     3678.170     &     2.295  &         0.022        &     1230.000    &     1.56e+04     \\\\\n",
       "\\textbf{4251.0}           &    1.088e+04  &     4135.671     &     2.630  &         0.009        &     2769.630    &      1.9e+04     \\\\\n",
       "\\textbf{4265.0}           &    1.008e+04  &     4108.597     &     2.453  &         0.014        &     2024.501    &     1.81e+04     \\\\\n",
       "\\textbf{4274.0}           &    5602.3767  &     3433.578     &     1.632  &         0.103        &    -1127.959    &     1.23e+04     \\\\\n",
       "\\textbf{4321.0}           &    2518.2649  &     3032.594     &     0.830  &         0.406        &    -3426.080    &     8462.610     \\\\\n",
       "\\textbf{4335.0}           &    6984.8365  &     4954.407     &     1.410  &         0.159        &    -2726.555    &     1.67e+04     \\\\\n",
       "\\textbf{4340.0}           &    6389.0278  &     3424.452     &     1.866  &         0.062        &     -323.419    &     1.31e+04     \\\\\n",
       "\\textbf{4371.0}           &    4744.8951  &     3344.844     &     1.419  &         0.156        &    -1811.509    &     1.13e+04     \\\\\n",
       "\\textbf{4415.0}           &    1.048e+04  &     4162.124     &     2.519  &         0.012        &     2326.029    &     1.86e+04     \\\\\n",
       "\\textbf{4450.0}           &    1.042e+04  &     4042.227     &     2.579  &         0.010        &     2501.120    &     1.83e+04     \\\\\n",
       "\\textbf{4476.0}           &    3409.8999  &     3264.845     &     1.044  &         0.296        &    -2989.694    &     9809.493     \\\\\n",
       "\\textbf{4510.0}           &   -1483.8606  &     2986.044     &    -0.497  &         0.619        &    -7336.961    &     4369.240     \\\\\n",
       "\\textbf{4520.0}           &    5466.5214  &     3271.905     &     1.671  &         0.095        &     -946.911    &     1.19e+04     \\\\\n",
       "\\textbf{4551.0}           &    9684.9935  &     7598.552     &     1.275  &         0.202        &    -5209.325    &     2.46e+04     \\\\\n",
       "\\textbf{4568.0}           &    7745.7072  &     3724.777     &     2.080  &         0.038        &      444.577    &      1.5e+04     \\\\\n",
       "\\textbf{4579.0}           &    1.293e+04  &     4538.177     &     2.850  &         0.004        &     4036.981    &     2.18e+04     \\\\\n",
       "\\textbf{4585.0}           &    1.123e+04  &     4250.879     &     2.642  &         0.008        &     2899.048    &     1.96e+04     \\\\\n",
       "\\textbf{4595.0}           &    7537.0673  &     3563.928     &     2.115  &         0.034        &      551.227    &     1.45e+04     \\\\\n",
       "\\textbf{4600.0}           &    6442.9841  &     3535.061     &     1.823  &         0.068        &     -486.273    &     1.34e+04     \\\\\n",
       "\\textbf{4607.0}           &    1.098e+04  &     4160.760     &     2.639  &         0.008        &     2823.984    &     1.91e+04     \\\\\n",
       "\\textbf{4608.0}           &    8172.4690  &     3636.861     &     2.247  &         0.025        &     1043.669    &     1.53e+04     \\\\\n",
       "\\textbf{4622.0}           &    1427.8834  &     2973.671     &     0.480  &         0.631        &    -4400.964    &     7256.731     \\\\\n",
       "\\textbf{4623.0}           &    1.065e+04  &     4180.459     &     2.547  &         0.011        &     2453.751    &     1.88e+04     \\\\\n",
       "\\textbf{4768.0}           &    7792.0053  &     3781.774     &     2.060  &         0.039        &      379.153    &     1.52e+04     \\\\\n",
       "\\textbf{4771.0}           &    1.153e+04  &     4273.534     &     2.697  &         0.007        &     3149.114    &     1.99e+04     \\\\\n",
       "\\textbf{4800.0}           &    1.009e+04  &     4286.046     &     2.354  &         0.019        &     1689.827    &     1.85e+04     \\\\\n",
       "\\textbf{4802.0}           &     1.02e+04  &     4013.159     &     2.541  &         0.011        &     2331.904    &     1.81e+04     \\\\\n",
       "\\textbf{4807.0}           &    9213.6890  &     3923.273     &     2.348  &         0.019        &     1523.476    &     1.69e+04     \\\\\n",
       "\\textbf{4839.0}           &   -1.455e+05  &     4319.284     &   -33.685  &         0.000        &    -1.54e+05    &    -1.37e+05     \\\\\n",
       "\\textbf{4843.0}           &    1.067e+04  &     3887.642     &     2.745  &         0.006        &     3051.533    &     1.83e+04     \\\\\n",
       "\\textbf{4881.0}           &    5934.0337  &     3330.016     &     1.782  &         0.075        &     -593.304    &     1.25e+04     \\\\\n",
       "\\textbf{4900.0}           &    1.034e+04  &     4039.223     &     2.559  &         0.011        &     2419.087    &     1.83e+04     \\\\\n",
       "\\textbf{4926.0}           &    4935.9631  &     3265.920     &     1.511  &         0.131        &    -1465.736    &     1.13e+04     \\\\\n",
       "\\textbf{4941.0}           &    7424.2795  &     3741.649     &     1.984  &         0.047        &       90.079    &     1.48e+04     \\\\\n",
       "\\textbf{4961.0}           &   -5213.7396  &     3833.242     &    -1.360  &         0.174        &    -1.27e+04    &     2299.997     \\\\\n",
       "\\textbf{4988.0}           &    1.739e+04  &     4299.529     &     4.045  &         0.000        &     8963.828    &     2.58e+04     \\\\\n",
       "\\textbf{4993.0}           &    1.283e+04  &     4538.407     &     2.828  &         0.005        &     3938.733    &     2.17e+04     \\\\\n",
       "\\textbf{5018.0}           &   -2996.3265  &     2963.717     &    -1.011  &         0.312        &    -8805.664    &     2813.011     \\\\\n",
       "\\textbf{5020.0}           &    1.124e+04  &     3584.056     &     3.137  &         0.002        &     4218.320    &     1.83e+04     \\\\\n",
       "\\textbf{5027.0}           &    5890.7880  &     3367.220     &     1.749  &         0.080        &     -709.475    &     1.25e+04     \\\\\n",
       "\\textbf{5032.0}           &    8682.1785  &     3795.147     &     2.288  &         0.022        &     1243.113    &     1.61e+04     \\\\\n",
       "\\textbf{5043.0}           &    6755.2178  &     3532.992     &     1.912  &         0.056        &     -169.984    &     1.37e+04     \\\\\n",
       "\\textbf{5046.0}           &   -6585.9934  &     2994.407     &    -2.199  &         0.028        &    -1.25e+04    &     -716.501     \\\\\n",
       "\\textbf{5047.0}           &    4.461e+04  &     4003.739     &    11.142  &         0.000        &     3.68e+04    &     5.25e+04     \\\\\n",
       "\\textbf{5065.0}           &    1.074e+04  &     4425.978     &     2.427  &         0.015        &     2067.109    &     1.94e+04     \\\\\n",
       "\\textbf{5071.0}           &    9434.0249  &     4306.851     &     2.190  &         0.029        &      991.941    &     1.79e+04     \\\\\n",
       "\\textbf{5073.0}           &   -2.062e+05  &     6352.355     &   -32.459  &         0.000        &    -2.19e+05    &    -1.94e+05     \\\\\n",
       "\\textbf{5087.0}           &    5704.1523  &     3525.290     &     1.618  &         0.106        &    -1205.953    &     1.26e+04     \\\\\n",
       "\\textbf{5109.0}           &    1.138e+04  &     4303.060     &     2.645  &         0.008        &     2944.822    &     1.98e+04     \\\\\n",
       "\\textbf{5116.0}           &    6984.6176  &     3536.143     &     1.975  &         0.048        &       53.239    &     1.39e+04     \\\\\n",
       "\\textbf{5122.0}           &    5068.8879  &     3304.120     &     1.534  &         0.125        &    -1407.690    &     1.15e+04     \\\\\n",
       "\\textbf{5134.0}           &      65.7087  &     3427.998     &     0.019  &         0.985        &    -6653.689    &     6785.107     \\\\\n",
       "\\textbf{5142.0}           &    7326.5923  &     4310.388     &     1.700  &         0.089        &    -1122.423    &     1.58e+04     \\\\\n",
       "\\textbf{5165.0}           &    1.177e+04  &     4553.903     &     2.584  &         0.010        &     2840.692    &     2.07e+04     \\\\\n",
       "\\textbf{5169.0}           &     2.07e+04  &     4033.456     &     5.132  &         0.000        &     1.28e+04    &     2.86e+04     \\\\\n",
       "\\textbf{5174.0}           &    8533.9796  &     3945.822     &     2.163  &         0.031        &      799.568    &     1.63e+04     \\\\\n",
       "\\textbf{5179.0}           &    1.093e+04  &     4116.086     &     2.656  &         0.008        &     2862.704    &      1.9e+04     \\\\\n",
       "\\textbf{5181.0}           &    1.125e+04  &     4306.955     &     2.613  &         0.009        &     2811.335    &     1.97e+04     \\\\\n",
       "\\textbf{5187.0}           &    1.166e+04  &     4521.494     &     2.579  &         0.010        &     2797.023    &     2.05e+04     \\\\\n",
       "\\textbf{5229.0}           &     -63.7561  &     2986.566     &    -0.021  &         0.983        &    -5917.879    &     5790.367     \\\\\n",
       "\\textbf{5234.0}           &   -7623.9666  &     3323.390     &    -2.294  &         0.022        &    -1.41e+04    &    -1109.617     \\\\\n",
       "\\textbf{5237.0}           &    9923.3867  &     3942.736     &     2.517  &         0.012        &     2195.024    &     1.77e+04     \\\\\n",
       "\\textbf{5252.0}           &    7558.9349  &     3558.107     &     2.124  &         0.034        &      584.503    &     1.45e+04     \\\\\n",
       "\\textbf{5254.0}           &    7539.1361  &     3570.188     &     2.112  &         0.035        &      541.024    &     1.45e+04     \\\\\n",
       "\\textbf{5306.0}           &    2084.0427  &     2954.633     &     0.705  &         0.481        &    -3707.487    &     7875.572     \\\\\n",
       "\\textbf{5338.0}           &    9445.9791  &     3859.542     &     2.447  &         0.014        &     1880.689    &      1.7e+04     \\\\\n",
       "\\textbf{5377.0}           &    1.154e+04  &     4349.504     &     2.653  &         0.008        &     3013.763    &     2.01e+04     \\\\\n",
       "\\textbf{5439.0}           &    8864.7143  &     4057.848     &     2.185  &         0.029        &      910.715    &     1.68e+04     \\\\\n",
       "\\textbf{5456.0}           &    1.274e+04  &     4551.099     &     2.800  &         0.005        &     3822.865    &     2.17e+04     \\\\\n",
       "\\textbf{5464.0}           &    7275.3968  &     4173.456     &     1.743  &         0.081        &     -905.213    &     1.55e+04     \\\\\n",
       "\\textbf{5476.0}           &    1.339e+04  &     4581.284     &     2.922  &         0.003        &     4407.281    &     2.24e+04     \\\\\n",
       "\\textbf{5492.0}           &   -4549.3336  &     3052.005     &    -1.491  &         0.136        &    -1.05e+04    &     1433.060     \\\\\n",
       "\\textbf{5496.0}           &    8244.9060  &     3727.831     &     2.212  &         0.027        &      937.791    &     1.56e+04     \\\\\n",
       "\\textbf{5505.0}           &    1.081e+04  &     4241.696     &     2.548  &         0.011        &     2494.547    &     1.91e+04     \\\\\n",
       "\\textbf{5518.0}           &    8692.2261  &     4307.097     &     2.018  &         0.044        &      249.661    &     1.71e+04     \\\\\n",
       "\\textbf{5520.0}           &    8145.9612  &     3692.089     &     2.206  &         0.027        &      908.905    &     1.54e+04     \\\\\n",
       "\\textbf{5545.0}           &     1.01e+04  &     4060.744     &     2.487  &         0.013        &     2138.140    &     1.81e+04     \\\\\n",
       "\\textbf{5568.0}           &    1.208e+04  &     3810.286     &     3.172  &         0.002        &     4615.917    &     1.96e+04     \\\\\n",
       "\\textbf{5569.0}           &     1.09e+04  &     4177.942     &     2.609  &         0.009        &     2708.894    &     1.91e+04     \\\\\n",
       "\\textbf{5578.0}           &    1.051e+04  &     4029.722     &     2.609  &         0.009        &     2612.725    &     1.84e+04     \\\\\n",
       "\\textbf{5581.0}           &     1.02e+04  &     3910.445     &     2.609  &         0.009        &     2536.038    &     1.79e+04     \\\\\n",
       "\\textbf{5589.0}           &     -78.8477  &     2944.356     &    -0.027  &         0.979        &    -5850.234    &     5692.538     \\\\\n",
       "\\textbf{5597.0}           &    1.083e+04  &     4249.786     &     2.549  &         0.011        &     2504.453    &     1.92e+04     \\\\\n",
       "\\textbf{5606.0}           &   -2.861e+04  &     3146.602     &    -9.092  &         0.000        &    -3.48e+04    &    -2.24e+04     \\\\\n",
       "\\textbf{5639.0}           &    1.236e+04  &     4328.856     &     2.856  &         0.004        &     3878.499    &     2.08e+04     \\\\\n",
       "\\textbf{5667.0}           &    1.291e+04  &     4602.692     &     2.805  &         0.005        &     3890.075    &     2.19e+04     \\\\\n",
       "\\textbf{5690.0}           &    1.004e+04  &     3981.493     &     2.522  &         0.012        &     2238.699    &     1.78e+04     \\\\\n",
       "\\textbf{5709.0}           &    9814.3704  &     4087.748     &     2.401  &         0.016        &     1801.762    &     1.78e+04     \\\\\n",
       "\\textbf{5726.0}           &    1.092e+04  &     4112.433     &     2.655  &         0.008        &     2856.723    &      1.9e+04     \\\\\n",
       "\\textbf{5764.0}           &    5926.0852  &     3214.035     &     1.844  &         0.065        &     -373.911    &     1.22e+04     \\\\\n",
       "\\textbf{5772.0}           &    8435.6372  &     3722.716     &     2.266  &         0.023        &     1138.547    &     1.57e+04     \\\\\n",
       "\\textbf{5860.0}           &   -1.639e+04  &     3104.532     &    -5.278  &         0.000        &    -2.25e+04    &    -1.03e+04     \\\\\n",
       "\\textbf{5878.0}           &    9767.1273  &     3351.289     &     2.914  &         0.004        &     3198.091    &     1.63e+04     \\\\\n",
       "\\textbf{5903.0}           &    8484.1600  &     3929.427     &     2.159  &         0.031        &      781.885    &     1.62e+04     \\\\\n",
       "\\textbf{5905.0}           &    5019.2603  &     3455.387     &     1.453  &         0.146        &    -1753.823    &     1.18e+04     \\\\\n",
       "\\textbf{5959.0}           &    1950.1034  &     3270.756     &     0.596  &         0.551        &    -4461.076    &     8361.283     \\\\\n",
       "\\textbf{6008.0}           &    2.543e+04  &     3008.929     &     8.450  &         0.000        &     1.95e+04    &     3.13e+04     \\\\\n",
       "\\textbf{6034.0}           &     835.4076  &     3061.073     &     0.273  &         0.785        &    -5164.761    &     6835.576     \\\\\n",
       "\\textbf{6035.0}           &    9294.2228  &     4264.637     &     2.179  &         0.029        &      934.885    &     1.77e+04     \\\\\n",
       "\\textbf{6036.0}           &    1789.1188  &     3089.325     &     0.579  &         0.563        &    -4266.429    &     7844.667     \\\\\n",
       "\\textbf{6039.0}           &    9810.8574  &     3952.369     &     2.482  &         0.013        &     2063.612    &     1.76e+04     \\\\\n",
       "\\textbf{6044.0}           &    1.186e+04  &     4578.145     &     2.590  &         0.010        &     2881.663    &     2.08e+04     \\\\\n",
       "\\textbf{6066.0}           &   -3.183e+04  &     4377.392     &    -7.272  &         0.000        &    -4.04e+04    &    -2.33e+04     \\\\\n",
       "\\textbf{6078.0}           &     1.12e+04  &     3846.718     &     2.911  &         0.004        &     3656.829    &     1.87e+04     \\\\\n",
       "\\textbf{6081.0}           &   -4720.5814  &     2959.879     &    -1.595  &         0.111        &    -1.05e+04    &     1081.232     \\\\\n",
       "\\textbf{60893.0}          &    9199.3006  &     5076.118     &     1.812  &         0.070        &     -750.663    &     1.91e+04     \\\\\n",
       "\\textbf{6097.0}           &    1.263e+04  &     4369.572     &     2.890  &         0.004        &     4064.557    &     2.12e+04     \\\\\n",
       "\\textbf{6102.0}           &    9052.2338  &     4019.358     &     2.252  &         0.024        &     1173.680    &     1.69e+04     \\\\\n",
       "\\textbf{6104.0}           &   -1187.3521  &     3482.045     &    -0.341  &         0.733        &    -8012.690    &     5637.986     \\\\\n",
       "\\textbf{6109.0}           &    3213.2067  &     3050.268     &     1.053  &         0.292        &    -2765.782    &     9192.195     \\\\\n",
       "\\textbf{6127.0}           &    2228.4004  &     3709.238     &     0.601  &         0.548        &    -5042.271    &     9499.072     \\\\\n",
       "\\textbf{61552.0}          &   -1975.3039  &     4309.096     &    -0.458  &         0.647        &    -1.04e+04    &     6471.180     \\\\\n",
       "\\textbf{6158.0}           &    6941.3184  &     3594.907     &     1.931  &         0.054        &     -105.246    &      1.4e+04     \\\\\n",
       "\\textbf{6171.0}           &    9725.1938  &     3892.892     &     2.498  &         0.012        &     2094.532    &     1.74e+04     \\\\\n",
       "\\textbf{61780.0}          &    6319.1379  &     5239.208     &     1.206  &         0.228        &    -3950.508    &     1.66e+04     \\\\\n",
       "\\textbf{6207.0}           &    1.056e+04  &     4083.381     &     2.587  &         0.010        &     2558.543    &     1.86e+04     \\\\\n",
       "\\textbf{6214.0}           &    9359.9377  &     3855.444     &     2.428  &         0.015        &     1802.682    &     1.69e+04     \\\\\n",
       "\\textbf{6216.0}           &    1.232e+04  &     4532.307     &     2.719  &         0.007        &     3437.128    &     2.12e+04     \\\\\n",
       "\\textbf{62221.0}          &    9709.7805  &     5083.029     &     1.910  &         0.056        &     -253.730    &     1.97e+04     \\\\\n",
       "\\textbf{6259.0}           &    1.154e+04  &     4450.023     &     2.593  &         0.010        &     2818.012    &     2.03e+04     \\\\\n",
       "\\textbf{62599.0}          &   -2.896e+04  &     5015.920     &    -5.773  &         0.000        &    -3.88e+04    &    -1.91e+04     \\\\\n",
       "\\textbf{6266.0}           &     1.33e+04  &     3145.921     &     4.229  &         0.000        &     7137.443    &     1.95e+04     \\\\\n",
       "\\textbf{6268.0}           &   -1276.6700  &     3136.449     &    -0.407  &         0.684        &    -7424.587    &     4871.247     \\\\\n",
       "\\textbf{6288.0}           &    7204.7187  &     3492.523     &     2.063  &         0.039        &      358.843    &     1.41e+04     \\\\\n",
       "\\textbf{6297.0}           &    8422.0184  &     3780.383     &     2.228  &         0.026        &     1011.893    &     1.58e+04     \\\\\n",
       "\\textbf{6307.0}           &   -1.498e+04  &     3955.696     &    -3.788  &         0.000        &    -2.27e+04    &    -7231.134     \\\\\n",
       "\\textbf{6313.0}           &     1.04e+04  &     5133.926     &     2.025  &         0.043        &      332.897    &     2.05e+04     \\\\\n",
       "\\textbf{6314.0}           &    1.172e+04  &     4311.513     &     2.718  &         0.007        &     3269.269    &     2.02e+04     \\\\\n",
       "\\textbf{6326.0}           &    1065.9030  &     2959.552     &     0.360  &         0.719        &    -4735.269    &     6867.075     \\\\\n",
       "\\textbf{6349.0}           &    9045.9152  &     3821.348     &     2.367  &         0.018        &     1555.492    &     1.65e+04     \\\\\n",
       "\\textbf{6357.0}           &    1.133e+04  &     4303.700     &     2.632  &         0.009        &     2889.397    &     1.98e+04     \\\\\n",
       "\\textbf{6375.0}           &    1.517e+04  &     4101.820     &     3.699  &         0.000        &     7133.811    &     2.32e+04     \\\\\n",
       "\\textbf{6376.0}           &    1.082e+04  &     4258.242     &     2.541  &         0.011        &     2474.895    &     1.92e+04     \\\\\n",
       "\\textbf{6379.0}           &     942.3993  &     6905.998     &     0.136  &         0.891        &    -1.26e+04    &     1.45e+04     \\\\\n",
       "\\textbf{6386.0}           &    9756.4149  &     3934.848     &     2.479  &         0.013        &     2043.513    &     1.75e+04     \\\\\n",
       "\\textbf{6403.0}           &    7896.0643  &     3900.575     &     2.024  &         0.043        &      250.343    &     1.55e+04     \\\\\n",
       "\\textbf{6410.0}           &    1.246e+04  &     4473.422     &     2.786  &         0.005        &     3695.738    &     2.12e+04     \\\\\n",
       "\\textbf{6416.0}           &    8278.6855  &     3814.267     &     2.170  &         0.030        &      802.142    &     1.58e+04     \\\\\n",
       "\\textbf{6424.0}           &     1.07e+04  &     4143.301     &     2.581  &         0.010        &     2574.325    &     1.88e+04     \\\\\n",
       "\\textbf{6433.0}           &    1.238e+04  &     4473.315     &     2.767  &         0.006        &     3611.317    &     2.11e+04     \\\\\n",
       "\\textbf{6435.0}           &    7739.4594  &     3195.857     &     2.422  &         0.015        &     1475.094    &      1.4e+04     \\\\\n",
       "\\textbf{6492.0}           &    3058.4908  &     3102.949     &     0.986  &         0.324        &    -3023.761    &     9140.743     \\\\\n",
       "\\textbf{6497.0}           &     1.16e+04  &     4271.985     &     2.714  &         0.007        &     3222.039    &        2e+04     \\\\\n",
       "\\textbf{6500.0}           &    1.238e+04  &     7941.667     &     1.559  &         0.119        &    -3184.200    &     2.79e+04     \\\\\n",
       "\\textbf{6509.0}           &    8036.7520  &     3678.547     &     2.185  &         0.029        &      826.241    &     1.52e+04     \\\\\n",
       "\\textbf{6527.0}           &    1.133e+04  &     4450.159     &     2.546  &         0.011        &     2605.843    &     2.01e+04     \\\\\n",
       "\\textbf{6528.0}           &    1.032e+04  &     4188.627     &     2.463  &         0.014        &     2107.083    &     1.85e+04     \\\\\n",
       "\\textbf{6531.0}           &   -4200.0683  &     3016.386     &    -1.392  &         0.164        &    -1.01e+04    &     1712.508     \\\\\n",
       "\\textbf{6532.0}           &    6540.4623  &     3539.456     &     1.848  &         0.065        &     -397.410    &     1.35e+04     \\\\\n",
       "\\textbf{6543.0}           &    1.105e+04  &     4207.876     &     2.626  &         0.009        &     2801.752    &     1.93e+04     \\\\\n",
       "\\textbf{6548.0}           &    1.103e+04  &     4220.182     &     2.613  &         0.009        &     2754.830    &     1.93e+04     \\\\\n",
       "\\textbf{6550.0}           &    9640.8987  &     4124.209     &     2.338  &         0.019        &     1556.821    &     1.77e+04     \\\\\n",
       "\\textbf{6552.0}           &    1.098e+04  &     4402.914     &     2.494  &         0.013        &     2350.328    &     1.96e+04     \\\\\n",
       "\\textbf{6565.0}           &     106.4800  &     3104.123     &     0.034  &         0.973        &    -5978.073    &     6191.033     \\\\\n",
       "\\textbf{6571.0}           &    9703.4230  &     3948.030     &     2.458  &         0.014        &     1964.683    &     1.74e+04     \\\\\n",
       "\\textbf{6573.0}           &    9345.6370  &     3833.374     &     2.438  &         0.015        &     1831.641    &     1.69e+04     \\\\\n",
       "\\textbf{6641.0}           &    9731.3147  &     5847.278     &     1.664  &         0.096        &    -1730.239    &     2.12e+04     \\\\\n",
       "\\textbf{6649.0}           &    1.201e+04  &     4326.600     &     2.775  &         0.006        &     3526.284    &     2.05e+04     \\\\\n",
       "\\textbf{6730.0}           &    1.398e+04  &     3155.542     &     4.431  &         0.000        &     7797.210    &     2.02e+04     \\\\\n",
       "\\textbf{6731.0}           &    3159.2103  &     3140.716     &     1.006  &         0.314        &    -2997.071    &     9315.492     \\\\\n",
       "\\textbf{6742.0}           &    1.325e+04  &     5704.105     &     2.324  &         0.020        &     2073.387    &     2.44e+04     \\\\\n",
       "\\textbf{6745.0}           &    1.087e+04  &     4172.152     &     2.606  &         0.009        &     2694.673    &     1.91e+04     \\\\\n",
       "\\textbf{6756.0}           &    1.218e+04  &     4382.072     &     2.779  &         0.005        &     3587.033    &     2.08e+04     \\\\\n",
       "\\textbf{6765.0}           &   -1623.7257  &     3020.376     &    -0.538  &         0.591        &    -7544.123    &     4296.672     \\\\\n",
       "\\textbf{6768.0}           &    1.376e+04  &     4605.552     &     2.987  &         0.003        &     4729.602    &     2.28e+04     \\\\\n",
       "\\textbf{6774.0}           &   -1.068e+04  &     3459.448     &    -3.087  &         0.002        &    -1.75e+04    &    -3899.693     \\\\\n",
       "\\textbf{6797.0}           &    1.167e+04  &     4644.903     &     2.511  &         0.012        &     2560.590    &     2.08e+04     \\\\\n",
       "\\textbf{6803.0}           &     1.15e+04  &     4294.555     &     2.677  &         0.007        &     3077.736    &     1.99e+04     \\\\\n",
       "\\textbf{6821.0}           &    9550.7894  &     3972.501     &     2.404  &         0.016        &     1764.084    &     1.73e+04     \\\\\n",
       "\\textbf{6830.0}           &    1.067e+04  &     4175.203     &     2.555  &         0.011        &     2483.547    &     1.89e+04     \\\\\n",
       "\\textbf{6845.0}           &    2794.6874  &     3024.603     &     0.924  &         0.356        &    -3133.995    &     8723.370     \\\\\n",
       "\\textbf{6848.0}           &    1.194e+04  &     4512.653     &     2.647  &         0.008        &     3099.269    &     2.08e+04     \\\\\n",
       "\\textbf{6873.0}           &    4102.7272  &     4083.423     &     1.005  &         0.315        &    -3901.402    &     1.21e+04     \\\\\n",
       "\\textbf{6900.0}           &    4912.8458  &     3212.206     &     1.529  &         0.126        &    -1383.567    &     1.12e+04     \\\\\n",
       "\\textbf{6908.0}           &    5531.9600  &     3268.028     &     1.693  &         0.091        &     -873.873    &     1.19e+04     \\\\\n",
       "\\textbf{6994.0}           &    1.113e+04  &     4166.476     &     2.672  &         0.008        &     2967.783    &     1.93e+04     \\\\\n",
       "\\textbf{7045.0}           &    7685.9806  &     4363.589     &     1.761  &         0.078        &     -867.317    &     1.62e+04     \\\\\n",
       "\\textbf{7065.0}           &    1.265e+04  &     3474.516     &     3.642  &         0.000        &     5842.833    &     1.95e+04     \\\\\n",
       "\\textbf{7085.0}           &    1.076e+04  &     3676.094     &     2.926  &         0.003        &     3549.393    &      1.8e+04     \\\\\n",
       "\\textbf{7107.0}           &    5981.2294  &     3491.103     &     1.713  &         0.087        &     -861.863    &     1.28e+04     \\\\\n",
       "\\textbf{7116.0}           &    1.242e+04  &     4487.009     &     2.768  &         0.006        &     3626.309    &     2.12e+04     \\\\\n",
       "\\textbf{7117.0}           &    1.293e+04  &     5088.069     &     2.541  &         0.011        &     2954.216    &     2.29e+04     \\\\\n",
       "\\textbf{7121.0}           &        1e+04  &     3995.575     &     2.504  &         0.012        &     2172.638    &     1.78e+04     \\\\\n",
       "\\textbf{7127.0}           &    1.031e+04  &     4620.297     &     2.231  &         0.026        &     1252.392    &     1.94e+04     \\\\\n",
       "\\textbf{7139.0}           &    8759.6584  &     3847.940     &     2.276  &         0.023        &     1217.111    &     1.63e+04     \\\\\n",
       "\\textbf{7146.0}           &    1.058e+04  &     4082.559     &     2.591  &         0.010        &     2576.987    &     1.86e+04     \\\\\n",
       "\\textbf{7163.0}           &    1.392e+04  &     4153.892     &     3.351  &         0.001        &     5778.922    &     2.21e+04     \\\\\n",
       "\\textbf{7180.0}           &    5301.0622  &     3502.912     &     1.513  &         0.130        &    -1565.178    &     1.22e+04     \\\\\n",
       "\\textbf{7183.0}           &    4120.5527  &     3188.548     &     1.292  &         0.196        &    -2129.486    &     1.04e+04     \\\\\n",
       "\\textbf{7228.0}           &    1.969e+04  &     4266.255     &     4.614  &         0.000        &     1.13e+04    &      2.8e+04     \\\\\n",
       "\\textbf{7232.0}           &    1.219e+04  &     5085.387     &     2.396  &         0.017        &     2217.046    &     2.22e+04     \\\\\n",
       "\\textbf{7250.0}           &    8856.4192  &     4031.036     &     2.197  &         0.028        &      954.976    &     1.68e+04     \\\\\n",
       "\\textbf{7257.0}           &    3.532e+04  &     3220.846     &    10.966  &         0.000        &      2.9e+04    &     4.16e+04     \\\\\n",
       "\\textbf{7260.0}           &    9192.0479  &     3774.724     &     2.435  &         0.015        &     1793.015    &     1.66e+04     \\\\\n",
       "\\textbf{7267.0}           &    1.111e+04  &     4501.757     &     2.469  &         0.014        &     2288.679    &     1.99e+04     \\\\\n",
       "\\textbf{7268.0}           &    1.087e+04  &     4196.367     &     2.591  &         0.010        &     2649.108    &     1.91e+04     \\\\\n",
       "\\textbf{7281.0}           &    1.135e+04  &     4815.400     &     2.358  &         0.018        &     1915.397    &     2.08e+04     \\\\\n",
       "\\textbf{7291.0}           &    5098.6077  &     3246.491     &     1.570  &         0.116        &    -1265.010    &     1.15e+04     \\\\\n",
       "\\textbf{7343.0}           &    7842.5035  &     3534.507     &     2.219  &         0.027        &      914.333    &     1.48e+04     \\\\\n",
       "\\textbf{7346.0}           &    7723.8849  &     3633.104     &     2.126  &         0.034        &      602.448    &     1.48e+04     \\\\\n",
       "\\textbf{7401.0}           &    9328.3374  &     3905.660     &     2.388  &         0.017        &     1672.650    &      1.7e+04     \\\\\n",
       "\\textbf{7409.0}           &    7397.1967  &     3546.545     &     2.086  &         0.037        &      445.428    &     1.43e+04     \\\\\n",
       "\\textbf{7420.0}           &    5897.1210  &     3362.452     &     1.754  &         0.079        &     -693.797    &     1.25e+04     \\\\\n",
       "\\textbf{7435.0}           &    1078.3592  &     3147.847     &     0.343  &         0.732        &    -5091.901    &     7248.619     \\\\\n",
       "\\textbf{7466.0}           &     1.04e+04  &     4250.324     &     2.446  &         0.014        &     2063.728    &     1.87e+04     \\\\\n",
       "\\textbf{7486.0}           &    9520.0452  &     3911.185     &     2.434  &         0.015        &     1853.528    &     1.72e+04     \\\\\n",
       "\\textbf{7503.0}           &    8453.4334  &     4930.048     &     1.715  &         0.086        &    -1210.210    &     1.81e+04     \\\\\n",
       "\\textbf{7506.0}           &    8867.4155  &     3529.618     &     2.512  &         0.012        &     1948.827    &     1.58e+04     \\\\\n",
       "\\textbf{7537.0}           &    1.144e+04  &     4255.597     &     2.689  &         0.007        &     3102.766    &     1.98e+04     \\\\\n",
       "\\textbf{7549.0}           &    5151.4501  &     3281.945     &     1.570  &         0.117        &    -1281.661    &     1.16e+04     \\\\\n",
       "\\textbf{7554.0}           &    1.029e+04  &     4183.920     &     2.459  &         0.014        &     2089.086    &     1.85e+04     \\\\\n",
       "\\textbf{7557.0}           &    4008.4871  &     3177.140     &     1.262  &         0.207        &    -2219.191    &     1.02e+04     \\\\\n",
       "\\textbf{7585.0}           &   -1.479e+04  &     3233.253     &    -4.575  &         0.000        &    -2.11e+04    &    -8454.038     \\\\\n",
       "\\textbf{7602.0}           &    1.019e+04  &     4004.646     &     2.544  &         0.011        &     2339.317    &      1.8e+04     \\\\\n",
       "\\textbf{7620.0}           &    1.137e+04  &     4519.517     &     2.516  &         0.012        &     2510.606    &     2.02e+04     \\\\\n",
       "\\textbf{7636.0}           &    1.076e+04  &     4116.216     &     2.614  &         0.009        &     2690.138    &     1.88e+04     \\\\\n",
       "\\textbf{7646.0}           &    1.076e+04  &     4153.781     &     2.591  &         0.010        &     2620.102    &     1.89e+04     \\\\\n",
       "\\textbf{7658.0}           &    3359.7419  &     3088.730     &     1.088  &         0.277        &    -2694.638    &     9414.122     \\\\\n",
       "\\textbf{7683.0}           &      1.1e+04  &     4267.354     &     2.579  &         0.010        &     2640.333    &     1.94e+04     \\\\\n",
       "\\textbf{7685.0}           &    8204.8310  &     3806.973     &     2.155  &         0.031        &      742.585    &     1.57e+04     \\\\\n",
       "\\textbf{7692.0}           &    5703.7807  &     3354.251     &     1.700  &         0.089        &     -871.062    &     1.23e+04     \\\\\n",
       "\\textbf{7762.0}           &    9143.7308  &     3775.228     &     2.422  &         0.015        &     1743.710    &     1.65e+04     \\\\\n",
       "\\textbf{7772.0}           &   -4709.9246  &     3006.136     &    -1.567  &         0.117        &    -1.06e+04    &     1182.559     \\\\\n",
       "\\textbf{7773.0}           &    9293.3462  &     3912.452     &     2.375  &         0.018        &     1624.345    &      1.7e+04     \\\\\n",
       "\\textbf{7777.0}           &    5454.0280  &     3322.397     &     1.642  &         0.101        &    -1058.375    &      1.2e+04     \\\\\n",
       "\\textbf{7835.0}           &    1.042e+04  &     4050.637     &     2.572  &         0.010        &     2480.063    &     1.84e+04     \\\\\n",
       "\\textbf{7873.0}           &    8293.3585  &     3686.192     &     2.250  &         0.024        &     1067.862    &     1.55e+04     \\\\\n",
       "\\textbf{7883.0}           &    2715.7040  &     3007.334     &     0.903  &         0.367        &    -3179.127    &     8610.535     \\\\\n",
       "\\textbf{7904.0}           &    1919.1878  &     3040.146     &     0.631  &         0.528        &    -4039.961    &     7878.336     \\\\\n",
       "\\textbf{7906.0}           &    1.391e+04  &     4332.521     &     3.210  &         0.001        &     5416.523    &     2.24e+04     \\\\\n",
       "\\textbf{7921.0}           &    8796.3042  &     3660.684     &     2.403  &         0.016        &     1620.807    &      1.6e+04     \\\\\n",
       "\\textbf{7923.0}           &    9824.0520  &     4152.989     &     2.366  &         0.018        &     1683.561    &      1.8e+04     \\\\\n",
       "\\textbf{7935.0}           &    7306.5846  &     3602.685     &     2.028  &         0.043        &      244.774    &     1.44e+04     \\\\\n",
       "\\textbf{7938.0}           &    6665.8266  &     3616.131     &     1.843  &         0.065        &     -422.340    &     1.38e+04     \\\\\n",
       "\\textbf{7985.0}           &   -9857.0510  &     3135.543     &    -3.144  &         0.002        &     -1.6e+04    &    -3710.910     \\\\\n",
       "\\textbf{8014.0}           &    9728.9685  &     4012.019     &     2.425  &         0.015        &     1864.801    &     1.76e+04     \\\\\n",
       "\\textbf{8030.0}           &    1.285e+04  &     4423.355     &     2.904  &         0.004        &     4175.418    &     2.15e+04     \\\\\n",
       "\\textbf{8046.0}           &    8260.1941  &     3659.637     &     2.257  &         0.024        &     1086.748    &     1.54e+04     \\\\\n",
       "\\textbf{8047.0}           &    8521.1702  &     4219.749     &     2.019  &         0.043        &      249.820    &     1.68e+04     \\\\\n",
       "\\textbf{8062.0}           &    7568.8138  &     3635.126     &     2.082  &         0.037        &      443.413    &     1.47e+04     \\\\\n",
       "\\textbf{8068.0}           &   -5940.6066  &     3633.910     &    -1.635  &         0.102        &    -1.31e+04    &     1182.410     \\\\\n",
       "\\textbf{8087.0}           &    5782.3278  &     3379.903     &     1.711  &         0.087        &     -842.797    &     1.24e+04     \\\\\n",
       "\\textbf{8095.0}           &    8573.5924  &     3747.465     &     2.288  &         0.022        &     1227.991    &     1.59e+04     \\\\\n",
       "\\textbf{8096.0}           &    1.191e+04  &     4342.888     &     2.743  &         0.006        &     3399.602    &     2.04e+04     \\\\\n",
       "\\textbf{8109.0}           &    9871.9424  &     3953.432     &     2.497  &         0.013        &     2122.614    &     1.76e+04     \\\\\n",
       "\\textbf{8123.0}           &     -63.1503  &     2943.464     &    -0.021  &         0.983        &    -5832.788    &     5706.488     \\\\\n",
       "\\textbf{8150.0}           &    1.101e+04  &     4189.962     &     2.627  &         0.009        &     2795.062    &     1.92e+04     \\\\\n",
       "\\textbf{8163.0}           &    3268.2742  &     3100.134     &     1.054  &         0.292        &    -2808.460    &     9345.008     \\\\\n",
       "\\textbf{8176.0}           &    1.265e+04  &     4772.087     &     2.651  &         0.008        &     3294.603    &      2.2e+04     \\\\\n",
       "\\textbf{8202.0}           &    5157.2125  &     3345.753     &     1.541  &         0.123        &    -1400.972    &     1.17e+04     \\\\\n",
       "\\textbf{8214.0}           &    3394.2616  &     3294.579     &     1.030  &         0.303        &    -3063.615    &     9852.138     \\\\\n",
       "\\textbf{8215.0}           &     -54.6148  &     3158.648     &    -0.017  &         0.986        &    -6246.045    &     6136.815     \\\\\n",
       "\\textbf{8219.0}           &    1.203e+04  &     4458.083     &     2.699  &         0.007        &     3294.795    &     2.08e+04     \\\\\n",
       "\\textbf{8247.0}           &    2375.9926  &     3170.269     &     0.749  &         0.454        &    -3838.218    &     8590.203     \\\\\n",
       "\\textbf{8253.0}           &    2666.7976  &     3125.567     &     0.853  &         0.394        &    -3459.789    &     8793.385     \\\\\n",
       "\\textbf{8290.0}           &    6743.8027  &     3815.948     &     1.767  &         0.077        &     -736.037    &     1.42e+04     \\\\\n",
       "\\textbf{8293.0}           &    3811.2373  &     3095.166     &     1.231  &         0.218        &    -2255.759    &     9878.234     \\\\\n",
       "\\textbf{8304.0}           &    1.042e+04  &     3872.462     &     2.692  &         0.007        &     2833.174    &      1.8e+04     \\\\\n",
       "\\textbf{8334.0}           &    1.273e+04  &     4602.405     &     2.766  &         0.006        &     3710.745    &     2.18e+04     \\\\\n",
       "\\textbf{8348.0}           &    8859.6583  &     3798.952     &     2.332  &         0.020        &     1413.135    &     1.63e+04     \\\\\n",
       "\\textbf{8357.0}           &    8358.1549  &     3677.448     &     2.273  &         0.023        &     1149.797    &     1.56e+04     \\\\\n",
       "\\textbf{8358.0}           &    4807.9611  &     3297.333     &     1.458  &         0.145        &    -1655.312    &     1.13e+04     \\\\\n",
       "\\textbf{8446.0}           &   -3937.5812  &     3829.797     &    -1.028  &         0.304        &    -1.14e+04    &     3569.403     \\\\\n",
       "\\textbf{8460.0}           &    1.303e+04  &     4819.099     &     2.705  &         0.007        &     3587.830    &     2.25e+04     \\\\\n",
       "\\textbf{8463.0}           &    9571.6362  &     3984.079     &     2.402  &         0.016        &     1762.235    &     1.74e+04     \\\\\n",
       "\\textbf{8479.0}           &    9664.9227  &     4452.608     &     2.171  &         0.030        &      937.133    &     1.84e+04     \\\\\n",
       "\\textbf{8530.0}           &    2.662e+04  &     3200.718     &     8.316  &         0.000        &     2.03e+04    &     3.29e+04     \\\\\n",
       "\\textbf{8536.0}           &    2847.7871  &     3111.934     &     0.915  &         0.360        &    -3252.076    &     8947.650     \\\\\n",
       "\\textbf{8543.0}           &    3.097e+04  &     4334.476     &     7.144  &         0.000        &     2.25e+04    &     3.95e+04     \\\\\n",
       "\\textbf{8549.0}           &    1064.3040  &     3405.646     &     0.313  &         0.755        &    -5611.280    &     7739.888     \\\\\n",
       "\\textbf{8551.0}           &    1.143e+04  &     4342.977     &     2.632  &         0.009        &     2917.232    &     1.99e+04     \\\\\n",
       "\\textbf{8559.0}           &    1.097e+04  &     4260.535     &     2.576  &         0.010        &     2622.868    &     1.93e+04     \\\\\n",
       "\\textbf{8573.0}           &    1.106e+04  &     4277.534     &     2.587  &         0.010        &     2679.397    &     1.94e+04     \\\\\n",
       "\\textbf{8606.0}           &    8852.5352  &     3572.763     &     2.478  &         0.013        &     1849.375    &     1.59e+04     \\\\\n",
       "\\textbf{8607.0}           &    1.211e+04  &     4438.488     &     2.727  &         0.006        &     3405.146    &     2.08e+04     \\\\\n",
       "\\textbf{8648.0}           &    8539.2383  &     3716.683     &     2.298  &         0.022        &     1253.975    &     1.58e+04     \\\\\n",
       "\\textbf{8657.0}           &    5903.3861  &     3583.915     &     1.647  &         0.100        &    -1121.633    &     1.29e+04     \\\\\n",
       "\\textbf{8675.0}           &    1.141e+04  &     5303.064     &     2.152  &         0.031        &     1018.708    &     2.18e+04     \\\\\n",
       "\\textbf{8681.0}           &    5266.6759  &     3251.575     &     1.620  &         0.105        &    -1106.905    &     1.16e+04     \\\\\n",
       "\\textbf{8687.0}           &    6525.4736  &     3566.937     &     1.829  &         0.067        &     -466.265    &     1.35e+04     \\\\\n",
       "\\textbf{8692.0}           &    5590.8047  &     3384.259     &     1.652  &         0.099        &    -1042.858    &     1.22e+04     \\\\\n",
       "\\textbf{8699.0}           &    9524.4686  &     3890.257     &     2.448  &         0.014        &     1898.974    &     1.71e+04     \\\\\n",
       "\\textbf{8717.0}           &    1.113e+04  &     4176.221     &     2.664  &         0.008        &     2939.597    &     1.93e+04     \\\\\n",
       "\\textbf{8759.0}           &    1.096e+04  &     4208.368     &     2.605  &         0.009        &     2713.368    &     1.92e+04     \\\\\n",
       "\\textbf{8762.0}           &     1.12e+04  &     3656.399     &     3.062  &         0.002        &     4028.997    &     1.84e+04     \\\\\n",
       "\\textbf{8819.0}           &    1.298e+04  &     4615.110     &     2.813  &         0.005        &     3937.940    &      2.2e+04     \\\\\n",
       "\\textbf{8850.0}           &    9181.1749  &     3848.090     &     2.386  &         0.017        &     1638.333    &     1.67e+04     \\\\\n",
       "\\textbf{8852.0}           &    9627.9364  &     3911.676     &     2.461  &         0.014        &     1960.456    &     1.73e+04     \\\\\n",
       "\\textbf{8859.0}           &    1.208e+04  &     4432.894     &     2.725  &         0.006        &     3390.509    &     2.08e+04     \\\\\n",
       "\\textbf{8867.0}           &    3661.8316  &     3646.213     &     1.004  &         0.315        &    -3485.301    &     1.08e+04     \\\\\n",
       "\\textbf{8881.0}           &    7769.9386  &     3624.646     &     2.144  &         0.032        &      665.082    &     1.49e+04     \\\\\n",
       "\\textbf{8958.0}           &    8951.7960  &     3784.276     &     2.366  &         0.018        &     1534.039    &     1.64e+04     \\\\\n",
       "\\textbf{8972.0}           &   -8091.6358  &     3040.544     &    -2.661  &         0.008        &    -1.41e+04    &    -2131.707     \\\\\n",
       "\\textbf{8990.0}           &    5547.1038  &     3395.854     &     1.633  &         0.102        &    -1109.288    &     1.22e+04     \\\\\n",
       "\\textbf{9004.0}           &    1.154e+04  &     4462.107     &     2.587  &         0.010        &     2797.496    &     2.03e+04     \\\\\n",
       "\\textbf{9016.0}           &    7357.7328  &     3517.351     &     2.092  &         0.036        &      463.190    &     1.43e+04     \\\\\n",
       "\\textbf{9048.0}           &    4707.7775  &     3184.670     &     1.478  &         0.139        &    -1534.661    &      1.1e+04     \\\\\n",
       "\\textbf{9051.0}           &    6169.5902  &     3713.425     &     1.661  &         0.097        &    -1109.287    &     1.34e+04     \\\\\n",
       "\\textbf{9071.0}           &     877.5078  &     2975.713     &     0.295  &         0.768        &    -4955.343    &     6710.359     \\\\\n",
       "\\textbf{9112.0}           &    6934.4051  &     3514.220     &     1.973  &         0.048        &       45.999    &     1.38e+04     \\\\\n",
       "\\textbf{9114.0}           &    1029.6249  &     3180.276     &     0.324  &         0.746        &    -5204.201    &     7263.450     \\\\\n",
       "\\textbf{9132.0}           &    1.297e+04  &     5496.010     &     2.359  &         0.018        &     2192.891    &     2.37e+04     \\\\\n",
       "\\textbf{9173.0}           &    7935.3986  &     3848.198     &     2.062  &         0.039        &      392.346    &     1.55e+04     \\\\\n",
       "\\textbf{9180.0}           &    1.117e+04  &     4295.561     &     2.599  &         0.009        &     2745.212    &     1.96e+04     \\\\\n",
       "\\textbf{9186.0}           &    7783.4450  &     3611.743     &     2.155  &         0.031        &      703.880    &     1.49e+04     \\\\\n",
       "\\textbf{9191.0}           &    2427.7345  &     4179.686     &     0.581  &         0.561        &    -5765.085    &     1.06e+04     \\\\\n",
       "\\textbf{9216.0}           &     161.8933  &     2939.543     &     0.055  &         0.956        &    -5600.057    &     5923.844     \\\\\n",
       "\\textbf{9217.0}           &   -1862.4721  &     2944.618     &    -0.633  &         0.527        &    -7634.372    &     3909.427     \\\\\n",
       "\\textbf{9225.0}           &    9577.3064  &     3862.968     &     2.479  &         0.013        &     2005.301    &     1.71e+04     \\\\\n",
       "\\textbf{9230.0}           &    1.207e+04  &     4753.482     &     2.540  &         0.011        &     2756.935    &     2.14e+04     \\\\\n",
       "\\textbf{9259.0}           &    1.294e+04  &     4561.875     &     2.837  &         0.005        &     4000.503    &     2.19e+04     \\\\\n",
       "\\textbf{9293.0}           &    1.064e+04  &     4105.021     &     2.592  &         0.010        &     2592.742    &     1.87e+04     \\\\\n",
       "\\textbf{9299.0}           &    3233.5415  &     3419.949     &     0.945  &         0.344        &    -3470.080    &     9937.163     \\\\\n",
       "\\textbf{9308.0}           &     1.15e+04  &     4307.956     &     2.670  &         0.008        &     3059.867    &     1.99e+04     \\\\\n",
       "\\textbf{9311.0}           &    8250.1694  &     4502.595     &     1.832  &         0.067        &     -575.601    &     1.71e+04     \\\\\n",
       "\\textbf{9313.0}           &    5615.0259  &     3514.373     &     1.598  &         0.110        &    -1273.679    &     1.25e+04     \\\\\n",
       "\\textbf{9325.0}           &    8184.6337  &     3690.329     &     2.218  &         0.027        &      951.026    &     1.54e+04     \\\\\n",
       "\\textbf{9332.0}           &    5773.7851  &     3330.914     &     1.733  &         0.083        &     -755.313    &     1.23e+04     \\\\\n",
       "\\textbf{9340.0}           &     379.0420  &     3897.387     &     0.097  &         0.923        &    -7260.429    &     8018.513     \\\\\n",
       "\\textbf{9372.0}           &    1.291e+04  &     4648.350     &     2.777  &         0.005        &     3798.287    &      2.2e+04     \\\\\n",
       "\\textbf{9411.0}           &    6573.8418  &     3745.839     &     1.755  &         0.079        &     -768.573    &     1.39e+04     \\\\\n",
       "\\textbf{9459.0}           &    1.104e+04  &     3107.094     &     3.554  &         0.000        &     4953.235    &     1.71e+04     \\\\\n",
       "\\textbf{9465.0}           &    1.003e+04  &     3232.899     &     3.103  &         0.002        &     3693.095    &     1.64e+04     \\\\\n",
       "\\textbf{9472.0}           &    5266.1813  &     3238.912     &     1.626  &         0.104        &    -1082.580    &     1.16e+04     \\\\\n",
       "\\textbf{9483.0}           &    5490.3001  &     3265.877     &     1.681  &         0.093        &     -911.315    &     1.19e+04     \\\\\n",
       "\\textbf{9563.0}           &   -1.465e+04  &     4841.183     &    -3.026  &         0.002        &    -2.41e+04    &    -5157.834     \\\\\n",
       "\\textbf{9590.0}           &    1.113e+04  &     4184.411     &     2.660  &         0.008        &     2930.217    &     1.93e+04     \\\\\n",
       "\\textbf{9598.0}           &    5799.6371  &     3924.945     &     1.478  &         0.140        &    -1893.852    &     1.35e+04     \\\\\n",
       "\\textbf{9599.0}           &    7653.2768  &     3540.545     &     2.162  &         0.031        &      713.270    &     1.46e+04     \\\\\n",
       "\\textbf{9602.0}           &    5222.4602  &     4344.338     &     1.202  &         0.229        &    -3293.102    &     1.37e+04     \\\\\n",
       "\\textbf{9619.0}           &    1.294e+04  &     4579.938     &     2.826  &         0.005        &     3967.053    &     2.19e+04     \\\\\n",
       "\\textbf{9643.0}           &    2786.9648  &     3094.998     &     0.900  &         0.368        &    -3279.702    &     8853.631     \\\\\n",
       "\\textbf{9650.0}           &     696.8935  &     2942.311     &     0.237  &         0.813        &    -5070.484    &     6464.271     \\\\\n",
       "\\textbf{9653.0}           &   -6797.1413  &     5279.218     &    -1.288  &         0.198        &    -1.71e+04    &     3550.929     \\\\\n",
       "\\textbf{9667.0}           &    2075.9287  &     2998.392     &     0.692  &         0.489        &    -3801.376    &     7953.233     \\\\\n",
       "\\textbf{9698.0}           &    1.078e+04  &     4118.761     &     2.617  &         0.009        &     2705.388    &     1.89e+04     \\\\\n",
       "\\textbf{9699.0}           &     1.07e+04  &     3842.617     &     2.784  &         0.005        &     3164.148    &     1.82e+04     \\\\\n",
       "\\textbf{9719.0}           &    3103.5465  &     3044.700     &     1.019  &         0.308        &    -2864.530    &     9071.623     \\\\\n",
       "\\textbf{9742.0}           &    6948.3637  &     3504.437     &     1.983  &         0.047        &       79.134    &     1.38e+04     \\\\\n",
       "\\textbf{9761.0}           &    8658.0344  &     3760.543     &     2.302  &         0.021        &     1286.797    &      1.6e+04     \\\\\n",
       "\\textbf{9771.0}           &    1994.4442  &     3022.632     &     0.660  &         0.509        &    -3930.375    &     7919.264     \\\\\n",
       "\\textbf{9772.0}           &    1.174e+04  &     4284.250     &     2.741  &         0.006        &     3344.095    &     2.01e+04     \\\\\n",
       "\\textbf{9778.0}           &    5796.5913  &     3278.106     &     1.768  &         0.077        &     -628.996    &     1.22e+04     \\\\\n",
       "\\textbf{9799.0}           &     722.1063  &     3107.287     &     0.232  &         0.816        &    -5368.648    &     6812.861     \\\\\n",
       "\\textbf{9815.0}           &    8538.2025  &     3759.120     &     2.271  &         0.023        &     1169.755    &     1.59e+04     \\\\\n",
       "\\textbf{9818.0}           &   -3.283e+04  &     3444.849     &    -9.531  &         0.000        &    -3.96e+04    &    -2.61e+04     \\\\\n",
       "\\textbf{9837.0}           &    1.252e+04  &     4529.032     &     2.765  &         0.006        &     3644.228    &     2.14e+04     \\\\\n",
       "\\textbf{9922.0}           &    5934.5649  &     3333.050     &     1.781  &         0.075        &     -598.720    &     1.25e+04     \\\\\n",
       "\\textbf{9954.0}           &     775.4874  &     3649.467     &     0.212  &         0.832        &    -6378.023    &     7928.998     \\\\\n",
       "\\textbf{9963.0}           &    5664.6187  &     3452.978     &     1.641  &         0.101        &    -1103.743    &     1.24e+04     \\\\\n",
       "\\textbf{9988.0}           &    1.221e+04  &     4531.381     &     2.694  &         0.007        &     3327.463    &     2.11e+04     \\\\\n",
       "\\textbf{gspilltecIVX1981} &      -0.0528  &        0.065     &    -0.807  &         0.419        &       -0.181    &        0.075     \\\\\n",
       "\\textbf{gspilltecIVX1982} &      -0.0262  &        0.065     &    -0.406  &         0.685        &       -0.153    &        0.100     \\\\\n",
       "\\textbf{gspilltecIVX1983} &      -0.0435  &        0.064     &    -0.683  &         0.494        &       -0.168    &        0.081     \\\\\n",
       "\\textbf{gspilltecIVX1984} &      -0.1019  &        0.063     &    -1.606  &         0.108        &       -0.226    &        0.022     \\\\\n",
       "\\textbf{gspilltecIVX1985} &      -0.1423  &        0.064     &    -2.236  &         0.025        &       -0.267    &       -0.018     \\\\\n",
       "\\textbf{gspilltecIVX1986} &      -0.1942  &        0.064     &    -3.030  &         0.002        &       -0.320    &       -0.069     \\\\\n",
       "\\textbf{gspilltecIVX1987} &      -0.2053  &        0.065     &    -3.167  &         0.002        &       -0.332    &       -0.078     \\\\\n",
       "\\textbf{gspilltecIVX1988} &      -0.2385  &        0.065     &    -3.643  &         0.000        &       -0.367    &       -0.110     \\\\\n",
       "\\textbf{gspilltecIVX1989} &      -0.2438  &        0.066     &    -3.672  &         0.000        &       -0.374    &       -0.114     \\\\\n",
       "\\textbf{gspilltecIVX1990} &      -0.2782  &        0.067     &    -4.138  &         0.000        &       -0.410    &       -0.146     \\\\\n",
       "\\textbf{gspilltecIVX1991} &      -0.2604  &        0.068     &    -3.830  &         0.000        &       -0.394    &       -0.127     \\\\\n",
       "\\textbf{gspilltecIVX1992} &      -0.2623  &        0.069     &    -3.810  &         0.000        &       -0.397    &       -0.127     \\\\\n",
       "\\textbf{gspilltecIVX1993} &      -0.2406  &        0.070     &    -3.430  &         0.001        &       -0.378    &       -0.103     \\\\\n",
       "\\textbf{gspilltecIVX1994} &      -0.2479  &        0.071     &    -3.482  &         0.000        &       -0.387    &       -0.108     \\\\\n",
       "\\textbf{gspilltecIVX1995} &      -0.2435  &        0.073     &    -3.348  &         0.001        &       -0.386    &       -0.101     \\\\\n",
       "\\textbf{gspilltecIVX1996} &      -0.2201  &        0.075     &    -2.945  &         0.003        &       -0.367    &       -0.074     \\\\\n",
       "\\textbf{gspilltecIVX1997} &      -0.2168  &        0.077     &    -2.820  &         0.005        &       -0.368    &       -0.066     \\\\\n",
       "\\textbf{gspilltecIVX1998} &      -0.1931  &        0.079     &    -2.446  &         0.014        &       -0.348    &       -0.038     \\\\\n",
       "\\textbf{gspilltecIVX1999} &      -0.0952  &        0.081     &    -1.180  &         0.238        &       -0.253    &        0.063     \\\\\n",
       "\\textbf{gspilltecIVX2000} &      -0.1748  &        0.082     &    -2.127  &         0.033        &       -0.336    &       -0.014     \\\\\n",
       "\\textbf{gspilltecIVX2001} &      -0.2609  &        0.083     &    -3.134  &         0.002        &       -0.424    &       -0.098     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21422.439 & \\textbf{  Durbin-Watson:     } &      0.544    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 46586321.471  \\\\\n",
       "\\textbf{Skew:}          &    9.916  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  291.337  & \\textbf{  Cond. No.          } &   2.69e+07    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.69e+07. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.667\n",
       "Model:                            OLS   Adj. R-squared:                  0.647\n",
       "Method:                 Least Squares   F-statistic:                     32.62\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        02:03:00   Log-Likelihood:            -1.4153e+05\n",
       "No. Observations:               13385   AIC:                         2.846e+05\n",
       "Df Residuals:                   12609   BIC:                         2.904e+05\n",
       "Df Model:                         775                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -1.404e+04   4166.434     -3.371      0.001   -2.22e+04   -5877.761\n",
       "gspilltecIV          0.4517      0.123      3.677      0.000       0.211       0.693\n",
       "pat_count          -27.4800      1.908    -14.400      0.000     -31.221     -23.739\n",
       "rsales               0.7669      0.037     20.684      0.000       0.694       0.840\n",
       "rppent               0.5705      0.084      6.780      0.000       0.406       0.735\n",
       "emp                 12.9965      7.160      1.815      0.070      -1.038      27.031\n",
       "rxrd                19.7263      0.598     32.993      0.000      18.554      20.898\n",
       "1981               110.3657   1080.039      0.102      0.919   -2006.675    2227.406\n",
       "1982              -243.4640   1072.952     -0.227      0.820   -2346.613    1859.685\n",
       "1983              -114.3389   1063.325     -0.108      0.914   -2198.618    1969.940\n",
       "1984               115.2448   1057.289      0.109      0.913   -1957.203    2187.692\n",
       "1985               542.6004   1055.171      0.514      0.607   -1525.695    2610.895\n",
       "1986              1069.3608   1045.395      1.023      0.306    -979.773    3118.495\n",
       "1987              1025.4005   1040.967      0.985      0.325   -1015.054    3065.855\n",
       "1988              1291.3045   1041.029      1.240      0.215    -749.270    3331.879\n",
       "1989              1514.1450   1036.323      1.461      0.144    -517.206    3545.496\n",
       "1990              1689.8009   1031.752      1.638      0.101    -332.589    3712.191\n",
       "1991              1757.6723   1031.035      1.705      0.088    -263.313    3778.657\n",
       "1992              1663.9210   1030.301      1.615      0.106    -355.627    3683.469\n",
       "1993              1275.2181   1026.193      1.243      0.214    -736.277    3286.713\n",
       "1994              1193.1166   1028.610      1.160      0.246    -823.115    3209.348\n",
       "1995              1591.0463   1026.610      1.550      0.121    -421.265    3603.357\n",
       "1996              1293.6561   1028.769      1.257      0.209    -722.888    3310.200\n",
       "1997              1571.4228   1033.392      1.521      0.128    -454.183    3597.029\n",
       "1998              1015.9035   1036.611      0.980      0.327   -1016.012    3047.819\n",
       "1999             -1264.8357   1047.178     -1.208      0.227   -3317.464     787.792\n",
       "2000               683.4387   1067.982      0.640      0.522   -1409.969    2776.846\n",
       "2001              1290.2571   1113.123      1.159      0.246    -891.633    3472.147\n",
       "10005.0           8336.4171   3699.630      2.253      0.024    1084.579    1.56e+04\n",
       "10006.0           7128.2051   3884.307      1.835      0.067    -485.627    1.47e+04\n",
       "10008.0           6433.2934   3402.253      1.891      0.059    -235.640    1.31e+04\n",
       "10016.0           7164.4693   3479.856      2.059      0.040     343.423     1.4e+04\n",
       "10030.0           1.086e+04   4142.866      2.621      0.009    2739.409     1.9e+04\n",
       "1004.0            9386.8209   3877.670      2.421      0.016    1785.997     1.7e+04\n",
       "10056.0           7896.2518   3668.891      2.152      0.031     704.667    1.51e+04\n",
       "10085.0           5990.2646   3332.521      1.798      0.072    -541.984    1.25e+04\n",
       "10092.0           1.098e+04   5959.766      1.842      0.066    -705.369    2.27e+04\n",
       "10097.0           1809.1528   3207.129      0.564      0.573   -4477.308    8095.614\n",
       "1010.0            7643.0741   5674.555      1.347      0.178   -3479.917    1.88e+04\n",
       "10109.0           1.308e+04   4550.294      2.875      0.004    4161.111     2.2e+04\n",
       "10115.0           1.006e+04   3768.736      2.670      0.008    2676.196    1.75e+04\n",
       "10124.0           1.321e+04   4579.363      2.886      0.004    4238.105    2.22e+04\n",
       "1013.0            4740.3993   3172.561      1.494      0.135   -1478.302     1.1e+04\n",
       "10150.0           4494.4694   3784.080      1.188      0.235   -2922.902    1.19e+04\n",
       "10159.0           1.165e+04   5152.687      2.260      0.024    1546.547    2.17e+04\n",
       "10174.0           1.061e+04   4285.422      2.477      0.013    2214.472     1.9e+04\n",
       "10185.0           1.119e+04   4410.048      2.538      0.011    2547.094    1.98e+04\n",
       "10195.0           9918.8258   3937.320      2.519      0.012    2201.079    1.76e+04\n",
       "10198.0           1.066e+04   4103.271      2.598      0.009    2617.745    1.87e+04\n",
       "10215.0           1.282e+04   4541.798      2.823      0.005    3919.267    2.17e+04\n",
       "10232.0           1.259e+04   4488.212      2.804      0.005    3787.767    2.14e+04\n",
       "10236.0           1.113e+04   4185.336      2.659      0.008    2926.278    1.93e+04\n",
       "10286.0           6944.5078   3499.189      1.985      0.047      85.565    1.38e+04\n",
       "10301.0          -1.406e+04   3043.938     -4.618      0.000      -2e+04   -8091.002\n",
       "10312.0           8882.7969   3795.864      2.340      0.019    1442.327    1.63e+04\n",
       "10332.0          -4179.0338   3944.302     -1.060      0.289   -1.19e+04    3552.399\n",
       "1036.0            9705.2346   4210.687      2.305      0.021    1451.648     1.8e+04\n",
       "10374.0           8442.5866   3705.690      2.278      0.023    1178.869    1.57e+04\n",
       "10386.0           1560.4253   2977.524      0.524      0.600   -4275.976    7396.826\n",
       "10391.0          -2882.9016   2983.643     -0.966      0.334   -8731.297    2965.493\n",
       "10407.0           7373.1369   3485.250      2.116      0.034     541.517    1.42e+04\n",
       "10420.0           1.144e+04   3948.088      2.898      0.004    3703.384    1.92e+04\n",
       "10422.0           7742.9911   3805.122      2.035      0.042     284.373    1.52e+04\n",
       "10426.0           1.184e+04   4573.963      2.588      0.010    2873.904    2.08e+04\n",
       "10441.0           1.192e+04   4357.147      2.735      0.006    3375.210    2.05e+04\n",
       "1045.0           -2088.3861   3668.933     -0.569      0.569   -9280.053    5103.280\n",
       "10453.0           1226.7046   2981.021      0.412      0.681   -4616.550    7069.959\n",
       "10482.0          -1.245e+04   3822.048     -3.258      0.001   -1.99e+04   -4961.414\n",
       "10498.0           1.049e+04   4138.590      2.535      0.011    2377.788    1.86e+04\n",
       "10499.0          -3707.8662   3195.067     -1.160      0.246   -9970.684    2554.952\n",
       "10511.0           1.272e+04   4629.963      2.747      0.006    3643.423    2.18e+04\n",
       "10519.0          -3690.5599   2990.879     -1.234      0.217   -9553.138    2172.018\n",
       "10530.0           2107.4873   3002.129      0.702      0.483   -3777.142    7992.116\n",
       "10537.0           4731.3617   3496.655      1.353      0.176   -2122.614    1.16e+04\n",
       "10540.0           6573.0163   3380.260      1.945      0.052     -52.807    1.32e+04\n",
       "10541.0           8401.1498   3790.382      2.216      0.027     971.424    1.58e+04\n",
       "10550.0           5503.3473   6140.198      0.896      0.370   -6532.375    1.75e+04\n",
       "10553.0           2834.0974   3318.240      0.854      0.393   -3670.158    9338.352\n",
       "10565.0            1.14e+04   4148.160      2.747      0.006    3265.326    1.95e+04\n",
       "10580.0            1.27e+04   4343.865      2.925      0.003    4189.164    2.12e+04\n",
       "10581.0           8236.5095   3945.775      2.087      0.037     502.189     1.6e+04\n",
       "10588.0           3146.3468   3044.889      1.033      0.301   -2822.100    9114.793\n",
       "10597.0           9207.2402   3889.421      2.367      0.018    1583.382    1.68e+04\n",
       "10599.0           9329.2321   3923.775      2.378      0.017    1638.037     1.7e+04\n",
       "10618.0           8960.0187   3856.497      2.323      0.020    1400.697    1.65e+04\n",
       "10656.0           7268.4839   3511.216      2.070      0.038     385.967    1.42e+04\n",
       "10658.0           7005.8260   3479.247      2.014      0.044     185.972    1.38e+04\n",
       "10726.0           1.437e+04   4403.463      3.264      0.001    5743.323     2.3e+04\n",
       "10734.0           9878.1281   4446.542      2.222      0.026    1162.229    1.86e+04\n",
       "10735.0           1.202e+04   4438.906      2.708      0.007    3320.678    2.07e+04\n",
       "10764.0           1.234e+04   4559.888      2.707      0.007    3406.665    2.13e+04\n",
       "10777.0           6970.8867   3477.112      2.005      0.045     155.219    1.38e+04\n",
       "1078.0            1.116e+04   3066.828      3.638      0.000    5146.780    1.72e+04\n",
       "10793.0           9218.8031   4147.854      2.223      0.026    1088.377    1.73e+04\n",
       "10816.0           7007.4075   3721.310      1.883      0.060    -286.926    1.43e+04\n",
       "10839.0           9251.2564   3809.506      2.428      0.015    1784.046    1.67e+04\n",
       "10857.0          -5584.6220   3018.547     -1.850      0.064   -1.15e+04     332.189\n",
       "10867.0           6266.9034   4195.822      1.494      0.135   -1957.547    1.45e+04\n",
       "10906.0           9612.5911   3916.798      2.454      0.014    1935.071    1.73e+04\n",
       "10950.0           9574.1163   4743.479      2.018      0.044     276.177    1.89e+04\n",
       "10983.0          -2.613e+04   3208.797     -8.143      0.000   -3.24e+04   -1.98e+04\n",
       "1099.0            8222.4417   3701.273      2.222      0.026     967.384    1.55e+04\n",
       "10991.0           1.145e+04   4668.265      2.453      0.014    2301.167    2.06e+04\n",
       "11012.0           1.018e+04   4079.198      2.496      0.013    2186.232    1.82e+04\n",
       "11038.0           6908.5529   4237.646      1.630      0.103   -1397.878    1.52e+04\n",
       "1104.0            1.139e+04   4232.982      2.691      0.007    3091.881    1.97e+04\n",
       "11060.0           1.027e+04   4117.218      2.494      0.013    2198.570    1.83e+04\n",
       "11094.0           8968.6893   3818.676      2.349      0.019    1483.504    1.65e+04\n",
       "11096.0           5736.6141   3342.590      1.716      0.086    -815.370    1.23e+04\n",
       "11113.0           1.022e+04   4226.040      2.417      0.016    1932.709    1.85e+04\n",
       "1115.0            8098.0254   3678.632      2.201      0.028     887.347    1.53e+04\n",
       "11161.0           8338.9787   3682.770      2.264      0.024    1120.190    1.56e+04\n",
       "11225.0           1.202e+04   4392.061      2.736      0.006    3406.259    2.06e+04\n",
       "11228.0           1.056e+04   3964.185      2.663      0.008    2785.075    1.83e+04\n",
       "11236.0            432.3561   4531.433      0.095      0.924   -8449.942    9314.654\n",
       "11288.0           6685.4946   3144.248      2.126      0.034     522.291    1.28e+04\n",
       "11312.0           3341.2076   3149.887      1.061      0.289   -2833.050    9515.465\n",
       "11361.0           5797.5155   3313.162      1.750      0.080    -696.786    1.23e+04\n",
       "11399.0           1791.0186   3171.702      0.565      0.572   -4426.001    8008.038\n",
       "114303.0         -8554.7210   5395.081     -1.586      0.113   -1.91e+04    2020.459\n",
       "11456.0            655.3210   3182.529      0.206      0.837   -5582.920    6893.562\n",
       "11465.0           4678.7849   3824.265      1.223      0.221   -2817.356    1.22e+04\n",
       "11502.0           1.186e+04   4447.890      2.667      0.008    3143.313    2.06e+04\n",
       "11506.0           1427.5921   3092.088      0.462      0.644   -4633.370    7488.555\n",
       "11537.0           8750.9564   3758.034      2.329      0.020    1384.639    1.61e+04\n",
       "11566.0           1.275e+04   4525.949      2.818      0.005    3883.438    2.16e+04\n",
       "11573.0           6824.1547   3447.624      1.979      0.048      66.287    1.36e+04\n",
       "11580.0            133.1965   3351.986      0.040      0.968   -6437.205    6703.598\n",
       "11600.0           1.196e+04   4334.154      2.760      0.006    3464.914    2.05e+04\n",
       "11609.0           1.459e+04   4199.329      3.475      0.001    6359.859    2.28e+04\n",
       "1161.0           -2819.4272   2954.270     -0.954      0.340   -8610.245    2971.391\n",
       "11636.0          -9598.0267   3635.579     -2.640      0.008   -1.67e+04   -2471.739\n",
       "11670.0            1.29e+04   4579.826      2.816      0.005    3919.660    2.19e+04\n",
       "11678.0           3653.3124   3268.448      1.118      0.264   -2753.342    1.01e+04\n",
       "11682.0           5697.0009   3338.579      1.706      0.088    -847.122    1.22e+04\n",
       "11694.0           9646.8768   4026.329      2.396      0.017    1754.660    1.75e+04\n",
       "11720.0           8111.0790   4501.821      1.802      0.072    -713.174    1.69e+04\n",
       "11721.0           6037.3698   3627.836      1.664      0.096   -1073.741    1.31e+04\n",
       "11722.0           7005.1179   3697.572      1.895      0.058    -242.685    1.43e+04\n",
       "11793.0           3172.6538   6017.047      0.527      0.598   -8621.674     1.5e+04\n",
       "11797.0           1.205e+04   4718.873      2.553      0.011    2797.242    2.13e+04\n",
       "11914.0           9708.1373   4476.264      2.169      0.030     933.979    1.85e+04\n",
       "1209.0            5758.0179   3254.162      1.769      0.077    -620.634    1.21e+04\n",
       "12136.0          -1345.1719   3228.391     -0.417      0.677   -7673.310    4982.966\n",
       "12141.0           9.132e+04   3418.316     26.715      0.000    8.46e+04     9.8e+04\n",
       "12181.0           5925.9690   4393.720      1.349      0.177   -2686.390    1.45e+04\n",
       "12215.0           1497.9987   3243.616      0.462      0.644   -4859.983    7855.980\n",
       "12216.0           7768.8755   3429.726      2.265      0.024    1046.090    1.45e+04\n",
       "12256.0           4432.5743   3460.578      1.281      0.200   -2350.685    1.12e+04\n",
       "12262.0           1.193e+04   4581.767      2.603      0.009    2947.382    2.09e+04\n",
       "12389.0           5632.2076   3333.542      1.690      0.091    -902.041    1.22e+04\n",
       "1239.0            7169.2164   3514.315      2.040      0.041     280.625    1.41e+04\n",
       "12390.0           7378.0172   4065.874      1.815      0.070    -591.714    1.53e+04\n",
       "12397.0           6407.6851   5492.646      1.167      0.243   -4358.737    1.72e+04\n",
       "1243.0            5287.9136   4067.600      1.300      0.194   -2685.201    1.33e+04\n",
       "12548.0           8984.0388   4241.517      2.118      0.034     670.020    1.73e+04\n",
       "12570.0           1.033e+04   4391.485      2.351      0.019    1717.135    1.89e+04\n",
       "12581.0           4742.3562   3893.599      1.218      0.223   -2889.690    1.24e+04\n",
       "12592.0           9733.1829   4214.514      2.309      0.021    1472.094     1.8e+04\n",
       "12604.0           9261.3599   6722.913      1.378      0.168   -3916.572    2.24e+04\n",
       "12656.0           1.275e+04   4797.150      2.658      0.008    3345.860    2.22e+04\n",
       "12679.0          -1467.2846   3335.617     -0.440      0.660   -8005.602    5071.033\n",
       "1278.0            1.187e+04   4568.977      2.598      0.009    2912.156    2.08e+04\n",
       "12788.0           1.027e+04   4533.225      2.266      0.023    1385.836    1.92e+04\n",
       "1283.0            1.308e+04   4581.350      2.855      0.004    4101.178    2.21e+04\n",
       "1297.0            9383.3666   3915.787      2.396      0.017    1707.829    1.71e+04\n",
       "12992.0           1.093e+04   4453.163      2.454      0.014    2199.830    1.97e+04\n",
       "13135.0           7207.4649   4052.732      1.778      0.075    -736.507    1.52e+04\n",
       "1327.0            9423.6690   3915.403      2.407      0.016    1748.884    1.71e+04\n",
       "13282.0           3509.7671   5497.179      0.638      0.523   -7265.539    1.43e+04\n",
       "1334.0            1.289e+04   4607.074      2.799      0.005    3862.866    2.19e+04\n",
       "13351.0           1135.7044   4077.886      0.279      0.781   -6857.572    9128.981\n",
       "13365.0           2862.8486   3342.703      0.856      0.392   -3689.358    9415.056\n",
       "13369.0           3079.9892   3388.253      0.909      0.363   -3561.502    9721.481\n",
       "13406.0           9632.3318   4151.138      2.320      0.020    1495.471    1.78e+04\n",
       "13407.0           6001.9300   3683.087      1.630      0.103   -1217.480    1.32e+04\n",
       "13417.0           1.207e+04   4762.098      2.535      0.011    2736.437    2.14e+04\n",
       "13525.0           1.146e+04   4599.907      2.492      0.013    2448.079    2.05e+04\n",
       "13554.0            1.27e+04   4752.213      2.673      0.008    3385.257     2.2e+04\n",
       "1359.0            1.051e+04   3699.533      2.840      0.005    3254.467    1.78e+04\n",
       "13623.0           9234.8077   4123.463      2.240      0.025    1152.194    1.73e+04\n",
       "1372.0            8934.5926   3806.329      2.347      0.019    1473.610    1.64e+04\n",
       "1380.0            5440.6218   3743.636      1.453      0.146   -1897.474    1.28e+04\n",
       "13923.0           8736.2570   4274.096      2.044      0.041     358.378    1.71e+04\n",
       "13932.0           1.118e+04   5116.978      2.185      0.029    1148.847    2.12e+04\n",
       "13941.0           1019.1180   3337.043      0.305      0.760   -5521.994    7560.230\n",
       "1397.0            6617.9837   3587.349      1.845      0.065    -413.766    1.36e+04\n",
       "14064.0           1.049e+04   4317.313      2.431      0.015    2032.134     1.9e+04\n",
       "14084.0           4235.9708   3485.117      1.215      0.224   -2595.388    1.11e+04\n",
       "14324.0           7822.4027   3672.860      2.130      0.033     623.039     1.5e+04\n",
       "14462.0           2250.0033   3404.219      0.661      0.509   -4422.784    8922.791\n",
       "1447.0            1.349e+04   5104.500      2.643      0.008    3483.368    2.35e+04\n",
       "14531.0           3181.5998      1e+04      0.318      0.751   -1.64e+04    2.28e+04\n",
       "14593.0           1.076e+04   4576.649      2.352      0.019    1793.113    1.97e+04\n",
       "14622.0           7006.0702   7326.553      0.956      0.339   -7355.088    2.14e+04\n",
       "1465.0            1.192e+04   4771.898      2.498      0.012    2568.838    2.13e+04\n",
       "1468.0            1.027e+04   4449.925      2.309      0.021    1552.365     1.9e+04\n",
       "14897.0           1.042e+04   5971.789      1.745      0.081   -1282.470    2.21e+04\n",
       "14954.0            1.06e+04   4476.738      2.368      0.018    1825.159    1.94e+04\n",
       "1496.0            1.104e+04   4166.134      2.651      0.008    2878.667    1.92e+04\n",
       "15267.0           9090.7110   4197.561      2.166      0.030     862.852    1.73e+04\n",
       "15354.0           3967.3799   3742.828      1.060      0.289   -3369.133    1.13e+04\n",
       "1542.0            7923.6795   3639.706      2.177      0.029     789.302    1.51e+04\n",
       "15459.0           1006.0868   3431.298      0.293      0.769   -5719.779    7731.953\n",
       "1554.0            1.105e+04   4161.285      2.655      0.008    2891.119    1.92e+04\n",
       "15708.0           1345.3597   3415.184      0.394      0.694   -5348.921    8039.640\n",
       "15711.0           6104.6576   3935.001      1.551      0.121   -1608.543    1.38e+04\n",
       "15761.0           9451.7981   4802.983      1.968      0.049      37.220    1.89e+04\n",
       "1581.0           -2.976e+04   4038.352     -7.369      0.000   -3.77e+04   -2.18e+04\n",
       "1593.0            5515.0972   3304.175      1.669      0.095    -961.588     1.2e+04\n",
       "1602.0            1.395e+04   3396.118      4.109      0.000    7296.484    2.06e+04\n",
       "1613.0            1.101e+04   4186.090      2.629      0.009    2799.971    1.92e+04\n",
       "16188.0           8669.1429   4170.880      2.078      0.038     493.583    1.68e+04\n",
       "1632.0            4256.0786   3098.060      1.374      0.170   -1816.591    1.03e+04\n",
       "1633.0            8908.1974   3817.027      2.334      0.020    1426.244    1.64e+04\n",
       "1635.0            7450.2837   3526.786      2.112      0.035     537.246    1.44e+04\n",
       "16401.0          -4061.5505   3576.531     -1.136      0.256   -1.11e+04    2948.994\n",
       "16437.0           3430.9607   4052.273      0.847      0.397   -4512.110    1.14e+04\n",
       "1651.0            7112.8693   3456.619      2.058      0.040     337.371    1.39e+04\n",
       "1655.0            9935.4841   3981.206      2.496      0.013    2131.715    1.77e+04\n",
       "1663.0            1.363e+04   3679.861      3.704      0.000    6415.407    2.08e+04\n",
       "16710.0           5871.3980   3911.464      1.501      0.133   -1795.666    1.35e+04\n",
       "16729.0           6628.1263   3941.492      1.682      0.093   -1097.799    1.44e+04\n",
       "1690.0           -5003.5820   3105.268     -1.611      0.107   -1.11e+04    1083.216\n",
       "1703.0            6095.9361   3462.476      1.761      0.078    -691.044    1.29e+04\n",
       "17101.0           4955.5819      1e+04      0.495      0.621   -1.47e+04    2.46e+04\n",
       "17202.0           1.037e+04   4459.996      2.325      0.020    1629.279    1.91e+04\n",
       "1722.0            7914.1858   3882.992      2.038      0.042     302.930    1.55e+04\n",
       "1728.0            1.051e+04   4109.935      2.558      0.011    2457.721    1.86e+04\n",
       "1743.0            1.061e+04   5016.063      2.116      0.034     782.085    2.04e+04\n",
       "1754.0            9871.2919   4014.380      2.459      0.014    2002.497    1.77e+04\n",
       "1762.0           -1360.6791   2978.263     -0.457      0.648   -7198.529    4477.170\n",
       "1773.0            1.168e+04   4569.107      2.556      0.011    2720.433    2.06e+04\n",
       "1786.0           -1787.4250   3044.853     -0.587      0.557   -7755.799    4180.950\n",
       "18100.0           9369.5941   4277.263      2.191      0.029     985.507    1.78e+04\n",
       "1820.0            1.025e+04   4017.719      2.552      0.011    2376.254    1.81e+04\n",
       "1848.0            -174.7376   3641.414     -0.048      0.962   -7312.463    6962.988\n",
       "18654.0           9895.0330   5108.443      1.937      0.053    -118.292    1.99e+04\n",
       "1875.0            8101.0355   4958.513      1.634      0.102   -1618.405    1.78e+04\n",
       "1884.0            1.173e+04   4391.837      2.672      0.008    3124.748    2.03e+04\n",
       "1913.0            1494.6254   2975.110      0.502      0.615   -4337.042    7326.293\n",
       "1919.0            8651.6988   4021.054      2.152      0.031     769.821    1.65e+04\n",
       "1920.0            4718.3260   3144.201      1.501      0.133   -1444.787    1.09e+04\n",
       "1968.0            5476.8899   3274.829      1.672      0.094    -942.272    1.19e+04\n",
       "1976.0            1.191e+04   4073.536      2.923      0.003    3923.878    1.99e+04\n",
       "1981.0            1.044e+04   4077.941      2.559      0.011    2441.799    1.84e+04\n",
       "1988.0           -5069.7376   3891.247     -1.303      0.193   -1.27e+04    2557.699\n",
       "1992.0            5039.6675   3216.455      1.567      0.117   -1265.073    1.13e+04\n",
       "2008.0            7226.1408   3451.123      2.094      0.036     461.414     1.4e+04\n",
       "2033.0            9390.2572   4347.121      2.160      0.031     869.239    1.79e+04\n",
       "2044.0            8829.3117   3722.148      2.372      0.018    1533.335    1.61e+04\n",
       "2049.0            7251.7676   3540.296      2.048      0.041     312.249    1.42e+04\n",
       "2061.0            1.251e+04   4483.089      2.790      0.005    3718.981    2.13e+04\n",
       "20779.0           5.344e+04   3925.737     13.614      0.000    4.57e+04    6.11e+04\n",
       "2085.0            4409.4735   3154.407      1.398      0.162   -1773.645    1.06e+04\n",
       "2086.0            5974.6876   3423.325      1.745      0.081    -735.550    1.27e+04\n",
       "2111.0            7052.1036   3445.832      2.047      0.041     297.749    1.38e+04\n",
       "21204.0           1.256e+04   4928.367      2.548      0.011    2898.240    2.22e+04\n",
       "21238.0           1.239e+04   4811.460      2.575      0.010    2959.156    2.18e+04\n",
       "2124.0            5697.0681   3402.162      1.675      0.094    -971.688    1.24e+04\n",
       "2146.0            2.541e+04   4992.641      5.089      0.000    1.56e+04    3.52e+04\n",
       "21496.0           1455.5099   3547.770      0.410      0.682   -5498.658    8409.678\n",
       "2154.0            9090.5296   3869.063      2.350      0.019    1506.577    1.67e+04\n",
       "2176.0            5.038e+04   4320.920     11.660      0.000    4.19e+04    5.89e+04\n",
       "2188.0            1.238e+04   4515.888      2.740      0.006    3523.399    2.12e+04\n",
       "2189.0            -562.1249   3162.012     -0.178      0.859   -6760.150    5635.900\n",
       "2220.0            7170.2235   3562.804      2.013      0.044     186.585    1.42e+04\n",
       "22205.0            1.25e+04   4887.973      2.557      0.011    2915.532    2.21e+04\n",
       "2226.0            6694.5745   6256.551      1.070      0.285   -5569.217     1.9e+04\n",
       "2230.0            1.276e+04   4478.745      2.850      0.004    3983.822    2.15e+04\n",
       "22325.0           9261.0607   3787.917      2.445      0.015    1836.167    1.67e+04\n",
       "2255.0            6563.5332   3644.695      1.801      0.072    -580.623    1.37e+04\n",
       "22619.0           9490.2470   4525.918      2.097      0.036     618.759    1.84e+04\n",
       "2267.0            4985.7416   3253.204      1.533      0.125   -1391.034    1.14e+04\n",
       "22815.0           6901.8595   3991.510      1.729      0.084    -922.108    1.47e+04\n",
       "2285.0           -2.214e+04   3272.038     -6.766      0.000   -2.86e+04   -1.57e+04\n",
       "2290.0            3654.5615   3530.734      1.035      0.301   -3266.215    1.06e+04\n",
       "2295.0              1.3e+04   5600.714      2.322      0.020    2026.157     2.4e+04\n",
       "2316.0            3158.2020   3401.547      0.928      0.353   -3509.348    9825.752\n",
       "23220.0           6844.3205   4048.020      1.691      0.091   -1090.414    1.48e+04\n",
       "23224.0           1.102e+04   4772.809      2.308      0.021    1662.511    2.04e+04\n",
       "2343.0            8651.7177   5862.373      1.476      0.140   -2839.425    2.01e+04\n",
       "2352.0            3701.1087   3232.290      1.145      0.252   -2634.671       1e+04\n",
       "23700.0          -2730.5076   4771.481     -0.572      0.567   -1.21e+04    6622.322\n",
       "2390.0            1.128e+04   4210.584      2.680      0.007    3029.690    1.95e+04\n",
       "2393.0             927.7677   2948.534      0.315      0.753   -4851.808    6707.343\n",
       "2403.0            2.172e+04   3176.339      6.839      0.000    1.55e+04    2.79e+04\n",
       "2435.0            1.411e+04   4527.637      3.116      0.002    5235.229     2.3e+04\n",
       "2444.0            2387.4015   3090.909      0.772      0.440   -3671.251    8446.054\n",
       "2448.0            7993.4498   3623.624      2.206      0.027     890.595    1.51e+04\n",
       "2469.0            1.061e+04   5183.112      2.046      0.041     445.737    2.08e+04\n",
       "24720.0           7113.8480   4153.805      1.713      0.087   -1028.242    1.53e+04\n",
       "24800.0            1.15e+04   3897.050      2.951      0.003    3860.020    1.91e+04\n",
       "2482.0            1.153e+04   4261.214      2.705      0.007    3173.514    1.99e+04\n",
       "24969.0            1.11e+04   5242.485      2.117      0.034     821.772    2.14e+04\n",
       "2498.0            5950.1850   3496.509      1.702      0.089    -903.504    1.28e+04\n",
       "2504.0           -1.316e+04   3179.409     -4.140      0.000   -1.94e+04   -6930.258\n",
       "2508.0            9444.1082   4064.529      2.324      0.020    1477.012    1.74e+04\n",
       "25124.0           8152.1151   4336.426      1.880      0.060    -347.939    1.67e+04\n",
       "2518.0            9780.3988   3987.017      2.453      0.014    1965.239    1.76e+04\n",
       "25224.0           7547.9150   7367.374      1.025      0.306   -6893.258     2.2e+04\n",
       "25279.0           1.073e+04   4396.741      2.441      0.015    2115.387    1.94e+04\n",
       "2537.0            6190.5805   3390.712      1.826      0.068    -455.730    1.28e+04\n",
       "2538.0            1.165e+04   5117.482      2.276      0.023    1614.429    2.17e+04\n",
       "25389.0           1.075e+04   6659.968      1.614      0.107   -2304.315    2.38e+04\n",
       "2547.0             -72.1448   3461.233     -0.021      0.983   -6856.688    6712.398\n",
       "2553.0             1.08e+04   4155.736      2.600      0.009    2657.280    1.89e+04\n",
       "2574.0            3872.3138   4394.763      0.881      0.378   -4742.090    1.25e+04\n",
       "25747.0           8533.8385   4417.126      1.932      0.053    -124.400    1.72e+04\n",
       "2577.0            5974.7236   3339.669      1.789      0.074    -571.536    1.25e+04\n",
       "2593.0            6166.0952   3437.442      1.794      0.073    -571.814    1.29e+04\n",
       "2596.0            1.235e+04   4438.456      2.782      0.005    3646.159     2.1e+04\n",
       "2663.0            1.216e+04   3876.313      3.138      0.002    4564.624    1.98e+04\n",
       "2771.0            1.193e+04   4407.974      2.706      0.007    3287.864    2.06e+04\n",
       "2787.0            9469.7837   3959.582      2.392      0.017    1708.400    1.72e+04\n",
       "2797.0            1966.6171   3049.874      0.645      0.519   -4011.601    7944.835\n",
       "2802.0            1.127e+04   4221.976      2.670      0.008    2996.835    1.95e+04\n",
       "2817.0           -8930.8400   3046.906     -2.931      0.003   -1.49e+04   -2958.440\n",
       "28678.0          -1537.9801   3864.197     -0.398      0.691   -9112.395    6036.434\n",
       "28701.0           8387.4495   3874.922      2.165      0.030     792.013     1.6e+04\n",
       "28742.0           4282.1391   4229.061      1.013      0.311   -4007.463    1.26e+04\n",
       "2888.0            6760.4694   3567.651      1.895      0.058    -232.670    1.38e+04\n",
       "2897.0            9862.4574   4460.678      2.211      0.027    1118.850    1.86e+04\n",
       "2917.0            4638.9279   3766.582      1.232      0.218   -2744.147     1.2e+04\n",
       "29392.0          -6393.4066   3894.751     -1.642      0.101    -1.4e+04    1240.897\n",
       "2950.0            7545.1302   4190.581      1.800      0.072    -669.046    1.58e+04\n",
       "2951.0             1.16e+04   4618.097      2.513      0.012    2552.014    2.07e+04\n",
       "2953.0            8985.2452   3782.157      2.376      0.018    1571.641    1.64e+04\n",
       "2960.0            5222.5971   4059.135      1.287      0.198   -2733.925    1.32e+04\n",
       "2975.0            6256.0595   3431.701      1.823      0.068    -470.597     1.3e+04\n",
       "2982.0            1.023e+04   4075.609      2.510      0.012    2239.386    1.82e+04\n",
       "2991.0             501.0336   3925.088      0.128      0.898   -7192.737    8194.804\n",
       "3011.0            7688.4059   3704.565      2.075      0.038     426.894    1.49e+04\n",
       "3015.0            1.268e+04   4380.448      2.894      0.004    4091.619    2.13e+04\n",
       "3026.0            9521.4095   3885.510      2.450      0.014    1905.218    1.71e+04\n",
       "3031.0            3433.1702   3896.852      0.881      0.378   -4205.253    1.11e+04\n",
       "3062.0            8372.4668   3560.764      2.351      0.019    1392.827    1.54e+04\n",
       "3093.0            9456.1844   4013.928      2.356      0.018    1588.274    1.73e+04\n",
       "3107.0            6693.9808   4904.836      1.365      0.172   -2920.245    1.63e+04\n",
       "3121.0            1.116e+04   3895.822      2.866      0.004    3527.714    1.88e+04\n",
       "3126.0            7440.8391   3566.309      2.086      0.037     450.331    1.44e+04\n",
       "3144.0             6.25e+04   4012.827     15.576      0.000    5.46e+04    7.04e+04\n",
       "3156.0            1.129e+04   4493.279      2.513      0.012    2482.732    2.01e+04\n",
       "3157.0            8993.0023   3828.880      2.349      0.019    1487.815    1.65e+04\n",
       "3170.0            1.304e+04   3799.293      3.433      0.001    5597.506    2.05e+04\n",
       "3178.0            1.137e+04   4347.953      2.616      0.009    2851.078    1.99e+04\n",
       "3206.0            4493.9310   3713.576      1.210      0.226   -2785.242    1.18e+04\n",
       "3229.0            1.114e+04   4280.634      2.602      0.009    2748.862    1.95e+04\n",
       "3235.0            7123.2217   3595.742      1.981      0.048      75.020    1.42e+04\n",
       "3246.0            1.031e+04   4194.329      2.458      0.014    2087.307    1.85e+04\n",
       "3248.0            8084.0530   3694.773      2.188      0.029     841.736    1.53e+04\n",
       "3282.0            -1.52e+04   3154.401     -4.819      0.000   -2.14e+04   -9018.369\n",
       "3362.0            2024.3697   4165.120      0.486      0.627   -6139.899    1.02e+04\n",
       "3372.0            9456.8896   4228.253      2.237      0.025    1168.871    1.77e+04\n",
       "3422.0            7340.0009   3512.525      2.090      0.037     454.918    1.42e+04\n",
       "3497.0            1221.2387   3007.116      0.406      0.685   -4673.166    7115.643\n",
       "3502.0            6705.2576   3499.979      1.916      0.055    -155.233    1.36e+04\n",
       "3504.0            5886.5006   3980.714      1.479      0.139   -1916.304    1.37e+04\n",
       "3505.0            3746.5464   3176.698      1.179      0.238   -2480.265    9973.358\n",
       "3532.0            9082.7373   3451.767      2.631      0.009    2316.749    1.58e+04\n",
       "3574.0            1.135e+04   5613.448      2.022      0.043     347.319    2.24e+04\n",
       "3580.0            5970.7232   3441.810      1.735      0.083    -775.749    1.27e+04\n",
       "3612.0             1.31e+04   4548.023      2.880      0.004    4181.503     2.2e+04\n",
       "3619.0            8659.8696   4076.577      2.124      0.034     669.159    1.67e+04\n",
       "3622.0            1.137e+04   4311.284      2.637      0.008    2917.477    1.98e+04\n",
       "3639.0            3712.0637   3103.375      1.196      0.232   -2371.024    9795.151\n",
       "3650.0           -4218.2473   2945.040     -1.432      0.152   -9990.974    1554.479\n",
       "3662.0            2096.9656   2973.505      0.705      0.481   -3731.557    7925.488\n",
       "3734.0            -618.7346   3131.546     -0.198      0.843   -6757.041    5519.571\n",
       "3735.0            1714.5441   3273.573      0.524      0.600   -4702.157    8131.245\n",
       "3761.0            2948.2889   3076.541      0.958      0.338   -3082.201    8978.778\n",
       "3779.0            8398.0005   3769.925      2.228      0.026    1008.373    1.58e+04\n",
       "3781.0            9848.7736   4341.478      2.269      0.023    1338.816    1.84e+04\n",
       "3782.0            2652.2226   3142.421      0.844      0.399   -3507.401    8811.846\n",
       "3786.0            9975.5723   3988.454      2.501      0.012    2157.595    1.78e+04\n",
       "3796.0            8412.7977   3868.706      2.175      0.030     829.545     1.6e+04\n",
       "3821.0            1.076e+04   4284.698      2.510      0.012    2356.627    1.92e+04\n",
       "3835.0             325.3667   3469.095      0.094      0.925   -6474.587    7125.321\n",
       "3839.0            1627.1748   3742.626      0.435      0.664   -5708.942    8963.291\n",
       "3840.0            9349.7452   3871.757      2.415      0.016    1760.512    1.69e+04\n",
       "3895.0            1.011e+04   4002.931      2.526      0.012    2266.773     1.8e+04\n",
       "3908.0            1.179e+04   5017.974      2.350      0.019    1956.045    2.16e+04\n",
       "3911.0            1074.0551   3033.956      0.354      0.723   -4872.960    7021.070\n",
       "3917.0            1.084e+04   4187.474      2.589      0.010    2634.498    1.91e+04\n",
       "3946.0            1.182e+04   4349.230      2.717      0.007    3293.217    2.03e+04\n",
       "3971.0            8785.2598   3826.746      2.296      0.022    1284.255    1.63e+04\n",
       "3980.0            2.001e+04   3707.464      5.396      0.000    1.27e+04    2.73e+04\n",
       "4034.0            3089.9753   3148.117      0.982      0.326   -3080.813    9260.764\n",
       "4036.0            1.078e+04   4106.787      2.626      0.009    2734.110    1.88e+04\n",
       "4040.0            9522.3439   3858.347      2.468      0.014    1959.397    1.71e+04\n",
       "4058.0            9224.0234   3774.366      2.444      0.015    1825.692    1.66e+04\n",
       "4060.0           -1.239e+04   3046.591     -4.068      0.000   -1.84e+04   -6422.563\n",
       "4062.0            1.461e+04   4513.229      3.238      0.001    5765.432    2.35e+04\n",
       "4077.0            1.029e+04   4923.332      2.090      0.037     639.909    1.99e+04\n",
       "4087.0           -2.318e+04   3393.341     -6.830      0.000   -2.98e+04   -1.65e+04\n",
       "4091.0            8767.6889   4182.215      2.096      0.036     569.911     1.7e+04\n",
       "4127.0            5139.3960   3231.497      1.590      0.112   -1194.830    1.15e+04\n",
       "4138.0            1.158e+04   4678.133      2.475      0.013    2406.218    2.07e+04\n",
       "4162.0            9400.3071   4301.461      2.185      0.029     968.789    1.78e+04\n",
       "4186.0             1.14e+04   4217.295      2.702      0.007    3130.468    1.97e+04\n",
       "4194.0           -1943.8093   3510.323     -0.554      0.580   -8824.576    4936.957\n",
       "4199.0           -5041.3250   3091.922     -1.630      0.103   -1.11e+04    1019.313\n",
       "4213.0            8197.8902   3566.600      2.299      0.022    1206.811    1.52e+04\n",
       "4222.0            5080.5991   3227.534      1.574      0.115   -1245.859    1.14e+04\n",
       "4223.0            8439.7735   3678.170      2.295      0.022    1230.000    1.56e+04\n",
       "4251.0            1.088e+04   4135.671      2.630      0.009    2769.630     1.9e+04\n",
       "4265.0            1.008e+04   4108.597      2.453      0.014    2024.501    1.81e+04\n",
       "4274.0            5602.3767   3433.578      1.632      0.103   -1127.959    1.23e+04\n",
       "4321.0            2518.2649   3032.594      0.830      0.406   -3426.080    8462.610\n",
       "4335.0            6984.8365   4954.407      1.410      0.159   -2726.555    1.67e+04\n",
       "4340.0            6389.0278   3424.452      1.866      0.062    -323.419    1.31e+04\n",
       "4371.0            4744.8951   3344.844      1.419      0.156   -1811.509    1.13e+04\n",
       "4415.0            1.048e+04   4162.124      2.519      0.012    2326.029    1.86e+04\n",
       "4450.0            1.042e+04   4042.227      2.579      0.010    2501.120    1.83e+04\n",
       "4476.0            3409.8999   3264.845      1.044      0.296   -2989.694    9809.493\n",
       "4510.0           -1483.8606   2986.044     -0.497      0.619   -7336.961    4369.240\n",
       "4520.0            5466.5214   3271.905      1.671      0.095    -946.911    1.19e+04\n",
       "4551.0            9684.9935   7598.552      1.275      0.202   -5209.325    2.46e+04\n",
       "4568.0            7745.7072   3724.777      2.080      0.038     444.577     1.5e+04\n",
       "4579.0            1.293e+04   4538.177      2.850      0.004    4036.981    2.18e+04\n",
       "4585.0            1.123e+04   4250.879      2.642      0.008    2899.048    1.96e+04\n",
       "4595.0            7537.0673   3563.928      2.115      0.034     551.227    1.45e+04\n",
       "4600.0            6442.9841   3535.061      1.823      0.068    -486.273    1.34e+04\n",
       "4607.0            1.098e+04   4160.760      2.639      0.008    2823.984    1.91e+04\n",
       "4608.0            8172.4690   3636.861      2.247      0.025    1043.669    1.53e+04\n",
       "4622.0            1427.8834   2973.671      0.480      0.631   -4400.964    7256.731\n",
       "4623.0            1.065e+04   4180.459      2.547      0.011    2453.751    1.88e+04\n",
       "4768.0            7792.0053   3781.774      2.060      0.039     379.153    1.52e+04\n",
       "4771.0            1.153e+04   4273.534      2.697      0.007    3149.114    1.99e+04\n",
       "4800.0            1.009e+04   4286.046      2.354      0.019    1689.827    1.85e+04\n",
       "4802.0             1.02e+04   4013.159      2.541      0.011    2331.904    1.81e+04\n",
       "4807.0            9213.6890   3923.273      2.348      0.019    1523.476    1.69e+04\n",
       "4839.0           -1.455e+05   4319.284    -33.685      0.000   -1.54e+05   -1.37e+05\n",
       "4843.0            1.067e+04   3887.642      2.745      0.006    3051.533    1.83e+04\n",
       "4881.0            5934.0337   3330.016      1.782      0.075    -593.304    1.25e+04\n",
       "4900.0            1.034e+04   4039.223      2.559      0.011    2419.087    1.83e+04\n",
       "4926.0            4935.9631   3265.920      1.511      0.131   -1465.736    1.13e+04\n",
       "4941.0            7424.2795   3741.649      1.984      0.047      90.079    1.48e+04\n",
       "4961.0           -5213.7396   3833.242     -1.360      0.174   -1.27e+04    2299.997\n",
       "4988.0            1.739e+04   4299.529      4.045      0.000    8963.828    2.58e+04\n",
       "4993.0            1.283e+04   4538.407      2.828      0.005    3938.733    2.17e+04\n",
       "5018.0           -2996.3265   2963.717     -1.011      0.312   -8805.664    2813.011\n",
       "5020.0            1.124e+04   3584.056      3.137      0.002    4218.320    1.83e+04\n",
       "5027.0            5890.7880   3367.220      1.749      0.080    -709.475    1.25e+04\n",
       "5032.0            8682.1785   3795.147      2.288      0.022    1243.113    1.61e+04\n",
       "5043.0            6755.2178   3532.992      1.912      0.056    -169.984    1.37e+04\n",
       "5046.0           -6585.9934   2994.407     -2.199      0.028   -1.25e+04    -716.501\n",
       "5047.0            4.461e+04   4003.739     11.142      0.000    3.68e+04    5.25e+04\n",
       "5065.0            1.074e+04   4425.978      2.427      0.015    2067.109    1.94e+04\n",
       "5071.0            9434.0249   4306.851      2.190      0.029     991.941    1.79e+04\n",
       "5073.0           -2.062e+05   6352.355    -32.459      0.000   -2.19e+05   -1.94e+05\n",
       "5087.0            5704.1523   3525.290      1.618      0.106   -1205.953    1.26e+04\n",
       "5109.0            1.138e+04   4303.060      2.645      0.008    2944.822    1.98e+04\n",
       "5116.0            6984.6176   3536.143      1.975      0.048      53.239    1.39e+04\n",
       "5122.0            5068.8879   3304.120      1.534      0.125   -1407.690    1.15e+04\n",
       "5134.0              65.7087   3427.998      0.019      0.985   -6653.689    6785.107\n",
       "5142.0            7326.5923   4310.388      1.700      0.089   -1122.423    1.58e+04\n",
       "5165.0            1.177e+04   4553.903      2.584      0.010    2840.692    2.07e+04\n",
       "5169.0             2.07e+04   4033.456      5.132      0.000    1.28e+04    2.86e+04\n",
       "5174.0            8533.9796   3945.822      2.163      0.031     799.568    1.63e+04\n",
       "5179.0            1.093e+04   4116.086      2.656      0.008    2862.704     1.9e+04\n",
       "5181.0            1.125e+04   4306.955      2.613      0.009    2811.335    1.97e+04\n",
       "5187.0            1.166e+04   4521.494      2.579      0.010    2797.023    2.05e+04\n",
       "5229.0             -63.7561   2986.566     -0.021      0.983   -5917.879    5790.367\n",
       "5234.0           -7623.9666   3323.390     -2.294      0.022   -1.41e+04   -1109.617\n",
       "5237.0            9923.3867   3942.736      2.517      0.012    2195.024    1.77e+04\n",
       "5252.0            7558.9349   3558.107      2.124      0.034     584.503    1.45e+04\n",
       "5254.0            7539.1361   3570.188      2.112      0.035     541.024    1.45e+04\n",
       "5306.0            2084.0427   2954.633      0.705      0.481   -3707.487    7875.572\n",
       "5338.0            9445.9791   3859.542      2.447      0.014    1880.689     1.7e+04\n",
       "5377.0            1.154e+04   4349.504      2.653      0.008    3013.763    2.01e+04\n",
       "5439.0            8864.7143   4057.848      2.185      0.029     910.715    1.68e+04\n",
       "5456.0            1.274e+04   4551.099      2.800      0.005    3822.865    2.17e+04\n",
       "5464.0            7275.3968   4173.456      1.743      0.081    -905.213    1.55e+04\n",
       "5476.0            1.339e+04   4581.284      2.922      0.003    4407.281    2.24e+04\n",
       "5492.0           -4549.3336   3052.005     -1.491      0.136   -1.05e+04    1433.060\n",
       "5496.0            8244.9060   3727.831      2.212      0.027     937.791    1.56e+04\n",
       "5505.0            1.081e+04   4241.696      2.548      0.011    2494.547    1.91e+04\n",
       "5518.0            8692.2261   4307.097      2.018      0.044     249.661    1.71e+04\n",
       "5520.0            8145.9612   3692.089      2.206      0.027     908.905    1.54e+04\n",
       "5545.0             1.01e+04   4060.744      2.487      0.013    2138.140    1.81e+04\n",
       "5568.0            1.208e+04   3810.286      3.172      0.002    4615.917    1.96e+04\n",
       "5569.0             1.09e+04   4177.942      2.609      0.009    2708.894    1.91e+04\n",
       "5578.0            1.051e+04   4029.722      2.609      0.009    2612.725    1.84e+04\n",
       "5581.0             1.02e+04   3910.445      2.609      0.009    2536.038    1.79e+04\n",
       "5589.0             -78.8477   2944.356     -0.027      0.979   -5850.234    5692.538\n",
       "5597.0            1.083e+04   4249.786      2.549      0.011    2504.453    1.92e+04\n",
       "5606.0           -2.861e+04   3146.602     -9.092      0.000   -3.48e+04   -2.24e+04\n",
       "5639.0            1.236e+04   4328.856      2.856      0.004    3878.499    2.08e+04\n",
       "5667.0            1.291e+04   4602.692      2.805      0.005    3890.075    2.19e+04\n",
       "5690.0            1.004e+04   3981.493      2.522      0.012    2238.699    1.78e+04\n",
       "5709.0            9814.3704   4087.748      2.401      0.016    1801.762    1.78e+04\n",
       "5726.0            1.092e+04   4112.433      2.655      0.008    2856.723     1.9e+04\n",
       "5764.0            5926.0852   3214.035      1.844      0.065    -373.911    1.22e+04\n",
       "5772.0            8435.6372   3722.716      2.266      0.023    1138.547    1.57e+04\n",
       "5860.0           -1.639e+04   3104.532     -5.278      0.000   -2.25e+04   -1.03e+04\n",
       "5878.0            9767.1273   3351.289      2.914      0.004    3198.091    1.63e+04\n",
       "5903.0            8484.1600   3929.427      2.159      0.031     781.885    1.62e+04\n",
       "5905.0            5019.2603   3455.387      1.453      0.146   -1753.823    1.18e+04\n",
       "5959.0            1950.1034   3270.756      0.596      0.551   -4461.076    8361.283\n",
       "6008.0            2.543e+04   3008.929      8.450      0.000    1.95e+04    3.13e+04\n",
       "6034.0             835.4076   3061.073      0.273      0.785   -5164.761    6835.576\n",
       "6035.0            9294.2228   4264.637      2.179      0.029     934.885    1.77e+04\n",
       "6036.0            1789.1188   3089.325      0.579      0.563   -4266.429    7844.667\n",
       "6039.0            9810.8574   3952.369      2.482      0.013    2063.612    1.76e+04\n",
       "6044.0            1.186e+04   4578.145      2.590      0.010    2881.663    2.08e+04\n",
       "6066.0           -3.183e+04   4377.392     -7.272      0.000   -4.04e+04   -2.33e+04\n",
       "6078.0             1.12e+04   3846.718      2.911      0.004    3656.829    1.87e+04\n",
       "6081.0           -4720.5814   2959.879     -1.595      0.111   -1.05e+04    1081.232\n",
       "60893.0           9199.3006   5076.118      1.812      0.070    -750.663    1.91e+04\n",
       "6097.0            1.263e+04   4369.572      2.890      0.004    4064.557    2.12e+04\n",
       "6102.0            9052.2338   4019.358      2.252      0.024    1173.680    1.69e+04\n",
       "6104.0           -1187.3521   3482.045     -0.341      0.733   -8012.690    5637.986\n",
       "6109.0            3213.2067   3050.268      1.053      0.292   -2765.782    9192.195\n",
       "6127.0            2228.4004   3709.238      0.601      0.548   -5042.271    9499.072\n",
       "61552.0          -1975.3039   4309.096     -0.458      0.647   -1.04e+04    6471.180\n",
       "6158.0            6941.3184   3594.907      1.931      0.054    -105.246     1.4e+04\n",
       "6171.0            9725.1938   3892.892      2.498      0.012    2094.532    1.74e+04\n",
       "61780.0           6319.1379   5239.208      1.206      0.228   -3950.508    1.66e+04\n",
       "6207.0            1.056e+04   4083.381      2.587      0.010    2558.543    1.86e+04\n",
       "6214.0            9359.9377   3855.444      2.428      0.015    1802.682    1.69e+04\n",
       "6216.0            1.232e+04   4532.307      2.719      0.007    3437.128    2.12e+04\n",
       "62221.0           9709.7805   5083.029      1.910      0.056    -253.730    1.97e+04\n",
       "6259.0            1.154e+04   4450.023      2.593      0.010    2818.012    2.03e+04\n",
       "62599.0          -2.896e+04   5015.920     -5.773      0.000   -3.88e+04   -1.91e+04\n",
       "6266.0             1.33e+04   3145.921      4.229      0.000    7137.443    1.95e+04\n",
       "6268.0           -1276.6700   3136.449     -0.407      0.684   -7424.587    4871.247\n",
       "6288.0            7204.7187   3492.523      2.063      0.039     358.843    1.41e+04\n",
       "6297.0            8422.0184   3780.383      2.228      0.026    1011.893    1.58e+04\n",
       "6307.0           -1.498e+04   3955.696     -3.788      0.000   -2.27e+04   -7231.134\n",
       "6313.0             1.04e+04   5133.926      2.025      0.043     332.897    2.05e+04\n",
       "6314.0            1.172e+04   4311.513      2.718      0.007    3269.269    2.02e+04\n",
       "6326.0            1065.9030   2959.552      0.360      0.719   -4735.269    6867.075\n",
       "6349.0            9045.9152   3821.348      2.367      0.018    1555.492    1.65e+04\n",
       "6357.0            1.133e+04   4303.700      2.632      0.009    2889.397    1.98e+04\n",
       "6375.0            1.517e+04   4101.820      3.699      0.000    7133.811    2.32e+04\n",
       "6376.0            1.082e+04   4258.242      2.541      0.011    2474.895    1.92e+04\n",
       "6379.0             942.3993   6905.998      0.136      0.891   -1.26e+04    1.45e+04\n",
       "6386.0            9756.4149   3934.848      2.479      0.013    2043.513    1.75e+04\n",
       "6403.0            7896.0643   3900.575      2.024      0.043     250.343    1.55e+04\n",
       "6410.0            1.246e+04   4473.422      2.786      0.005    3695.738    2.12e+04\n",
       "6416.0            8278.6855   3814.267      2.170      0.030     802.142    1.58e+04\n",
       "6424.0             1.07e+04   4143.301      2.581      0.010    2574.325    1.88e+04\n",
       "6433.0            1.238e+04   4473.315      2.767      0.006    3611.317    2.11e+04\n",
       "6435.0            7739.4594   3195.857      2.422      0.015    1475.094     1.4e+04\n",
       "6492.0            3058.4908   3102.949      0.986      0.324   -3023.761    9140.743\n",
       "6497.0             1.16e+04   4271.985      2.714      0.007    3222.039       2e+04\n",
       "6500.0            1.238e+04   7941.667      1.559      0.119   -3184.200    2.79e+04\n",
       "6509.0            8036.7520   3678.547      2.185      0.029     826.241    1.52e+04\n",
       "6527.0            1.133e+04   4450.159      2.546      0.011    2605.843    2.01e+04\n",
       "6528.0            1.032e+04   4188.627      2.463      0.014    2107.083    1.85e+04\n",
       "6531.0           -4200.0683   3016.386     -1.392      0.164   -1.01e+04    1712.508\n",
       "6532.0            6540.4623   3539.456      1.848      0.065    -397.410    1.35e+04\n",
       "6543.0            1.105e+04   4207.876      2.626      0.009    2801.752    1.93e+04\n",
       "6548.0            1.103e+04   4220.182      2.613      0.009    2754.830    1.93e+04\n",
       "6550.0            9640.8987   4124.209      2.338      0.019    1556.821    1.77e+04\n",
       "6552.0            1.098e+04   4402.914      2.494      0.013    2350.328    1.96e+04\n",
       "6565.0             106.4800   3104.123      0.034      0.973   -5978.073    6191.033\n",
       "6571.0            9703.4230   3948.030      2.458      0.014    1964.683    1.74e+04\n",
       "6573.0            9345.6370   3833.374      2.438      0.015    1831.641    1.69e+04\n",
       "6641.0            9731.3147   5847.278      1.664      0.096   -1730.239    2.12e+04\n",
       "6649.0            1.201e+04   4326.600      2.775      0.006    3526.284    2.05e+04\n",
       "6730.0            1.398e+04   3155.542      4.431      0.000    7797.210    2.02e+04\n",
       "6731.0            3159.2103   3140.716      1.006      0.314   -2997.071    9315.492\n",
       "6742.0            1.325e+04   5704.105      2.324      0.020    2073.387    2.44e+04\n",
       "6745.0            1.087e+04   4172.152      2.606      0.009    2694.673    1.91e+04\n",
       "6756.0            1.218e+04   4382.072      2.779      0.005    3587.033    2.08e+04\n",
       "6765.0           -1623.7257   3020.376     -0.538      0.591   -7544.123    4296.672\n",
       "6768.0            1.376e+04   4605.552      2.987      0.003    4729.602    2.28e+04\n",
       "6774.0           -1.068e+04   3459.448     -3.087      0.002   -1.75e+04   -3899.693\n",
       "6797.0            1.167e+04   4644.903      2.511      0.012    2560.590    2.08e+04\n",
       "6803.0             1.15e+04   4294.555      2.677      0.007    3077.736    1.99e+04\n",
       "6821.0            9550.7894   3972.501      2.404      0.016    1764.084    1.73e+04\n",
       "6830.0            1.067e+04   4175.203      2.555      0.011    2483.547    1.89e+04\n",
       "6845.0            2794.6874   3024.603      0.924      0.356   -3133.995    8723.370\n",
       "6848.0            1.194e+04   4512.653      2.647      0.008    3099.269    2.08e+04\n",
       "6873.0            4102.7272   4083.423      1.005      0.315   -3901.402    1.21e+04\n",
       "6900.0            4912.8458   3212.206      1.529      0.126   -1383.567    1.12e+04\n",
       "6908.0            5531.9600   3268.028      1.693      0.091    -873.873    1.19e+04\n",
       "6994.0            1.113e+04   4166.476      2.672      0.008    2967.783    1.93e+04\n",
       "7045.0            7685.9806   4363.589      1.761      0.078    -867.317    1.62e+04\n",
       "7065.0            1.265e+04   3474.516      3.642      0.000    5842.833    1.95e+04\n",
       "7085.0            1.076e+04   3676.094      2.926      0.003    3549.393     1.8e+04\n",
       "7107.0            5981.2294   3491.103      1.713      0.087    -861.863    1.28e+04\n",
       "7116.0            1.242e+04   4487.009      2.768      0.006    3626.309    2.12e+04\n",
       "7117.0            1.293e+04   5088.069      2.541      0.011    2954.216    2.29e+04\n",
       "7121.0                1e+04   3995.575      2.504      0.012    2172.638    1.78e+04\n",
       "7127.0            1.031e+04   4620.297      2.231      0.026    1252.392    1.94e+04\n",
       "7139.0            8759.6584   3847.940      2.276      0.023    1217.111    1.63e+04\n",
       "7146.0            1.058e+04   4082.559      2.591      0.010    2576.987    1.86e+04\n",
       "7163.0            1.392e+04   4153.892      3.351      0.001    5778.922    2.21e+04\n",
       "7180.0            5301.0622   3502.912      1.513      0.130   -1565.178    1.22e+04\n",
       "7183.0            4120.5527   3188.548      1.292      0.196   -2129.486    1.04e+04\n",
       "7228.0            1.969e+04   4266.255      4.614      0.000    1.13e+04     2.8e+04\n",
       "7232.0            1.219e+04   5085.387      2.396      0.017    2217.046    2.22e+04\n",
       "7250.0            8856.4192   4031.036      2.197      0.028     954.976    1.68e+04\n",
       "7257.0            3.532e+04   3220.846     10.966      0.000     2.9e+04    4.16e+04\n",
       "7260.0            9192.0479   3774.724      2.435      0.015    1793.015    1.66e+04\n",
       "7267.0            1.111e+04   4501.757      2.469      0.014    2288.679    1.99e+04\n",
       "7268.0            1.087e+04   4196.367      2.591      0.010    2649.108    1.91e+04\n",
       "7281.0            1.135e+04   4815.400      2.358      0.018    1915.397    2.08e+04\n",
       "7291.0            5098.6077   3246.491      1.570      0.116   -1265.010    1.15e+04\n",
       "7343.0            7842.5035   3534.507      2.219      0.027     914.333    1.48e+04\n",
       "7346.0            7723.8849   3633.104      2.126      0.034     602.448    1.48e+04\n",
       "7401.0            9328.3374   3905.660      2.388      0.017    1672.650     1.7e+04\n",
       "7409.0            7397.1967   3546.545      2.086      0.037     445.428    1.43e+04\n",
       "7420.0            5897.1210   3362.452      1.754      0.079    -693.797    1.25e+04\n",
       "7435.0            1078.3592   3147.847      0.343      0.732   -5091.901    7248.619\n",
       "7466.0             1.04e+04   4250.324      2.446      0.014    2063.728    1.87e+04\n",
       "7486.0            9520.0452   3911.185      2.434      0.015    1853.528    1.72e+04\n",
       "7503.0            8453.4334   4930.048      1.715      0.086   -1210.210    1.81e+04\n",
       "7506.0            8867.4155   3529.618      2.512      0.012    1948.827    1.58e+04\n",
       "7537.0            1.144e+04   4255.597      2.689      0.007    3102.766    1.98e+04\n",
       "7549.0            5151.4501   3281.945      1.570      0.117   -1281.661    1.16e+04\n",
       "7554.0            1.029e+04   4183.920      2.459      0.014    2089.086    1.85e+04\n",
       "7557.0            4008.4871   3177.140      1.262      0.207   -2219.191    1.02e+04\n",
       "7585.0           -1.479e+04   3233.253     -4.575      0.000   -2.11e+04   -8454.038\n",
       "7602.0            1.019e+04   4004.646      2.544      0.011    2339.317     1.8e+04\n",
       "7620.0            1.137e+04   4519.517      2.516      0.012    2510.606    2.02e+04\n",
       "7636.0            1.076e+04   4116.216      2.614      0.009    2690.138    1.88e+04\n",
       "7646.0            1.076e+04   4153.781      2.591      0.010    2620.102    1.89e+04\n",
       "7658.0            3359.7419   3088.730      1.088      0.277   -2694.638    9414.122\n",
       "7683.0              1.1e+04   4267.354      2.579      0.010    2640.333    1.94e+04\n",
       "7685.0            8204.8310   3806.973      2.155      0.031     742.585    1.57e+04\n",
       "7692.0            5703.7807   3354.251      1.700      0.089    -871.062    1.23e+04\n",
       "7762.0            9143.7308   3775.228      2.422      0.015    1743.710    1.65e+04\n",
       "7772.0           -4709.9246   3006.136     -1.567      0.117   -1.06e+04    1182.559\n",
       "7773.0            9293.3462   3912.452      2.375      0.018    1624.345     1.7e+04\n",
       "7777.0            5454.0280   3322.397      1.642      0.101   -1058.375     1.2e+04\n",
       "7835.0            1.042e+04   4050.637      2.572      0.010    2480.063    1.84e+04\n",
       "7873.0            8293.3585   3686.192      2.250      0.024    1067.862    1.55e+04\n",
       "7883.0            2715.7040   3007.334      0.903      0.367   -3179.127    8610.535\n",
       "7904.0            1919.1878   3040.146      0.631      0.528   -4039.961    7878.336\n",
       "7906.0            1.391e+04   4332.521      3.210      0.001    5416.523    2.24e+04\n",
       "7921.0            8796.3042   3660.684      2.403      0.016    1620.807     1.6e+04\n",
       "7923.0            9824.0520   4152.989      2.366      0.018    1683.561     1.8e+04\n",
       "7935.0            7306.5846   3602.685      2.028      0.043     244.774    1.44e+04\n",
       "7938.0            6665.8266   3616.131      1.843      0.065    -422.340    1.38e+04\n",
       "7985.0           -9857.0510   3135.543     -3.144      0.002    -1.6e+04   -3710.910\n",
       "8014.0            9728.9685   4012.019      2.425      0.015    1864.801    1.76e+04\n",
       "8030.0            1.285e+04   4423.355      2.904      0.004    4175.418    2.15e+04\n",
       "8046.0            8260.1941   3659.637      2.257      0.024    1086.748    1.54e+04\n",
       "8047.0            8521.1702   4219.749      2.019      0.043     249.820    1.68e+04\n",
       "8062.0            7568.8138   3635.126      2.082      0.037     443.413    1.47e+04\n",
       "8068.0           -5940.6066   3633.910     -1.635      0.102   -1.31e+04    1182.410\n",
       "8087.0            5782.3278   3379.903      1.711      0.087    -842.797    1.24e+04\n",
       "8095.0            8573.5924   3747.465      2.288      0.022    1227.991    1.59e+04\n",
       "8096.0            1.191e+04   4342.888      2.743      0.006    3399.602    2.04e+04\n",
       "8109.0            9871.9424   3953.432      2.497      0.013    2122.614    1.76e+04\n",
       "8123.0             -63.1503   2943.464     -0.021      0.983   -5832.788    5706.488\n",
       "8150.0            1.101e+04   4189.962      2.627      0.009    2795.062    1.92e+04\n",
       "8163.0            3268.2742   3100.134      1.054      0.292   -2808.460    9345.008\n",
       "8176.0            1.265e+04   4772.087      2.651      0.008    3294.603     2.2e+04\n",
       "8202.0            5157.2125   3345.753      1.541      0.123   -1400.972    1.17e+04\n",
       "8214.0            3394.2616   3294.579      1.030      0.303   -3063.615    9852.138\n",
       "8215.0             -54.6148   3158.648     -0.017      0.986   -6246.045    6136.815\n",
       "8219.0            1.203e+04   4458.083      2.699      0.007    3294.795    2.08e+04\n",
       "8247.0            2375.9926   3170.269      0.749      0.454   -3838.218    8590.203\n",
       "8253.0            2666.7976   3125.567      0.853      0.394   -3459.789    8793.385\n",
       "8290.0            6743.8027   3815.948      1.767      0.077    -736.037    1.42e+04\n",
       "8293.0            3811.2373   3095.166      1.231      0.218   -2255.759    9878.234\n",
       "8304.0            1.042e+04   3872.462      2.692      0.007    2833.174     1.8e+04\n",
       "8334.0            1.273e+04   4602.405      2.766      0.006    3710.745    2.18e+04\n",
       "8348.0            8859.6583   3798.952      2.332      0.020    1413.135    1.63e+04\n",
       "8357.0            8358.1549   3677.448      2.273      0.023    1149.797    1.56e+04\n",
       "8358.0            4807.9611   3297.333      1.458      0.145   -1655.312    1.13e+04\n",
       "8446.0           -3937.5812   3829.797     -1.028      0.304   -1.14e+04    3569.403\n",
       "8460.0            1.303e+04   4819.099      2.705      0.007    3587.830    2.25e+04\n",
       "8463.0            9571.6362   3984.079      2.402      0.016    1762.235    1.74e+04\n",
       "8479.0            9664.9227   4452.608      2.171      0.030     937.133    1.84e+04\n",
       "8530.0            2.662e+04   3200.718      8.316      0.000    2.03e+04    3.29e+04\n",
       "8536.0            2847.7871   3111.934      0.915      0.360   -3252.076    8947.650\n",
       "8543.0            3.097e+04   4334.476      7.144      0.000    2.25e+04    3.95e+04\n",
       "8549.0            1064.3040   3405.646      0.313      0.755   -5611.280    7739.888\n",
       "8551.0            1.143e+04   4342.977      2.632      0.009    2917.232    1.99e+04\n",
       "8559.0            1.097e+04   4260.535      2.576      0.010    2622.868    1.93e+04\n",
       "8573.0            1.106e+04   4277.534      2.587      0.010    2679.397    1.94e+04\n",
       "8606.0            8852.5352   3572.763      2.478      0.013    1849.375    1.59e+04\n",
       "8607.0            1.211e+04   4438.488      2.727      0.006    3405.146    2.08e+04\n",
       "8648.0            8539.2383   3716.683      2.298      0.022    1253.975    1.58e+04\n",
       "8657.0            5903.3861   3583.915      1.647      0.100   -1121.633    1.29e+04\n",
       "8675.0            1.141e+04   5303.064      2.152      0.031    1018.708    2.18e+04\n",
       "8681.0            5266.6759   3251.575      1.620      0.105   -1106.905    1.16e+04\n",
       "8687.0            6525.4736   3566.937      1.829      0.067    -466.265    1.35e+04\n",
       "8692.0            5590.8047   3384.259      1.652      0.099   -1042.858    1.22e+04\n",
       "8699.0            9524.4686   3890.257      2.448      0.014    1898.974    1.71e+04\n",
       "8717.0            1.113e+04   4176.221      2.664      0.008    2939.597    1.93e+04\n",
       "8759.0            1.096e+04   4208.368      2.605      0.009    2713.368    1.92e+04\n",
       "8762.0             1.12e+04   3656.399      3.062      0.002    4028.997    1.84e+04\n",
       "8819.0            1.298e+04   4615.110      2.813      0.005    3937.940     2.2e+04\n",
       "8850.0            9181.1749   3848.090      2.386      0.017    1638.333    1.67e+04\n",
       "8852.0            9627.9364   3911.676      2.461      0.014    1960.456    1.73e+04\n",
       "8859.0            1.208e+04   4432.894      2.725      0.006    3390.509    2.08e+04\n",
       "8867.0            3661.8316   3646.213      1.004      0.315   -3485.301    1.08e+04\n",
       "8881.0            7769.9386   3624.646      2.144      0.032     665.082    1.49e+04\n",
       "8958.0            8951.7960   3784.276      2.366      0.018    1534.039    1.64e+04\n",
       "8972.0           -8091.6358   3040.544     -2.661      0.008   -1.41e+04   -2131.707\n",
       "8990.0            5547.1038   3395.854      1.633      0.102   -1109.288    1.22e+04\n",
       "9004.0            1.154e+04   4462.107      2.587      0.010    2797.496    2.03e+04\n",
       "9016.0            7357.7328   3517.351      2.092      0.036     463.190    1.43e+04\n",
       "9048.0            4707.7775   3184.670      1.478      0.139   -1534.661     1.1e+04\n",
       "9051.0            6169.5902   3713.425      1.661      0.097   -1109.287    1.34e+04\n",
       "9071.0             877.5078   2975.713      0.295      0.768   -4955.343    6710.359\n",
       "9112.0            6934.4051   3514.220      1.973      0.048      45.999    1.38e+04\n",
       "9114.0            1029.6249   3180.276      0.324      0.746   -5204.201    7263.450\n",
       "9132.0            1.297e+04   5496.010      2.359      0.018    2192.891    2.37e+04\n",
       "9173.0            7935.3986   3848.198      2.062      0.039     392.346    1.55e+04\n",
       "9180.0            1.117e+04   4295.561      2.599      0.009    2745.212    1.96e+04\n",
       "9186.0            7783.4450   3611.743      2.155      0.031     703.880    1.49e+04\n",
       "9191.0            2427.7345   4179.686      0.581      0.561   -5765.085    1.06e+04\n",
       "9216.0             161.8933   2939.543      0.055      0.956   -5600.057    5923.844\n",
       "9217.0           -1862.4721   2944.618     -0.633      0.527   -7634.372    3909.427\n",
       "9225.0            9577.3064   3862.968      2.479      0.013    2005.301    1.71e+04\n",
       "9230.0            1.207e+04   4753.482      2.540      0.011    2756.935    2.14e+04\n",
       "9259.0            1.294e+04   4561.875      2.837      0.005    4000.503    2.19e+04\n",
       "9293.0            1.064e+04   4105.021      2.592      0.010    2592.742    1.87e+04\n",
       "9299.0            3233.5415   3419.949      0.945      0.344   -3470.080    9937.163\n",
       "9308.0             1.15e+04   4307.956      2.670      0.008    3059.867    1.99e+04\n",
       "9311.0            8250.1694   4502.595      1.832      0.067    -575.601    1.71e+04\n",
       "9313.0            5615.0259   3514.373      1.598      0.110   -1273.679    1.25e+04\n",
       "9325.0            8184.6337   3690.329      2.218      0.027     951.026    1.54e+04\n",
       "9332.0            5773.7851   3330.914      1.733      0.083    -755.313    1.23e+04\n",
       "9340.0             379.0420   3897.387      0.097      0.923   -7260.429    8018.513\n",
       "9372.0            1.291e+04   4648.350      2.777      0.005    3798.287     2.2e+04\n",
       "9411.0            6573.8418   3745.839      1.755      0.079    -768.573    1.39e+04\n",
       "9459.0            1.104e+04   3107.094      3.554      0.000    4953.235    1.71e+04\n",
       "9465.0            1.003e+04   3232.899      3.103      0.002    3693.095    1.64e+04\n",
       "9472.0            5266.1813   3238.912      1.626      0.104   -1082.580    1.16e+04\n",
       "9483.0            5490.3001   3265.877      1.681      0.093    -911.315    1.19e+04\n",
       "9563.0           -1.465e+04   4841.183     -3.026      0.002   -2.41e+04   -5157.834\n",
       "9590.0            1.113e+04   4184.411      2.660      0.008    2930.217    1.93e+04\n",
       "9598.0            5799.6371   3924.945      1.478      0.140   -1893.852    1.35e+04\n",
       "9599.0            7653.2768   3540.545      2.162      0.031     713.270    1.46e+04\n",
       "9602.0            5222.4602   4344.338      1.202      0.229   -3293.102    1.37e+04\n",
       "9619.0            1.294e+04   4579.938      2.826      0.005    3967.053    2.19e+04\n",
       "9643.0            2786.9648   3094.998      0.900      0.368   -3279.702    8853.631\n",
       "9650.0             696.8935   2942.311      0.237      0.813   -5070.484    6464.271\n",
       "9653.0           -6797.1413   5279.218     -1.288      0.198   -1.71e+04    3550.929\n",
       "9667.0            2075.9287   2998.392      0.692      0.489   -3801.376    7953.233\n",
       "9698.0            1.078e+04   4118.761      2.617      0.009    2705.388    1.89e+04\n",
       "9699.0             1.07e+04   3842.617      2.784      0.005    3164.148    1.82e+04\n",
       "9719.0            3103.5465   3044.700      1.019      0.308   -2864.530    9071.623\n",
       "9742.0            6948.3637   3504.437      1.983      0.047      79.134    1.38e+04\n",
       "9761.0            8658.0344   3760.543      2.302      0.021    1286.797     1.6e+04\n",
       "9771.0            1994.4442   3022.632      0.660      0.509   -3930.375    7919.264\n",
       "9772.0            1.174e+04   4284.250      2.741      0.006    3344.095    2.01e+04\n",
       "9778.0            5796.5913   3278.106      1.768      0.077    -628.996    1.22e+04\n",
       "9799.0             722.1063   3107.287      0.232      0.816   -5368.648    6812.861\n",
       "9815.0            8538.2025   3759.120      2.271      0.023    1169.755    1.59e+04\n",
       "9818.0           -3.283e+04   3444.849     -9.531      0.000   -3.96e+04   -2.61e+04\n",
       "9837.0            1.252e+04   4529.032      2.765      0.006    3644.228    2.14e+04\n",
       "9922.0            5934.5649   3333.050      1.781      0.075    -598.720    1.25e+04\n",
       "9954.0             775.4874   3649.467      0.212      0.832   -6378.023    7928.998\n",
       "9963.0            5664.6187   3452.978      1.641      0.101   -1103.743    1.24e+04\n",
       "9988.0            1.221e+04   4531.381      2.694      0.007    3327.463    2.11e+04\n",
       "gspilltecIVX1981    -0.0528      0.065     -0.807      0.419      -0.181       0.075\n",
       "gspilltecIVX1982    -0.0262      0.065     -0.406      0.685      -0.153       0.100\n",
       "gspilltecIVX1983    -0.0435      0.064     -0.683      0.494      -0.168       0.081\n",
       "gspilltecIVX1984    -0.1019      0.063     -1.606      0.108      -0.226       0.022\n",
       "gspilltecIVX1985    -0.1423      0.064     -2.236      0.025      -0.267      -0.018\n",
       "gspilltecIVX1986    -0.1942      0.064     -3.030      0.002      -0.320      -0.069\n",
       "gspilltecIVX1987    -0.2053      0.065     -3.167      0.002      -0.332      -0.078\n",
       "gspilltecIVX1988    -0.2385      0.065     -3.643      0.000      -0.367      -0.110\n",
       "gspilltecIVX1989    -0.2438      0.066     -3.672      0.000      -0.374      -0.114\n",
       "gspilltecIVX1990    -0.2782      0.067     -4.138      0.000      -0.410      -0.146\n",
       "gspilltecIVX1991    -0.2604      0.068     -3.830      0.000      -0.394      -0.127\n",
       "gspilltecIVX1992    -0.2623      0.069     -3.810      0.000      -0.397      -0.127\n",
       "gspilltecIVX1993    -0.2406      0.070     -3.430      0.001      -0.378      -0.103\n",
       "gspilltecIVX1994    -0.2479      0.071     -3.482      0.000      -0.387      -0.108\n",
       "gspilltecIVX1995    -0.2435      0.073     -3.348      0.001      -0.386      -0.101\n",
       "gspilltecIVX1996    -0.2201      0.075     -2.945      0.003      -0.367      -0.074\n",
       "gspilltecIVX1997    -0.2168      0.077     -2.820      0.005      -0.368      -0.066\n",
       "gspilltecIVX1998    -0.1931      0.079     -2.446      0.014      -0.348      -0.038\n",
       "gspilltecIVX1999    -0.0952      0.081     -1.180      0.238      -0.253       0.063\n",
       "gspilltecIVX2000    -0.1748      0.082     -2.127      0.033      -0.336      -0.014\n",
       "gspilltecIVX2001    -0.2609      0.083     -3.134      0.002      -0.424      -0.098\n",
       "==============================================================================\n",
       "Omnibus:                    21422.439   Durbin-Watson:                   0.544\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         46586321.471\n",
       "Skew:                           9.916   Prob(JB):                         0.00\n",
       "Kurtosis:                     291.337   Cond. No.                     2.69e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.69e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tech spillovers model, firm FE's\n",
    "x_vars_fe = x_vars.drop(columns=drop_columns)\n",
    "\n",
    "year_model3 = sm.OLS(y_var,x_vars_fe).fit()\n",
    "year_model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db840de-c87c-4b76-8a30-c3cd1c80f6af",
   "metadata": {},
   "source": [
    "Results highly depend on the firm FE's. Wrong sign for tech, insignificant for product market. Unobserved firm-related variables play a role?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ad88ded-6932-4961-82f2-53719a5b2d75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.357</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.356</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   274.6</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:03:01</td>     <th>  Log-Likelihood:    </th> <td>-1.4594e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13385</td>      <th>  AIC:               </th>  <td>2.919e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 13357</td>      <th>  BIC:               </th>  <td>2.921e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    27</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>  946.3154</td> <td>  147.629</td> <td>    6.410</td> <td> 0.000</td> <td>  656.941</td> <td> 1235.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th>      <td>   -0.0600</td> <td>    0.102</td> <td>   -0.586</td> <td> 0.558</td> <td>   -0.260</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -11.1614</td> <td>    1.793</td> <td>   -6.225</td> <td> 0.000</td> <td>  -14.676</td> <td>   -7.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.5112</td> <td>    0.034</td> <td>   15.126</td> <td> 0.000</td> <td>    0.445</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.4669</td> <td>    0.048</td> <td>    9.689</td> <td> 0.000</td> <td>    0.372</td> <td>    0.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>  -39.4619</td> <td>    3.475</td> <td>  -11.355</td> <td> 0.000</td> <td>  -46.274</td> <td>  -32.650</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>   11.4031</td> <td>    0.470</td> <td>   24.284</td> <td> 0.000</td> <td>   10.483</td> <td>   12.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1981</th> <td>   -0.0376</td> <td>    0.136</td> <td>   -0.278</td> <td> 0.781</td> <td>   -0.303</td> <td>    0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1982</th> <td>   -0.0165</td> <td>    0.133</td> <td>   -0.124</td> <td> 0.902</td> <td>   -0.277</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1983</th> <td>    0.0010</td> <td>    0.129</td> <td>    0.008</td> <td> 0.994</td> <td>   -0.253</td> <td>    0.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1984</th> <td>   -0.0235</td> <td>    0.126</td> <td>   -0.186</td> <td> 0.852</td> <td>   -0.271</td> <td>    0.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1985</th> <td>   -0.0079</td> <td>    0.124</td> <td>   -0.064</td> <td> 0.949</td> <td>   -0.251</td> <td>    0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1986</th> <td>    0.0070</td> <td>    0.121</td> <td>    0.058</td> <td> 0.954</td> <td>   -0.230</td> <td>    0.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1987</th> <td>    0.0052</td> <td>    0.118</td> <td>    0.044</td> <td> 0.965</td> <td>   -0.227</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1988</th> <td>   -0.0086</td> <td>    0.117</td> <td>   -0.073</td> <td> 0.942</td> <td>   -0.239</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1989</th> <td>    0.0257</td> <td>    0.116</td> <td>    0.222</td> <td> 0.825</td> <td>   -0.201</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1990</th> <td>    0.0151</td> <td>    0.114</td> <td>    0.132</td> <td> 0.895</td> <td>   -0.209</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1991</th> <td>    0.0634</td> <td>    0.113</td> <td>    0.563</td> <td> 0.573</td> <td>   -0.157</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1992</th> <td>    0.0415</td> <td>    0.112</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.178</td> <td>    0.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1993</th> <td>    0.0532</td> <td>    0.111</td> <td>    0.479</td> <td> 0.632</td> <td>   -0.164</td> <td>    0.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1994</th> <td>    0.0527</td> <td>    0.111</td> <td>    0.476</td> <td> 0.634</td> <td>   -0.164</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1995</th> <td>    0.1206</td> <td>    0.110</td> <td>    1.098</td> <td> 0.272</td> <td>   -0.095</td> <td>    0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1996</th> <td>    0.1585</td> <td>    0.109</td> <td>    1.451</td> <td> 0.147</td> <td>   -0.056</td> <td>    0.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1997</th> <td>    0.2513</td> <td>    0.109</td> <td>    2.315</td> <td> 0.021</td> <td>    0.039</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1998</th> <td>    0.3606</td> <td>    0.108</td> <td>    3.339</td> <td> 0.001</td> <td>    0.149</td> <td>    0.572</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1999</th> <td>    0.4666</td> <td>    0.108</td> <td>    4.328</td> <td> 0.000</td> <td>    0.255</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX2000</th> <td>    0.5090</td> <td>    0.108</td> <td>    4.718</td> <td> 0.000</td> <td>    0.298</td> <td>    0.720</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX2001</th> <td>    0.3331</td> <td>    0.108</td> <td>    3.073</td> <td> 0.002</td> <td>    0.121</td> <td>    0.546</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>24023.522</td> <th>  Durbin-Watson:     </th>   <td>   0.353</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>61844043.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>12.758</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>335.022</td>  <th>  Cond. No.          </th>   <td>1.57e+04</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.57e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.357    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.356    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      274.6    \\\\\n",
       "\\textbf{Date:}             & Wed, 16 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     02:03:01     & \\textbf{  Log-Likelihood:    } & -1.4594e+05   \\\\\n",
       "\\textbf{No. Observations:} &       13385      & \\textbf{  AIC:               } &  2.919e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       13357      & \\textbf{  BIC:               } &  2.921e+05    \\\\\n",
       "\\textbf{Df Model:}         &          27      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &     946.3154  &      147.629     &     6.410  &         0.000        &      656.941    &     1235.689     \\\\\n",
       "\\textbf{gspillsicIV}      &      -0.0600  &        0.102     &    -0.586  &         0.558        &       -0.260    &        0.141     \\\\\n",
       "\\textbf{pat\\_count}       &     -11.1614  &        1.793     &    -6.225  &         0.000        &      -14.676    &       -7.647     \\\\\n",
       "\\textbf{rsales}           &       0.5112  &        0.034     &    15.126  &         0.000        &        0.445    &        0.577     \\\\\n",
       "\\textbf{rppent}           &       0.4669  &        0.048     &     9.689  &         0.000        &        0.372    &        0.561     \\\\\n",
       "\\textbf{emp}              &     -39.4619  &        3.475     &   -11.355  &         0.000        &      -46.274    &      -32.650     \\\\\n",
       "\\textbf{rxrd}             &      11.4031  &        0.470     &    24.284  &         0.000        &       10.483    &       12.324     \\\\\n",
       "\\textbf{gspillsicIVX1981} &      -0.0376  &        0.136     &    -0.278  &         0.781        &       -0.303    &        0.228     \\\\\n",
       "\\textbf{gspillsicIVX1982} &      -0.0165  &        0.133     &    -0.124  &         0.902        &       -0.277    &        0.244     \\\\\n",
       "\\textbf{gspillsicIVX1983} &       0.0010  &        0.129     &     0.008  &         0.994        &       -0.253    &        0.255     \\\\\n",
       "\\textbf{gspillsicIVX1984} &      -0.0235  &        0.126     &    -0.186  &         0.852        &       -0.271    &        0.224     \\\\\n",
       "\\textbf{gspillsicIVX1985} &      -0.0079  &        0.124     &    -0.064  &         0.949        &       -0.251    &        0.235     \\\\\n",
       "\\textbf{gspillsicIVX1986} &       0.0070  &        0.121     &     0.058  &         0.954        &       -0.230    &        0.244     \\\\\n",
       "\\textbf{gspillsicIVX1987} &       0.0052  &        0.118     &     0.044  &         0.965        &       -0.227    &        0.237     \\\\\n",
       "\\textbf{gspillsicIVX1988} &      -0.0086  &        0.117     &    -0.073  &         0.942        &       -0.239    &        0.222     \\\\\n",
       "\\textbf{gspillsicIVX1989} &       0.0257  &        0.116     &     0.222  &         0.825        &       -0.201    &        0.253     \\\\\n",
       "\\textbf{gspillsicIVX1990} &       0.0151  &        0.114     &     0.132  &         0.895        &       -0.209    &        0.239     \\\\\n",
       "\\textbf{gspillsicIVX1991} &       0.0634  &        0.113     &     0.563  &         0.573        &       -0.157    &        0.284     \\\\\n",
       "\\textbf{gspillsicIVX1992} &       0.0415  &        0.112     &     0.370  &         0.711        &       -0.178    &        0.261     \\\\\n",
       "\\textbf{gspillsicIVX1993} &       0.0532  &        0.111     &     0.479  &         0.632        &       -0.164    &        0.271     \\\\\n",
       "\\textbf{gspillsicIVX1994} &       0.0527  &        0.111     &     0.476  &         0.634        &       -0.164    &        0.270     \\\\\n",
       "\\textbf{gspillsicIVX1995} &       0.1206  &        0.110     &     1.098  &         0.272        &       -0.095    &        0.336     \\\\\n",
       "\\textbf{gspillsicIVX1996} &       0.1585  &        0.109     &     1.451  &         0.147        &       -0.056    &        0.372     \\\\\n",
       "\\textbf{gspillsicIVX1997} &       0.2513  &        0.109     &     2.315  &         0.021        &        0.039    &        0.464     \\\\\n",
       "\\textbf{gspillsicIVX1998} &       0.3606  &        0.108     &     3.339  &         0.001        &        0.149    &        0.572     \\\\\n",
       "\\textbf{gspillsicIVX1999} &       0.4666  &        0.108     &     4.328  &         0.000        &        0.255    &        0.678     \\\\\n",
       "\\textbf{gspillsicIVX2000} &       0.5090  &        0.108     &     4.718  &         0.000        &        0.298    &        0.720     \\\\\n",
       "\\textbf{gspillsicIVX2001} &       0.3331  &        0.108     &     3.073  &         0.002        &        0.121    &        0.546     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 24023.522 & \\textbf{  Durbin-Watson:     } &      0.353    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 61844043.678  \\\\\n",
       "\\textbf{Skew:}          &   12.758  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  335.022  & \\textbf{  Cond. No.          } &   1.57e+04    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.57e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.357\n",
       "Model:                            OLS   Adj. R-squared:                  0.356\n",
       "Method:                 Least Squares   F-statistic:                     274.6\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        02:03:01   Log-Likelihood:            -1.4594e+05\n",
       "No. Observations:               13385   AIC:                         2.919e+05\n",
       "Df Residuals:                   13357   BIC:                         2.921e+05\n",
       "Df Model:                          27                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const              946.3154    147.629      6.410      0.000     656.941    1235.689\n",
       "gspillsicIV         -0.0600      0.102     -0.586      0.558      -0.260       0.141\n",
       "pat_count          -11.1614      1.793     -6.225      0.000     -14.676      -7.647\n",
       "rsales               0.5112      0.034     15.126      0.000       0.445       0.577\n",
       "rppent               0.4669      0.048      9.689      0.000       0.372       0.561\n",
       "emp                -39.4619      3.475    -11.355      0.000     -46.274     -32.650\n",
       "rxrd                11.4031      0.470     24.284      0.000      10.483      12.324\n",
       "gspillsicIVX1981    -0.0376      0.136     -0.278      0.781      -0.303       0.228\n",
       "gspillsicIVX1982    -0.0165      0.133     -0.124      0.902      -0.277       0.244\n",
       "gspillsicIVX1983     0.0010      0.129      0.008      0.994      -0.253       0.255\n",
       "gspillsicIVX1984    -0.0235      0.126     -0.186      0.852      -0.271       0.224\n",
       "gspillsicIVX1985    -0.0079      0.124     -0.064      0.949      -0.251       0.235\n",
       "gspillsicIVX1986     0.0070      0.121      0.058      0.954      -0.230       0.244\n",
       "gspillsicIVX1987     0.0052      0.118      0.044      0.965      -0.227       0.237\n",
       "gspillsicIVX1988    -0.0086      0.117     -0.073      0.942      -0.239       0.222\n",
       "gspillsicIVX1989     0.0257      0.116      0.222      0.825      -0.201       0.253\n",
       "gspillsicIVX1990     0.0151      0.114      0.132      0.895      -0.209       0.239\n",
       "gspillsicIVX1991     0.0634      0.113      0.563      0.573      -0.157       0.284\n",
       "gspillsicIVX1992     0.0415      0.112      0.370      0.711      -0.178       0.261\n",
       "gspillsicIVX1993     0.0532      0.111      0.479      0.632      -0.164       0.271\n",
       "gspillsicIVX1994     0.0527      0.111      0.476      0.634      -0.164       0.270\n",
       "gspillsicIVX1995     0.1206      0.110      1.098      0.272      -0.095       0.336\n",
       "gspillsicIVX1996     0.1585      0.109      1.451      0.147      -0.056       0.372\n",
       "gspillsicIVX1997     0.2513      0.109      2.315      0.021       0.039       0.464\n",
       "gspillsicIVX1998     0.3606      0.108      3.339      0.001       0.149       0.572\n",
       "gspillsicIVX1999     0.4666      0.108      4.328      0.000       0.255       0.678\n",
       "gspillsicIVX2000     0.5090      0.108      4.718      0.000       0.298       0.720\n",
       "gspillsicIVX2001     0.3331      0.108      3.073      0.002       0.121       0.546\n",
       "==============================================================================\n",
       "Omnibus:                    24023.522   Durbin-Watson:                   0.353\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         61844043.678\n",
       "Skew:                          12.758   Prob(JB):                         0.00\n",
       "Kurtosis:                     335.022   Cond. No.                     1.57e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.57e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product spillovers model, no firm FE's\n",
    "drop_columns = [col for col in x_vars.columns if 'gspilltecIV' in col]\n",
    "x_vars_nofe = x_vars.drop(columns=fixed_effects)\n",
    "x_vars_nofe = x_vars_nofe.drop(columns=drop_columns)\n",
    "\n",
    "year_model4 = sm.OLS(y_var,x_vars_nofe).fit()\n",
    "year_model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bc5fbf8-de34-465f-8097-71775a7443d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>rmkvaf</td>      <th>  R-squared:         </th>  <td>   0.666</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.646</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   32.48</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 16 Oct 2024</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:03:03</td>     <th>  Log-Likelihood:    </th> <td>-1.4155e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 13385</td>      <th>  AIC:               </th>  <td>2.847e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 12609</td>      <th>  BIC:               </th>  <td>2.905e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   775</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>-1.059e+04</td> <td> 2858.143</td> <td>   -3.705</td> <td> 0.000</td> <td>-1.62e+04</td> <td>-4986.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIV</th>      <td>    0.8770</td> <td>    0.199</td> <td>    4.412</td> <td> 0.000</td> <td>    0.487</td> <td>    1.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pat_count</th>        <td>  -29.6820</td> <td>    1.861</td> <td>  -15.948</td> <td> 0.000</td> <td>  -33.330</td> <td>  -26.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rsales</th>           <td>    0.7906</td> <td>    0.037</td> <td>   21.566</td> <td> 0.000</td> <td>    0.719</td> <td>    0.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rppent</th>           <td>    0.6416</td> <td>    0.084</td> <td>    7.603</td> <td> 0.000</td> <td>    0.476</td> <td>    0.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>emp</th>              <td>   12.1477</td> <td>    7.031</td> <td>    1.728</td> <td> 0.084</td> <td>   -1.633</td> <td>   25.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rxrd</th>             <td>   18.7054</td> <td>    0.614</td> <td>   30.479</td> <td> 0.000</td> <td>   17.502</td> <td>   19.908</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981</th>             <td> -323.9459</td> <td>  749.124</td> <td>   -0.432</td> <td> 0.665</td> <td>-1792.342</td> <td> 1144.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1982</th>             <td> -175.3555</td> <td>  747.071</td> <td>   -0.235</td> <td> 0.814</td> <td>-1639.729</td> <td> 1289.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1983</th>             <td> -106.8254</td> <td>  738.681</td> <td>   -0.145</td> <td> 0.885</td> <td>-1554.753</td> <td> 1341.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1984</th>             <td> -358.0670</td> <td>  734.939</td> <td>   -0.487</td> <td> 0.626</td> <td>-1798.660</td> <td> 1082.526</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1985</th>             <td> -339.9200</td> <td>  734.792</td> <td>   -0.463</td> <td> 0.644</td> <td>-1780.223</td> <td> 1100.383</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1986</th>             <td> -333.0589</td> <td>  729.764</td> <td>   -0.456</td> <td> 0.648</td> <td>-1763.507</td> <td> 1097.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1987</th>             <td> -418.4831</td> <td>  726.215</td> <td>   -0.576</td> <td> 0.564</td> <td>-1841.975</td> <td> 1005.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988</th>             <td> -537.8235</td> <td>  725.635</td> <td>   -0.741</td> <td> 0.459</td> <td>-1960.179</td> <td>  884.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1989</th>             <td> -394.3985</td> <td>  723.074</td> <td>   -0.545</td> <td> 0.585</td> <td>-1811.734</td> <td> 1022.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1990</th>             <td> -723.8705</td> <td>  719.942</td> <td>   -1.005</td> <td> 0.315</td> <td>-2135.066</td> <td>  687.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1991</th>             <td> -405.2199</td> <td>  718.078</td> <td>   -0.564</td> <td> 0.573</td> <td>-1812.763</td> <td> 1002.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992</th>             <td> -200.8668</td> <td>  716.324</td> <td>   -0.280</td> <td> 0.779</td> <td>-1604.970</td> <td> 1203.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1993</th>             <td>  -60.0251</td> <td>  714.587</td> <td>   -0.084</td> <td> 0.933</td> <td>-1460.725</td> <td> 1340.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1994</th>             <td> -213.6207</td> <td>  716.211</td> <td>   -0.298</td> <td> 0.766</td> <td>-1617.503</td> <td> 1190.261</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1995</th>             <td>  162.8964</td> <td>  716.020</td> <td>    0.228</td> <td> 0.820</td> <td>-1240.612</td> <td> 1566.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1996</th>             <td>  673.3490</td> <td>  718.687</td> <td>    0.937</td> <td> 0.349</td> <td> -735.387</td> <td> 2082.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1997</th>             <td>  981.4864</td> <td>  721.476</td> <td>    1.360</td> <td> 0.174</td> <td> -432.716</td> <td> 2395.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1998</th>             <td>  793.8169</td> <td>  725.956</td> <td>    1.093</td> <td> 0.274</td> <td> -629.167</td> <td> 2216.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1999</th>             <td> 1042.6699</td> <td>  732.548</td> <td>    1.423</td> <td> 0.155</td> <td> -393.235</td> <td> 2478.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2000</th>             <td>  473.6464</td> <td>  750.844</td> <td>    0.631</td> <td> 0.528</td> <td> -998.123</td> <td> 1945.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2001</th>             <td>  461.9434</td> <td>  779.875</td> <td>    0.592</td> <td> 0.554</td> <td>-1066.731</td> <td> 1990.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10005.0</th>          <td> 9659.3047</td> <td> 3429.640</td> <td>    2.816</td> <td> 0.005</td> <td> 2936.688</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10006.0</th>          <td> 9654.2887</td> <td> 3848.511</td> <td>    2.509</td> <td> 0.012</td> <td> 2110.621</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10008.0</th>          <td> 9065.5709</td> <td> 3369.549</td> <td>    2.690</td> <td> 0.007</td> <td> 2460.743</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10016.0</th>          <td> 1.032e+04</td> <td> 3471.639</td> <td>    2.971</td> <td> 0.003</td> <td> 3510.375</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10030.0</th>          <td> 1.032e+04</td> <td> 3484.519</td> <td>    2.962</td> <td> 0.003</td> <td> 3490.881</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1004.0</th>           <td>  1.04e+04</td> <td> 3516.374</td> <td>    2.956</td> <td> 0.003</td> <td> 3502.541</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10056.0</th>          <td> 8114.8321</td> <td> 3313.170</td> <td>    2.449</td> <td> 0.014</td> <td> 1620.514</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10085.0</th>          <td> 2996.3025</td> <td> 2984.784</td> <td>    1.004</td> <td> 0.315</td> <td>-2854.329</td> <td> 8846.934</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10092.0</th>          <td>     1e+04</td> <td> 5579.277</td> <td>    1.793</td> <td> 0.073</td> <td> -932.045</td> <td> 2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10097.0</th>          <td> 2913.9955</td> <td> 3124.866</td> <td>    0.933</td> <td> 0.351</td> <td>-3211.217</td> <td> 9039.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1010.0</th>           <td>  1.01e+04</td> <td> 5629.803</td> <td>    1.794</td> <td> 0.073</td> <td> -935.137</td> <td> 2.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10109.0</th>          <td> 1.061e+04</td> <td> 3495.834</td> <td>    3.035</td> <td> 0.002</td> <td> 3757.360</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10115.0</th>          <td> 7108.6747</td> <td> 3121.346</td> <td>    2.277</td> <td> 0.023</td> <td>  990.362</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10124.0</th>          <td> 1.071e+04</td> <td> 3505.368</td> <td>    3.055</td> <td> 0.002</td> <td> 3837.145</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1013.0</th>           <td> 5204.0952</td> <td> 3088.093</td> <td>    1.685</td> <td> 0.092</td> <td> -849.037</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10150.0</th>          <td> 3126.6112</td> <td> 3528.442</td> <td>    0.886</td> <td> 0.376</td> <td>-3789.672</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10159.0</th>          <td>-3942.8076</td> <td> 4398.018</td> <td>   -0.896</td> <td> 0.370</td> <td>-1.26e+04</td> <td> 4677.977</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10174.0</th>          <td> 1.059e+04</td> <td> 3728.330</td> <td>    2.840</td> <td> 0.005</td> <td> 3281.435</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10185.0</th>          <td> 6933.6784</td> <td> 3401.405</td> <td>    2.038</td> <td> 0.042</td> <td>  266.407</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10195.0</th>          <td>-2419.7055</td> <td> 2991.651</td> <td>   -0.809</td> <td> 0.419</td> <td>-8283.797</td> <td> 3444.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10198.0</th>          <td>  1.04e+04</td> <td> 3490.406</td> <td>    2.980</td> <td> 0.003</td> <td> 3559.496</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10215.0</th>          <td> 1.047e+04</td> <td> 3502.877</td> <td>    2.989</td> <td> 0.003</td> <td> 3603.939</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10232.0</th>          <td> 3412.4149</td> <td> 3139.375</td> <td>    1.087</td> <td> 0.277</td> <td>-2741.238</td> <td> 9566.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10236.0</th>          <td> 9729.1672</td> <td> 3425.098</td> <td>    2.841</td> <td> 0.005</td> <td> 3015.455</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10286.0</th>          <td> 9948.2372</td> <td> 3494.179</td> <td>    2.847</td> <td> 0.004</td> <td> 3099.114</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10301.0</th>          <td>-1.568e+04</td> <td> 3092.129</td> <td>   -5.072</td> <td> 0.000</td> <td>-2.17e+04</td> <td>-9623.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10312.0</th>          <td> 1.037e+04</td> <td> 3515.620</td> <td>    2.951</td> <td> 0.003</td> <td> 3483.546</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10332.0</th>          <td>  1.05e+04</td> <td> 4320.420</td> <td>    2.430</td> <td> 0.015</td> <td> 2028.470</td> <td>  1.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1036.0</th>           <td> 6315.6838</td> <td> 3359.575</td> <td>    1.880</td> <td> 0.060</td> <td> -269.594</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10374.0</th>          <td> 8620.1908</td> <td> 3326.131</td> <td>    2.592</td> <td> 0.010</td> <td> 2100.469</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10386.0</th>          <td> 6270.3101</td> <td> 3177.083</td> <td>    1.974</td> <td> 0.048</td> <td>   42.745</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10391.0</th>          <td> 3673.5113</td> <td> 3158.308</td> <td>    1.163</td> <td> 0.245</td> <td>-2517.254</td> <td> 9864.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10407.0</th>          <td> 3552.8502</td> <td> 2995.765</td> <td>    1.186</td> <td> 0.236</td> <td>-2319.306</td> <td> 9425.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10420.0</th>          <td> 7609.5888</td> <td> 3141.904</td> <td>    2.422</td> <td> 0.015</td> <td> 1450.979</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10422.0</th>          <td> 7032.1079</td> <td> 3412.655</td> <td>    2.061</td> <td> 0.039</td> <td>  342.784</td> <td> 1.37e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10426.0</th>          <td> 8969.2563</td> <td> 3539.383</td> <td>    2.534</td> <td> 0.011</td> <td> 2031.527</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10441.0</th>          <td>  1.01e+04</td> <td> 3465.889</td> <td>    2.913</td> <td> 0.004</td> <td> 3301.710</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1045.0</th>           <td>  263.6677</td> <td> 3653.113</td> <td>    0.072</td> <td> 0.942</td> <td>-6896.990</td> <td> 7424.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10453.0</th>          <td> 6252.1220</td> <td> 3193.893</td> <td>    1.958</td> <td> 0.050</td> <td>   -8.395</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10482.0</th>          <td>-1.574e+04</td> <td> 3499.420</td> <td>   -4.499</td> <td> 0.000</td> <td>-2.26e+04</td> <td>-8883.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10498.0</th>          <td> 9516.6376</td> <td> 3463.766</td> <td>    2.747</td> <td> 0.006</td> <td> 2727.130</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10499.0</th>          <td> 1905.7296</td> <td> 3036.020</td> <td>    0.628</td> <td> 0.530</td> <td>-4045.331</td> <td> 7856.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10511.0</th>          <td> 1.036e+04</td> <td> 3584.248</td> <td>    2.890</td> <td> 0.004</td> <td> 3331.750</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10519.0</th>          <td>-6995.2218</td> <td> 2994.130</td> <td>   -2.336</td> <td> 0.019</td> <td>-1.29e+04</td> <td>-1126.271</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10530.0</th>          <td> 7992.8778</td> <td> 3302.808</td> <td>    2.420</td> <td> 0.016</td> <td> 1518.871</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10537.0</th>          <td> 7966.2215</td> <td> 3612.886</td> <td>    2.205</td> <td> 0.027</td> <td>  884.416</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10540.0</th>          <td> 8942.0924</td> <td> 3329.146</td> <td>    2.686</td> <td> 0.007</td> <td> 2416.459</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10541.0</th>          <td>  1.02e+04</td> <td> 3607.567</td> <td>    2.826</td> <td> 0.005</td> <td> 3125.335</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10550.0</th>          <td> 7172.0022</td> <td> 6120.797</td> <td>    1.172</td> <td> 0.241</td> <td>-4825.691</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10553.0</th>          <td> 4910.8820</td> <td> 3309.233</td> <td>    1.484</td> <td> 0.138</td> <td>-1575.719</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10565.0</th>          <td> 1.095e+04</td> <td> 3502.584</td> <td>    3.125</td> <td> 0.002</td> <td> 4080.237</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10580.0</th>          <td> 1.074e+04</td> <td> 3496.131</td> <td>    3.071</td> <td> 0.002</td> <td> 3883.162</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10581.0</th>          <td> 8286.5040</td> <td> 3446.834</td> <td>    2.404</td> <td> 0.016</td> <td> 1530.184</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10588.0</th>          <td> -175.8462</td> <td> 2944.608</td> <td>   -0.060</td> <td> 0.952</td> <td>-5947.726</td> <td> 5596.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10597.0</th>          <td> 9656.1515</td> <td> 3467.223</td> <td>    2.785</td> <td> 0.005</td> <td> 2859.867</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10599.0</th>          <td> 1.023e+04</td> <td> 3528.354</td> <td>    2.900</td> <td> 0.004</td> <td> 3315.237</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10618.0</th>          <td> 9074.0716</td> <td> 3399.306</td> <td>    2.669</td> <td> 0.008</td> <td> 2410.914</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10656.0</th>          <td> 1.046e+04</td> <td> 3495.542</td> <td>    2.993</td> <td> 0.003</td> <td> 3610.766</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10658.0</th>          <td> 1.045e+04</td> <td> 3496.829</td> <td>    2.987</td> <td> 0.003</td> <td> 3590.733</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10726.0</th>          <td> 1.227e+04</td> <td> 3523.531</td> <td>    3.481</td> <td> 0.001</td> <td> 5359.204</td> <td> 1.92e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10734.0</th>          <td> 7760.9997</td> <td> 3836.548</td> <td>    2.023</td> <td> 0.043</td> <td>  240.781</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10735.0</th>          <td> 9750.1704</td> <td> 3465.560</td> <td>    2.813</td> <td> 0.005</td> <td> 2957.146</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10764.0</th>          <td>  1.03e+04</td> <td> 3584.360</td> <td>    2.874</td> <td> 0.004</td> <td> 3274.591</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10777.0</th>          <td> 1.038e+04</td> <td> 3498.480</td> <td>    2.966</td> <td> 0.003</td> <td> 3519.464</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1078.0</th>           <td>  536.0262</td> <td> 3370.038</td> <td>    0.159</td> <td> 0.874</td> <td>-6069.762</td> <td> 7141.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10793.0</th>          <td> 8667.7588</td> <td> 3500.015</td> <td>    2.476</td> <td> 0.013</td> <td> 1807.197</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10816.0</th>          <td> 8810.4118</td> <td> 3495.532</td> <td>    2.520</td> <td> 0.012</td> <td> 1958.637</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10839.0</th>          <td> 1.057e+04</td> <td> 3494.111</td> <td>    3.026</td> <td> 0.002</td> <td> 3724.674</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10857.0</th>          <td>  267.8678</td> <td> 3201.082</td> <td>    0.084</td> <td> 0.933</td> <td>-6006.741</td> <td> 6542.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10867.0</th>          <td> 4873.3848</td> <td> 3659.413</td> <td>    1.332</td> <td> 0.183</td> <td>-2299.621</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10906.0</th>          <td> 9336.4858</td> <td> 3395.787</td> <td>    2.749</td> <td> 0.006</td> <td> 2680.227</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10950.0</th>          <td> 1.032e+04</td> <td> 4451.546</td> <td>    2.319</td> <td> 0.020</td> <td> 1598.643</td> <td> 1.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10983.0</th>          <td>-2.185e+04</td> <td> 3294.297</td> <td>   -6.633</td> <td> 0.000</td> <td>-2.83e+04</td> <td>-1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1099.0</th>           <td> 9612.8017</td> <td> 3450.007</td> <td>    2.786</td> <td> 0.005</td> <td> 2850.264</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>10991.0</th>          <td> 4300.1083</td> <td> 3524.484</td> <td>    1.220</td> <td> 0.222</td> <td>-2608.417</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11012.0</th>          <td> 7003.0744</td> <td> 3245.625</td> <td>    2.158</td> <td> 0.031</td> <td>  641.155</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11038.0</th>          <td> 1584.9939</td> <td> 3243.140</td> <td>    0.489</td> <td> 0.625</td> <td>-4772.054</td> <td> 7942.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1104.0</th>           <td> 9087.1045</td> <td> 3363.632</td> <td>    2.702</td> <td> 0.007</td> <td> 2493.873</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11060.0</th>          <td> 9940.4467</td> <td> 3499.969</td> <td>    2.840</td> <td> 0.005</td> <td> 3079.975</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11094.0</th>          <td> 9322.6430</td> <td> 3404.847</td> <td>    2.738</td> <td> 0.006</td> <td> 2648.624</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11096.0</th>          <td> 8070.0491</td> <td> 3306.301</td> <td>    2.441</td> <td> 0.015</td> <td> 1589.196</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11113.0</th>          <td> 9320.6769</td> <td> 3620.346</td> <td>    2.575</td> <td> 0.010</td> <td> 2224.247</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1115.0</th>           <td> 8436.8675</td> <td> 3367.347</td> <td>    2.505</td> <td> 0.012</td> <td> 1836.355</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11161.0</th>          <td> 6021.3457</td> <td> 3130.858</td> <td>    1.923</td> <td> 0.054</td> <td> -115.612</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11225.0</th>          <td> 1.019e+04</td> <td> 3520.620</td> <td>    2.896</td> <td> 0.004</td> <td> 3294.007</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11228.0</th>          <td> 1.058e+04</td> <td> 3455.736</td> <td>    3.062</td> <td> 0.002</td> <td> 3807.075</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11236.0</th>          <td> 8487.3536</td> <td> 4807.653</td> <td>    1.765</td> <td> 0.078</td> <td> -936.377</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11288.0</th>          <td>-4285.1438</td> <td> 3433.102</td> <td>   -1.248</td> <td> 0.212</td> <td> -1.1e+04</td> <td> 2444.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11312.0</th>          <td>-1488.0720</td> <td> 3086.728</td> <td>   -0.482</td> <td> 0.630</td> <td>-7538.528</td> <td> 4562.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11361.0</th>          <td>  1.05e+04</td> <td> 3501.988</td> <td>    2.998</td> <td> 0.003</td> <td> 3632.949</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11399.0</th>          <td> 2347.5209</td> <td> 3077.670</td> <td>    0.763</td> <td> 0.446</td> <td>-3685.180</td> <td> 8380.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>114303.0</th>         <td>-9571.3693</td> <td> 5399.892</td> <td>   -1.773</td> <td> 0.076</td> <td>-2.02e+04</td> <td> 1013.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11456.0</th>          <td> 5586.1634</td> <td> 3461.222</td> <td>    1.614</td> <td> 0.107</td> <td>-1198.358</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11465.0</th>          <td> 4659.0121</td> <td> 3374.297</td> <td>    1.381</td> <td> 0.167</td> <td>-1955.123</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11502.0</th>          <td> 8668.1917</td> <td> 3397.049</td> <td>    2.552</td> <td> 0.011</td> <td> 2009.459</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11506.0</th>          <td> 6603.9028</td> <td> 3378.886</td> <td>    1.954</td> <td> 0.051</td> <td>  -19.228</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11537.0</th>          <td> 1.023e+04</td> <td> 3479.312</td> <td>    2.939</td> <td> 0.003</td> <td> 3407.335</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11566.0</th>          <td> 1.036e+04</td> <td> 3491.031</td> <td>    2.968</td> <td> 0.003</td> <td> 3518.073</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11573.0</th>          <td> 9761.2159</td> <td> 3429.072</td> <td>    2.847</td> <td> 0.004</td> <td> 3039.712</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11580.0</th>          <td> 7866.4408</td> <td> 3666.583</td> <td>    2.145</td> <td> 0.032</td> <td>  679.380</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11600.0</th>          <td> 9665.2370</td> <td> 3432.503</td> <td>    2.816</td> <td> 0.005</td> <td> 2937.009</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11609.0</th>          <td> 1.356e+04</td> <td> 3467.960</td> <td>    3.911</td> <td> 0.000</td> <td> 6766.557</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1161.0</th>           <td> -480.8907</td> <td> 2968.996</td> <td>   -0.162</td> <td> 0.871</td> <td>-6300.575</td> <td> 5338.793</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11636.0</th>          <td>-1.146e+04</td> <td> 3207.307</td> <td>   -3.574</td> <td> 0.000</td> <td>-1.78e+04</td> <td>-5177.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11670.0</th>          <td> 9809.1236</td> <td> 3451.905</td> <td>    2.842</td> <td> 0.004</td> <td> 3042.865</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11678.0</th>          <td>-3033.7459</td> <td> 3115.647</td> <td>   -0.974</td> <td> 0.330</td> <td>-9140.887</td> <td> 3073.396</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11682.0</th>          <td>  1.06e+04</td> <td> 3611.779</td> <td>    2.935</td> <td> 0.003</td> <td> 3521.726</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11694.0</th>          <td> 1.041e+04</td> <td> 3619.335</td> <td>    2.877</td> <td> 0.004</td> <td> 3317.257</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11720.0</th>          <td> -762.2755</td> <td> 3867.445</td> <td>   -0.197</td> <td> 0.844</td> <td>-8343.057</td> <td> 6818.506</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11721.0</th>          <td>-9711.2464</td> <td> 3644.527</td> <td>   -2.665</td> <td> 0.008</td> <td>-1.69e+04</td> <td>-2567.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11722.0</th>          <td> 9216.8593</td> <td> 3613.105</td> <td>    2.551</td> <td> 0.011</td> <td> 2134.624</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11793.0</th>          <td> 9959.7365</td> <td> 6257.374</td> <td>    1.592</td> <td> 0.111</td> <td>-2305.669</td> <td> 2.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11797.0</th>          <td> 1.057e+04</td> <td> 3841.148</td> <td>    2.752</td> <td> 0.006</td> <td> 3040.013</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>11914.0</th>          <td> 9570.5529</td> <td> 4017.374</td> <td>    2.382</td> <td> 0.017</td> <td> 1695.889</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1209.0</th>           <td> 8145.5808</td> <td> 3258.527</td> <td>    2.500</td> <td> 0.012</td> <td> 1758.373</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12136.0</th>          <td>-8658.1861</td> <td> 3530.681</td> <td>   -2.452</td> <td> 0.014</td> <td>-1.56e+04</td> <td>-1737.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12141.0</th>          <td> 8.845e+04</td> <td> 3265.400</td> <td>   27.087</td> <td> 0.000</td> <td>  8.2e+04</td> <td> 9.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12181.0</th>          <td> 7785.2247</td> <td> 4432.555</td> <td>    1.756</td> <td> 0.079</td> <td> -903.258</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12215.0</th>          <td>-1543.0409</td> <td> 3214.227</td> <td>   -0.480</td> <td> 0.631</td> <td>-7843.415</td> <td> 4757.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12216.0</th>          <td> 1926.4346</td> <td> 3215.418</td> <td>    0.599</td> <td> 0.549</td> <td>-4376.274</td> <td> 8229.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12256.0</th>          <td> 2154.7127</td> <td> 3244.711</td> <td>    0.664</td> <td> 0.507</td> <td>-4205.414</td> <td> 8514.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12262.0</th>          <td> 8366.2379</td> <td> 3552.028</td> <td>    2.355</td> <td> 0.019</td> <td> 1403.724</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12389.0</th>          <td> 1.212e+04</td> <td> 3710.289</td> <td>    3.266</td> <td> 0.001</td> <td> 4843.769</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1239.0</th>           <td> 6859.3033</td> <td> 3194.471</td> <td>    2.147</td> <td> 0.032</td> <td>  597.654</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12390.0</th>          <td> 8144.3311</td> <td> 3827.435</td> <td>    2.128</td> <td> 0.033</td> <td>  641.976</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12397.0</th>          <td> 7091.0057</td> <td> 5435.028</td> <td>    1.305</td> <td> 0.192</td> <td>-3562.476</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1243.0</th>           <td> 4740.1661</td> <td> 3505.474</td> <td>    1.352</td> <td> 0.176</td> <td>-2131.096</td> <td> 1.16e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12548.0</th>          <td> 9622.3915</td> <td> 3913.321</td> <td>    2.459</td> <td> 0.014</td> <td> 1951.687</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12570.0</th>          <td> 8478.9178</td> <td> 3680.537</td> <td>    2.304</td> <td> 0.021</td> <td> 1264.506</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12581.0</th>          <td> 8072.2136</td> <td> 3959.674</td> <td>    2.039</td> <td> 0.042</td> <td>  310.651</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12592.0</th>          <td> 7180.3739</td> <td> 3562.401</td> <td>    2.016</td> <td> 0.044</td> <td>  197.525</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12604.0</th>          <td> 6302.1541</td> <td> 6167.016</td> <td>    1.022</td> <td> 0.307</td> <td>-5786.135</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12656.0</th>          <td> 9831.1782</td> <td> 3729.089</td> <td>    2.636</td> <td> 0.008</td> <td> 2521.597</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12679.0</th>          <td>-1.207e+04</td> <td> 3575.885</td> <td>   -3.374</td> <td> 0.001</td> <td>-1.91e+04</td> <td>-5057.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1278.0</th>           <td> 9272.1183</td> <td> 3554.622</td> <td>    2.608</td> <td> 0.009</td> <td> 2304.519</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12788.0</th>          <td>-1.081e+04</td> <td> 3704.281</td> <td>   -2.917</td> <td> 0.004</td> <td>-1.81e+04</td> <td>-3545.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1283.0</th>           <td> 8454.8366</td> <td> 3314.507</td> <td>    2.551</td> <td> 0.011</td> <td> 1957.900</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1297.0</th>           <td> 9358.7113</td> <td> 3457.490</td> <td>    2.707</td> <td> 0.007</td> <td> 2581.505</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>12992.0</th>          <td> 1.015e+04</td> <td> 3767.816</td> <td>    2.693</td> <td> 0.007</td> <td> 2762.823</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13135.0</th>          <td> 5448.9172</td> <td> 3564.090</td> <td>    1.529</td> <td> 0.126</td> <td>-1537.242</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1327.0</th>           <td>-1643.9351</td> <td> 3038.632</td> <td>   -0.541</td> <td> 0.589</td> <td>-7600.117</td> <td> 4312.247</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13282.0</th>          <td> 4445.8876</td> <td> 5417.023</td> <td>    0.821</td> <td> 0.412</td> <td>-6172.301</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1334.0</th>           <td>-5548.4156</td> <td> 3198.783</td> <td>   -1.735</td> <td> 0.083</td> <td>-1.18e+04</td> <td>  721.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13351.0</th>          <td> 6518.8735</td> <td> 4245.842</td> <td>    1.535</td> <td> 0.125</td> <td>-1803.622</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13365.0</th>          <td>-1.383e+04</td> <td> 4121.711</td> <td>   -3.356</td> <td> 0.001</td> <td>-2.19e+04</td> <td>-5754.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13369.0</th>          <td> 9139.3518</td> <td> 3706.917</td> <td>    2.465</td> <td> 0.014</td> <td> 1873.230</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13406.0</th>          <td> 9939.8470</td> <td> 3720.720</td> <td>    2.671</td> <td> 0.008</td> <td> 2646.670</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13407.0</th>          <td> 2417.5901</td> <td> 3312.421</td> <td>    0.730</td> <td> 0.465</td> <td>-4075.258</td> <td> 8910.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13417.0</th>          <td> 9899.0300</td> <td> 3860.871</td> <td>    2.564</td> <td> 0.010</td> <td> 2331.135</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13525.0</th>          <td>-2774.8050</td> <td> 3294.923</td> <td>   -0.842</td> <td> 0.400</td> <td>-9233.355</td> <td> 3683.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13554.0</th>          <td>  1.04e+04</td> <td> 3768.695</td> <td>    2.759</td> <td> 0.006</td> <td> 3009.340</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1359.0</th>           <td>-7579.7543</td> <td> 3532.881</td> <td>   -2.145</td> <td> 0.032</td> <td>-1.45e+04</td> <td> -654.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13623.0</th>          <td> 7282.1206</td> <td> 3530.867</td> <td>    2.062</td> <td> 0.039</td> <td>  361.084</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1372.0</th>           <td>-1024.5227</td> <td> 2952.643</td> <td>   -0.347</td> <td> 0.729</td> <td>-6812.153</td> <td> 4763.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1380.0</th>           <td>  933.6613</td> <td> 3077.529</td> <td>    0.303</td> <td> 0.762</td> <td>-5098.765</td> <td> 6966.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13923.0</th>          <td> 9652.5025</td> <td> 4001.225</td> <td>    2.412</td> <td> 0.016</td> <td> 1809.493</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13932.0</th>          <td> 9693.4571</td> <td> 4463.929</td> <td>    2.172</td> <td> 0.030</td> <td>  943.476</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>13941.0</th>          <td>-1234.2215</td> <td> 3351.972</td> <td>   -0.368</td> <td> 0.713</td> <td>-7804.596</td> <td> 5336.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1397.0</th>           <td> 8978.2149</td> <td> 3604.283</td> <td>    2.491</td> <td> 0.013</td> <td> 1913.271</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14064.0</th>          <td> 6496.0923</td> <td> 3427.729</td> <td>    1.895</td> <td> 0.058</td> <td> -222.777</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14084.0</th>          <td> 9938.4584</td> <td> 3773.668</td> <td>    2.634</td> <td> 0.008</td> <td> 2541.495</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14324.0</th>          <td> -448.3298</td> <td> 3367.341</td> <td>   -0.133</td> <td> 0.894</td> <td>-7048.831</td> <td> 6152.171</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14462.0</th>          <td> 9253.3530</td> <td> 3776.161</td> <td>    2.450</td> <td> 0.014</td> <td> 1851.504</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1447.0</th>           <td> 1.462e+04</td> <td> 4904.563</td> <td>    2.981</td> <td> 0.003</td> <td> 5007.306</td> <td> 2.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14531.0</th>          <td> 8818.8233</td> <td> 1.01e+04</td> <td>    0.870</td> <td> 0.385</td> <td>-1.11e+04</td> <td> 2.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14593.0</th>          <td> 9416.8572</td> <td> 3820.901</td> <td>    2.465</td> <td> 0.014</td> <td> 1927.310</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14622.0</th>          <td> 1.034e+04</td> <td> 7410.721</td> <td>    1.395</td> <td> 0.163</td> <td>-4188.065</td> <td> 2.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1465.0</th>           <td> 8580.7095</td> <td> 3881.320</td> <td>    2.211</td> <td> 0.027</td> <td>  972.732</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1468.0</th>           <td> 1.025e+04</td> <td> 3913.165</td> <td>    2.618</td> <td> 0.009</td> <td> 2575.683</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14897.0</th>          <td> 8143.8094</td> <td> 5481.373</td> <td>    1.486</td> <td> 0.137</td> <td>-2600.516</td> <td> 1.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>14954.0</th>          <td> 8879.7142</td> <td> 3787.153</td> <td>    2.345</td> <td> 0.019</td> <td> 1456.318</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1496.0</th>           <td> 1.056e+04</td> <td> 3504.246</td> <td>    3.014</td> <td> 0.003</td> <td> 3691.980</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15267.0</th>          <td> 8769.1983</td> <td> 3768.994</td> <td>    2.327</td> <td> 0.020</td> <td> 1381.397</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15354.0</th>          <td> 4573.2807</td> <td> 3644.445</td> <td>    1.255</td> <td> 0.210</td> <td>-2570.386</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1542.0</th>           <td> 9271.0954</td> <td> 3433.382</td> <td>    2.700</td> <td> 0.007</td> <td> 2541.144</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15459.0</th>          <td> 8320.3467</td> <td> 3771.104</td> <td>    2.206</td> <td> 0.027</td> <td>  928.408</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1554.0</th>           <td> 1.037e+04</td> <td> 3482.162</td> <td>    2.977</td> <td> 0.003</td> <td> 3539.471</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15708.0</th>          <td>-1.427e+04</td> <td> 4364.184</td> <td>   -3.269</td> <td> 0.001</td> <td>-2.28e+04</td> <td>-5713.790</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15711.0</th>          <td> 8305.0899</td> <td> 3828.438</td> <td>    2.169</td> <td> 0.030</td> <td>  800.770</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>15761.0</th>          <td> 9943.8050</td> <td> 4432.762</td> <td>    2.243</td> <td> 0.025</td> <td> 1254.916</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1581.0</th>           <td>-1.894e+04</td> <td> 4232.063</td> <td>   -4.476</td> <td> 0.000</td> <td>-2.72e+04</td> <td>-1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1593.0</th>           <td> 9341.0130</td> <td> 3412.180</td> <td>    2.738</td> <td> 0.006</td> <td> 2652.621</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1602.0</th>           <td> 1.675e+04</td> <td> 3401.742</td> <td>    4.925</td> <td> 0.000</td> <td> 1.01e+04</td> <td> 2.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1613.0</th>           <td> 9426.6340</td> <td> 3409.769</td> <td>    2.765</td> <td> 0.006</td> <td> 2742.967</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16188.0</th>          <td> 5067.9193</td> <td> 3620.452</td> <td>    1.400</td> <td> 0.162</td> <td>-2028.717</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1632.0</th>           <td> 1695.0991</td> <td> 2948.865</td> <td>    0.575</td> <td> 0.565</td> <td>-4085.124</td> <td> 7475.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1633.0</th>           <td> 7476.1562</td> <td> 3246.672</td> <td>    2.303</td> <td> 0.021</td> <td> 1112.185</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1635.0</th>           <td>-9570.9410</td> <td> 3483.344</td> <td>   -2.748</td> <td> 0.006</td> <td>-1.64e+04</td> <td>-2743.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16401.0</th>          <td>  154.0311</td> <td> 3522.272</td> <td>    0.044</td> <td> 0.965</td> <td>-6750.159</td> <td> 7058.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16437.0</th>          <td> 3278.1817</td> <td> 4055.398</td> <td>    0.808</td> <td> 0.419</td> <td>-4671.016</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1651.0</th>           <td> 4853.9574</td> <td> 3050.757</td> <td>    1.591</td> <td> 0.112</td> <td>-1125.990</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1655.0</th>           <td> 1.039e+04</td> <td> 3500.961</td> <td>    2.969</td> <td> 0.003</td> <td> 3531.377</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1663.0</th>           <td> 1.499e+04</td> <td> 3493.288</td> <td>    4.290</td> <td> 0.000</td> <td> 8137.781</td> <td> 2.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16710.0</th>          <td> 4680.4781</td> <td> 3643.868</td> <td>    1.284</td> <td> 0.199</td> <td>-2462.057</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>16729.0</th>          <td> 3279.1787</td> <td> 3496.933</td> <td>    0.938</td> <td> 0.348</td> <td>-3575.341</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1690.0</th>           <td> -1.29e+04</td> <td> 3126.881</td> <td>   -4.125</td> <td> 0.000</td> <td> -1.9e+04</td> <td>-6769.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1703.0</th>           <td> 8580.7670</td> <td> 3451.436</td> <td>    2.486</td> <td> 0.013</td> <td> 1815.426</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17101.0</th>          <td>-2.018e+04</td> <td> 1.11e+04</td> <td>   -1.826</td> <td> 0.068</td> <td>-4.19e+04</td> <td> 1486.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>17202.0</th>          <td> 7809.9940</td> <td> 3719.926</td> <td>    2.100</td> <td> 0.036</td> <td>  518.372</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1722.0</th>           <td> 8452.8790</td> <td> 3492.118</td> <td>    2.421</td> <td> 0.016</td> <td> 1607.796</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1728.0</th>           <td> 1.027e+04</td> <td> 3498.334</td> <td>    2.935</td> <td> 0.003</td> <td> 3409.289</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1743.0</th>           <td> 8906.5506</td> <td> 4307.712</td> <td>    2.068</td> <td> 0.039</td> <td>  462.779</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1754.0</th>           <td> 9834.3119</td> <td> 3546.280</td> <td>    2.773</td> <td> 0.006</td> <td> 2883.064</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1762.0</th>           <td> 7435.8004</td> <td> 3401.849</td> <td>    2.186</td> <td> 0.029</td> <td>  767.658</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1773.0</th>           <td> 8891.4429</td> <td> 3479.243</td> <td>    2.556</td> <td> 0.011</td> <td> 2071.596</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1786.0</th>           <td>-3461.9294</td> <td> 3081.001</td> <td>   -1.124</td> <td> 0.261</td> <td>-9501.160</td> <td> 2577.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18100.0</th>          <td> 6947.5759</td> <td> 3657.295</td> <td>    1.900</td> <td> 0.058</td> <td> -221.279</td> <td> 1.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1820.0</th>           <td> 5465.5030</td> <td> 3097.241</td> <td>    1.765</td> <td> 0.078</td> <td> -605.561</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1848.0</th>           <td>-1886.5500</td> <td> 3530.970</td> <td>   -0.534</td> <td> 0.593</td> <td>-8807.788</td> <td> 5034.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>18654.0</th>          <td> 9606.5788</td> <td> 4646.011</td> <td>    2.068</td> <td> 0.039</td> <td>  499.691</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1875.0</th>           <td> 3764.7167</td> <td> 4537.792</td> <td>    0.830</td> <td> 0.407</td> <td>-5130.045</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1884.0</th>           <td> 8893.5871</td> <td> 3449.113</td> <td>    2.579</td> <td> 0.010</td> <td> 2132.801</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1913.0</th>           <td> 8167.5315</td> <td> 3325.843</td> <td>    2.456</td> <td> 0.014</td> <td> 1648.373</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1919.0</th>           <td> 9105.8754</td> <td> 3573.766</td> <td>    2.548</td> <td> 0.011</td> <td> 2100.751</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1920.0</th>           <td> 7091.9048</td> <td> 3175.931</td> <td>    2.233</td> <td> 0.026</td> <td>  866.596</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1968.0</th>           <td> 9763.7597</td> <td> 3432.129</td> <td>    2.845</td> <td> 0.004</td> <td> 3036.264</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1976.0</th>           <td> 1.072e+04</td> <td> 3385.906</td> <td>    3.167</td> <td> 0.002</td> <td> 4085.647</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1981.0</th>           <td> 9179.0457</td> <td> 3386.527</td> <td>    2.710</td> <td> 0.007</td> <td> 2540.938</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1988.0</th>           <td> 4480.5582</td> <td> 4066.528</td> <td>    1.102</td> <td> 0.271</td> <td>-3490.456</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1992.0</th>           <td> 1.038e+04</td> <td> 3487.074</td> <td>    2.977</td> <td> 0.003</td> <td> 3545.153</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2008.0</th>           <td> 9454.9287</td> <td> 3367.152</td> <td>    2.808</td> <td> 0.005</td> <td> 2854.798</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2033.0</th>           <td> 9562.2420</td> <td> 3880.534</td> <td>    2.464</td> <td> 0.014</td> <td> 1955.805</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2044.0</th>           <td> 7026.6396</td> <td> 3177.011</td> <td>    2.212</td> <td> 0.027</td> <td>  799.214</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2049.0</th>           <td> 9228.3043</td> <td> 3397.286</td> <td>    2.716</td> <td> 0.007</td> <td> 2569.107</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2061.0</th>           <td> 1.047e+04</td> <td> 3505.208</td> <td>    2.986</td> <td> 0.003</td> <td> 3594.353</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>20779.0</th>          <td> 5.253e+04</td> <td> 3656.911</td> <td>   14.363</td> <td> 0.000</td> <td> 4.54e+04</td> <td> 5.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2085.0</th>           <td>-8160.7724</td> <td> 3333.012</td> <td>   -2.448</td> <td> 0.014</td> <td>-1.47e+04</td> <td>-1627.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2086.0</th>           <td> 7159.5200</td> <td> 3250.961</td> <td>    2.202</td> <td> 0.028</td> <td>  787.141</td> <td> 1.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2111.0</th>           <td> 7686.8490</td> <td> 3220.397</td> <td>    2.387</td> <td> 0.017</td> <td> 1374.380</td> <td>  1.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21204.0</th>          <td>  384.4065</td> <td> 3516.538</td> <td>    0.109</td> <td> 0.913</td> <td>-6508.543</td> <td> 7277.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21238.0</th>          <td> 8142.5703</td> <td> 3798.818</td> <td>    2.143</td> <td> 0.032</td> <td>  696.310</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2124.0</th>           <td> 9948.2780</td> <td> 3545.061</td> <td>    2.806</td> <td> 0.005</td> <td> 2999.419</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2146.0</th>           <td> 1.384e+04</td> <td> 3784.701</td> <td>    3.656</td> <td> 0.000</td> <td> 6419.923</td> <td> 2.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>21496.0</th>          <td>-1.592e+04</td> <td> 4394.801</td> <td>   -3.622</td> <td> 0.000</td> <td>-2.45e+04</td> <td>-7302.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2154.0</th>           <td> 9073.6815</td> <td> 3398.606</td> <td>    2.670</td> <td> 0.008</td> <td> 2411.896</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2176.0</th>           <td> 4.929e+04</td> <td> 3844.965</td> <td>   12.820</td> <td> 0.000</td> <td> 4.18e+04</td> <td> 5.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2188.0</th>           <td> 1.044e+04</td> <td> 3575.743</td> <td>    2.921</td> <td> 0.004</td> <td> 3434.167</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2189.0</th>           <td> 4635.5911</td> <td> 3442.148</td> <td>    1.347</td> <td> 0.178</td> <td>-2111.542</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2220.0</th>           <td> 9562.6108</td> <td> 3447.803</td> <td>    2.774</td> <td> 0.006</td> <td> 2804.392</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22205.0</th>          <td> 1.025e+04</td> <td> 3981.986</td> <td>    2.575</td> <td> 0.010</td> <td> 2448.797</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2226.0</th>           <td> 6832.7747</td> <td> 6127.983</td> <td>    1.115</td> <td> 0.265</td> <td>-5179.003</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2230.0</th>           <td> 8034.0356</td> <td> 3473.326</td> <td>    2.313</td> <td> 0.021</td> <td> 1225.788</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22325.0</th>          <td>-1992.9417</td> <td> 3644.785</td> <td>   -0.547</td> <td> 0.585</td> <td>-9137.275</td> <td> 5151.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2255.0</th>           <td> 8674.1665</td> <td> 3467.604</td> <td>    2.501</td> <td> 0.012</td> <td> 1877.134</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22619.0</th>          <td> 9952.2455</td> <td> 4149.838</td> <td>    2.398</td> <td> 0.016</td> <td> 1817.932</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2267.0</th>           <td> -805.0746</td> <td> 3029.653</td> <td>   -0.266</td> <td> 0.790</td> <td>-6743.655</td> <td> 5133.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>22815.0</th>          <td> 4148.6717</td> <td> 3597.517</td> <td>    1.153</td> <td> 0.249</td> <td>-2903.008</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2285.0</th>           <td>-1.943e+04</td> <td> 3290.519</td> <td>   -5.906</td> <td> 0.000</td> <td>-2.59e+04</td> <td> -1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2290.0</th>           <td> 5559.2176</td> <td> 3411.874</td> <td>    1.629</td> <td> 0.103</td> <td>-1128.575</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2295.0</th>           <td> 1.008e+04</td> <td> 4816.118</td> <td>    2.093</td> <td> 0.036</td> <td>  640.045</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2316.0</th>           <td> 5178.5854</td> <td> 3441.299</td> <td>    1.505</td> <td> 0.132</td> <td>-1566.883</td> <td> 1.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23220.0</th>          <td> 8950.6027</td> <td> 3920.186</td> <td>    2.283</td> <td> 0.022</td> <td> 1266.442</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23224.0</th>          <td>-9039.2043</td> <td> 3854.373</td> <td>   -2.345</td> <td> 0.019</td> <td>-1.66e+04</td> <td>-1484.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2343.0</th>           <td>-4325.9227</td> <td> 5346.106</td> <td>   -0.809</td> <td> 0.418</td> <td>-1.48e+04</td> <td> 6153.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2352.0</th>           <td> 1.003e+04</td> <td> 3595.998</td> <td>    2.790</td> <td> 0.005</td> <td> 2983.330</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>23700.0</th>          <td>-2370.0832</td> <td> 4559.670</td> <td>   -0.520</td> <td> 0.603</td> <td>-1.13e+04</td> <td> 6567.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2390.0</th>           <td> 1.048e+04</td> <td> 3494.768</td> <td>    2.997</td> <td> 0.003</td> <td> 3624.802</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2393.0</th>           <td> 8653.3534</td> <td> 3349.179</td> <td>    2.584</td> <td> 0.010</td> <td> 2088.453</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2403.0</th>           <td> 9935.9405</td> <td> 3390.435</td> <td>    2.931</td> <td> 0.003</td> <td> 3290.172</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2435.0</th>           <td> 1.183e+04</td> <td> 3523.767</td> <td>    3.357</td> <td> 0.001</td> <td> 4921.422</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2444.0</th>           <td> 8334.6736</td> <td> 3418.105</td> <td>    2.438</td> <td> 0.015</td> <td> 1634.667</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2448.0</th>           <td> 8079.7608</td> <td> 3280.572</td> <td>    2.463</td> <td> 0.014</td> <td> 1649.340</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2469.0</th>           <td> 9319.5696</td> <td> 4619.391</td> <td>    2.017</td> <td> 0.044</td> <td>  264.861</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24720.0</th>          <td> 1.002e+04</td> <td> 4081.147</td> <td>    2.455</td> <td> 0.014</td> <td> 2018.551</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24800.0</th>          <td>-2368.8265</td> <td> 4068.738</td> <td>   -0.582</td> <td> 0.560</td> <td>-1.03e+04</td> <td> 5606.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2482.0</th>           <td> 1.054e+04</td> <td> 3503.339</td> <td>    3.009</td> <td> 0.003</td> <td> 3675.623</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>24969.0</th>          <td> 9638.1580</td> <td> 4606.058</td> <td>    2.092</td> <td> 0.036</td> <td>  609.584</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2498.0</th>           <td> 1009.6808</td> <td> 3154.855</td> <td>    0.320</td> <td> 0.749</td> <td>-5174.316</td> <td> 7193.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2504.0</th>           <td> -596.5201</td> <td> 3326.842</td> <td>   -0.179</td> <td> 0.858</td> <td>-7117.636</td> <td> 5924.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2508.0</th>           <td> 1.021e+04</td> <td> 3663.087</td> <td>    2.788</td> <td> 0.005</td> <td> 3034.214</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25124.0</th>          <td> 9907.2768</td> <td> 4155.005</td> <td>    2.384</td> <td> 0.017</td> <td> 1762.836</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2518.0</th>           <td>  1.02e+04</td> <td> 3499.266</td> <td>    2.914</td> <td> 0.004</td> <td> 3337.153</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25224.0</th>          <td> 1.085e+04</td> <td> 7432.065</td> <td>    1.459</td> <td> 0.145</td> <td>-3722.732</td> <td> 2.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25279.0</th>          <td> 7235.2571</td> <td> 3849.249</td> <td>    1.880</td> <td> 0.060</td> <td> -309.857</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2537.0</th>           <td>-6799.0456</td> <td> 3253.919</td> <td>   -2.089</td> <td> 0.037</td> <td>-1.32e+04</td> <td> -420.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2538.0</th>           <td> 9856.5848</td> <td> 4266.509</td> <td>    2.310</td> <td> 0.021</td> <td> 1493.578</td> <td> 1.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25389.0</th>          <td> 9764.1891</td> <td> 6252.235</td> <td>    1.562</td> <td> 0.118</td> <td>-2491.144</td> <td>  2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2547.0</th>           <td> 3557.0844</td> <td> 3568.631</td> <td>    0.997</td> <td> 0.319</td> <td>-3437.976</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2553.0</th>           <td> 8873.3357</td> <td> 3380.697</td> <td>    2.625</td> <td> 0.009</td> <td> 2246.655</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2574.0</th>           <td> 2969.8852</td> <td> 3914.372</td> <td>    0.759</td> <td> 0.448</td> <td>-4702.880</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>25747.0</th>          <td> 9693.6935</td> <td> 4140.844</td> <td>    2.341</td> <td> 0.019</td> <td> 1577.010</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2577.0</th>           <td> 8965.0512</td> <td> 3358.735</td> <td>    2.669</td> <td> 0.008</td> <td> 2381.419</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2593.0</th>           <td> 9363.1907</td> <td> 3451.006</td> <td>    2.713</td> <td> 0.007</td> <td> 2598.694</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2596.0</th>           <td> 4211.3081</td> <td> 3037.853</td> <td>    1.386</td> <td> 0.166</td> <td>-1743.346</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2663.0</th>           <td> 1.312e+04</td> <td> 3506.203</td> <td>    3.743</td> <td> 0.000</td> <td> 6250.849</td> <td>    2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2771.0</th>           <td> 4584.3418</td> <td> 3064.314</td> <td>    1.496</td> <td> 0.135</td> <td>-1422.180</td> <td> 1.06e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2787.0</th>           <td> 9312.7499</td> <td> 3431.787</td> <td>    2.714</td> <td> 0.007</td> <td> 2585.925</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2797.0</th>           <td>-3679.5566</td> <td> 3112.073</td> <td>   -1.182</td> <td> 0.237</td> <td>-9779.693</td> <td> 2420.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2802.0</th>           <td> 1.005e+04</td> <td> 3460.168</td> <td>    2.905</td> <td> 0.004</td> <td> 3267.980</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2817.0</th>           <td> 2283.4997</td> <td> 3416.197</td> <td>    0.668</td> <td> 0.504</td> <td>-4412.766</td> <td> 8979.766</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28678.0</th>          <td>-9271.0040</td> <td> 4084.380</td> <td>   -2.270</td> <td> 0.023</td> <td>-1.73e+04</td> <td>-1264.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28701.0</th>          <td> 7716.1013</td> <td> 3370.690</td> <td>    2.289</td> <td> 0.022</td> <td> 1109.037</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>28742.0</th>          <td>-9267.6116</td> <td> 4027.724</td> <td>   -2.301</td> <td> 0.021</td> <td>-1.72e+04</td> <td>-1372.660</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2888.0</th>           <td> 1.007e+04</td> <td> 3594.695</td> <td>    2.802</td> <td> 0.005</td> <td> 3025.153</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2897.0</th>           <td> 1.066e+04</td> <td> 4157.956</td> <td>    2.564</td> <td> 0.010</td> <td> 2511.181</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2917.0</th>           <td> 4909.1440</td> <td> 3468.408</td> <td>    1.415</td> <td> 0.157</td> <td>-1889.463</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>29392.0</th>          <td>-2347.8239</td> <td> 3892.943</td> <td>   -0.603</td> <td> 0.546</td> <td>-9978.585</td> <td> 5282.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2950.0</th>           <td>-1.893e+04</td> <td> 4792.383</td> <td>   -3.951</td> <td> 0.000</td> <td>-2.83e+04</td> <td>-9540.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2951.0</th>           <td> 1.016e+04</td> <td> 3824.854</td> <td>    2.656</td> <td> 0.008</td> <td> 2663.376</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2953.0</th>           <td> 9261.6627</td> <td> 3377.554</td> <td>    2.742</td> <td> 0.006</td> <td> 2641.143</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2960.0</th>           <td> 8815.4539</td> <td> 4097.258</td> <td>    2.152</td> <td> 0.031</td> <td>  784.204</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2975.0</th>           <td> 2967.2731</td> <td> 3055.386</td> <td>    0.971</td> <td> 0.331</td> <td>-3021.748</td> <td> 8956.294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2982.0</th>           <td> 8378.8566</td> <td> 3335.690</td> <td>    2.512</td> <td> 0.012</td> <td> 1840.397</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>2991.0</th>           <td>-3542.1410</td> <td> 3614.739</td> <td>   -0.980</td> <td> 0.327</td> <td>-1.06e+04</td> <td> 3543.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3011.0</th>           <td>-4619.7729</td> <td> 3181.777</td> <td>   -1.452</td> <td> 0.147</td> <td>-1.09e+04</td> <td> 1616.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3015.0</th>           <td> 1.033e+04</td> <td> 3445.213</td> <td>    2.998</td> <td> 0.003</td> <td> 3575.672</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3026.0</th>           <td> 8115.5273</td> <td> 3298.350</td> <td>    2.460</td> <td> 0.014</td> <td> 1650.259</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3031.0</th>           <td> -1.31e+04</td> <td> 4243.942</td> <td>   -3.087</td> <td> 0.002</td> <td>-2.14e+04</td> <td>-4783.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3062.0</th>           <td> 1.218e+04</td> <td> 3636.770</td> <td>    3.350</td> <td> 0.001</td> <td> 5056.024</td> <td> 1.93e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3093.0</th>           <td> 1154.0983</td> <td> 3273.649</td> <td>    0.353</td> <td> 0.724</td> <td>-5262.751</td> <td> 7570.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3107.0</th>           <td> 1.003e+04</td> <td> 4887.548</td> <td>    2.052</td> <td> 0.040</td> <td>  449.992</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3121.0</th>           <td> 1.131e+04</td> <td> 3424.546</td> <td>    3.304</td> <td> 0.001</td> <td> 4601.086</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3126.0</th>           <td> 1.029e+04</td> <td> 3500.672</td> <td>    2.940</td> <td> 0.003</td> <td> 3430.420</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3144.0</th>           <td> 6.226e+04</td> <td> 3492.741</td> <td>   17.824</td> <td> 0.000</td> <td> 5.54e+04</td> <td> 6.91e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3156.0</th>           <td> 8439.9508</td> <td> 3686.328</td> <td>    2.290</td> <td> 0.022</td> <td> 1214.187</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3157.0</th>           <td> 8909.5602</td> <td> 3367.099</td> <td>    2.646</td> <td> 0.008</td> <td> 2309.534</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3170.0</th>           <td> 1.049e+04</td> <td> 3163.430</td> <td>    3.317</td> <td> 0.001</td> <td> 4292.419</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3178.0</th>           <td>  476.3824</td> <td> 3058.630</td> <td>    0.156</td> <td> 0.876</td> <td>-5518.997</td> <td> 6471.762</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3206.0</th>           <td> 6647.9203</td> <td> 3682.551</td> <td>    1.805</td> <td> 0.071</td> <td> -570.439</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3229.0</th>           <td> 4699.0928</td> <td> 3154.197</td> <td>    1.490</td> <td> 0.136</td> <td>-1483.614</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3235.0</th>           <td> 1.043e+04</td> <td> 3672.530</td> <td>    2.840</td> <td> 0.005</td> <td> 3232.613</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3246.0</th>           <td> 8999.9332</td> <td> 3441.630</td> <td>    2.615</td> <td> 0.009</td> <td> 2253.815</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3248.0</th>           <td> 1.017e+04</td> <td> 3536.773</td> <td>    2.875</td> <td> 0.004</td> <td> 3235.421</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3282.0</th>           <td>-2.157e+04</td> <td> 3279.699</td> <td>   -6.578</td> <td> 0.000</td> <td> -2.8e+04</td> <td>-1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3362.0</th>           <td> 2423.5362</td> <td> 3770.430</td> <td>    0.643</td> <td> 0.520</td> <td>-4967.079</td> <td> 9814.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3372.0</th>           <td> 9627.7372</td> <td> 3873.154</td> <td>    2.486</td> <td> 0.013</td> <td> 2035.766</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3422.0</th>           <td> 9480.0410</td> <td> 3437.691</td> <td>    2.758</td> <td> 0.006</td> <td> 2741.643</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3497.0</th>           <td> 5252.3369</td> <td> 3141.534</td> <td>    1.672</td> <td> 0.095</td> <td> -905.547</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3502.0</th>           <td> 3238.2765</td> <td> 3011.798</td> <td>    1.075</td> <td> 0.282</td> <td>-2665.307</td> <td> 9141.860</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3504.0</th>           <td> 8995.7334</td> <td> 3981.420</td> <td>    2.259</td> <td> 0.024</td> <td> 1191.544</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3505.0</th>           <td> 9143.3764</td> <td> 3461.357</td> <td>    2.642</td> <td> 0.008</td> <td> 2358.589</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3532.0</th>           <td> 1.003e+04</td> <td> 3245.402</td> <td>    3.091</td> <td> 0.002</td> <td> 3669.185</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3574.0</th>           <td> 1.069e+04</td> <td> 5195.998</td> <td>    2.058</td> <td> 0.040</td> <td>  509.765</td> <td> 2.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3580.0</th>           <td> 6068.2467</td> <td> 3187.564</td> <td>    1.904</td> <td> 0.057</td> <td> -179.864</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3612.0</th>           <td>  1.02e+04</td> <td> 3452.504</td> <td>    2.955</td> <td> 0.003</td> <td> 3436.160</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3619.0</th>           <td> 7934.3369</td> <td> 3447.904</td> <td>    2.301</td> <td> 0.021</td> <td> 1175.921</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3622.0</th>           <td> 1.043e+04</td> <td> 3580.387</td> <td>    2.913</td> <td> 0.004</td> <td> 3411.652</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3639.0</th>           <td>-1512.7133</td> <td> 2960.576</td> <td>   -0.511</td> <td> 0.609</td> <td>-7315.893</td> <td> 4290.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3650.0</th>           <td> 3115.5849</td> <td> 3263.925</td> <td>    0.955</td> <td> 0.340</td> <td>-3282.206</td> <td> 9513.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3662.0</th>           <td> 1.006e+04</td> <td> 3445.692</td> <td>    2.919</td> <td> 0.004</td> <td> 3304.170</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3734.0</th>           <td>-7319.5906</td> <td> 3017.118</td> <td>   -2.426</td> <td> 0.015</td> <td>-1.32e+04</td> <td>-1405.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3735.0</th>           <td> 9975.8401</td> <td> 3692.019</td> <td>    2.702</td> <td> 0.007</td> <td> 2738.921</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3761.0</th>           <td> 6349.2844</td> <td> 3193.756</td> <td>    1.988</td> <td> 0.047</td> <td>   89.036</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3779.0</th>           <td>-8181.0780</td> <td> 3599.088</td> <td>   -2.273</td> <td> 0.023</td> <td>-1.52e+04</td> <td>-1126.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3781.0</th>           <td> -618.0552</td> <td> 3532.836</td> <td>   -0.175</td> <td> 0.861</td> <td>-7542.951</td> <td> 6306.840</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3782.0</th>           <td>-3555.6311</td> <td> 3135.968</td> <td>   -1.134</td> <td> 0.257</td> <td>-9702.606</td> <td> 2591.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3786.0</th>           <td> 7499.2694</td> <td> 3240.654</td> <td>    2.314</td> <td> 0.021</td> <td> 1147.094</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3796.0</th>           <td>-9059.0070</td> <td> 3588.977</td> <td>   -2.524</td> <td> 0.012</td> <td>-1.61e+04</td> <td>-2024.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3821.0</th>           <td> 9604.8814</td> <td> 3519.854</td> <td>    2.729</td> <td> 0.006</td> <td> 2705.432</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3835.0</th>           <td> 3076.1366</td> <td> 3410.513</td> <td>    0.902</td> <td> 0.367</td> <td>-3608.988</td> <td> 9761.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3839.0</th>           <td> 8341.2256</td> <td> 4032.054</td> <td>    2.069</td> <td> 0.039</td> <td>  437.786</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3840.0</th>           <td> -969.3453</td> <td> 2951.072</td> <td>   -0.328</td> <td> 0.743</td> <td>-6753.895</td> <td> 4815.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3895.0</th>           <td> 9871.3541</td> <td> 3445.298</td> <td>    2.865</td> <td> 0.004</td> <td> 3118.046</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3908.0</th>           <td> 3036.1131</td> <td> 4046.355</td> <td>    0.750</td> <td> 0.453</td> <td>-4895.359</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3911.0</th>           <td> 6304.1002</td> <td> 3262.976</td> <td>    1.932</td> <td> 0.053</td> <td>  -91.829</td> <td> 1.27e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3917.0</th>           <td> 9590.5921</td> <td> 3473.229</td> <td>    2.761</td> <td> 0.006</td> <td> 2782.534</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3946.0</th>           <td> 1.011e+04</td> <td> 3473.206</td> <td>    2.910</td> <td> 0.004</td> <td> 3300.234</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3971.0</th>           <td> 9572.0036</td> <td> 3496.097</td> <td>    2.738</td> <td> 0.006</td> <td> 2719.122</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>3980.0</th>           <td> 1.938e+04</td> <td> 3338.403</td> <td>    5.804</td> <td> 0.000</td> <td> 1.28e+04</td> <td> 2.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4034.0</th>           <td> 7772.7623</td> <td> 3372.613</td> <td>    2.305</td> <td> 0.021</td> <td> 1161.929</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4036.0</th>           <td>  1.05e+04</td> <td> 3491.567</td> <td>    3.008</td> <td> 0.003</td> <td> 3659.649</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4040.0</th>           <td> 1558.3206</td> <td> 2961.665</td> <td>    0.526</td> <td> 0.599</td> <td>-4246.994</td> <td> 7363.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4058.0</th>           <td> 8856.4981</td> <td> 3314.736</td> <td>    2.672</td> <td> 0.008</td> <td> 2359.111</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4060.0</th>           <td>-6647.6270</td> <td> 3194.633</td> <td>   -2.081</td> <td> 0.037</td> <td>-1.29e+04</td> <td> -385.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4062.0</th>           <td> 1.204e+04</td> <td> 3470.424</td> <td>    3.469</td> <td> 0.001</td> <td> 5237.107</td> <td> 1.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4077.0</th>           <td> 7312.3454</td> <td> 4387.204</td> <td>    1.667</td> <td> 0.096</td> <td>-1287.241</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4087.0</th>           <td>-1.765e+04</td> <td> 3493.298</td> <td>   -5.052</td> <td> 0.000</td> <td>-2.45e+04</td> <td>-1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4091.0</th>           <td> 7653.2198</td> <td> 3705.070</td> <td>    2.066</td> <td> 0.039</td> <td>  390.718</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4127.0</th>           <td> 3525.2756</td> <td> 3006.526</td> <td>    1.173</td> <td> 0.241</td> <td>-2367.972</td> <td> 9418.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4138.0</th>           <td> 1.059e+04</td> <td> 4042.581</td> <td>    2.620</td> <td> 0.009</td> <td> 2666.318</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4162.0</th>           <td> 9146.0786</td> <td> 3916.758</td> <td>    2.335</td> <td> 0.020</td> <td> 1468.636</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4186.0</th>           <td>  1.05e+04</td> <td> 3488.695</td> <td>    3.009</td> <td> 0.003</td> <td> 3658.523</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4194.0</th>           <td> -120.8974</td> <td> 3365.019</td> <td>   -0.036</td> <td> 0.971</td> <td>-6716.846</td> <td> 6475.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4199.0</th>           <td>-2262.4254</td> <td> 2969.652</td> <td>   -0.762</td> <td> 0.446</td> <td>-8083.396</td> <td> 3558.545</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4213.0</th>           <td> 1.047e+04</td> <td> 3437.887</td> <td>    3.045</td> <td> 0.002</td> <td> 3728.458</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4222.0</th>           <td>-4565.7812</td> <td> 3081.867</td> <td>   -1.481</td> <td> 0.138</td> <td>-1.06e+04</td> <td> 1475.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4223.0</th>           <td> 9852.3412</td> <td> 3425.086</td> <td>    2.877</td> <td> 0.004</td> <td> 3138.651</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4251.0</th>           <td> 1.021e+04</td> <td> 3471.456</td> <td>    2.940</td> <td> 0.003</td> <td> 3402.138</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4265.0</th>           <td> 7812.5812</td> <td> 3452.112</td> <td>    2.263</td> <td> 0.024</td> <td> 1045.916</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4274.0</th>           <td> 9412.2701</td> <td> 3542.496</td> <td>    2.657</td> <td> 0.008</td> <td> 2468.439</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4321.0</th>           <td> 9164.6583</td> <td> 3367.888</td> <td>    2.721</td> <td> 0.007</td> <td> 2563.085</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4335.0</th>           <td> 8190.6964</td> <td> 4783.515</td> <td>    1.712</td> <td> 0.087</td> <td>-1185.720</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4340.0</th>           <td> 6767.9340</td> <td> 3214.191</td> <td>    2.106</td> <td> 0.035</td> <td>  467.630</td> <td> 1.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4371.0</th>           <td> 8758.7466</td> <td> 3469.648</td> <td>    2.524</td> <td> 0.012</td> <td> 1957.708</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4415.0</th>           <td> 9054.9012</td> <td> 3466.775</td> <td>    2.612</td> <td> 0.009</td> <td> 2259.495</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4450.0</th>           <td> 7004.7705</td> <td> 3192.340</td> <td>    2.194</td> <td> 0.028</td> <td>  747.298</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4476.0</th>           <td> -537.3034</td> <td> 3155.397</td> <td>   -0.170</td> <td> 0.865</td> <td>-6722.362</td> <td> 5647.755</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4510.0</th>           <td> 5574.8780</td> <td> 3357.655</td> <td>    1.660</td> <td> 0.097</td> <td>-1006.636</td> <td> 1.22e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4520.0</th>           <td> 1.049e+04</td> <td> 3503.110</td> <td>    2.996</td> <td> 0.003</td> <td> 3627.664</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4551.0</th>           <td> 7925.1651</td> <td> 7335.131</td> <td>    1.080</td> <td> 0.280</td> <td>-6452.807</td> <td> 2.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4568.0</th>           <td> 1.039e+04</td> <td> 3626.066</td> <td>    2.865</td> <td> 0.004</td> <td> 3281.685</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4579.0</th>           <td>  1.01e+04</td> <td> 3454.228</td> <td>    2.924</td> <td> 0.003</td> <td> 3327.659</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4585.0</th>           <td> 1.023e+04</td> <td> 3532.907</td> <td>    2.895</td> <td> 0.004</td> <td> 3303.805</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4595.0</th>           <td> 7017.4950</td> <td> 3203.870</td> <td>    2.190</td> <td> 0.029</td> <td>  737.422</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4600.0</th>           <td>-2913.4675</td> <td> 3188.550</td> <td>   -0.914</td> <td> 0.361</td> <td>-9163.510</td> <td> 3336.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4607.0</th>           <td>  1.01e+04</td> <td> 3461.607</td> <td>    2.918</td> <td> 0.004</td> <td> 3314.622</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4608.0</th>           <td>-2467.3480</td> <td> 2996.140</td> <td>   -0.824</td> <td> 0.410</td> <td>-8340.238</td> <td> 3405.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4622.0</th>           <td> 8052.4631</td> <td> 3317.026</td> <td>    2.428</td> <td> 0.015</td> <td> 1550.588</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4623.0</th>           <td> 8958.3286</td> <td> 3477.238</td> <td>    2.576</td> <td> 0.010</td> <td> 2142.413</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4768.0</th>           <td> 9434.6639</td> <td> 3524.936</td> <td>    2.677</td> <td> 0.007</td> <td> 2525.254</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4771.0</th>           <td> 1.027e+04</td> <td> 3483.066</td> <td>    2.949</td> <td> 0.003</td> <td> 3445.939</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4800.0</th>           <td> 8501.1614</td> <td> 3530.706</td> <td>    2.408</td> <td> 0.016</td> <td> 1580.441</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4802.0</th>           <td> 1.039e+04</td> <td> 3495.460</td> <td>    2.971</td> <td> 0.003</td> <td> 3535.120</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4807.0</th>           <td> 1.021e+04</td> <td> 3570.231</td> <td>    2.859</td> <td> 0.004</td> <td> 3209.789</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4839.0</th>           <td>-1.356e+05</td> <td> 4533.275</td> <td>  -29.907</td> <td> 0.000</td> <td>-1.44e+05</td> <td>-1.27e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4843.0</th>           <td>-8167.6858</td> <td> 3490.884</td> <td>   -2.340</td> <td> 0.019</td> <td> -1.5e+04</td> <td>-1325.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4881.0</th>           <td> 8989.8462</td> <td> 3358.611</td> <td>    2.677</td> <td> 0.007</td> <td> 2406.458</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4900.0</th>           <td> 7258.8031</td> <td> 3240.831</td> <td>    2.240</td> <td> 0.025</td> <td>  906.282</td> <td> 1.36e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4926.0</th>           <td> 9046.0784</td> <td> 3409.453</td> <td>    2.653</td> <td> 0.008</td> <td> 2363.032</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4941.0</th>           <td> 8846.7949</td> <td> 3497.185</td> <td>    2.530</td> <td> 0.011</td> <td> 1991.780</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4961.0</th>           <td>-7068.5016</td> <td> 3842.536</td> <td>   -1.840</td> <td> 0.066</td> <td>-1.46e+04</td> <td>  463.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4988.0</th>           <td> 1.557e+04</td> <td> 3453.820</td> <td>    4.509</td> <td> 0.000</td> <td> 8802.201</td> <td> 2.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>4993.0</th>           <td> 1.045e+04</td> <td> 3497.931</td> <td>    2.988</td> <td> 0.003</td> <td> 3594.338</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5018.0</th>           <td> 7179.5577</td> <td> 3370.938</td> <td>    2.130</td> <td> 0.033</td> <td>  572.005</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5020.0</th>           <td>-5446.9435</td> <td> 3426.169</td> <td>   -1.590</td> <td> 0.112</td> <td>-1.22e+04</td> <td> 1268.869</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5027.0</th>           <td> 5845.9537</td> <td> 3181.465</td> <td>    1.838</td> <td> 0.066</td> <td> -390.201</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5032.0</th>           <td> 9857.7384</td> <td> 3470.574</td> <td>    2.840</td> <td> 0.005</td> <td> 3054.886</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5043.0</th>           <td> 5610.0072</td> <td> 3176.436</td> <td>    1.766</td> <td> 0.077</td> <td> -616.291</td> <td> 1.18e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5046.0</th>           <td>-1341.8874</td> <td> 3062.059</td> <td>   -0.438</td> <td> 0.661</td> <td>-7343.989</td> <td> 4660.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5047.0</th>           <td> 5.447e+04</td> <td> 4066.247</td> <td>   13.397</td> <td> 0.000</td> <td> 4.65e+04</td> <td> 6.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5065.0</th>           <td> 1.006e+04</td> <td> 3830.583</td> <td>    2.625</td> <td> 0.009</td> <td> 2547.975</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5071.0</th>           <td> 9512.9670</td> <td> 3843.083</td> <td>    2.475</td> <td> 0.013</td> <td> 1979.940</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5073.0</th>           <td>-2.008e+05</td> <td> 6312.374</td> <td>  -31.807</td> <td> 0.000</td> <td>-2.13e+05</td> <td>-1.88e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5087.0</th>           <td> 4797.4041</td> <td> 3156.086</td> <td>    1.520</td> <td> 0.129</td> <td>-1389.005</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5109.0</th>           <td> 1.013e+04</td> <td> 3496.987</td> <td>    2.898</td> <td> 0.004</td> <td> 3278.094</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5116.0</th>           <td>-3937.9453</td> <td> 3224.186</td> <td>   -1.221</td> <td> 0.222</td> <td>-1.03e+04</td> <td> 2381.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5122.0</th>           <td> 5517.2539</td> <td> 3154.948</td> <td>    1.749</td> <td> 0.080</td> <td> -666.925</td> <td> 1.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5134.0</th>           <td> 3884.2245</td> <td> 3594.240</td> <td>    1.081</td> <td> 0.280</td> <td>-3161.033</td> <td> 1.09e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5142.0</th>           <td> 6838.4021</td> <td> 3976.061</td> <td>    1.720</td> <td> 0.085</td> <td> -955.281</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5165.0</th>           <td> 4497.2071</td> <td> 3311.638</td> <td>    1.358</td> <td> 0.174</td> <td>-1994.108</td> <td>  1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5169.0</th>           <td> 1.916e+04</td> <td> 3342.007</td> <td>    5.734</td> <td> 0.000</td> <td> 1.26e+04</td> <td> 2.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5174.0</th>           <td> 6293.7590</td> <td> 3406.431</td> <td>    1.848</td> <td> 0.065</td> <td> -383.364</td> <td>  1.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5179.0</th>           <td> 9463.8885</td> <td> 3387.278</td> <td>    2.794</td> <td> 0.005</td> <td> 2824.307</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5181.0</th>           <td> 1.047e+04</td> <td> 3593.512</td> <td>    2.915</td> <td> 0.004</td> <td> 3430.631</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5187.0</th>           <td> 1.028e+04</td> <td> 3835.771</td> <td>    2.680</td> <td> 0.007</td> <td> 2762.068</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5229.0</th>           <td> 3372.9553</td> <td> 3100.033</td> <td>    1.088</td> <td> 0.277</td> <td>-2703.582</td> <td> 9449.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5234.0</th>           <td>-2373.8375</td> <td> 3557.256</td> <td>   -0.667</td> <td> 0.505</td> <td>-9346.600</td> <td> 4598.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5237.0</th>           <td> 9274.2111</td> <td> 3376.757</td> <td>    2.746</td> <td> 0.006</td> <td> 2655.255</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5252.0</th>           <td> 9398.7830</td> <td> 3395.434</td> <td>    2.768</td> <td> 0.006</td> <td> 2743.215</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5254.0</th>           <td> 9895.7852</td> <td> 3468.171</td> <td>    2.853</td> <td> 0.004</td> <td> 3097.642</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5306.0</th>           <td> 9991.3769</td> <td> 3380.819</td> <td>    2.955</td> <td> 0.003</td> <td> 3364.457</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5338.0</th>           <td> 1.017e+04</td> <td> 3461.718</td> <td>    2.937</td> <td> 0.003</td> <td> 3379.876</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5377.0</th>           <td> 1.013e+04</td> <td> 3504.604</td> <td>    2.891</td> <td> 0.004</td> <td> 3263.929</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5439.0</th>           <td> 7549.1619</td> <td> 3383.094</td> <td>    2.231</td> <td> 0.026</td> <td>  917.783</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5456.0</th>           <td> 1.043e+04</td> <td> 3529.032</td> <td>    2.955</td> <td> 0.003</td> <td> 3510.479</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5464.0</th>           <td> 9243.4459</td> <td> 4033.476</td> <td>    2.292</td> <td> 0.022</td> <td> 1337.220</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5476.0</th>           <td> 9316.0567</td> <td> 3359.046</td> <td>    2.773</td> <td> 0.006</td> <td> 2731.816</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5492.0</th>           <td>-1.083e+04</td> <td> 3431.449</td> <td>   -3.157</td> <td> 0.002</td> <td>-1.76e+04</td> <td>-4106.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5496.0</th>           <td> 9305.5943</td> <td> 3421.714</td> <td>    2.720</td> <td> 0.007</td> <td> 2598.514</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5505.0</th>           <td> 9887.5592</td> <td> 3496.472</td> <td>    2.828</td> <td> 0.005</td> <td> 3033.942</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5518.0</th>           <td> 8043.4491</td> <td> 3674.641</td> <td>    2.189</td> <td> 0.029</td> <td>  840.594</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5520.0</th>           <td> 7086.8585</td> <td> 3241.750</td> <td>    2.186</td> <td> 0.029</td> <td>  732.536</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5545.0</th>           <td> 1.061e+04</td> <td> 3626.502</td> <td>    2.926</td> <td> 0.003</td> <td> 3503.952</td> <td> 1.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5568.0</th>           <td> 1.329e+04</td> <td> 3515.859</td> <td>    3.779</td> <td> 0.000</td> <td> 6394.215</td> <td> 2.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5569.0</th>           <td> 1.038e+04</td> <td> 3521.677</td> <td>    2.947</td> <td> 0.003</td> <td> 3474.427</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5578.0</th>           <td> 9794.0748</td> <td> 3412.408</td> <td>    2.870</td> <td> 0.004</td> <td> 3105.235</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5581.0</th>           <td> 9726.6032</td> <td> 3379.409</td> <td>    2.878</td> <td> 0.004</td> <td> 3102.448</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5589.0</th>           <td> 4310.6488</td> <td> 3071.264</td> <td>    1.404</td> <td> 0.160</td> <td>-1709.495</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5597.0</th>           <td> 1.225e+04</td> <td> 3985.852</td> <td>    3.073</td> <td> 0.002</td> <td> 4436.767</td> <td> 2.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5606.0</th>           <td>-2.818e+04</td> <td> 3156.085</td> <td>   -8.928</td> <td> 0.000</td> <td>-3.44e+04</td> <td> -2.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5639.0</th>           <td> 1.079e+04</td> <td> 3475.889</td> <td>    3.103</td> <td> 0.002</td> <td> 3972.430</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5667.0</th>           <td> 4408.3690</td> <td> 3081.766</td> <td>    1.430</td> <td> 0.153</td> <td>-1632.362</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5690.0</th>           <td> 1.044e+04</td> <td> 3495.787</td> <td>    2.986</td> <td> 0.003</td> <td> 3586.243</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5709.0</th>           <td> 9489.9110</td> <td> 3484.902</td> <td>    2.723</td> <td> 0.006</td> <td> 2658.972</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5726.0</th>           <td> 8843.5967</td> <td> 3350.233</td> <td>    2.640</td> <td> 0.008</td> <td> 2276.631</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5764.0</th>           <td> 9607.0617</td> <td> 3330.031</td> <td>    2.885</td> <td> 0.004</td> <td> 3079.694</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5772.0</th>           <td> 1.009e+04</td> <td> 3477.233</td> <td>    2.901</td> <td> 0.004</td> <td> 3272.074</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5860.0</th>           <td>-2.137e+04</td> <td> 3183.508</td> <td>   -6.714</td> <td> 0.000</td> <td>-2.76e+04</td> <td>-1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5878.0</th>           <td> 1.288e+04</td> <td> 3373.262</td> <td>    3.819</td> <td> 0.000</td> <td> 6271.821</td> <td> 1.95e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5903.0</th>           <td> 3056.9152</td> <td> 3093.343</td> <td>    0.988</td> <td> 0.323</td> <td>-3006.507</td> <td> 9120.338</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5905.0</th>           <td> 8382.9179</td> <td> 3504.017</td> <td>    2.392</td> <td> 0.017</td> <td> 1514.512</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>5959.0</th>           <td> 6222.0789</td> <td> 3427.640</td> <td>    1.815</td> <td> 0.070</td> <td> -496.618</td> <td> 1.29e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6008.0</th>           <td> 2.945e+04</td> <td> 3053.695</td> <td>    9.643</td> <td> 0.000</td> <td> 2.35e+04</td> <td> 3.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6034.0</th>           <td> 1.011e+04</td> <td> 3585.086</td> <td>    2.820</td> <td> 0.005</td> <td> 3081.561</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6035.0</th>           <td> 1483.7234</td> <td> 3506.418</td> <td>    0.423</td> <td> 0.672</td> <td>-5389.389</td> <td> 8356.836</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6036.0</th>           <td>  221.6732</td> <td> 2991.364</td> <td>    0.074</td> <td> 0.941</td> <td>-5641.855</td> <td> 6085.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6039.0</th>           <td> 9544.5704</td> <td> 3432.253</td> <td>    2.781</td> <td> 0.005</td> <td> 2816.832</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6044.0</th>           <td> 1.016e+04</td> <td> 3681.651</td> <td>    2.760</td> <td> 0.006</td> <td> 2942.992</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6066.0</th>           <td>-2.346e+04</td> <td> 4482.310</td> <td>   -5.233</td> <td> 0.000</td> <td>-3.22e+04</td> <td>-1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6078.0</th>           <td> 9706.7472</td> <td> 3249.116</td> <td>    2.988</td> <td> 0.003</td> <td> 3337.985</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6081.0</th>           <td>-1.153e+04</td> <td> 3146.489</td> <td>   -3.664</td> <td> 0.000</td> <td>-1.77e+04</td> <td>-5361.713</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>60893.0</th>          <td> -1.28e+04</td> <td> 4529.427</td> <td>   -2.826</td> <td> 0.005</td> <td>-2.17e+04</td> <td>-3921.010</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6097.0</th>           <td> 9210.7131</td> <td> 3343.462</td> <td>    2.755</td> <td> 0.006</td> <td> 2657.018</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6102.0</th>           <td> 9066.2170</td> <td> 3480.579</td> <td>    2.605</td> <td> 0.009</td> <td> 2243.752</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6104.0</th>           <td>  232.2733</td> <td> 3423.042</td> <td>    0.068</td> <td> 0.946</td> <td>-6477.409</td> <td> 6941.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6109.0</th>           <td> 1010.3528</td> <td> 2947.059</td> <td>    0.343</td> <td> 0.732</td> <td>-4766.331</td> <td> 6787.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6127.0</th>           <td> 4701.1928</td> <td> 3757.688</td> <td>    1.251</td> <td> 0.211</td> <td>-2664.447</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61552.0</th>          <td>-4432.5561</td> <td> 4243.880</td> <td>   -1.044</td> <td> 0.296</td> <td>-1.28e+04</td> <td> 3886.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6158.0</th>           <td> 6664.1488</td> <td> 3326.127</td> <td>    2.004</td> <td> 0.045</td> <td>  144.434</td> <td> 1.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6171.0</th>           <td> 9441.2528</td> <td> 3383.795</td> <td>    2.790</td> <td> 0.005</td> <td> 2808.500</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>61780.0</th>          <td> 9463.2455</td> <td> 5197.187</td> <td>    1.821</td> <td> 0.069</td> <td> -724.032</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6207.0</th>           <td> 8858.3816</td> <td> 3348.787</td> <td>    2.645</td> <td> 0.008</td> <td> 2294.250</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6214.0</th>           <td> 1.051e+04</td> <td> 3500.804</td> <td>    3.002</td> <td> 0.003</td> <td> 3647.047</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6216.0</th>           <td> 9305.6197</td> <td> 3433.501</td> <td>    2.710</td> <td> 0.007</td> <td> 2575.436</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62221.0</th>          <td> 9636.3504</td> <td> 4648.363</td> <td>    2.073</td> <td> 0.038</td> <td>  524.852</td> <td> 1.87e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6259.0</th>           <td> 1362.8316</td> <td> 3271.611</td> <td>    0.417</td> <td> 0.677</td> <td>-5050.023</td> <td> 7775.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>62599.0</th>          <td>-2.068e+04</td> <td> 4912.844</td> <td>   -4.210</td> <td> 0.000</td> <td>-3.03e+04</td> <td>-1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6266.0</th>           <td> 5707.1489</td> <td> 3198.854</td> <td>    1.784</td> <td> 0.074</td> <td> -563.092</td> <td>  1.2e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6268.0</th>           <td> 5194.0633</td> <td> 3510.777</td> <td>    1.479</td> <td> 0.139</td> <td>-1687.594</td> <td> 1.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6288.0</th>           <td> 9663.1685</td> <td> 3447.626</td> <td>    2.803</td> <td> 0.005</td> <td> 2905.297</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6297.0</th>           <td> 1.056e+04</td> <td> 3595.867</td> <td>    2.937</td> <td> 0.003</td> <td> 3511.586</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6307.0</th>           <td>-1.092e+04</td> <td> 4063.132</td> <td>   -2.688</td> <td> 0.007</td> <td>-1.89e+04</td> <td>-2956.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6313.0</th>           <td> 8891.8083</td> <td> 4582.140</td> <td>    1.941</td> <td> 0.052</td> <td>  -89.882</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6314.0</th>           <td> 9311.8048</td> <td> 3390.368</td> <td>    2.747</td> <td> 0.006</td> <td> 2666.168</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6326.0</th>           <td> 6072.5560</td> <td> 3163.322</td> <td>    1.920</td> <td> 0.055</td> <td> -128.036</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6349.0</th>           <td> 9207.5642</td> <td> 3389.098</td> <td>    2.717</td> <td> 0.007</td> <td> 2564.416</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6357.0</th>           <td> 1.024e+04</td> <td> 3585.280</td> <td>    2.857</td> <td> 0.004</td> <td> 3216.274</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6375.0</th>           <td> 1.444e+04</td> <td> 3464.702</td> <td>    4.168</td> <td> 0.000</td> <td> 7650.844</td> <td> 2.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6376.0</th>           <td> 9678.6118</td> <td> 3484.646</td> <td>    2.778</td> <td> 0.005</td> <td> 2848.175</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6379.0</th>           <td>   26.6996</td> <td> 6333.401</td> <td>    0.004</td> <td> 0.997</td> <td>-1.24e+04</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6386.0</th>           <td> 1.048e+04</td> <td> 3498.806</td> <td>    2.994</td> <td> 0.003</td> <td> 3618.477</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6403.0</th>           <td> 4977.6386</td> <td> 3259.208</td> <td>    1.527</td> <td> 0.127</td> <td>-1410.906</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6410.0</th>           <td>  1.05e+04</td> <td> 3527.394</td> <td>    2.977</td> <td> 0.003</td> <td> 3586.498</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6416.0</th>           <td> 2347.3189</td> <td> 3090.659</td> <td>    0.759</td> <td> 0.448</td> <td>-3710.844</td> <td> 8405.482</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6424.0</th>           <td> 9621.0992</td> <td> 3436.334</td> <td>    2.800</td> <td> 0.005</td> <td> 2885.363</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6433.0</th>           <td> 8661.7986</td> <td> 3343.259</td> <td>    2.591</td> <td> 0.010</td> <td> 2108.503</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6435.0</th>           <td> 1.183e+04</td> <td> 3360.879</td> <td>    3.519</td> <td> 0.000</td> <td> 5240.102</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6492.0</th>           <td> 8974.9088</td> <td> 3432.021</td> <td>    2.615</td> <td> 0.009</td> <td> 2247.626</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6497.0</th>           <td>-3053.3190</td> <td> 3013.220</td> <td>   -1.013</td> <td> 0.311</td> <td>-8959.689</td> <td> 2853.051</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6500.0</th>           <td> 6150.1570</td> <td> 7269.626</td> <td>    0.846</td> <td> 0.398</td> <td>-8099.415</td> <td> 2.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6509.0</th>           <td> 9556.9483</td> <td> 3435.340</td> <td>    2.782</td> <td> 0.005</td> <td> 2823.159</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6527.0</th>           <td> 1.047e+04</td> <td> 3729.698</td> <td>    2.807</td> <td> 0.005</td> <td> 3158.019</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6528.0</th>           <td> 8269.4671</td> <td> 3499.188</td> <td>    2.363</td> <td> 0.018</td> <td> 1410.526</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6531.0</th>           <td> 3609.6244</td> <td> 3468.218</td> <td>    1.041</td> <td> 0.298</td> <td>-3188.611</td> <td> 1.04e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6532.0</th>           <td> 5989.2996</td> <td> 3226.740</td> <td>    1.856</td> <td> 0.063</td> <td> -335.601</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6543.0</th>           <td> 1.019e+04</td> <td> 3488.637</td> <td>    2.922</td> <td> 0.003</td> <td> 3355.752</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6548.0</th>           <td> 9642.4815</td> <td> 3462.712</td> <td>    2.785</td> <td> 0.005</td> <td> 2855.040</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6550.0</th>           <td> 1.029e+04</td> <td> 3711.831</td> <td>    2.772</td> <td> 0.006</td> <td> 3013.513</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6552.0</th>           <td> 9481.6941</td> <td> 3579.910</td> <td>    2.649</td> <td> 0.008</td> <td> 2464.527</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6565.0</th>           <td> 8646.7354</td> <td> 3544.299</td> <td>    2.440</td> <td> 0.015</td> <td> 1699.371</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6571.0</th>           <td> 9933.4396</td> <td> 3461.508</td> <td>    2.870</td> <td> 0.004</td> <td> 3148.358</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6573.0</th>           <td> 9777.5503</td> <td> 3422.534</td> <td>    2.857</td> <td> 0.004</td> <td> 3068.863</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6641.0</th>           <td> 6780.5002</td> <td> 5403.773</td> <td>    1.255</td> <td> 0.210</td> <td>-3811.717</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6649.0</th>           <td> 1.063e+04</td> <td> 3496.881</td> <td>    3.041</td> <td> 0.002</td> <td> 3779.297</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6730.0</th>           <td> 3503.1057</td> <td> 3291.625</td> <td>    1.064</td> <td> 0.287</td> <td>-2948.979</td> <td> 9955.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6731.0</th>           <td> 9345.3044</td> <td> 3489.365</td> <td>    2.678</td> <td> 0.007</td> <td> 2505.618</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6742.0</th>           <td> 9178.2023</td> <td> 4815.789</td> <td>    1.906</td> <td> 0.057</td> <td> -261.476</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6745.0</th>           <td> 1.058e+04</td> <td> 3564.650</td> <td>    2.968</td> <td> 0.003</td> <td> 3592.600</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6756.0</th>           <td> 9146.2023</td> <td> 3364.036</td> <td>    2.719</td> <td> 0.007</td> <td> 2552.180</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6765.0</th>           <td>-6705.0520</td> <td> 3084.029</td> <td>   -2.174</td> <td> 0.030</td> <td>-1.28e+04</td> <td> -659.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6768.0</th>           <td> 1.103e+04</td> <td> 3514.640</td> <td>    3.140</td> <td> 0.002</td> <td> 4145.597</td> <td> 1.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6774.0</th>           <td>-1.801e+04</td> <td> 3543.800</td> <td>   -5.083</td> <td> 0.000</td> <td> -2.5e+04</td> <td>-1.11e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6797.0</th>           <td> 1.026e+04</td> <td> 3843.351</td> <td>    2.670</td> <td> 0.008</td> <td> 2729.513</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6803.0</th>           <td> 9607.2274</td> <td> 3435.986</td> <td>    2.796</td> <td> 0.005</td> <td> 2872.172</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6821.0</th>           <td> 9500.5977</td> <td> 3458.877</td> <td>    2.747</td> <td> 0.006</td> <td> 2720.672</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6830.0</th>           <td> 7437.6025</td> <td> 3255.407</td> <td>    2.285</td> <td> 0.022</td> <td> 1056.509</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6845.0</th>           <td> 9803.0965</td> <td> 3438.070</td> <td>    2.851</td> <td> 0.004</td> <td> 3063.956</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6848.0</th>           <td> 8333.6403</td> <td> 3496.836</td> <td>    2.383</td> <td> 0.017</td> <td> 1479.309</td> <td> 1.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6873.0</th>           <td> 6772.6473</td> <td> 4025.271</td> <td>    1.683</td> <td> 0.092</td> <td>-1117.495</td> <td> 1.47e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6900.0</th>           <td> 9887.2618</td> <td> 3448.768</td> <td>    2.867</td> <td> 0.004</td> <td> 3127.151</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6908.0</th>           <td> 9670.9945</td> <td> 3412.881</td> <td>    2.834</td> <td> 0.005</td> <td> 2981.228</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>6994.0</th>           <td> 7027.9999</td> <td> 3190.940</td> <td>    2.202</td> <td> 0.028</td> <td>  773.272</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7045.0</th>           <td>-1365.3935</td> <td> 3609.225</td> <td>   -0.378</td> <td> 0.705</td> <td>-8440.023</td> <td> 5709.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7065.0</th>           <td> 1.611e+04</td> <td> 3506.227</td> <td>    4.596</td> <td> 0.000</td> <td> 9241.876</td> <td>  2.3e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7085.0</th>           <td> 1.278e+04</td> <td> 3490.166</td> <td>    3.663</td> <td> 0.000</td> <td> 5943.361</td> <td> 1.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7107.0</th>           <td> 1.016e+04</td> <td> 3620.898</td> <td>    2.807</td> <td> 0.005</td> <td> 3065.851</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7116.0</th>           <td> 9950.6944</td> <td> 3458.392</td> <td>    2.877</td> <td> 0.004</td> <td> 3171.720</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7117.0</th>           <td> 9962.5160</td> <td> 4171.308</td> <td>    2.388</td> <td> 0.017</td> <td> 1786.117</td> <td> 1.81e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7121.0</th>           <td> 8806.0934</td> <td> 3370.809</td> <td>    2.612</td> <td> 0.009</td> <td> 2198.794</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7127.0</th>           <td> 5683.8420</td> <td> 3400.609</td> <td>    1.671</td> <td> 0.095</td> <td> -981.868</td> <td> 1.23e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7139.0</th>           <td> 9692.1878</td> <td> 3477.874</td> <td>    2.787</td> <td> 0.005</td> <td> 2875.026</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7146.0</th>           <td> 1.012e+04</td> <td> 3465.747</td> <td>    2.919</td> <td> 0.004</td> <td> 3323.612</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7163.0</th>           <td>  1.31e+04</td> <td> 3467.196</td> <td>    3.777</td> <td> 0.000</td> <td> 6299.856</td> <td> 1.99e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7180.0</th>           <td> 7580.6776</td> <td> 3426.253</td> <td>    2.213</td> <td> 0.027</td> <td>  864.700</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7183.0</th>           <td> 9122.0661</td> <td> 3445.881</td> <td>    2.647</td> <td> 0.008</td> <td> 2367.616</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7228.0</th>           <td> 1.691e+04</td> <td> 3336.789</td> <td>    5.067</td> <td> 0.000</td> <td> 1.04e+04</td> <td> 2.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7232.0</th>           <td> 6459.3309</td> <td> 4154.287</td> <td>    1.555</td> <td> 0.120</td> <td>-1683.704</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7250.0</th>           <td> 5903.5751</td> <td> 3433.908</td> <td>    1.719</td> <td> 0.086</td> <td> -827.408</td> <td> 1.26e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7257.0</th>           <td> 2.454e+04</td> <td> 3288.256</td> <td>    7.464</td> <td> 0.000</td> <td> 1.81e+04</td> <td>  3.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7260.0</th>           <td> 9960.5244</td> <td> 3417.721</td> <td>    2.914</td> <td> 0.004</td> <td> 3261.272</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7267.0</th>           <td> 8789.4484</td> <td> 3543.555</td> <td>    2.480</td> <td> 0.013</td> <td> 1843.541</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7268.0</th>           <td>-4864.0786</td> <td> 3160.501</td> <td>   -1.539</td> <td> 0.124</td> <td>-1.11e+04</td> <td> 1330.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7281.0</th>           <td> 1.024e+04</td> <td> 4245.639</td> <td>    2.411</td> <td> 0.016</td> <td> 1915.827</td> <td> 1.86e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7291.0</th>           <td> 9738.4799</td> <td> 3446.044</td> <td>    2.826</td> <td> 0.005</td> <td> 2983.710</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7343.0</th>           <td> 2811.3153</td> <td> 3688.762</td> <td>    0.762</td> <td> 0.446</td> <td>-4419.220</td> <td>    1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7346.0</th>           <td>  510.6111</td> <td> 2979.289</td> <td>    0.171</td> <td> 0.864</td> <td>-5329.248</td> <td> 6350.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7401.0</th>           <td> 1.037e+04</td> <td> 3532.412</td> <td>    2.935</td> <td> 0.003</td> <td> 3442.141</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7409.0</th>           <td> 9957.6138</td> <td> 3452.343</td> <td>    2.884</td> <td> 0.004</td> <td> 3190.497</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7420.0</th>           <td> 7030.1192</td> <td> 3222.589</td> <td>    2.182</td> <td> 0.029</td> <td>  713.354</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7435.0</th>           <td> 9960.7449</td> <td> 3257.812</td> <td>    3.057</td> <td> 0.002</td> <td> 3574.937</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7466.0</th>           <td> 8121.3534</td> <td> 3464.126</td> <td>    2.344</td> <td> 0.019</td> <td> 1331.140</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7486.0</th>           <td>-3690.8571</td> <td> 3031.916</td> <td>   -1.217</td> <td> 0.223</td> <td>-9633.873</td> <td> 2252.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7503.0</th>           <td> 9995.7891</td> <td> 4813.311</td> <td>    2.077</td> <td> 0.038</td> <td>  560.967</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7506.0</th>           <td> 1.114e+04</td> <td> 3408.724</td> <td>    3.269</td> <td> 0.001</td> <td> 4461.736</td> <td> 1.78e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7537.0</th>           <td> 9056.8261</td> <td> 3396.774</td> <td>    2.666</td> <td> 0.008</td> <td> 2398.632</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7549.0</th>           <td> 9421.4226</td> <td> 3438.414</td> <td>    2.740</td> <td> 0.006</td> <td> 2681.608</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7554.0</th>           <td> 9079.1061</td> <td> 3444.815</td> <td>    2.636</td> <td> 0.008</td> <td> 2326.745</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7557.0</th>           <td> 9586.5450</td> <td> 3506.696</td> <td>    2.734</td> <td> 0.006</td> <td> 2712.887</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7585.0</th>           <td>-1.767e+04</td> <td> 3412.507</td> <td>   -5.179</td> <td> 0.000</td> <td>-2.44e+04</td> <td> -1.1e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7602.0</th>           <td> 8879.0584</td> <td> 3347.634</td> <td>    2.652</td> <td> 0.008</td> <td> 2317.187</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7620.0</th>           <td> 4824.0196</td> <td> 3317.870</td> <td>    1.454</td> <td> 0.146</td> <td>-1679.510</td> <td> 1.13e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7636.0</th>           <td> 9195.8774</td> <td> 3376.389</td> <td>    2.724</td> <td> 0.006</td> <td> 2577.642</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7646.0</th>           <td> 9544.3596</td> <td> 3443.343</td> <td>    2.772</td> <td> 0.006</td> <td> 2794.883</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7658.0</th>           <td> 8846.6138</td> <td> 3378.292</td> <td>    2.619</td> <td> 0.009</td> <td> 2224.648</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7683.0</th>           <td> 1.075e+04</td> <td> 3675.409</td> <td>    2.925</td> <td> 0.003</td> <td> 3546.037</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7685.0</th>           <td> 9783.4865</td> <td> 3575.982</td> <td>    2.736</td> <td> 0.006</td> <td> 2774.018</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7692.0</th>           <td> 5094.7382</td> <td> 3102.002</td> <td>    1.642</td> <td> 0.101</td> <td> -985.659</td> <td> 1.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7762.0</th>           <td> 1.038e+04</td> <td> 3464.947</td> <td>    2.995</td> <td> 0.003</td> <td> 3586.497</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7772.0</th>           <td>-1371.3064</td> <td> 3005.129</td> <td>   -0.456</td> <td> 0.648</td> <td>-7261.817</td> <td> 4519.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7773.0</th>           <td> 9554.4855</td> <td> 3463.000</td> <td>    2.759</td> <td> 0.006</td> <td> 2766.478</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7777.0</th>           <td> 6152.5489</td> <td> 3204.358</td> <td>    1.920</td> <td> 0.055</td> <td> -128.480</td> <td> 1.24e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7835.0</th>           <td> 1.054e+04</td> <td> 3518.681</td> <td>    2.996</td> <td> 0.003</td> <td> 3643.560</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7873.0</th>           <td>-2813.4654</td> <td> 2999.335</td> <td>   -0.938</td> <td> 0.348</td> <td>-8692.619</td> <td> 3065.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7883.0</th>           <td> 9932.7429</td> <td> 3433.142</td> <td>    2.893</td> <td> 0.004</td> <td> 3203.262</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7904.0</th>           <td> 9043.3757</td> <td> 3486.366</td> <td>    2.594</td> <td> 0.009</td> <td> 2209.568</td> <td> 1.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7906.0</th>           <td> 1.253e+04</td> <td> 3521.991</td> <td>    3.558</td> <td> 0.000</td> <td> 5627.449</td> <td> 1.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7921.0</th>           <td> 1.079e+04</td> <td> 3466.265</td> <td>    3.112</td> <td> 0.002</td> <td> 3993.016</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7923.0</th>           <td> 8674.4178</td> <td> 3539.688</td> <td>    2.451</td> <td> 0.014</td> <td> 1736.090</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7935.0</th>           <td> 7444.2116</td> <td> 3273.992</td> <td>    2.274</td> <td> 0.023</td> <td> 1026.689</td> <td> 1.39e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7938.0</th>           <td> 8845.2456</td> <td> 3467.407</td> <td>    2.551</td> <td> 0.011</td> <td> 2048.600</td> <td> 1.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>7985.0</th>           <td>-1.091e+04</td> <td> 3104.873</td> <td>   -3.515</td> <td> 0.000</td> <td> -1.7e+04</td> <td>-4828.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8014.0</th>           <td> 7773.1550</td> <td> 3358.207</td> <td>    2.315</td> <td> 0.021</td> <td> 1190.559</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8030.0</th>           <td> 1.024e+04</td> <td> 3432.790</td> <td>    2.983</td> <td> 0.003</td> <td> 3510.766</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8046.0</th>           <td>-2361.1775</td> <td> 3083.758</td> <td>   -0.766</td> <td> 0.444</td> <td>-8405.812</td> <td> 3683.457</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8047.0</th>           <td> 9758.5162</td> <td> 3946.299</td> <td>    2.473</td> <td> 0.013</td> <td> 2023.169</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8062.0</th>           <td> 8420.4733</td> <td> 3407.990</td> <td>    2.471</td> <td> 0.013</td> <td> 1740.294</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8068.0</th>           <td>-5735.8824</td> <td> 3541.043</td> <td>   -1.620</td> <td> 0.105</td> <td>-1.27e+04</td> <td> 1205.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8087.0</th>           <td>-6967.8963</td> <td> 3297.326</td> <td>   -2.113</td> <td> 0.035</td> <td>-1.34e+04</td> <td> -504.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8095.0</th>           <td> 1.036e+04</td> <td> 3503.810</td> <td>    2.956</td> <td> 0.003</td> <td> 3490.766</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8096.0</th>           <td> 9442.4911</td> <td> 3396.980</td> <td>    2.780</td> <td> 0.005</td> <td> 2783.893</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8109.0</th>           <td> 1.031e+04</td> <td> 3485.190</td> <td>    2.957</td> <td> 0.003</td> <td> 3474.529</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8123.0</th>           <td> 9497.2222</td> <td> 3457.064</td> <td>    2.747</td> <td> 0.006</td> <td> 2720.850</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8150.0</th>           <td> 1.035e+04</td> <td> 3499.858</td> <td>    2.956</td> <td> 0.003</td> <td> 3486.945</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8163.0</th>           <td>  1.04e+04</td> <td> 3545.844</td> <td>    2.934</td> <td> 0.003</td> <td> 3451.826</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8176.0</th>           <td> 1380.9099</td> <td> 3340.324</td> <td>    0.413</td> <td> 0.679</td> <td>-5166.633</td> <td> 7928.453</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8202.0</th>           <td> 8928.8530</td> <td> 3496.105</td> <td>    2.554</td> <td> 0.011</td> <td> 2075.956</td> <td> 1.58e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8214.0</th>           <td> 7951.0174</td> <td> 3472.900</td> <td>    2.289</td> <td> 0.022</td> <td> 1143.605</td> <td> 1.48e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8215.0</th>           <td> 6869.4339</td> <td> 3533.851</td> <td>    1.944</td> <td> 0.052</td> <td>  -57.451</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8219.0</th>           <td>  1.01e+04</td> <td> 3521.286</td> <td>    2.868</td> <td> 0.004</td> <td> 3195.379</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8247.0</th>           <td> 7517.0591</td> <td> 3412.455</td> <td>    2.203</td> <td> 0.028</td> <td>  828.128</td> <td> 1.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8253.0</th>           <td>-5960.1192</td> <td> 3082.870</td> <td>   -1.933</td> <td> 0.053</td> <td> -1.2e+04</td> <td>   82.775</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8290.0</th>           <td> 7523.9306</td> <td> 3581.453</td> <td>    2.101</td> <td> 0.036</td> <td>  503.737</td> <td> 1.45e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8293.0</th>           <td> 1.049e+04</td> <td> 3497.907</td> <td>    2.999</td> <td> 0.003</td> <td> 3632.477</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8304.0</th>           <td> 1.052e+04</td> <td> 3404.658</td> <td>    3.090</td> <td> 0.002</td> <td> 3846.046</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8334.0</th>           <td> 8459.9005</td> <td> 3390.789</td> <td>    2.495</td> <td> 0.013</td> <td> 1813.438</td> <td> 1.51e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8348.0</th>           <td> 1.033e+04</td> <td> 3541.616</td> <td>    2.918</td> <td> 0.004</td> <td> 3392.106</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8357.0</th>           <td> 9916.7234</td> <td> 3441.737</td> <td>    2.881</td> <td> 0.004</td> <td> 3170.395</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8358.0</th>           <td> 8999.4511</td> <td> 3440.726</td> <td>    2.616</td> <td> 0.009</td> <td> 2255.105</td> <td> 1.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8446.0</th>           <td>-1264.4703</td> <td> 3768.625</td> <td>   -0.336</td> <td> 0.737</td> <td>-8651.548</td> <td> 6122.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8460.0</th>           <td> 1.089e+04</td> <td> 3837.834</td> <td>    2.838</td> <td> 0.005</td> <td> 3368.395</td> <td> 1.84e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8463.0</th>           <td> 9275.8747</td> <td> 3429.308</td> <td>    2.705</td> <td> 0.007</td> <td> 2553.909</td> <td>  1.6e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8479.0</th>           <td> 1.002e+04</td> <td> 4084.448</td> <td>    2.454</td> <td> 0.014</td> <td> 2015.762</td> <td>  1.8e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8530.0</th>           <td>  1.51e+04</td> <td> 3389.501</td> <td>    4.455</td> <td> 0.000</td> <td> 8454.778</td> <td> 2.17e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8536.0</th>           <td> 7703.1720</td> <td> 3360.553</td> <td>    2.292</td> <td> 0.022</td> <td> 1115.978</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8543.0</th>           <td> 2.968e+04</td> <td> 3685.258</td> <td>    8.053</td> <td> 0.000</td> <td> 2.25e+04</td> <td> 3.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8549.0</th>           <td> -314.6492</td> <td> 3224.852</td> <td>   -0.098</td> <td> 0.922</td> <td>-6635.849</td> <td> 6006.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8551.0</th>           <td> 9873.1363</td> <td> 3485.903</td> <td>    2.832</td> <td> 0.005</td> <td> 3040.235</td> <td> 1.67e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8559.0</th>           <td> 4491.5178</td> <td> 3212.952</td> <td>    1.398</td> <td> 0.162</td> <td>-1806.358</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8573.0</th>           <td>-3760.3739</td> <td> 3245.720</td> <td>   -1.159</td> <td> 0.247</td> <td>-1.01e+04</td> <td> 2601.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8606.0</th>           <td> 1.163e+04</td> <td> 3484.800</td> <td>    3.338</td> <td> 0.001</td> <td> 4802.881</td> <td> 1.85e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8607.0</th>           <td> 9805.6659</td> <td> 3477.735</td> <td>    2.820</td> <td> 0.005</td> <td> 2988.777</td> <td> 1.66e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8648.0</th>           <td> 9658.8439</td> <td> 3419.758</td> <td>    2.824</td> <td> 0.005</td> <td> 2955.598</td> <td> 1.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8657.0</th>           <td> 2916.5634</td> <td> 3074.140</td> <td>    0.949</td> <td> 0.343</td> <td>-3109.219</td> <td> 8942.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8675.0</th>           <td> 8484.9073</td> <td> 4577.829</td> <td>    1.853</td> <td> 0.064</td> <td> -488.333</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8681.0</th>           <td> 5340.5429</td> <td> 3103.645</td> <td>    1.721</td> <td> 0.085</td> <td> -743.073</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8687.0</th>           <td> 7564.8392</td> <td> 3485.445</td> <td>    2.170</td> <td> 0.030</td> <td>  732.837</td> <td> 1.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8692.0</th>           <td> 8275.6065</td> <td> 3374.689</td> <td>    2.452</td> <td> 0.014</td> <td> 1660.703</td> <td> 1.49e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8699.0</th>           <td> 1.015e+04</td> <td> 3469.228</td> <td>    2.926</td> <td> 0.003</td> <td> 3350.051</td> <td>  1.7e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8717.0</th>           <td> 1.034e+04</td> <td> 3480.922</td> <td>    2.970</td> <td> 0.003</td> <td> 3515.999</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8759.0</th>           <td> 3195.8327</td> <td> 3002.715</td> <td>    1.064</td> <td> 0.287</td> <td>-2689.946</td> <td> 9081.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8762.0</th>           <td> 9229.2761</td> <td> 3211.469</td> <td>    2.874</td> <td> 0.004</td> <td> 2934.309</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8819.0</th>           <td> 1.045e+04</td> <td> 3554.911</td> <td>    2.941</td> <td> 0.003</td> <td> 3485.338</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8850.0</th>           <td> 1.006e+04</td> <td> 3470.331</td> <td>    2.900</td> <td> 0.004</td> <td> 3260.321</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8852.0</th>           <td> 1.037e+04</td> <td> 3514.880</td> <td>    2.952</td> <td> 0.003</td> <td> 3485.138</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8859.0</th>           <td> 9504.3144</td> <td> 3432.430</td> <td>    2.769</td> <td> 0.006</td> <td> 2776.229</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8867.0</th>           <td> 2804.6769</td> <td> 3365.235</td> <td>    0.833</td> <td> 0.405</td> <td>-3791.695</td> <td> 9401.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8881.0</th>           <td> 8810.4771</td> <td> 3373.327</td> <td>    2.612</td> <td> 0.009</td> <td> 2198.244</td> <td> 1.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8958.0</th>           <td> 6294.1180</td> <td> 3147.355</td> <td>    2.000</td> <td> 0.046</td> <td>  124.823</td> <td> 1.25e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8972.0</th>           <td>-1.304e+04</td> <td> 3292.688</td> <td>   -3.960</td> <td> 0.000</td> <td>-1.95e+04</td> <td>-6584.568</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>8990.0</th>           <td>-3233.5185</td> <td> 3122.100</td> <td>   -1.036</td> <td> 0.300</td> <td>-9353.310</td> <td> 2886.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9004.0</th>           <td> 1.028e+04</td> <td> 3671.630</td> <td>    2.799</td> <td> 0.005</td> <td> 3078.424</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9016.0</th>           <td> 7909.5405</td> <td> 3260.736</td> <td>    2.426</td> <td> 0.015</td> <td> 1518.003</td> <td> 1.43e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9048.0</th>           <td> 8117.5466</td> <td> 3282.531</td> <td>    2.473</td> <td> 0.013</td> <td> 1683.285</td> <td> 1.46e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9051.0</th>           <td> -285.8187</td> <td> 3339.687</td> <td>   -0.086</td> <td> 0.932</td> <td>-6832.114</td> <td> 6260.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9071.0</th>           <td> 1.047e+04</td> <td> 3494.598</td> <td>    2.995</td> <td> 0.003</td> <td> 3616.763</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9112.0</th>           <td> 5268.4764</td> <td> 3107.901</td> <td>    1.695</td> <td> 0.090</td> <td> -823.483</td> <td> 1.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9114.0</th>           <td> 6528.6201</td> <td> 3498.644</td> <td>    1.866</td> <td> 0.062</td> <td> -329.255</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9132.0</th>           <td> 7679.5102</td> <td> 4515.180</td> <td>    1.701</td> <td> 0.089</td> <td>-1170.930</td> <td> 1.65e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9173.0</th>           <td> 9871.3241</td> <td> 3749.193</td> <td>    2.633</td> <td> 0.008</td> <td> 2522.335</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9180.0</th>           <td> 1.023e+04</td> <td> 3560.674</td> <td>    2.873</td> <td> 0.004</td> <td> 3251.695</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9186.0</th>           <td> 9579.4763</td> <td> 3435.987</td> <td>    2.788</td> <td> 0.005</td> <td> 2844.420</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9191.0</th>           <td> 7809.2449</td> <td> 4335.647</td> <td>    1.801</td> <td> 0.072</td> <td> -689.283</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9216.0</th>           <td> 5465.3263</td> <td> 3093.356</td> <td>    1.767</td> <td> 0.077</td> <td> -598.121</td> <td> 1.15e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9217.0</th>           <td> 1777.0444</td> <td> 3009.252</td> <td>    0.591</td> <td> 0.555</td> <td>-4121.547</td> <td> 7675.635</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9225.0</th>           <td> 1.071e+04</td> <td> 3503.029</td> <td>    3.058</td> <td> 0.002</td> <td> 3845.352</td> <td> 1.76e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9230.0</th>           <td> 9410.1365</td> <td> 3829.849</td> <td>    2.457</td> <td> 0.014</td> <td> 1903.050</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9259.0</th>           <td> 1.043e+04</td> <td> 3496.754</td> <td>    2.983</td> <td> 0.003</td> <td> 3576.686</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9293.0</th>           <td> 1.034e+04</td> <td> 3492.063</td> <td>    2.960</td> <td> 0.003</td> <td> 3491.788</td> <td> 1.72e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9299.0</th>           <td> 6897.8306</td> <td> 3506.720</td> <td>    1.967</td> <td> 0.049</td> <td>   24.126</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9308.0</th>           <td> 1741.3332</td> <td> 3024.635</td> <td>    0.576</td> <td> 0.565</td> <td>-4187.411</td> <td> 7670.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9311.0</th>           <td> 5296.9888</td> <td> 4107.271</td> <td>    1.290</td> <td> 0.197</td> <td>-2753.887</td> <td> 1.33e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9313.0</th>           <td> 3977.4922</td> <td> 3104.734</td> <td>    1.281</td> <td> 0.200</td> <td>-2108.259</td> <td> 1.01e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9325.0</th>           <td> 9578.9967</td> <td> 3442.888</td> <td>    2.782</td> <td> 0.005</td> <td> 2830.413</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9332.0</th>           <td> 1.001e+04</td> <td> 3470.784</td> <td>    2.883</td> <td> 0.004</td> <td> 3203.517</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9340.0</th>           <td>-2.037e+04</td> <td> 5011.621</td> <td>   -4.064</td> <td> 0.000</td> <td>-3.02e+04</td> <td>-1.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9372.0</th>           <td> 9148.2969</td> <td> 3621.080</td> <td>    2.526</td> <td> 0.012</td> <td> 2050.429</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9411.0</th>           <td> 8088.5107</td> <td> 3524.109</td> <td>    2.295</td> <td> 0.022</td> <td> 1180.721</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9459.0</th>           <td> -666.6739</td> <td> 3356.430</td> <td>   -0.199</td> <td> 0.843</td> <td>-7245.788</td> <td> 5912.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9465.0</th>           <td> 1.471e+04</td> <td> 3419.252</td> <td>    4.303</td> <td> 0.000</td> <td> 8010.932</td> <td> 2.14e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9472.0</th>           <td> 3955.3257</td> <td> 3021.325</td> <td>    1.309</td> <td> 0.191</td> <td>-1966.930</td> <td> 9877.582</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9483.0</th>           <td>-4488.5082</td> <td> 3083.214</td> <td>   -1.456</td> <td> 0.145</td> <td>-1.05e+04</td> <td> 1555.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9563.0</th>           <td>-1.564e+04</td> <td> 4431.982</td> <td>   -3.530</td> <td> 0.000</td> <td>-2.43e+04</td> <td>-6956.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9590.0</th>           <td> 4752.8554</td> <td> 3056.595</td> <td>    1.555</td> <td> 0.120</td> <td>-1238.535</td> <td> 1.07e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9598.0</th>           <td> 2876.1361</td> <td> 3734.665</td> <td>    0.770</td> <td> 0.441</td> <td>-4444.375</td> <td> 1.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9599.0</th>           <td> 1309.9612</td> <td> 2948.034</td> <td>    0.444</td> <td> 0.657</td> <td>-4468.634</td> <td> 7088.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9602.0</th>           <td> 2279.3103</td> <td> 4077.970</td> <td>    0.559</td> <td> 0.576</td> <td>-5714.132</td> <td> 1.03e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9619.0</th>           <td> 8879.0387</td> <td> 3376.513</td> <td>    2.630</td> <td> 0.009</td> <td> 2260.559</td> <td> 1.55e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9643.0</th>           <td> 1.012e+04</td> <td> 3558.028</td> <td>    2.844</td> <td> 0.004</td> <td> 3146.293</td> <td> 1.71e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9650.0</th>           <td> 1.011e+04</td> <td> 3465.651</td> <td>    2.917</td> <td> 0.004</td> <td> 3316.603</td> <td> 1.69e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9653.0</th>           <td>-6037.0683</td> <td> 5294.152</td> <td>   -1.140</td> <td> 0.254</td> <td>-1.64e+04</td> <td> 4340.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9667.0</th>           <td> 9385.3308</td> <td> 3422.969</td> <td>    2.742</td> <td> 0.006</td> <td> 2675.790</td> <td> 1.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9698.0</th>           <td> 7457.0200</td> <td> 3229.332</td> <td>    2.309</td> <td> 0.021</td> <td> 1127.039</td> <td> 1.38e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9699.0</th>           <td> 9848.2959</td> <td> 3312.269</td> <td>    2.973</td> <td> 0.003</td> <td> 3355.744</td> <td> 1.63e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9719.0</th>           <td>  768.4108</td> <td> 2945.395</td> <td>    0.261</td> <td> 0.794</td> <td>-5005.011</td> <td> 6541.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9742.0</th>           <td>-2580.5961</td> <td> 3079.909</td> <td>   -0.838</td> <td> 0.402</td> <td>-8617.686</td> <td> 3456.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9761.0</th>           <td> 1.046e+04</td> <td> 3530.077</td> <td>    2.963</td> <td> 0.003</td> <td> 3539.740</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9771.0</th>           <td> 3049.1857</td> <td> 3017.146</td> <td>    1.011</td> <td> 0.312</td> <td>-2864.879</td> <td> 8963.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9772.0</th>           <td> 9577.3307</td> <td> 3397.420</td> <td>    2.819</td> <td> 0.005</td> <td> 2917.871</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9778.0</th>           <td> 8834.8250</td> <td> 3321.328</td> <td>    2.660</td> <td> 0.008</td> <td> 2324.517</td> <td> 1.53e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9799.0</th>           <td> 2300.8569</td> <td> 3111.941</td> <td>    0.739</td> <td> 0.460</td> <td>-3799.021</td> <td> 8400.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9815.0</th>           <td> 1.041e+04</td> <td> 3600.344</td> <td>    2.891</td> <td> 0.004</td> <td> 3349.939</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9818.0</th>           <td>-3.633e+04</td> <td> 3152.499</td> <td>  -11.524</td> <td> 0.000</td> <td>-4.25e+04</td> <td>-3.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9837.0</th>           <td> 1.034e+04</td> <td> 3554.223</td> <td>    2.908</td> <td> 0.004</td> <td> 3370.486</td> <td> 1.73e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9922.0</th>           <td> 1086.6389</td> <td> 2947.216</td> <td>    0.369</td> <td> 0.712</td> <td>-4690.353</td> <td> 6863.631</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9954.0</th>           <td> 9757.9764</td> <td> 3958.884</td> <td>    2.465</td> <td> 0.014</td> <td> 1997.961</td> <td> 1.75e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9963.0</th>           <td> 9335.3174</td> <td> 3515.814</td> <td>    2.655</td> <td> 0.008</td> <td> 2443.787</td> <td> 1.62e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>9988.0</th>           <td> 9912.8554</td> <td> 3503.871</td> <td>    2.829</td> <td> 0.005</td> <td> 3044.736</td> <td> 1.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1981</th> <td>   -0.0427</td> <td>    0.122</td> <td>   -0.349</td> <td> 0.727</td> <td>   -0.283</td> <td>    0.197</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1982</th> <td>   -0.0526</td> <td>    0.121</td> <td>   -0.436</td> <td> 0.663</td> <td>   -0.289</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1983</th> <td>   -0.0757</td> <td>    0.119</td> <td>   -0.638</td> <td> 0.523</td> <td>   -0.308</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1984</th> <td>   -0.1432</td> <td>    0.117</td> <td>   -1.222</td> <td> 0.222</td> <td>   -0.373</td> <td>    0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1985</th> <td>   -0.1742</td> <td>    0.117</td> <td>   -1.488</td> <td> 0.137</td> <td>   -0.404</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1986</th> <td>   -0.2309</td> <td>    0.117</td> <td>   -1.976</td> <td> 0.048</td> <td>   -0.460</td> <td>   -0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1987</th> <td>   -0.2508</td> <td>    0.117</td> <td>   -2.143</td> <td> 0.032</td> <td>   -0.480</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1988</th> <td>   -0.2858</td> <td>    0.117</td> <td>   -2.434</td> <td> 0.015</td> <td>   -0.516</td> <td>   -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1989</th> <td>   -0.2845</td> <td>    0.118</td> <td>   -2.410</td> <td> 0.016</td> <td>   -0.516</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1990</th> <td>   -0.3120</td> <td>    0.119</td> <td>   -2.624</td> <td> 0.009</td> <td>   -0.545</td> <td>   -0.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1991</th> <td>   -0.2929</td> <td>    0.120</td> <td>   -2.445</td> <td> 0.014</td> <td>   -0.528</td> <td>   -0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1992</th> <td>   -0.3439</td> <td>    0.121</td> <td>   -2.850</td> <td> 0.004</td> <td>   -0.580</td> <td>   -0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1993</th> <td>   -0.3505</td> <td>    0.122</td> <td>   -2.873</td> <td> 0.004</td> <td>   -0.590</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1994</th> <td>   -0.3584</td> <td>    0.123</td> <td>   -2.913</td> <td> 0.004</td> <td>   -0.600</td> <td>   -0.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1995</th> <td>   -0.3304</td> <td>    0.125</td> <td>   -2.644</td> <td> 0.008</td> <td>   -0.575</td> <td>   -0.085</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1996</th> <td>   -0.3542</td> <td>    0.127</td> <td>   -2.785</td> <td> 0.005</td> <td>   -0.603</td> <td>   -0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1997</th> <td>   -0.3330</td> <td>    0.130</td> <td>   -2.565</td> <td> 0.010</td> <td>   -0.587</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1998</th> <td>   -0.2876</td> <td>    0.133</td> <td>   -2.170</td> <td> 0.030</td> <td>   -0.547</td> <td>   -0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX1999</th> <td>   -0.2297</td> <td>    0.135</td> <td>   -1.700</td> <td> 0.089</td> <td>   -0.495</td> <td>    0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX2000</th> <td>   -0.2065</td> <td>    0.137</td> <td>   -1.504</td> <td> 0.133</td> <td>   -0.476</td> <td>    0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gspillsicIVX2001</th> <td>   -0.4205</td> <td>    0.139</td> <td>   -3.027</td> <td> 0.002</td> <td>   -0.693</td> <td>   -0.148</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>21719.318</td> <th>  Durbin-Watson:     </th>   <td>   0.545</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>50384141.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>10.195</td>   <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>302.876</td>  <th>  Cond. No.          </th>   <td>9.49e+06</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.49e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      rmkvaf      & \\textbf{  R-squared:         } &      0.666    \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &      0.646    \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &      32.48    \\\\\n",
       "\\textbf{Date:}             & Wed, 16 Oct 2024 & \\textbf{  Prob (F-statistic):} &      0.00     \\\\\n",
       "\\textbf{Time:}             &     02:03:03     & \\textbf{  Log-Likelihood:    } & -1.4155e+05   \\\\\n",
       "\\textbf{No. Observations:} &       13385      & \\textbf{  AIC:               } &  2.847e+05    \\\\\n",
       "\\textbf{Df Residuals:}     &       12609      & \\textbf{  BIC:               } &  2.905e+05    \\\\\n",
       "\\textbf{Df Model:}         &         775      & \\textbf{                     } &               \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &               \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}            &   -1.059e+04  &     2858.143     &    -3.705  &         0.000        &    -1.62e+04    &    -4986.834     \\\\\n",
       "\\textbf{gspillsicIV}      &       0.8770  &        0.199     &     4.412  &         0.000        &        0.487    &        1.267     \\\\\n",
       "\\textbf{pat\\_count}       &     -29.6820  &        1.861     &   -15.948  &         0.000        &      -33.330    &      -26.034     \\\\\n",
       "\\textbf{rsales}           &       0.7906  &        0.037     &    21.566  &         0.000        &        0.719    &        0.862     \\\\\n",
       "\\textbf{rppent}           &       0.6416  &        0.084     &     7.603  &         0.000        &        0.476    &        0.807     \\\\\n",
       "\\textbf{emp}              &      12.1477  &        7.031     &     1.728  &         0.084        &       -1.633    &       25.929     \\\\\n",
       "\\textbf{rxrd}             &      18.7054  &        0.614     &    30.479  &         0.000        &       17.502    &       19.908     \\\\\n",
       "\\textbf{1981}             &    -323.9459  &      749.124     &    -0.432  &         0.665        &    -1792.342    &     1144.450     \\\\\n",
       "\\textbf{1982}             &    -175.3555  &      747.071     &    -0.235  &         0.814        &    -1639.729    &     1289.018     \\\\\n",
       "\\textbf{1983}             &    -106.8254  &      738.681     &    -0.145  &         0.885        &    -1554.753    &     1341.102     \\\\\n",
       "\\textbf{1984}             &    -358.0670  &      734.939     &    -0.487  &         0.626        &    -1798.660    &     1082.526     \\\\\n",
       "\\textbf{1985}             &    -339.9200  &      734.792     &    -0.463  &         0.644        &    -1780.223    &     1100.383     \\\\\n",
       "\\textbf{1986}             &    -333.0589  &      729.764     &    -0.456  &         0.648        &    -1763.507    &     1097.389     \\\\\n",
       "\\textbf{1987}             &    -418.4831  &      726.215     &    -0.576  &         0.564        &    -1841.975    &     1005.009     \\\\\n",
       "\\textbf{1988}             &    -537.8235  &      725.635     &    -0.741  &         0.459        &    -1960.179    &      884.531     \\\\\n",
       "\\textbf{1989}             &    -394.3985  &      723.074     &    -0.545  &         0.585        &    -1811.734    &     1022.937     \\\\\n",
       "\\textbf{1990}             &    -723.8705  &      719.942     &    -1.005  &         0.315        &    -2135.066    &      687.325     \\\\\n",
       "\\textbf{1991}             &    -405.2199  &      718.078     &    -0.564  &         0.573        &    -1812.763    &     1002.323     \\\\\n",
       "\\textbf{1992}             &    -200.8668  &      716.324     &    -0.280  &         0.779        &    -1604.970    &     1203.236     \\\\\n",
       "\\textbf{1993}             &     -60.0251  &      714.587     &    -0.084  &         0.933        &    -1460.725    &     1340.675     \\\\\n",
       "\\textbf{1994}             &    -213.6207  &      716.211     &    -0.298  &         0.766        &    -1617.503    &     1190.261     \\\\\n",
       "\\textbf{1995}             &     162.8964  &      716.020     &     0.228  &         0.820        &    -1240.612    &     1566.405     \\\\\n",
       "\\textbf{1996}             &     673.3490  &      718.687     &     0.937  &         0.349        &     -735.387    &     2082.085     \\\\\n",
       "\\textbf{1997}             &     981.4864  &      721.476     &     1.360  &         0.174        &     -432.716    &     2395.689     \\\\\n",
       "\\textbf{1998}             &     793.8169  &      725.956     &     1.093  &         0.274        &     -629.167    &     2216.801     \\\\\n",
       "\\textbf{1999}             &    1042.6699  &      732.548     &     1.423  &         0.155        &     -393.235    &     2478.575     \\\\\n",
       "\\textbf{2000}             &     473.6464  &      750.844     &     0.631  &         0.528        &     -998.123    &     1945.415     \\\\\n",
       "\\textbf{2001}             &     461.9434  &      779.875     &     0.592  &         0.554        &    -1066.731    &     1990.618     \\\\\n",
       "\\textbf{10005.0}          &    9659.3047  &     3429.640     &     2.816  &         0.005        &     2936.688    &     1.64e+04     \\\\\n",
       "\\textbf{10006.0}          &    9654.2887  &     3848.511     &     2.509  &         0.012        &     2110.621    &     1.72e+04     \\\\\n",
       "\\textbf{10008.0}          &    9065.5709  &     3369.549     &     2.690  &         0.007        &     2460.743    &     1.57e+04     \\\\\n",
       "\\textbf{10016.0}          &    1.032e+04  &     3471.639     &     2.971  &         0.003        &     3510.375    &     1.71e+04     \\\\\n",
       "\\textbf{10030.0}          &    1.032e+04  &     3484.519     &     2.962  &         0.003        &     3490.881    &     1.72e+04     \\\\\n",
       "\\textbf{1004.0}           &     1.04e+04  &     3516.374     &     2.956  &         0.003        &     3502.541    &     1.73e+04     \\\\\n",
       "\\textbf{10056.0}          &    8114.8321  &     3313.170     &     2.449  &         0.014        &     1620.514    &     1.46e+04     \\\\\n",
       "\\textbf{10085.0}          &    2996.3025  &     2984.784     &     1.004  &         0.315        &    -2854.329    &     8846.934     \\\\\n",
       "\\textbf{10092.0}          &        1e+04  &     5579.277     &     1.793  &         0.073        &     -932.045    &     2.09e+04     \\\\\n",
       "\\textbf{10097.0}          &    2913.9955  &     3124.866     &     0.933  &         0.351        &    -3211.217    &     9039.208     \\\\\n",
       "\\textbf{1010.0}           &     1.01e+04  &     5629.803     &     1.794  &         0.073        &     -935.137    &     2.11e+04     \\\\\n",
       "\\textbf{10109.0}          &    1.061e+04  &     3495.834     &     3.035  &         0.002        &     3757.360    &     1.75e+04     \\\\\n",
       "\\textbf{10115.0}          &    7108.6747  &     3121.346     &     2.277  &         0.023        &      990.362    &     1.32e+04     \\\\\n",
       "\\textbf{10124.0}          &    1.071e+04  &     3505.368     &     3.055  &         0.002        &     3837.145    &     1.76e+04     \\\\\n",
       "\\textbf{1013.0}           &    5204.0952  &     3088.093     &     1.685  &         0.092        &     -849.037    &     1.13e+04     \\\\\n",
       "\\textbf{10150.0}          &    3126.6112  &     3528.442     &     0.886  &         0.376        &    -3789.672    &        1e+04     \\\\\n",
       "\\textbf{10159.0}          &   -3942.8076  &     4398.018     &    -0.896  &         0.370        &    -1.26e+04    &     4677.977     \\\\\n",
       "\\textbf{10174.0}          &    1.059e+04  &     3728.330     &     2.840  &         0.005        &     3281.435    &     1.79e+04     \\\\\n",
       "\\textbf{10185.0}          &    6933.6784  &     3401.405     &     2.038  &         0.042        &      266.407    &     1.36e+04     \\\\\n",
       "\\textbf{10195.0}          &   -2419.7055  &     2991.651     &    -0.809  &         0.419        &    -8283.797    &     3444.386     \\\\\n",
       "\\textbf{10198.0}          &     1.04e+04  &     3490.406     &     2.980  &         0.003        &     3559.496    &     1.72e+04     \\\\\n",
       "\\textbf{10215.0}          &    1.047e+04  &     3502.877     &     2.989  &         0.003        &     3603.939    &     1.73e+04     \\\\\n",
       "\\textbf{10232.0}          &    3412.4149  &     3139.375     &     1.087  &         0.277        &    -2741.238    &     9566.068     \\\\\n",
       "\\textbf{10236.0}          &    9729.1672  &     3425.098     &     2.841  &         0.005        &     3015.455    &     1.64e+04     \\\\\n",
       "\\textbf{10286.0}          &    9948.2372  &     3494.179     &     2.847  &         0.004        &     3099.114    &     1.68e+04     \\\\\n",
       "\\textbf{10301.0}          &   -1.568e+04  &     3092.129     &    -5.072  &         0.000        &    -2.17e+04    &    -9623.073     \\\\\n",
       "\\textbf{10312.0}          &    1.037e+04  &     3515.620     &     2.951  &         0.003        &     3483.546    &     1.73e+04     \\\\\n",
       "\\textbf{10332.0}          &     1.05e+04  &     4320.420     &     2.430  &         0.015        &     2028.470    &      1.9e+04     \\\\\n",
       "\\textbf{1036.0}           &    6315.6838  &     3359.575     &     1.880  &         0.060        &     -269.594    &     1.29e+04     \\\\\n",
       "\\textbf{10374.0}          &    8620.1908  &     3326.131     &     2.592  &         0.010        &     2100.469    &     1.51e+04     \\\\\n",
       "\\textbf{10386.0}          &    6270.3101  &     3177.083     &     1.974  &         0.048        &       42.745    &     1.25e+04     \\\\\n",
       "\\textbf{10391.0}          &    3673.5113  &     3158.308     &     1.163  &         0.245        &    -2517.254    &     9864.276     \\\\\n",
       "\\textbf{10407.0}          &    3552.8502  &     2995.765     &     1.186  &         0.236        &    -2319.306    &     9425.006     \\\\\n",
       "\\textbf{10420.0}          &    7609.5888  &     3141.904     &     2.422  &         0.015        &     1450.979    &     1.38e+04     \\\\\n",
       "\\textbf{10422.0}          &    7032.1079  &     3412.655     &     2.061  &         0.039        &      342.784    &     1.37e+04     \\\\\n",
       "\\textbf{10426.0}          &    8969.2563  &     3539.383     &     2.534  &         0.011        &     2031.527    &     1.59e+04     \\\\\n",
       "\\textbf{10441.0}          &     1.01e+04  &     3465.889     &     2.913  &         0.004        &     3301.710    &     1.69e+04     \\\\\n",
       "\\textbf{1045.0}           &     263.6677  &     3653.113     &     0.072  &         0.942        &    -6896.990    &     7424.325     \\\\\n",
       "\\textbf{10453.0}          &    6252.1220  &     3193.893     &     1.958  &         0.050        &       -8.395    &     1.25e+04     \\\\\n",
       "\\textbf{10482.0}          &   -1.574e+04  &     3499.420     &    -4.499  &         0.000        &    -2.26e+04    &    -8883.519     \\\\\n",
       "\\textbf{10498.0}          &    9516.6376  &     3463.766     &     2.747  &         0.006        &     2727.130    &     1.63e+04     \\\\\n",
       "\\textbf{10499.0}          &    1905.7296  &     3036.020     &     0.628  &         0.530        &    -4045.331    &     7856.790     \\\\\n",
       "\\textbf{10511.0}          &    1.036e+04  &     3584.248     &     2.890  &         0.004        &     3331.750    &     1.74e+04     \\\\\n",
       "\\textbf{10519.0}          &   -6995.2218  &     2994.130     &    -2.336  &         0.019        &    -1.29e+04    &    -1126.271     \\\\\n",
       "\\textbf{10530.0}          &    7992.8778  &     3302.808     &     2.420  &         0.016        &     1518.871    &     1.45e+04     \\\\\n",
       "\\textbf{10537.0}          &    7966.2215  &     3612.886     &     2.205  &         0.027        &      884.416    &      1.5e+04     \\\\\n",
       "\\textbf{10540.0}          &    8942.0924  &     3329.146     &     2.686  &         0.007        &     2416.459    &     1.55e+04     \\\\\n",
       "\\textbf{10541.0}          &     1.02e+04  &     3607.567     &     2.826  &         0.005        &     3125.335    &     1.73e+04     \\\\\n",
       "\\textbf{10550.0}          &    7172.0022  &     6120.797     &     1.172  &         0.241        &    -4825.691    &     1.92e+04     \\\\\n",
       "\\textbf{10553.0}          &    4910.8820  &     3309.233     &     1.484  &         0.138        &    -1575.719    &     1.14e+04     \\\\\n",
       "\\textbf{10565.0}          &    1.095e+04  &     3502.584     &     3.125  &         0.002        &     4080.237    &     1.78e+04     \\\\\n",
       "\\textbf{10580.0}          &    1.074e+04  &     3496.131     &     3.071  &         0.002        &     3883.162    &     1.76e+04     \\\\\n",
       "\\textbf{10581.0}          &    8286.5040  &     3446.834     &     2.404  &         0.016        &     1530.184    &      1.5e+04     \\\\\n",
       "\\textbf{10588.0}          &    -175.8462  &     2944.608     &    -0.060  &         0.952        &    -5947.726    &     5596.034     \\\\\n",
       "\\textbf{10597.0}          &    9656.1515  &     3467.223     &     2.785  &         0.005        &     2859.867    &     1.65e+04     \\\\\n",
       "\\textbf{10599.0}          &    1.023e+04  &     3528.354     &     2.900  &         0.004        &     3315.237    &     1.71e+04     \\\\\n",
       "\\textbf{10618.0}          &    9074.0716  &     3399.306     &     2.669  &         0.008        &     2410.914    &     1.57e+04     \\\\\n",
       "\\textbf{10656.0}          &    1.046e+04  &     3495.542     &     2.993  &         0.003        &     3610.766    &     1.73e+04     \\\\\n",
       "\\textbf{10658.0}          &    1.045e+04  &     3496.829     &     2.987  &         0.003        &     3590.733    &     1.73e+04     \\\\\n",
       "\\textbf{10726.0}          &    1.227e+04  &     3523.531     &     3.481  &         0.001        &     5359.204    &     1.92e+04     \\\\\n",
       "\\textbf{10734.0}          &    7760.9997  &     3836.548     &     2.023  &         0.043        &      240.781    &     1.53e+04     \\\\\n",
       "\\textbf{10735.0}          &    9750.1704  &     3465.560     &     2.813  &         0.005        &     2957.146    &     1.65e+04     \\\\\n",
       "\\textbf{10764.0}          &     1.03e+04  &     3584.360     &     2.874  &         0.004        &     3274.591    &     1.73e+04     \\\\\n",
       "\\textbf{10777.0}          &    1.038e+04  &     3498.480     &     2.966  &         0.003        &     3519.464    &     1.72e+04     \\\\\n",
       "\\textbf{1078.0}           &     536.0262  &     3370.038     &     0.159  &         0.874        &    -6069.762    &     7141.814     \\\\\n",
       "\\textbf{10793.0}          &    8667.7588  &     3500.015     &     2.476  &         0.013        &     1807.197    &     1.55e+04     \\\\\n",
       "\\textbf{10816.0}          &    8810.4118  &     3495.532     &     2.520  &         0.012        &     1958.637    &     1.57e+04     \\\\\n",
       "\\textbf{10839.0}          &    1.057e+04  &     3494.111     &     3.026  &         0.002        &     3724.674    &     1.74e+04     \\\\\n",
       "\\textbf{10857.0}          &     267.8678  &     3201.082     &     0.084  &         0.933        &    -6006.741    &     6542.476     \\\\\n",
       "\\textbf{10867.0}          &    4873.3848  &     3659.413     &     1.332  &         0.183        &    -2299.621    &      1.2e+04     \\\\\n",
       "\\textbf{10906.0}          &    9336.4858  &     3395.787     &     2.749  &         0.006        &     2680.227    &      1.6e+04     \\\\\n",
       "\\textbf{10950.0}          &    1.032e+04  &     4451.546     &     2.319  &         0.020        &     1598.643    &     1.91e+04     \\\\\n",
       "\\textbf{10983.0}          &   -2.185e+04  &     3294.297     &    -6.633  &         0.000        &    -2.83e+04    &    -1.54e+04     \\\\\n",
       "\\textbf{1099.0}           &    9612.8017  &     3450.007     &     2.786  &         0.005        &     2850.264    &     1.64e+04     \\\\\n",
       "\\textbf{10991.0}          &    4300.1083  &     3524.484     &     1.220  &         0.222        &    -2608.417    &     1.12e+04     \\\\\n",
       "\\textbf{11012.0}          &    7003.0744  &     3245.625     &     2.158  &         0.031        &      641.155    &     1.34e+04     \\\\\n",
       "\\textbf{11038.0}          &    1584.9939  &     3243.140     &     0.489  &         0.625        &    -4772.054    &     7942.042     \\\\\n",
       "\\textbf{1104.0}           &    9087.1045  &     3363.632     &     2.702  &         0.007        &     2493.873    &     1.57e+04     \\\\\n",
       "\\textbf{11060.0}          &    9940.4467  &     3499.969     &     2.840  &         0.005        &     3079.975    &     1.68e+04     \\\\\n",
       "\\textbf{11094.0}          &    9322.6430  &     3404.847     &     2.738  &         0.006        &     2648.624    &      1.6e+04     \\\\\n",
       "\\textbf{11096.0}          &    8070.0491  &     3306.301     &     2.441  &         0.015        &     1589.196    &     1.46e+04     \\\\\n",
       "\\textbf{11113.0}          &    9320.6769  &     3620.346     &     2.575  &         0.010        &     2224.247    &     1.64e+04     \\\\\n",
       "\\textbf{1115.0}           &    8436.8675  &     3367.347     &     2.505  &         0.012        &     1836.355    &      1.5e+04     \\\\\n",
       "\\textbf{11161.0}          &    6021.3457  &     3130.858     &     1.923  &         0.054        &     -115.612    &     1.22e+04     \\\\\n",
       "\\textbf{11225.0}          &    1.019e+04  &     3520.620     &     2.896  &         0.004        &     3294.007    &     1.71e+04     \\\\\n",
       "\\textbf{11228.0}          &    1.058e+04  &     3455.736     &     3.062  &         0.002        &     3807.075    &     1.74e+04     \\\\\n",
       "\\textbf{11236.0}          &    8487.3536  &     4807.653     &     1.765  &         0.078        &     -936.377    &     1.79e+04     \\\\\n",
       "\\textbf{11288.0}          &   -4285.1438  &     3433.102     &    -1.248  &         0.212        &     -1.1e+04    &     2444.258     \\\\\n",
       "\\textbf{11312.0}          &   -1488.0720  &     3086.728     &    -0.482  &         0.630        &    -7538.528    &     4562.384     \\\\\n",
       "\\textbf{11361.0}          &     1.05e+04  &     3501.988     &     2.998  &         0.003        &     3632.949    &     1.74e+04     \\\\\n",
       "\\textbf{11399.0}          &    2347.5209  &     3077.670     &     0.763  &         0.446        &    -3685.180    &     8380.222     \\\\\n",
       "\\textbf{114303.0}         &   -9571.3693  &     5399.892     &    -1.773  &         0.076        &    -2.02e+04    &     1013.241     \\\\\n",
       "\\textbf{11456.0}          &    5586.1634  &     3461.222     &     1.614  &         0.107        &    -1198.358    &     1.24e+04     \\\\\n",
       "\\textbf{11465.0}          &    4659.0121  &     3374.297     &     1.381  &         0.167        &    -1955.123    &     1.13e+04     \\\\\n",
       "\\textbf{11502.0}          &    8668.1917  &     3397.049     &     2.552  &         0.011        &     2009.459    &     1.53e+04     \\\\\n",
       "\\textbf{11506.0}          &    6603.9028  &     3378.886     &     1.954  &         0.051        &      -19.228    &     1.32e+04     \\\\\n",
       "\\textbf{11537.0}          &    1.023e+04  &     3479.312     &     2.939  &         0.003        &     3407.335    &      1.7e+04     \\\\\n",
       "\\textbf{11566.0}          &    1.036e+04  &     3491.031     &     2.968  &         0.003        &     3518.073    &     1.72e+04     \\\\\n",
       "\\textbf{11573.0}          &    9761.2159  &     3429.072     &     2.847  &         0.004        &     3039.712    &     1.65e+04     \\\\\n",
       "\\textbf{11580.0}          &    7866.4408  &     3666.583     &     2.145  &         0.032        &      679.380    &     1.51e+04     \\\\\n",
       "\\textbf{11600.0}          &    9665.2370  &     3432.503     &     2.816  &         0.005        &     2937.009    &     1.64e+04     \\\\\n",
       "\\textbf{11609.0}          &    1.356e+04  &     3467.960     &     3.911  &         0.000        &     6766.557    &     2.04e+04     \\\\\n",
       "\\textbf{1161.0}           &    -480.8907  &     2968.996     &    -0.162  &         0.871        &    -6300.575    &     5338.793     \\\\\n",
       "\\textbf{11636.0}          &   -1.146e+04  &     3207.307     &    -3.574  &         0.000        &    -1.78e+04    &    -5177.201     \\\\\n",
       "\\textbf{11670.0}          &    9809.1236  &     3451.905     &     2.842  &         0.004        &     3042.865    &     1.66e+04     \\\\\n",
       "\\textbf{11678.0}          &   -3033.7459  &     3115.647     &    -0.974  &         0.330        &    -9140.887    &     3073.396     \\\\\n",
       "\\textbf{11682.0}          &     1.06e+04  &     3611.779     &     2.935  &         0.003        &     3521.726    &     1.77e+04     \\\\\n",
       "\\textbf{11694.0}          &    1.041e+04  &     3619.335     &     2.877  &         0.004        &     3317.257    &     1.75e+04     \\\\\n",
       "\\textbf{11720.0}          &    -762.2755  &     3867.445     &    -0.197  &         0.844        &    -8343.057    &     6818.506     \\\\\n",
       "\\textbf{11721.0}          &   -9711.2464  &     3644.527     &    -2.665  &         0.008        &    -1.69e+04    &    -2567.419     \\\\\n",
       "\\textbf{11722.0}          &    9216.8593  &     3613.105     &     2.551  &         0.011        &     2134.624    &     1.63e+04     \\\\\n",
       "\\textbf{11793.0}          &    9959.7365  &     6257.374     &     1.592  &         0.111        &    -2305.669    &     2.22e+04     \\\\\n",
       "\\textbf{11797.0}          &    1.057e+04  &     3841.148     &     2.752  &         0.006        &     3040.013    &     1.81e+04     \\\\\n",
       "\\textbf{11914.0}          &    9570.5529  &     4017.374     &     2.382  &         0.017        &     1695.889    &     1.74e+04     \\\\\n",
       "\\textbf{1209.0}           &    8145.5808  &     3258.527     &     2.500  &         0.012        &     1758.373    &     1.45e+04     \\\\\n",
       "\\textbf{12136.0}          &   -8658.1861  &     3530.681     &    -2.452  &         0.014        &    -1.56e+04    &    -1737.514     \\\\\n",
       "\\textbf{12141.0}          &    8.845e+04  &     3265.400     &    27.087  &         0.000        &      8.2e+04    &     9.48e+04     \\\\\n",
       "\\textbf{12181.0}          &    7785.2247  &     4432.555     &     1.756  &         0.079        &     -903.258    &     1.65e+04     \\\\\n",
       "\\textbf{12215.0}          &   -1543.0409  &     3214.227     &    -0.480  &         0.631        &    -7843.415    &     4757.334     \\\\\n",
       "\\textbf{12216.0}          &    1926.4346  &     3215.418     &     0.599  &         0.549        &    -4376.274    &     8229.143     \\\\\n",
       "\\textbf{12256.0}          &    2154.7127  &     3244.711     &     0.664  &         0.507        &    -4205.414    &     8514.839     \\\\\n",
       "\\textbf{12262.0}          &    8366.2379  &     3552.028     &     2.355  &         0.019        &     1403.724    &     1.53e+04     \\\\\n",
       "\\textbf{12389.0}          &    1.212e+04  &     3710.289     &     3.266  &         0.001        &     4843.769    &     1.94e+04     \\\\\n",
       "\\textbf{1239.0}           &    6859.3033  &     3194.471     &     2.147  &         0.032        &      597.654    &     1.31e+04     \\\\\n",
       "\\textbf{12390.0}          &    8144.3311  &     3827.435     &     2.128  &         0.033        &      641.976    &     1.56e+04     \\\\\n",
       "\\textbf{12397.0}          &    7091.0057  &     5435.028     &     1.305  &         0.192        &    -3562.476    &     1.77e+04     \\\\\n",
       "\\textbf{1243.0}           &    4740.1661  &     3505.474     &     1.352  &         0.176        &    -2131.096    &     1.16e+04     \\\\\n",
       "\\textbf{12548.0}          &    9622.3915  &     3913.321     &     2.459  &         0.014        &     1951.687    &     1.73e+04     \\\\\n",
       "\\textbf{12570.0}          &    8478.9178  &     3680.537     &     2.304  &         0.021        &     1264.506    &     1.57e+04     \\\\\n",
       "\\textbf{12581.0}          &    8072.2136  &     3959.674     &     2.039  &         0.042        &      310.651    &     1.58e+04     \\\\\n",
       "\\textbf{12592.0}          &    7180.3739  &     3562.401     &     2.016  &         0.044        &      197.525    &     1.42e+04     \\\\\n",
       "\\textbf{12604.0}          &    6302.1541  &     6167.016     &     1.022  &         0.307        &    -5786.135    &     1.84e+04     \\\\\n",
       "\\textbf{12656.0}          &    9831.1782  &     3729.089     &     2.636  &         0.008        &     2521.597    &     1.71e+04     \\\\\n",
       "\\textbf{12679.0}          &   -1.207e+04  &     3575.885     &    -3.374  &         0.001        &    -1.91e+04    &    -5057.142     \\\\\n",
       "\\textbf{1278.0}           &    9272.1183  &     3554.622     &     2.608  &         0.009        &     2304.519    &     1.62e+04     \\\\\n",
       "\\textbf{12788.0}          &   -1.081e+04  &     3704.281     &    -2.917  &         0.004        &    -1.81e+04    &    -3545.393     \\\\\n",
       "\\textbf{1283.0}           &    8454.8366  &     3314.507     &     2.551  &         0.011        &     1957.900    &      1.5e+04     \\\\\n",
       "\\textbf{1297.0}           &    9358.7113  &     3457.490     &     2.707  &         0.007        &     2581.505    &     1.61e+04     \\\\\n",
       "\\textbf{12992.0}          &    1.015e+04  &     3767.816     &     2.693  &         0.007        &     2762.823    &     1.75e+04     \\\\\n",
       "\\textbf{13135.0}          &    5448.9172  &     3564.090     &     1.529  &         0.126        &    -1537.242    &     1.24e+04     \\\\\n",
       "\\textbf{1327.0}           &   -1643.9351  &     3038.632     &    -0.541  &         0.589        &    -7600.117    &     4312.247     \\\\\n",
       "\\textbf{13282.0}          &    4445.8876  &     5417.023     &     0.821  &         0.412        &    -6172.301    &     1.51e+04     \\\\\n",
       "\\textbf{1334.0}           &   -5548.4156  &     3198.783     &    -1.735  &         0.083        &    -1.18e+04    &      721.687     \\\\\n",
       "\\textbf{13351.0}          &    6518.8735  &     4245.842     &     1.535  &         0.125        &    -1803.622    &     1.48e+04     \\\\\n",
       "\\textbf{13365.0}          &   -1.383e+04  &     4121.711     &    -3.356  &         0.001        &    -2.19e+04    &    -5754.663     \\\\\n",
       "\\textbf{13369.0}          &    9139.3518  &     3706.917     &     2.465  &         0.014        &     1873.230    &     1.64e+04     \\\\\n",
       "\\textbf{13406.0}          &    9939.8470  &     3720.720     &     2.671  &         0.008        &     2646.670    &     1.72e+04     \\\\\n",
       "\\textbf{13407.0}          &    2417.5901  &     3312.421     &     0.730  &         0.465        &    -4075.258    &     8910.439     \\\\\n",
       "\\textbf{13417.0}          &    9899.0300  &     3860.871     &     2.564  &         0.010        &     2331.135    &     1.75e+04     \\\\\n",
       "\\textbf{13525.0}          &   -2774.8050  &     3294.923     &    -0.842  &         0.400        &    -9233.355    &     3683.745     \\\\\n",
       "\\textbf{13554.0}          &     1.04e+04  &     3768.695     &     2.759  &         0.006        &     3009.340    &     1.78e+04     \\\\\n",
       "\\textbf{1359.0}           &   -7579.7543  &     3532.881     &    -2.145  &         0.032        &    -1.45e+04    &     -654.770     \\\\\n",
       "\\textbf{13623.0}          &    7282.1206  &     3530.867     &     2.062  &         0.039        &      361.084    &     1.42e+04     \\\\\n",
       "\\textbf{1372.0}           &   -1024.5227  &     2952.643     &    -0.347  &         0.729        &    -6812.153    &     4763.107     \\\\\n",
       "\\textbf{1380.0}           &     933.6613  &     3077.529     &     0.303  &         0.762        &    -5098.765    &     6966.087     \\\\\n",
       "\\textbf{13923.0}          &    9652.5025  &     4001.225     &     2.412  &         0.016        &     1809.493    &     1.75e+04     \\\\\n",
       "\\textbf{13932.0}          &    9693.4571  &     4463.929     &     2.172  &         0.030        &      943.476    &     1.84e+04     \\\\\n",
       "\\textbf{13941.0}          &   -1234.2215  &     3351.972     &    -0.368  &         0.713        &    -7804.596    &     5336.153     \\\\\n",
       "\\textbf{1397.0}           &    8978.2149  &     3604.283     &     2.491  &         0.013        &     1913.271    &      1.6e+04     \\\\\n",
       "\\textbf{14064.0}          &    6496.0923  &     3427.729     &     1.895  &         0.058        &     -222.777    &     1.32e+04     \\\\\n",
       "\\textbf{14084.0}          &    9938.4584  &     3773.668     &     2.634  &         0.008        &     2541.495    &     1.73e+04     \\\\\n",
       "\\textbf{14324.0}          &    -448.3298  &     3367.341     &    -0.133  &         0.894        &    -7048.831    &     6152.171     \\\\\n",
       "\\textbf{14462.0}          &    9253.3530  &     3776.161     &     2.450  &         0.014        &     1851.504    &     1.67e+04     \\\\\n",
       "\\textbf{1447.0}           &    1.462e+04  &     4904.563     &     2.981  &         0.003        &     5007.306    &     2.42e+04     \\\\\n",
       "\\textbf{14531.0}          &    8818.8233  &     1.01e+04     &     0.870  &         0.385        &    -1.11e+04    &     2.87e+04     \\\\\n",
       "\\textbf{14593.0}          &    9416.8572  &     3820.901     &     2.465  &         0.014        &     1927.310    &     1.69e+04     \\\\\n",
       "\\textbf{14622.0}          &    1.034e+04  &     7410.721     &     1.395  &         0.163        &    -4188.065    &     2.49e+04     \\\\\n",
       "\\textbf{1465.0}           &    8580.7095  &     3881.320     &     2.211  &         0.027        &      972.732    &     1.62e+04     \\\\\n",
       "\\textbf{1468.0}           &    1.025e+04  &     3913.165     &     2.618  &         0.009        &     2575.683    &     1.79e+04     \\\\\n",
       "\\textbf{14897.0}          &    8143.8094  &     5481.373     &     1.486  &         0.137        &    -2600.516    &     1.89e+04     \\\\\n",
       "\\textbf{14954.0}          &    8879.7142  &     3787.153     &     2.345  &         0.019        &     1456.318    &     1.63e+04     \\\\\n",
       "\\textbf{1496.0}           &    1.056e+04  &     3504.246     &     3.014  &         0.003        &     3691.980    &     1.74e+04     \\\\\n",
       "\\textbf{15267.0}          &    8769.1983  &     3768.994     &     2.327  &         0.020        &     1381.397    &     1.62e+04     \\\\\n",
       "\\textbf{15354.0}          &    4573.2807  &     3644.445     &     1.255  &         0.210        &    -2570.386    &     1.17e+04     \\\\\n",
       "\\textbf{1542.0}           &    9271.0954  &     3433.382     &     2.700  &         0.007        &     2541.144    &      1.6e+04     \\\\\n",
       "\\textbf{15459.0}          &    8320.3467  &     3771.104     &     2.206  &         0.027        &      928.408    &     1.57e+04     \\\\\n",
       "\\textbf{1554.0}           &    1.037e+04  &     3482.162     &     2.977  &         0.003        &     3539.471    &     1.72e+04     \\\\\n",
       "\\textbf{15708.0}          &   -1.427e+04  &     4364.184     &    -3.269  &         0.001        &    -2.28e+04    &    -5713.790     \\\\\n",
       "\\textbf{15711.0}          &    8305.0899  &     3828.438     &     2.169  &         0.030        &      800.770    &     1.58e+04     \\\\\n",
       "\\textbf{15761.0}          &    9943.8050  &     4432.762     &     2.243  &         0.025        &     1254.916    &     1.86e+04     \\\\\n",
       "\\textbf{1581.0}           &   -1.894e+04  &     4232.063     &    -4.476  &         0.000        &    -2.72e+04    &    -1.06e+04     \\\\\n",
       "\\textbf{1593.0}           &    9341.0130  &     3412.180     &     2.738  &         0.006        &     2652.621    &      1.6e+04     \\\\\n",
       "\\textbf{1602.0}           &    1.675e+04  &     3401.742     &     4.925  &         0.000        &     1.01e+04    &     2.34e+04     \\\\\n",
       "\\textbf{1613.0}           &    9426.6340  &     3409.769     &     2.765  &         0.006        &     2742.967    &     1.61e+04     \\\\\n",
       "\\textbf{16188.0}          &    5067.9193  &     3620.452     &     1.400  &         0.162        &    -2028.717    &     1.22e+04     \\\\\n",
       "\\textbf{1632.0}           &    1695.0991  &     2948.865     &     0.575  &         0.565        &    -4085.124    &     7475.322     \\\\\n",
       "\\textbf{1633.0}           &    7476.1562  &     3246.672     &     2.303  &         0.021        &     1112.185    &     1.38e+04     \\\\\n",
       "\\textbf{1635.0}           &   -9570.9410  &     3483.344     &    -2.748  &         0.006        &    -1.64e+04    &    -2743.057     \\\\\n",
       "\\textbf{16401.0}          &     154.0311  &     3522.272     &     0.044  &         0.965        &    -6750.159    &     7058.221     \\\\\n",
       "\\textbf{16437.0}          &    3278.1817  &     4055.398     &     0.808  &         0.419        &    -4671.016    &     1.12e+04     \\\\\n",
       "\\textbf{1651.0}           &    4853.9574  &     3050.757     &     1.591  &         0.112        &    -1125.990    &     1.08e+04     \\\\\n",
       "\\textbf{1655.0}           &    1.039e+04  &     3500.961     &     2.969  &         0.003        &     3531.377    &     1.73e+04     \\\\\n",
       "\\textbf{1663.0}           &    1.499e+04  &     3493.288     &     4.290  &         0.000        &     8137.781    &     2.18e+04     \\\\\n",
       "\\textbf{16710.0}          &    4680.4781  &     3643.868     &     1.284  &         0.199        &    -2462.057    &     1.18e+04     \\\\\n",
       "\\textbf{16729.0}          &    3279.1787  &     3496.933     &     0.938  &         0.348        &    -3575.341    &     1.01e+04     \\\\\n",
       "\\textbf{1690.0}           &    -1.29e+04  &     3126.881     &    -4.125  &         0.000        &     -1.9e+04    &    -6769.159     \\\\\n",
       "\\textbf{1703.0}           &    8580.7670  &     3451.436     &     2.486  &         0.013        &     1815.426    &     1.53e+04     \\\\\n",
       "\\textbf{17101.0}          &   -2.018e+04  &     1.11e+04     &    -1.826  &         0.068        &    -4.19e+04    &     1486.592     \\\\\n",
       "\\textbf{17202.0}          &    7809.9940  &     3719.926     &     2.100  &         0.036        &      518.372    &     1.51e+04     \\\\\n",
       "\\textbf{1722.0}           &    8452.8790  &     3492.118     &     2.421  &         0.016        &     1607.796    &     1.53e+04     \\\\\n",
       "\\textbf{1728.0}           &    1.027e+04  &     3498.334     &     2.935  &         0.003        &     3409.289    &     1.71e+04     \\\\\n",
       "\\textbf{1743.0}           &    8906.5506  &     4307.712     &     2.068  &         0.039        &      462.779    &     1.74e+04     \\\\\n",
       "\\textbf{1754.0}           &    9834.3119  &     3546.280     &     2.773  &         0.006        &     2883.064    &     1.68e+04     \\\\\n",
       "\\textbf{1762.0}           &    7435.8004  &     3401.849     &     2.186  &         0.029        &      767.658    &     1.41e+04     \\\\\n",
       "\\textbf{1773.0}           &    8891.4429  &     3479.243     &     2.556  &         0.011        &     2071.596    &     1.57e+04     \\\\\n",
       "\\textbf{1786.0}           &   -3461.9294  &     3081.001     &    -1.124  &         0.261        &    -9501.160    &     2577.301     \\\\\n",
       "\\textbf{18100.0}          &    6947.5759  &     3657.295     &     1.900  &         0.058        &     -221.279    &     1.41e+04     \\\\\n",
       "\\textbf{1820.0}           &    5465.5030  &     3097.241     &     1.765  &         0.078        &     -605.561    &     1.15e+04     \\\\\n",
       "\\textbf{1848.0}           &   -1886.5500  &     3530.970     &    -0.534  &         0.593        &    -8807.788    &     5034.688     \\\\\n",
       "\\textbf{18654.0}          &    9606.5788  &     4646.011     &     2.068  &         0.039        &      499.691    &     1.87e+04     \\\\\n",
       "\\textbf{1875.0}           &    3764.7167  &     4537.792     &     0.830  &         0.407        &    -5130.045    &     1.27e+04     \\\\\n",
       "\\textbf{1884.0}           &    8893.5871  &     3449.113     &     2.579  &         0.010        &     2132.801    &     1.57e+04     \\\\\n",
       "\\textbf{1913.0}           &    8167.5315  &     3325.843     &     2.456  &         0.014        &     1648.373    &     1.47e+04     \\\\\n",
       "\\textbf{1919.0}           &    9105.8754  &     3573.766     &     2.548  &         0.011        &     2100.751    &     1.61e+04     \\\\\n",
       "\\textbf{1920.0}           &    7091.9048  &     3175.931     &     2.233  &         0.026        &      866.596    &     1.33e+04     \\\\\n",
       "\\textbf{1968.0}           &    9763.7597  &     3432.129     &     2.845  &         0.004        &     3036.264    &     1.65e+04     \\\\\n",
       "\\textbf{1976.0}           &    1.072e+04  &     3385.906     &     3.167  &         0.002        &     4085.647    &     1.74e+04     \\\\\n",
       "\\textbf{1981.0}           &    9179.0457  &     3386.527     &     2.710  &         0.007        &     2540.938    &     1.58e+04     \\\\\n",
       "\\textbf{1988.0}           &    4480.5582  &     4066.528     &     1.102  &         0.271        &    -3490.456    &     1.25e+04     \\\\\n",
       "\\textbf{1992.0}           &    1.038e+04  &     3487.074     &     2.977  &         0.003        &     3545.153    &     1.72e+04     \\\\\n",
       "\\textbf{2008.0}           &    9454.9287  &     3367.152     &     2.808  &         0.005        &     2854.798    &     1.61e+04     \\\\\n",
       "\\textbf{2033.0}           &    9562.2420  &     3880.534     &     2.464  &         0.014        &     1955.805    &     1.72e+04     \\\\\n",
       "\\textbf{2044.0}           &    7026.6396  &     3177.011     &     2.212  &         0.027        &      799.214    &     1.33e+04     \\\\\n",
       "\\textbf{2049.0}           &    9228.3043  &     3397.286     &     2.716  &         0.007        &     2569.107    &     1.59e+04     \\\\\n",
       "\\textbf{2061.0}           &    1.047e+04  &     3505.208     &     2.986  &         0.003        &     3594.353    &     1.73e+04     \\\\\n",
       "\\textbf{20779.0}          &    5.253e+04  &     3656.911     &    14.363  &         0.000        &     4.54e+04    &     5.97e+04     \\\\\n",
       "\\textbf{2085.0}           &   -8160.7724  &     3333.012     &    -2.448  &         0.014        &    -1.47e+04    &    -1627.563     \\\\\n",
       "\\textbf{2086.0}           &    7159.5200  &     3250.961     &     2.202  &         0.028        &      787.141    &     1.35e+04     \\\\\n",
       "\\textbf{2111.0}           &    7686.8490  &     3220.397     &     2.387  &         0.017        &     1374.380    &      1.4e+04     \\\\\n",
       "\\textbf{21204.0}          &     384.4065  &     3516.538     &     0.109  &         0.913        &    -6508.543    &     7277.356     \\\\\n",
       "\\textbf{21238.0}          &    8142.5703  &     3798.818     &     2.143  &         0.032        &      696.310    &     1.56e+04     \\\\\n",
       "\\textbf{2124.0}           &    9948.2780  &     3545.061     &     2.806  &         0.005        &     2999.419    &     1.69e+04     \\\\\n",
       "\\textbf{2146.0}           &    1.384e+04  &     3784.701     &     3.656  &         0.000        &     6419.923    &     2.13e+04     \\\\\n",
       "\\textbf{21496.0}          &   -1.592e+04  &     4394.801     &    -3.622  &         0.000        &    -2.45e+04    &    -7302.277     \\\\\n",
       "\\textbf{2154.0}           &    9073.6815  &     3398.606     &     2.670  &         0.008        &     2411.896    &     1.57e+04     \\\\\n",
       "\\textbf{2176.0}           &    4.929e+04  &     3844.965     &    12.820  &         0.000        &     4.18e+04    &     5.68e+04     \\\\\n",
       "\\textbf{2188.0}           &    1.044e+04  &     3575.743     &     2.921  &         0.004        &     3434.167    &     1.75e+04     \\\\\n",
       "\\textbf{2189.0}           &    4635.5911  &     3442.148     &     1.347  &         0.178        &    -2111.542    &     1.14e+04     \\\\\n",
       "\\textbf{2220.0}           &    9562.6108  &     3447.803     &     2.774  &         0.006        &     2804.392    &     1.63e+04     \\\\\n",
       "\\textbf{22205.0}          &    1.025e+04  &     3981.986     &     2.575  &         0.010        &     2448.797    &     1.81e+04     \\\\\n",
       "\\textbf{2226.0}           &    6832.7747  &     6127.983     &     1.115  &         0.265        &    -5179.003    &     1.88e+04     \\\\\n",
       "\\textbf{2230.0}           &    8034.0356  &     3473.326     &     2.313  &         0.021        &     1225.788    &     1.48e+04     \\\\\n",
       "\\textbf{22325.0}          &   -1992.9417  &     3644.785     &    -0.547  &         0.585        &    -9137.275    &     5151.392     \\\\\n",
       "\\textbf{2255.0}           &    8674.1665  &     3467.604     &     2.501  &         0.012        &     1877.134    &     1.55e+04     \\\\\n",
       "\\textbf{22619.0}          &    9952.2455  &     4149.838     &     2.398  &         0.016        &     1817.932    &     1.81e+04     \\\\\n",
       "\\textbf{2267.0}           &    -805.0746  &     3029.653     &    -0.266  &         0.790        &    -6743.655    &     5133.505     \\\\\n",
       "\\textbf{22815.0}          &    4148.6717  &     3597.517     &     1.153  &         0.249        &    -2903.008    &     1.12e+04     \\\\\n",
       "\\textbf{2285.0}           &   -1.943e+04  &     3290.519     &    -5.906  &         0.000        &    -2.59e+04    &     -1.3e+04     \\\\\n",
       "\\textbf{2290.0}           &    5559.2176  &     3411.874     &     1.629  &         0.103        &    -1128.575    &     1.22e+04     \\\\\n",
       "\\textbf{2295.0}           &    1.008e+04  &     4816.118     &     2.093  &         0.036        &      640.045    &     1.95e+04     \\\\\n",
       "\\textbf{2316.0}           &    5178.5854  &     3441.299     &     1.505  &         0.132        &    -1566.883    &     1.19e+04     \\\\\n",
       "\\textbf{23220.0}          &    8950.6027  &     3920.186     &     2.283  &         0.022        &     1266.442    &     1.66e+04     \\\\\n",
       "\\textbf{23224.0}          &   -9039.2043  &     3854.373     &    -2.345  &         0.019        &    -1.66e+04    &    -1484.047     \\\\\n",
       "\\textbf{2343.0}           &   -4325.9227  &     5346.106     &    -0.809  &         0.418        &    -1.48e+04    &     6153.259     \\\\\n",
       "\\textbf{2352.0}           &    1.003e+04  &     3595.998     &     2.790  &         0.005        &     2983.330    &     1.71e+04     \\\\\n",
       "\\textbf{23700.0}          &   -2370.0832  &     4559.670     &    -0.520  &         0.603        &    -1.13e+04    &     6567.565     \\\\\n",
       "\\textbf{2390.0}           &    1.048e+04  &     3494.768     &     2.997  &         0.003        &     3624.802    &     1.73e+04     \\\\\n",
       "\\textbf{2393.0}           &    8653.3534  &     3349.179     &     2.584  &         0.010        &     2088.453    &     1.52e+04     \\\\\n",
       "\\textbf{2403.0}           &    9935.9405  &     3390.435     &     2.931  &         0.003        &     3290.172    &     1.66e+04     \\\\\n",
       "\\textbf{2435.0}           &    1.183e+04  &     3523.767     &     3.357  &         0.001        &     4921.422    &     1.87e+04     \\\\\n",
       "\\textbf{2444.0}           &    8334.6736  &     3418.105     &     2.438  &         0.015        &     1634.667    &      1.5e+04     \\\\\n",
       "\\textbf{2448.0}           &    8079.7608  &     3280.572     &     2.463  &         0.014        &     1649.340    &     1.45e+04     \\\\\n",
       "\\textbf{2469.0}           &    9319.5696  &     4619.391     &     2.017  &         0.044        &      264.861    &     1.84e+04     \\\\\n",
       "\\textbf{24720.0}          &    1.002e+04  &     4081.147     &     2.455  &         0.014        &     2018.551    &      1.8e+04     \\\\\n",
       "\\textbf{24800.0}          &   -2368.8265  &     4068.738     &    -0.582  &         0.560        &    -1.03e+04    &     5606.520     \\\\\n",
       "\\textbf{2482.0}           &    1.054e+04  &     3503.339     &     3.009  &         0.003        &     3675.623    &     1.74e+04     \\\\\n",
       "\\textbf{24969.0}          &    9638.1580  &     4606.058     &     2.092  &         0.036        &      609.584    &     1.87e+04     \\\\\n",
       "\\textbf{2498.0}           &    1009.6808  &     3154.855     &     0.320  &         0.749        &    -5174.316    &     7193.678     \\\\\n",
       "\\textbf{2504.0}           &    -596.5201  &     3326.842     &    -0.179  &         0.858        &    -7117.636    &     5924.596     \\\\\n",
       "\\textbf{2508.0}           &    1.021e+04  &     3663.087     &     2.788  &         0.005        &     3034.214    &     1.74e+04     \\\\\n",
       "\\textbf{25124.0}          &    9907.2768  &     4155.005     &     2.384  &         0.017        &     1762.836    &     1.81e+04     \\\\\n",
       "\\textbf{2518.0}           &     1.02e+04  &     3499.266     &     2.914  &         0.004        &     3337.153    &     1.71e+04     \\\\\n",
       "\\textbf{25224.0}          &    1.085e+04  &     7432.065     &     1.459  &         0.145        &    -3722.732    &     2.54e+04     \\\\\n",
       "\\textbf{25279.0}          &    7235.2571  &     3849.249     &     1.880  &         0.060        &     -309.857    &     1.48e+04     \\\\\n",
       "\\textbf{2537.0}           &   -6799.0456  &     3253.919     &    -2.089  &         0.037        &    -1.32e+04    &     -420.869     \\\\\n",
       "\\textbf{2538.0}           &    9856.5848  &     4266.509     &     2.310  &         0.021        &     1493.578    &     1.82e+04     \\\\\n",
       "\\textbf{25389.0}          &    9764.1891  &     6252.235     &     1.562  &         0.118        &    -2491.144    &      2.2e+04     \\\\\n",
       "\\textbf{2547.0}           &    3557.0844  &     3568.631     &     0.997  &         0.319        &    -3437.976    &     1.06e+04     \\\\\n",
       "\\textbf{2553.0}           &    8873.3357  &     3380.697     &     2.625  &         0.009        &     2246.655    &     1.55e+04     \\\\\n",
       "\\textbf{2574.0}           &    2969.8852  &     3914.372     &     0.759  &         0.448        &    -4702.880    &     1.06e+04     \\\\\n",
       "\\textbf{25747.0}          &    9693.6935  &     4140.844     &     2.341  &         0.019        &     1577.010    &     1.78e+04     \\\\\n",
       "\\textbf{2577.0}           &    8965.0512  &     3358.735     &     2.669  &         0.008        &     2381.419    &     1.55e+04     \\\\\n",
       "\\textbf{2593.0}           &    9363.1907  &     3451.006     &     2.713  &         0.007        &     2598.694    &     1.61e+04     \\\\\n",
       "\\textbf{2596.0}           &    4211.3081  &     3037.853     &     1.386  &         0.166        &    -1743.346    &     1.02e+04     \\\\\n",
       "\\textbf{2663.0}           &    1.312e+04  &     3506.203     &     3.743  &         0.000        &     6250.849    &        2e+04     \\\\\n",
       "\\textbf{2771.0}           &    4584.3418  &     3064.314     &     1.496  &         0.135        &    -1422.180    &     1.06e+04     \\\\\n",
       "\\textbf{2787.0}           &    9312.7499  &     3431.787     &     2.714  &         0.007        &     2585.925    &      1.6e+04     \\\\\n",
       "\\textbf{2797.0}           &   -3679.5566  &     3112.073     &    -1.182  &         0.237        &    -9779.693    &     2420.580     \\\\\n",
       "\\textbf{2802.0}           &    1.005e+04  &     3460.168     &     2.905  &         0.004        &     3267.980    &     1.68e+04     \\\\\n",
       "\\textbf{2817.0}           &    2283.4997  &     3416.197     &     0.668  &         0.504        &    -4412.766    &     8979.766     \\\\\n",
       "\\textbf{28678.0}          &   -9271.0040  &     4084.380     &    -2.270  &         0.023        &    -1.73e+04    &    -1264.999     \\\\\n",
       "\\textbf{28701.0}          &    7716.1013  &     3370.690     &     2.289  &         0.022        &     1109.037    &     1.43e+04     \\\\\n",
       "\\textbf{28742.0}          &   -9267.6116  &     4027.724     &    -2.301  &         0.021        &    -1.72e+04    &    -1372.660     \\\\\n",
       "\\textbf{2888.0}           &    1.007e+04  &     3594.695     &     2.802  &         0.005        &     3025.153    &     1.71e+04     \\\\\n",
       "\\textbf{2897.0}           &    1.066e+04  &     4157.956     &     2.564  &         0.010        &     2511.181    &     1.88e+04     \\\\\n",
       "\\textbf{2917.0}           &    4909.1440  &     3468.408     &     1.415  &         0.157        &    -1889.463    &     1.17e+04     \\\\\n",
       "\\textbf{29392.0}          &   -2347.8239  &     3892.943     &    -0.603  &         0.546        &    -9978.585    &     5282.937     \\\\\n",
       "\\textbf{2950.0}           &   -1.893e+04  &     4792.383     &    -3.951  &         0.000        &    -2.83e+04    &    -9540.080     \\\\\n",
       "\\textbf{2951.0}           &    1.016e+04  &     3824.854     &     2.656  &         0.008        &     2663.376    &     1.77e+04     \\\\\n",
       "\\textbf{2953.0}           &    9261.6627  &     3377.554     &     2.742  &         0.006        &     2641.143    &     1.59e+04     \\\\\n",
       "\\textbf{2960.0}           &    8815.4539  &     4097.258     &     2.152  &         0.031        &      784.204    &     1.68e+04     \\\\\n",
       "\\textbf{2975.0}           &    2967.2731  &     3055.386     &     0.971  &         0.331        &    -3021.748    &     8956.294     \\\\\n",
       "\\textbf{2982.0}           &    8378.8566  &     3335.690     &     2.512  &         0.012        &     1840.397    &     1.49e+04     \\\\\n",
       "\\textbf{2991.0}           &   -3542.1410  &     3614.739     &    -0.980  &         0.327        &    -1.06e+04    &     3543.297     \\\\\n",
       "\\textbf{3011.0}           &   -4619.7729  &     3181.777     &    -1.452  &         0.147        &    -1.09e+04    &     1616.993     \\\\\n",
       "\\textbf{3015.0}           &    1.033e+04  &     3445.213     &     2.998  &         0.003        &     3575.672    &     1.71e+04     \\\\\n",
       "\\textbf{3026.0}           &    8115.5273  &     3298.350     &     2.460  &         0.014        &     1650.259    &     1.46e+04     \\\\\n",
       "\\textbf{3031.0}           &    -1.31e+04  &     4243.942     &    -3.087  &         0.002        &    -2.14e+04    &    -4783.026     \\\\\n",
       "\\textbf{3062.0}           &    1.218e+04  &     3636.770     &     3.350  &         0.001        &     5056.024    &     1.93e+04     \\\\\n",
       "\\textbf{3093.0}           &    1154.0983  &     3273.649     &     0.353  &         0.724        &    -5262.751    &     7570.948     \\\\\n",
       "\\textbf{3107.0}           &    1.003e+04  &     4887.548     &     2.052  &         0.040        &      449.992    &     1.96e+04     \\\\\n",
       "\\textbf{3121.0}           &    1.131e+04  &     3424.546     &     3.304  &         0.001        &     4601.086    &      1.8e+04     \\\\\n",
       "\\textbf{3126.0}           &    1.029e+04  &     3500.672     &     2.940  &         0.003        &     3430.420    &     1.72e+04     \\\\\n",
       "\\textbf{3144.0}           &    6.226e+04  &     3492.741     &    17.824  &         0.000        &     5.54e+04    &     6.91e+04     \\\\\n",
       "\\textbf{3156.0}           &    8439.9508  &     3686.328     &     2.290  &         0.022        &     1214.187    &     1.57e+04     \\\\\n",
       "\\textbf{3157.0}           &    8909.5602  &     3367.099     &     2.646  &         0.008        &     2309.534    &     1.55e+04     \\\\\n",
       "\\textbf{3170.0}           &    1.049e+04  &     3163.430     &     3.317  &         0.001        &     4292.419    &     1.67e+04     \\\\\n",
       "\\textbf{3178.0}           &     476.3824  &     3058.630     &     0.156  &         0.876        &    -5518.997    &     6471.762     \\\\\n",
       "\\textbf{3206.0}           &    6647.9203  &     3682.551     &     1.805  &         0.071        &     -570.439    &     1.39e+04     \\\\\n",
       "\\textbf{3229.0}           &    4699.0928  &     3154.197     &     1.490  &         0.136        &    -1483.614    &     1.09e+04     \\\\\n",
       "\\textbf{3235.0}           &    1.043e+04  &     3672.530     &     2.840  &         0.005        &     3232.613    &     1.76e+04     \\\\\n",
       "\\textbf{3246.0}           &    8999.9332  &     3441.630     &     2.615  &         0.009        &     2253.815    &     1.57e+04     \\\\\n",
       "\\textbf{3248.0}           &    1.017e+04  &     3536.773     &     2.875  &         0.004        &     3235.421    &     1.71e+04     \\\\\n",
       "\\textbf{3282.0}           &   -2.157e+04  &     3279.699     &    -6.578  &         0.000        &     -2.8e+04    &    -1.51e+04     \\\\\n",
       "\\textbf{3362.0}           &    2423.5362  &     3770.430     &     0.643  &         0.520        &    -4967.079    &     9814.152     \\\\\n",
       "\\textbf{3372.0}           &    9627.7372  &     3873.154     &     2.486  &         0.013        &     2035.766    &     1.72e+04     \\\\\n",
       "\\textbf{3422.0}           &    9480.0410  &     3437.691     &     2.758  &         0.006        &     2741.643    &     1.62e+04     \\\\\n",
       "\\textbf{3497.0}           &    5252.3369  &     3141.534     &     1.672  &         0.095        &     -905.547    &     1.14e+04     \\\\\n",
       "\\textbf{3502.0}           &    3238.2765  &     3011.798     &     1.075  &         0.282        &    -2665.307    &     9141.860     \\\\\n",
       "\\textbf{3504.0}           &    8995.7334  &     3981.420     &     2.259  &         0.024        &     1191.544    &     1.68e+04     \\\\\n",
       "\\textbf{3505.0}           &    9143.3764  &     3461.357     &     2.642  &         0.008        &     2358.589    &     1.59e+04     \\\\\n",
       "\\textbf{3532.0}           &    1.003e+04  &     3245.402     &     3.091  &         0.002        &     3669.185    &     1.64e+04     \\\\\n",
       "\\textbf{3574.0}           &    1.069e+04  &     5195.998     &     2.058  &         0.040        &      509.765    &     2.09e+04     \\\\\n",
       "\\textbf{3580.0}           &    6068.2467  &     3187.564     &     1.904  &         0.057        &     -179.864    &     1.23e+04     \\\\\n",
       "\\textbf{3612.0}           &     1.02e+04  &     3452.504     &     2.955  &         0.003        &     3436.160    &      1.7e+04     \\\\\n",
       "\\textbf{3619.0}           &    7934.3369  &     3447.904     &     2.301  &         0.021        &     1175.921    &     1.47e+04     \\\\\n",
       "\\textbf{3622.0}           &    1.043e+04  &     3580.387     &     2.913  &         0.004        &     3411.652    &     1.74e+04     \\\\\n",
       "\\textbf{3639.0}           &   -1512.7133  &     2960.576     &    -0.511  &         0.609        &    -7315.893    &     4290.467     \\\\\n",
       "\\textbf{3650.0}           &    3115.5849  &     3263.925     &     0.955  &         0.340        &    -3282.206    &     9513.375     \\\\\n",
       "\\textbf{3662.0}           &    1.006e+04  &     3445.692     &     2.919  &         0.004        &     3304.170    &     1.68e+04     \\\\\n",
       "\\textbf{3734.0}           &   -7319.5906  &     3017.118     &    -2.426  &         0.015        &    -1.32e+04    &    -1405.580     \\\\\n",
       "\\textbf{3735.0}           &    9975.8401  &     3692.019     &     2.702  &         0.007        &     2738.921    &     1.72e+04     \\\\\n",
       "\\textbf{3761.0}           &    6349.2844  &     3193.756     &     1.988  &         0.047        &       89.036    &     1.26e+04     \\\\\n",
       "\\textbf{3779.0}           &   -8181.0780  &     3599.088     &    -2.273  &         0.023        &    -1.52e+04    &    -1126.317     \\\\\n",
       "\\textbf{3781.0}           &    -618.0552  &     3532.836     &    -0.175  &         0.861        &    -7542.951    &     6306.840     \\\\\n",
       "\\textbf{3782.0}           &   -3555.6311  &     3135.968     &    -1.134  &         0.257        &    -9702.606    &     2591.344     \\\\\n",
       "\\textbf{3786.0}           &    7499.2694  &     3240.654     &     2.314  &         0.021        &     1147.094    &     1.39e+04     \\\\\n",
       "\\textbf{3796.0}           &   -9059.0070  &     3588.977     &    -2.524  &         0.012        &    -1.61e+04    &    -2024.065     \\\\\n",
       "\\textbf{3821.0}           &    9604.8814  &     3519.854     &     2.729  &         0.006        &     2705.432    &     1.65e+04     \\\\\n",
       "\\textbf{3835.0}           &    3076.1366  &     3410.513     &     0.902  &         0.367        &    -3608.988    &     9761.262     \\\\\n",
       "\\textbf{3839.0}           &    8341.2256  &     4032.054     &     2.069  &         0.039        &      437.786    &     1.62e+04     \\\\\n",
       "\\textbf{3840.0}           &    -969.3453  &     2951.072     &    -0.328  &         0.743        &    -6753.895    &     4815.204     \\\\\n",
       "\\textbf{3895.0}           &    9871.3541  &     3445.298     &     2.865  &         0.004        &     3118.046    &     1.66e+04     \\\\\n",
       "\\textbf{3908.0}           &    3036.1131  &     4046.355     &     0.750  &         0.453        &    -4895.359    &      1.1e+04     \\\\\n",
       "\\textbf{3911.0}           &    6304.1002  &     3262.976     &     1.932  &         0.053        &      -91.829    &     1.27e+04     \\\\\n",
       "\\textbf{3917.0}           &    9590.5921  &     3473.229     &     2.761  &         0.006        &     2782.534    &     1.64e+04     \\\\\n",
       "\\textbf{3946.0}           &    1.011e+04  &     3473.206     &     2.910  &         0.004        &     3300.234    &     1.69e+04     \\\\\n",
       "\\textbf{3971.0}           &    9572.0036  &     3496.097     &     2.738  &         0.006        &     2719.122    &     1.64e+04     \\\\\n",
       "\\textbf{3980.0}           &    1.938e+04  &     3338.403     &     5.804  &         0.000        &     1.28e+04    &     2.59e+04     \\\\\n",
       "\\textbf{4034.0}           &    7772.7623  &     3372.613     &     2.305  &         0.021        &     1161.929    &     1.44e+04     \\\\\n",
       "\\textbf{4036.0}           &     1.05e+04  &     3491.567     &     3.008  &         0.003        &     3659.649    &     1.73e+04     \\\\\n",
       "\\textbf{4040.0}           &    1558.3206  &     2961.665     &     0.526  &         0.599        &    -4246.994    &     7363.635     \\\\\n",
       "\\textbf{4058.0}           &    8856.4981  &     3314.736     &     2.672  &         0.008        &     2359.111    &     1.54e+04     \\\\\n",
       "\\textbf{4060.0}           &   -6647.6270  &     3194.633     &    -2.081  &         0.037        &    -1.29e+04    &     -385.661     \\\\\n",
       "\\textbf{4062.0}           &    1.204e+04  &     3470.424     &     3.469  &         0.001        &     5237.107    &     1.88e+04     \\\\\n",
       "\\textbf{4077.0}           &    7312.3454  &     4387.204     &     1.667  &         0.096        &    -1287.241    &     1.59e+04     \\\\\n",
       "\\textbf{4087.0}           &   -1.765e+04  &     3493.298     &    -5.052  &         0.000        &    -2.45e+04    &    -1.08e+04     \\\\\n",
       "\\textbf{4091.0}           &    7653.2198  &     3705.070     &     2.066  &         0.039        &      390.718    &     1.49e+04     \\\\\n",
       "\\textbf{4127.0}           &    3525.2756  &     3006.526     &     1.173  &         0.241        &    -2367.972    &     9418.523     \\\\\n",
       "\\textbf{4138.0}           &    1.059e+04  &     4042.581     &     2.620  &         0.009        &     2666.318    &     1.85e+04     \\\\\n",
       "\\textbf{4162.0}           &    9146.0786  &     3916.758     &     2.335  &         0.020        &     1468.636    &     1.68e+04     \\\\\n",
       "\\textbf{4186.0}           &     1.05e+04  &     3488.695     &     3.009  &         0.003        &     3658.523    &     1.73e+04     \\\\\n",
       "\\textbf{4194.0}           &    -120.8974  &     3365.019     &    -0.036  &         0.971        &    -6716.846    &     6475.051     \\\\\n",
       "\\textbf{4199.0}           &   -2262.4254  &     2969.652     &    -0.762  &         0.446        &    -8083.396    &     3558.545     \\\\\n",
       "\\textbf{4213.0}           &    1.047e+04  &     3437.887     &     3.045  &         0.002        &     3728.458    &     1.72e+04     \\\\\n",
       "\\textbf{4222.0}           &   -4565.7812  &     3081.867     &    -1.481  &         0.138        &    -1.06e+04    &     1475.148     \\\\\n",
       "\\textbf{4223.0}           &    9852.3412  &     3425.086     &     2.877  &         0.004        &     3138.651    &     1.66e+04     \\\\\n",
       "\\textbf{4251.0}           &    1.021e+04  &     3471.456     &     2.940  &         0.003        &     3402.138    &      1.7e+04     \\\\\n",
       "\\textbf{4265.0}           &    7812.5812  &     3452.112     &     2.263  &         0.024        &     1045.916    &     1.46e+04     \\\\\n",
       "\\textbf{4274.0}           &    9412.2701  &     3542.496     &     2.657  &         0.008        &     2468.439    &     1.64e+04     \\\\\n",
       "\\textbf{4321.0}           &    9164.6583  &     3367.888     &     2.721  &         0.007        &     2563.085    &     1.58e+04     \\\\\n",
       "\\textbf{4335.0}           &    8190.6964  &     4783.515     &     1.712  &         0.087        &    -1185.720    &     1.76e+04     \\\\\n",
       "\\textbf{4340.0}           &    6767.9340  &     3214.191     &     2.106  &         0.035        &      467.630    &     1.31e+04     \\\\\n",
       "\\textbf{4371.0}           &    8758.7466  &     3469.648     &     2.524  &         0.012        &     1957.708    &     1.56e+04     \\\\\n",
       "\\textbf{4415.0}           &    9054.9012  &     3466.775     &     2.612  &         0.009        &     2259.495    &     1.59e+04     \\\\\n",
       "\\textbf{4450.0}           &    7004.7705  &     3192.340     &     2.194  &         0.028        &      747.298    &     1.33e+04     \\\\\n",
       "\\textbf{4476.0}           &    -537.3034  &     3155.397     &    -0.170  &         0.865        &    -6722.362    &     5647.755     \\\\\n",
       "\\textbf{4510.0}           &    5574.8780  &     3357.655     &     1.660  &         0.097        &    -1006.636    &     1.22e+04     \\\\\n",
       "\\textbf{4520.0}           &    1.049e+04  &     3503.110     &     2.996  &         0.003        &     3627.664    &     1.74e+04     \\\\\n",
       "\\textbf{4551.0}           &    7925.1651  &     7335.131     &     1.080  &         0.280        &    -6452.807    &     2.23e+04     \\\\\n",
       "\\textbf{4568.0}           &    1.039e+04  &     3626.066     &     2.865  &         0.004        &     3281.685    &     1.75e+04     \\\\\n",
       "\\textbf{4579.0}           &     1.01e+04  &     3454.228     &     2.924  &         0.003        &     3327.659    &     1.69e+04     \\\\\n",
       "\\textbf{4585.0}           &    1.023e+04  &     3532.907     &     2.895  &         0.004        &     3303.805    &     1.72e+04     \\\\\n",
       "\\textbf{4595.0}           &    7017.4950  &     3203.870     &     2.190  &         0.029        &      737.422    &     1.33e+04     \\\\\n",
       "\\textbf{4600.0}           &   -2913.4675  &     3188.550     &    -0.914  &         0.361        &    -9163.510    &     3336.575     \\\\\n",
       "\\textbf{4607.0}           &     1.01e+04  &     3461.607     &     2.918  &         0.004        &     3314.622    &     1.69e+04     \\\\\n",
       "\\textbf{4608.0}           &   -2467.3480  &     2996.140     &    -0.824  &         0.410        &    -8340.238    &     3405.542     \\\\\n",
       "\\textbf{4622.0}           &    8052.4631  &     3317.026     &     2.428  &         0.015        &     1550.588    &     1.46e+04     \\\\\n",
       "\\textbf{4623.0}           &    8958.3286  &     3477.238     &     2.576  &         0.010        &     2142.413    &     1.58e+04     \\\\\n",
       "\\textbf{4768.0}           &    9434.6639  &     3524.936     &     2.677  &         0.007        &     2525.254    &     1.63e+04     \\\\\n",
       "\\textbf{4771.0}           &    1.027e+04  &     3483.066     &     2.949  &         0.003        &     3445.939    &     1.71e+04     \\\\\n",
       "\\textbf{4800.0}           &    8501.1614  &     3530.706     &     2.408  &         0.016        &     1580.441    &     1.54e+04     \\\\\n",
       "\\textbf{4802.0}           &    1.039e+04  &     3495.460     &     2.971  &         0.003        &     3535.120    &     1.72e+04     \\\\\n",
       "\\textbf{4807.0}           &    1.021e+04  &     3570.231     &     2.859  &         0.004        &     3209.789    &     1.72e+04     \\\\\n",
       "\\textbf{4839.0}           &   -1.356e+05  &     4533.275     &   -29.907  &         0.000        &    -1.44e+05    &    -1.27e+05     \\\\\n",
       "\\textbf{4843.0}           &   -8167.6858  &     3490.884     &    -2.340  &         0.019        &     -1.5e+04    &    -1325.021     \\\\\n",
       "\\textbf{4881.0}           &    8989.8462  &     3358.611     &     2.677  &         0.007        &     2406.458    &     1.56e+04     \\\\\n",
       "\\textbf{4900.0}           &    7258.8031  &     3240.831     &     2.240  &         0.025        &      906.282    &     1.36e+04     \\\\\n",
       "\\textbf{4926.0}           &    9046.0784  &     3409.453     &     2.653  &         0.008        &     2363.032    &     1.57e+04     \\\\\n",
       "\\textbf{4941.0}           &    8846.7949  &     3497.185     &     2.530  &         0.011        &     1991.780    &     1.57e+04     \\\\\n",
       "\\textbf{4961.0}           &   -7068.5016  &     3842.536     &    -1.840  &         0.066        &    -1.46e+04    &      463.454     \\\\\n",
       "\\textbf{4988.0}           &    1.557e+04  &     3453.820     &     4.509  &         0.000        &     8802.201    &     2.23e+04     \\\\\n",
       "\\textbf{4993.0}           &    1.045e+04  &     3497.931     &     2.988  &         0.003        &     3594.338    &     1.73e+04     \\\\\n",
       "\\textbf{5018.0}           &    7179.5577  &     3370.938     &     2.130  &         0.033        &      572.005    &     1.38e+04     \\\\\n",
       "\\textbf{5020.0}           &   -5446.9435  &     3426.169     &    -1.590  &         0.112        &    -1.22e+04    &     1268.869     \\\\\n",
       "\\textbf{5027.0}           &    5845.9537  &     3181.465     &     1.838  &         0.066        &     -390.201    &     1.21e+04     \\\\\n",
       "\\textbf{5032.0}           &    9857.7384  &     3470.574     &     2.840  &         0.005        &     3054.886    &     1.67e+04     \\\\\n",
       "\\textbf{5043.0}           &    5610.0072  &     3176.436     &     1.766  &         0.077        &     -616.291    &     1.18e+04     \\\\\n",
       "\\textbf{5046.0}           &   -1341.8874  &     3062.059     &    -0.438  &         0.661        &    -7343.989    &     4660.214     \\\\\n",
       "\\textbf{5047.0}           &    5.447e+04  &     4066.247     &    13.397  &         0.000        &     4.65e+04    &     6.24e+04     \\\\\n",
       "\\textbf{5065.0}           &    1.006e+04  &     3830.583     &     2.625  &         0.009        &     2547.975    &     1.76e+04     \\\\\n",
       "\\textbf{5071.0}           &    9512.9670  &     3843.083     &     2.475  &         0.013        &     1979.940    &      1.7e+04     \\\\\n",
       "\\textbf{5073.0}           &   -2.008e+05  &     6312.374     &   -31.807  &         0.000        &    -2.13e+05    &    -1.88e+05     \\\\\n",
       "\\textbf{5087.0}           &    4797.4041  &     3156.086     &     1.520  &         0.129        &    -1389.005    &      1.1e+04     \\\\\n",
       "\\textbf{5109.0}           &    1.013e+04  &     3496.987     &     2.898  &         0.004        &     3278.094    &      1.7e+04     \\\\\n",
       "\\textbf{5116.0}           &   -3937.9453  &     3224.186     &    -1.221  &         0.222        &    -1.03e+04    &     2381.951     \\\\\n",
       "\\textbf{5122.0}           &    5517.2539  &     3154.948     &     1.749  &         0.080        &     -666.925    &     1.17e+04     \\\\\n",
       "\\textbf{5134.0}           &    3884.2245  &     3594.240     &     1.081  &         0.280        &    -3161.033    &     1.09e+04     \\\\\n",
       "\\textbf{5142.0}           &    6838.4021  &     3976.061     &     1.720  &         0.085        &     -955.281    &     1.46e+04     \\\\\n",
       "\\textbf{5165.0}           &    4497.2071  &     3311.638     &     1.358  &         0.174        &    -1994.108    &      1.1e+04     \\\\\n",
       "\\textbf{5169.0}           &    1.916e+04  &     3342.007     &     5.734  &         0.000        &     1.26e+04    &     2.57e+04     \\\\\n",
       "\\textbf{5174.0}           &    6293.7590  &     3406.431     &     1.848  &         0.065        &     -383.364    &      1.3e+04     \\\\\n",
       "\\textbf{5179.0}           &    9463.8885  &     3387.278     &     2.794  &         0.005        &     2824.307    &     1.61e+04     \\\\\n",
       "\\textbf{5181.0}           &    1.047e+04  &     3593.512     &     2.915  &         0.004        &     3430.631    &     1.75e+04     \\\\\n",
       "\\textbf{5187.0}           &    1.028e+04  &     3835.771     &     2.680  &         0.007        &     2762.068    &     1.78e+04     \\\\\n",
       "\\textbf{5229.0}           &    3372.9553  &     3100.033     &     1.088  &         0.277        &    -2703.582    &     9449.492     \\\\\n",
       "\\textbf{5234.0}           &   -2373.8375  &     3557.256     &    -0.667  &         0.505        &    -9346.600    &     4598.925     \\\\\n",
       "\\textbf{5237.0}           &    9274.2111  &     3376.757     &     2.746  &         0.006        &     2655.255    &     1.59e+04     \\\\\n",
       "\\textbf{5252.0}           &    9398.7830  &     3395.434     &     2.768  &         0.006        &     2743.215    &     1.61e+04     \\\\\n",
       "\\textbf{5254.0}           &    9895.7852  &     3468.171     &     2.853  &         0.004        &     3097.642    &     1.67e+04     \\\\\n",
       "\\textbf{5306.0}           &    9991.3769  &     3380.819     &     2.955  &         0.003        &     3364.457    &     1.66e+04     \\\\\n",
       "\\textbf{5338.0}           &    1.017e+04  &     3461.718     &     2.937  &         0.003        &     3379.876    &      1.7e+04     \\\\\n",
       "\\textbf{5377.0}           &    1.013e+04  &     3504.604     &     2.891  &         0.004        &     3263.929    &      1.7e+04     \\\\\n",
       "\\textbf{5439.0}           &    7549.1619  &     3383.094     &     2.231  &         0.026        &      917.783    &     1.42e+04     \\\\\n",
       "\\textbf{5456.0}           &    1.043e+04  &     3529.032     &     2.955  &         0.003        &     3510.479    &     1.73e+04     \\\\\n",
       "\\textbf{5464.0}           &    9243.4459  &     4033.476     &     2.292  &         0.022        &     1337.220    &     1.71e+04     \\\\\n",
       "\\textbf{5476.0}           &    9316.0567  &     3359.046     &     2.773  &         0.006        &     2731.816    &     1.59e+04     \\\\\n",
       "\\textbf{5492.0}           &   -1.083e+04  &     3431.449     &    -3.157  &         0.002        &    -1.76e+04    &    -4106.851     \\\\\n",
       "\\textbf{5496.0}           &    9305.5943  &     3421.714     &     2.720  &         0.007        &     2598.514    &      1.6e+04     \\\\\n",
       "\\textbf{5505.0}           &    9887.5592  &     3496.472     &     2.828  &         0.005        &     3033.942    &     1.67e+04     \\\\\n",
       "\\textbf{5518.0}           &    8043.4491  &     3674.641     &     2.189  &         0.029        &      840.594    &     1.52e+04     \\\\\n",
       "\\textbf{5520.0}           &    7086.8585  &     3241.750     &     2.186  &         0.029        &      732.536    &     1.34e+04     \\\\\n",
       "\\textbf{5545.0}           &    1.061e+04  &     3626.502     &     2.926  &         0.003        &     3503.952    &     1.77e+04     \\\\\n",
       "\\textbf{5568.0}           &    1.329e+04  &     3515.859     &     3.779  &         0.000        &     6394.215    &     2.02e+04     \\\\\n",
       "\\textbf{5569.0}           &    1.038e+04  &     3521.677     &     2.947  &         0.003        &     3474.427    &     1.73e+04     \\\\\n",
       "\\textbf{5578.0}           &    9794.0748  &     3412.408     &     2.870  &         0.004        &     3105.235    &     1.65e+04     \\\\\n",
       "\\textbf{5581.0}           &    9726.6032  &     3379.409     &     2.878  &         0.004        &     3102.448    &     1.64e+04     \\\\\n",
       "\\textbf{5589.0}           &    4310.6488  &     3071.264     &     1.404  &         0.160        &    -1709.495    &     1.03e+04     \\\\\n",
       "\\textbf{5597.0}           &    1.225e+04  &     3985.852     &     3.073  &         0.002        &     4436.767    &     2.01e+04     \\\\\n",
       "\\textbf{5606.0}           &   -2.818e+04  &     3156.085     &    -8.928  &         0.000        &    -3.44e+04    &     -2.2e+04     \\\\\n",
       "\\textbf{5639.0}           &    1.079e+04  &     3475.889     &     3.103  &         0.002        &     3972.430    &     1.76e+04     \\\\\n",
       "\\textbf{5667.0}           &    4408.3690  &     3081.766     &     1.430  &         0.153        &    -1632.362    &     1.04e+04     \\\\\n",
       "\\textbf{5690.0}           &    1.044e+04  &     3495.787     &     2.986  &         0.003        &     3586.243    &     1.73e+04     \\\\\n",
       "\\textbf{5709.0}           &    9489.9110  &     3484.902     &     2.723  &         0.006        &     2658.972    &     1.63e+04     \\\\\n",
       "\\textbf{5726.0}           &    8843.5967  &     3350.233     &     2.640  &         0.008        &     2276.631    &     1.54e+04     \\\\\n",
       "\\textbf{5764.0}           &    9607.0617  &     3330.031     &     2.885  &         0.004        &     3079.694    &     1.61e+04     \\\\\n",
       "\\textbf{5772.0}           &    1.009e+04  &     3477.233     &     2.901  &         0.004        &     3272.074    &     1.69e+04     \\\\\n",
       "\\textbf{5860.0}           &   -2.137e+04  &     3183.508     &    -6.714  &         0.000        &    -2.76e+04    &    -1.51e+04     \\\\\n",
       "\\textbf{5878.0}           &    1.288e+04  &     3373.262     &     3.819  &         0.000        &     6271.821    &     1.95e+04     \\\\\n",
       "\\textbf{5903.0}           &    3056.9152  &     3093.343     &     0.988  &         0.323        &    -3006.507    &     9120.338     \\\\\n",
       "\\textbf{5905.0}           &    8382.9179  &     3504.017     &     2.392  &         0.017        &     1514.512    &     1.53e+04     \\\\\n",
       "\\textbf{5959.0}           &    6222.0789  &     3427.640     &     1.815  &         0.070        &     -496.618    &     1.29e+04     \\\\\n",
       "\\textbf{6008.0}           &    2.945e+04  &     3053.695     &     9.643  &         0.000        &     2.35e+04    &     3.54e+04     \\\\\n",
       "\\textbf{6034.0}           &    1.011e+04  &     3585.086     &     2.820  &         0.005        &     3081.561    &     1.71e+04     \\\\\n",
       "\\textbf{6035.0}           &    1483.7234  &     3506.418     &     0.423  &         0.672        &    -5389.389    &     8356.836     \\\\\n",
       "\\textbf{6036.0}           &     221.6732  &     2991.364     &     0.074  &         0.941        &    -5641.855    &     6085.201     \\\\\n",
       "\\textbf{6039.0}           &    9544.5704  &     3432.253     &     2.781  &         0.005        &     2816.832    &     1.63e+04     \\\\\n",
       "\\textbf{6044.0}           &    1.016e+04  &     3681.651     &     2.760  &         0.006        &     2942.992    &     1.74e+04     \\\\\n",
       "\\textbf{6066.0}           &   -2.346e+04  &     4482.310     &    -5.233  &         0.000        &    -3.22e+04    &    -1.47e+04     \\\\\n",
       "\\textbf{6078.0}           &    9706.7472  &     3249.116     &     2.988  &         0.003        &     3337.985    &     1.61e+04     \\\\\n",
       "\\textbf{6081.0}           &   -1.153e+04  &     3146.489     &    -3.664  &         0.000        &    -1.77e+04    &    -5361.713     \\\\\n",
       "\\textbf{60893.0}          &    -1.28e+04  &     4529.427     &    -2.826  &         0.005        &    -2.17e+04    &    -3921.010     \\\\\n",
       "\\textbf{6097.0}           &    9210.7131  &     3343.462     &     2.755  &         0.006        &     2657.018    &     1.58e+04     \\\\\n",
       "\\textbf{6102.0}           &    9066.2170  &     3480.579     &     2.605  &         0.009        &     2243.752    &     1.59e+04     \\\\\n",
       "\\textbf{6104.0}           &     232.2733  &     3423.042     &     0.068  &         0.946        &    -6477.409    &     6941.956     \\\\\n",
       "\\textbf{6109.0}           &    1010.3528  &     2947.059     &     0.343  &         0.732        &    -4766.331    &     6787.037     \\\\\n",
       "\\textbf{6127.0}           &    4701.1928  &     3757.688     &     1.251  &         0.211        &    -2664.447    &     1.21e+04     \\\\\n",
       "\\textbf{61552.0}          &   -4432.5561  &     4243.880     &    -1.044  &         0.296        &    -1.28e+04    &     3886.094     \\\\\n",
       "\\textbf{6158.0}           &    6664.1488  &     3326.127     &     2.004  &         0.045        &      144.434    &     1.32e+04     \\\\\n",
       "\\textbf{6171.0}           &    9441.2528  &     3383.795     &     2.790  &         0.005        &     2808.500    &     1.61e+04     \\\\\n",
       "\\textbf{61780.0}          &    9463.2455  &     5197.187     &     1.821  &         0.069        &     -724.032    &     1.97e+04     \\\\\n",
       "\\textbf{6207.0}           &    8858.3816  &     3348.787     &     2.645  &         0.008        &     2294.250    &     1.54e+04     \\\\\n",
       "\\textbf{6214.0}           &    1.051e+04  &     3500.804     &     3.002  &         0.003        &     3647.047    &     1.74e+04     \\\\\n",
       "\\textbf{6216.0}           &    9305.6197  &     3433.501     &     2.710  &         0.007        &     2575.436    &      1.6e+04     \\\\\n",
       "\\textbf{62221.0}          &    9636.3504  &     4648.363     &     2.073  &         0.038        &      524.852    &     1.87e+04     \\\\\n",
       "\\textbf{6259.0}           &    1362.8316  &     3271.611     &     0.417  &         0.677        &    -5050.023    &     7775.686     \\\\\n",
       "\\textbf{62599.0}          &   -2.068e+04  &     4912.844     &    -4.210  &         0.000        &    -3.03e+04    &    -1.11e+04     \\\\\n",
       "\\textbf{6266.0}           &    5707.1489  &     3198.854     &     1.784  &         0.074        &     -563.092    &      1.2e+04     \\\\\n",
       "\\textbf{6268.0}           &    5194.0633  &     3510.777     &     1.479  &         0.139        &    -1687.594    &     1.21e+04     \\\\\n",
       "\\textbf{6288.0}           &    9663.1685  &     3447.626     &     2.803  &         0.005        &     2905.297    &     1.64e+04     \\\\\n",
       "\\textbf{6297.0}           &    1.056e+04  &     3595.867     &     2.937  &         0.003        &     3511.586    &     1.76e+04     \\\\\n",
       "\\textbf{6307.0}           &   -1.092e+04  &     4063.132     &    -2.688  &         0.007        &    -1.89e+04    &    -2956.996     \\\\\n",
       "\\textbf{6313.0}           &    8891.8083  &     4582.140     &     1.941  &         0.052        &      -89.882    &     1.79e+04     \\\\\n",
       "\\textbf{6314.0}           &    9311.8048  &     3390.368     &     2.747  &         0.006        &     2666.168    &      1.6e+04     \\\\\n",
       "\\textbf{6326.0}           &    6072.5560  &     3163.322     &     1.920  &         0.055        &     -128.036    &     1.23e+04     \\\\\n",
       "\\textbf{6349.0}           &    9207.5642  &     3389.098     &     2.717  &         0.007        &     2564.416    &     1.59e+04     \\\\\n",
       "\\textbf{6357.0}           &    1.024e+04  &     3585.280     &     2.857  &         0.004        &     3216.274    &     1.73e+04     \\\\\n",
       "\\textbf{6375.0}           &    1.444e+04  &     3464.702     &     4.168  &         0.000        &     7650.844    &     2.12e+04     \\\\\n",
       "\\textbf{6376.0}           &    9678.6118  &     3484.646     &     2.778  &         0.005        &     2848.175    &     1.65e+04     \\\\\n",
       "\\textbf{6379.0}           &      26.6996  &     6333.401     &     0.004  &         0.997        &    -1.24e+04    &     1.24e+04     \\\\\n",
       "\\textbf{6386.0}           &    1.048e+04  &     3498.806     &     2.994  &         0.003        &     3618.477    &     1.73e+04     \\\\\n",
       "\\textbf{6403.0}           &    4977.6386  &     3259.208     &     1.527  &         0.127        &    -1410.906    &     1.14e+04     \\\\\n",
       "\\textbf{6410.0}           &     1.05e+04  &     3527.394     &     2.977  &         0.003        &     3586.498    &     1.74e+04     \\\\\n",
       "\\textbf{6416.0}           &    2347.3189  &     3090.659     &     0.759  &         0.448        &    -3710.844    &     8405.482     \\\\\n",
       "\\textbf{6424.0}           &    9621.0992  &     3436.334     &     2.800  &         0.005        &     2885.363    &     1.64e+04     \\\\\n",
       "\\textbf{6433.0}           &    8661.7986  &     3343.259     &     2.591  &         0.010        &     2108.503    &     1.52e+04     \\\\\n",
       "\\textbf{6435.0}           &    1.183e+04  &     3360.879     &     3.519  &         0.000        &     5240.102    &     1.84e+04     \\\\\n",
       "\\textbf{6492.0}           &    8974.9088  &     3432.021     &     2.615  &         0.009        &     2247.626    &     1.57e+04     \\\\\n",
       "\\textbf{6497.0}           &   -3053.3190  &     3013.220     &    -1.013  &         0.311        &    -8959.689    &     2853.051     \\\\\n",
       "\\textbf{6500.0}           &    6150.1570  &     7269.626     &     0.846  &         0.398        &    -8099.415    &     2.04e+04     \\\\\n",
       "\\textbf{6509.0}           &    9556.9483  &     3435.340     &     2.782  &         0.005        &     2823.159    &     1.63e+04     \\\\\n",
       "\\textbf{6527.0}           &    1.047e+04  &     3729.698     &     2.807  &         0.005        &     3158.019    &     1.78e+04     \\\\\n",
       "\\textbf{6528.0}           &    8269.4671  &     3499.188     &     2.363  &         0.018        &     1410.526    &     1.51e+04     \\\\\n",
       "\\textbf{6531.0}           &    3609.6244  &     3468.218     &     1.041  &         0.298        &    -3188.611    &     1.04e+04     \\\\\n",
       "\\textbf{6532.0}           &    5989.2996  &     3226.740     &     1.856  &         0.063        &     -335.601    &     1.23e+04     \\\\\n",
       "\\textbf{6543.0}           &    1.019e+04  &     3488.637     &     2.922  &         0.003        &     3355.752    &      1.7e+04     \\\\\n",
       "\\textbf{6548.0}           &    9642.4815  &     3462.712     &     2.785  &         0.005        &     2855.040    &     1.64e+04     \\\\\n",
       "\\textbf{6550.0}           &    1.029e+04  &     3711.831     &     2.772  &         0.006        &     3013.513    &     1.76e+04     \\\\\n",
       "\\textbf{6552.0}           &    9481.6941  &     3579.910     &     2.649  &         0.008        &     2464.527    &     1.65e+04     \\\\\n",
       "\\textbf{6565.0}           &    8646.7354  &     3544.299     &     2.440  &         0.015        &     1699.371    &     1.56e+04     \\\\\n",
       "\\textbf{6571.0}           &    9933.4396  &     3461.508     &     2.870  &         0.004        &     3148.358    &     1.67e+04     \\\\\n",
       "\\textbf{6573.0}           &    9777.5503  &     3422.534     &     2.857  &         0.004        &     3068.863    &     1.65e+04     \\\\\n",
       "\\textbf{6641.0}           &    6780.5002  &     5403.773     &     1.255  &         0.210        &    -3811.717    &     1.74e+04     \\\\\n",
       "\\textbf{6649.0}           &    1.063e+04  &     3496.881     &     3.041  &         0.002        &     3779.297    &     1.75e+04     \\\\\n",
       "\\textbf{6730.0}           &    3503.1057  &     3291.625     &     1.064  &         0.287        &    -2948.979    &     9955.191     \\\\\n",
       "\\textbf{6731.0}           &    9345.3044  &     3489.365     &     2.678  &         0.007        &     2505.618    &     1.62e+04     \\\\\n",
       "\\textbf{6742.0}           &    9178.2023  &     4815.789     &     1.906  &         0.057        &     -261.476    &     1.86e+04     \\\\\n",
       "\\textbf{6745.0}           &    1.058e+04  &     3564.650     &     2.968  &         0.003        &     3592.600    &     1.76e+04     \\\\\n",
       "\\textbf{6756.0}           &    9146.2023  &     3364.036     &     2.719  &         0.007        &     2552.180    &     1.57e+04     \\\\\n",
       "\\textbf{6765.0}           &   -6705.0520  &     3084.029     &    -2.174  &         0.030        &    -1.28e+04    &     -659.886     \\\\\n",
       "\\textbf{6768.0}           &    1.103e+04  &     3514.640     &     3.140  &         0.002        &     4145.597    &     1.79e+04     \\\\\n",
       "\\textbf{6774.0}           &   -1.801e+04  &     3543.800     &    -5.083  &         0.000        &     -2.5e+04    &    -1.11e+04     \\\\\n",
       "\\textbf{6797.0}           &    1.026e+04  &     3843.351     &     2.670  &         0.008        &     2729.513    &     1.78e+04     \\\\\n",
       "\\textbf{6803.0}           &    9607.2274  &     3435.986     &     2.796  &         0.005        &     2872.172    &     1.63e+04     \\\\\n",
       "\\textbf{6821.0}           &    9500.5977  &     3458.877     &     2.747  &         0.006        &     2720.672    &     1.63e+04     \\\\\n",
       "\\textbf{6830.0}           &    7437.6025  &     3255.407     &     2.285  &         0.022        &     1056.509    &     1.38e+04     \\\\\n",
       "\\textbf{6845.0}           &    9803.0965  &     3438.070     &     2.851  &         0.004        &     3063.956    &     1.65e+04     \\\\\n",
       "\\textbf{6848.0}           &    8333.6403  &     3496.836     &     2.383  &         0.017        &     1479.309    &     1.52e+04     \\\\\n",
       "\\textbf{6873.0}           &    6772.6473  &     4025.271     &     1.683  &         0.092        &    -1117.495    &     1.47e+04     \\\\\n",
       "\\textbf{6900.0}           &    9887.2618  &     3448.768     &     2.867  &         0.004        &     3127.151    &     1.66e+04     \\\\\n",
       "\\textbf{6908.0}           &    9670.9945  &     3412.881     &     2.834  &         0.005        &     2981.228    &     1.64e+04     \\\\\n",
       "\\textbf{6994.0}           &    7027.9999  &     3190.940     &     2.202  &         0.028        &      773.272    &     1.33e+04     \\\\\n",
       "\\textbf{7045.0}           &   -1365.3935  &     3609.225     &    -0.378  &         0.705        &    -8440.023    &     5709.236     \\\\\n",
       "\\textbf{7065.0}           &    1.611e+04  &     3506.227     &     4.596  &         0.000        &     9241.876    &      2.3e+04     \\\\\n",
       "\\textbf{7085.0}           &    1.278e+04  &     3490.166     &     3.663  &         0.000        &     5943.361    &     1.96e+04     \\\\\n",
       "\\textbf{7107.0}           &    1.016e+04  &     3620.898     &     2.807  &         0.005        &     3065.851    &     1.73e+04     \\\\\n",
       "\\textbf{7116.0}           &    9950.6944  &     3458.392     &     2.877  &         0.004        &     3171.720    &     1.67e+04     \\\\\n",
       "\\textbf{7117.0}           &    9962.5160  &     4171.308     &     2.388  &         0.017        &     1786.117    &     1.81e+04     \\\\\n",
       "\\textbf{7121.0}           &    8806.0934  &     3370.809     &     2.612  &         0.009        &     2198.794    &     1.54e+04     \\\\\n",
       "\\textbf{7127.0}           &    5683.8420  &     3400.609     &     1.671  &         0.095        &     -981.868    &     1.23e+04     \\\\\n",
       "\\textbf{7139.0}           &    9692.1878  &     3477.874     &     2.787  &         0.005        &     2875.026    &     1.65e+04     \\\\\n",
       "\\textbf{7146.0}           &    1.012e+04  &     3465.747     &     2.919  &         0.004        &     3323.612    &     1.69e+04     \\\\\n",
       "\\textbf{7163.0}           &     1.31e+04  &     3467.196     &     3.777  &         0.000        &     6299.856    &     1.99e+04     \\\\\n",
       "\\textbf{7180.0}           &    7580.6776  &     3426.253     &     2.213  &         0.027        &      864.700    &     1.43e+04     \\\\\n",
       "\\textbf{7183.0}           &    9122.0661  &     3445.881     &     2.647  &         0.008        &     2367.616    &     1.59e+04     \\\\\n",
       "\\textbf{7228.0}           &    1.691e+04  &     3336.789     &     5.067  &         0.000        &     1.04e+04    &     2.34e+04     \\\\\n",
       "\\textbf{7232.0}           &    6459.3309  &     4154.287     &     1.555  &         0.120        &    -1683.704    &     1.46e+04     \\\\\n",
       "\\textbf{7250.0}           &    5903.5751  &     3433.908     &     1.719  &         0.086        &     -827.408    &     1.26e+04     \\\\\n",
       "\\textbf{7257.0}           &    2.454e+04  &     3288.256     &     7.464  &         0.000        &     1.81e+04    &      3.1e+04     \\\\\n",
       "\\textbf{7260.0}           &    9960.5244  &     3417.721     &     2.914  &         0.004        &     3261.272    &     1.67e+04     \\\\\n",
       "\\textbf{7267.0}           &    8789.4484  &     3543.555     &     2.480  &         0.013        &     1843.541    &     1.57e+04     \\\\\n",
       "\\textbf{7268.0}           &   -4864.0786  &     3160.501     &    -1.539  &         0.124        &    -1.11e+04    &     1330.984     \\\\\n",
       "\\textbf{7281.0}           &    1.024e+04  &     4245.639     &     2.411  &         0.016        &     1915.827    &     1.86e+04     \\\\\n",
       "\\textbf{7291.0}           &    9738.4799  &     3446.044     &     2.826  &         0.005        &     2983.710    &     1.65e+04     \\\\\n",
       "\\textbf{7343.0}           &    2811.3153  &     3688.762     &     0.762  &         0.446        &    -4419.220    &        1e+04     \\\\\n",
       "\\textbf{7346.0}           &     510.6111  &     2979.289     &     0.171  &         0.864        &    -5329.248    &     6350.470     \\\\\n",
       "\\textbf{7401.0}           &    1.037e+04  &     3532.412     &     2.935  &         0.003        &     3442.141    &     1.73e+04     \\\\\n",
       "\\textbf{7409.0}           &    9957.6138  &     3452.343     &     2.884  &         0.004        &     3190.497    &     1.67e+04     \\\\\n",
       "\\textbf{7420.0}           &    7030.1192  &     3222.589     &     2.182  &         0.029        &      713.354    &     1.33e+04     \\\\\n",
       "\\textbf{7435.0}           &    9960.7449  &     3257.812     &     3.057  &         0.002        &     3574.937    &     1.63e+04     \\\\\n",
       "\\textbf{7466.0}           &    8121.3534  &     3464.126     &     2.344  &         0.019        &     1331.140    &     1.49e+04     \\\\\n",
       "\\textbf{7486.0}           &   -3690.8571  &     3031.916     &    -1.217  &         0.223        &    -9633.873    &     2252.159     \\\\\n",
       "\\textbf{7503.0}           &    9995.7891  &     4813.311     &     2.077  &         0.038        &      560.967    &     1.94e+04     \\\\\n",
       "\\textbf{7506.0}           &    1.114e+04  &     3408.724     &     3.269  &         0.001        &     4461.736    &     1.78e+04     \\\\\n",
       "\\textbf{7537.0}           &    9056.8261  &     3396.774     &     2.666  &         0.008        &     2398.632    &     1.57e+04     \\\\\n",
       "\\textbf{7549.0}           &    9421.4226  &     3438.414     &     2.740  &         0.006        &     2681.608    &     1.62e+04     \\\\\n",
       "\\textbf{7554.0}           &    9079.1061  &     3444.815     &     2.636  &         0.008        &     2326.745    &     1.58e+04     \\\\\n",
       "\\textbf{7557.0}           &    9586.5450  &     3506.696     &     2.734  &         0.006        &     2712.887    &     1.65e+04     \\\\\n",
       "\\textbf{7585.0}           &   -1.767e+04  &     3412.507     &    -5.179  &         0.000        &    -2.44e+04    &     -1.1e+04     \\\\\n",
       "\\textbf{7602.0}           &    8879.0584  &     3347.634     &     2.652  &         0.008        &     2317.187    &     1.54e+04     \\\\\n",
       "\\textbf{7620.0}           &    4824.0196  &     3317.870     &     1.454  &         0.146        &    -1679.510    &     1.13e+04     \\\\\n",
       "\\textbf{7636.0}           &    9195.8774  &     3376.389     &     2.724  &         0.006        &     2577.642    &     1.58e+04     \\\\\n",
       "\\textbf{7646.0}           &    9544.3596  &     3443.343     &     2.772  &         0.006        &     2794.883    &     1.63e+04     \\\\\n",
       "\\textbf{7658.0}           &    8846.6138  &     3378.292     &     2.619  &         0.009        &     2224.648    &     1.55e+04     \\\\\n",
       "\\textbf{7683.0}           &    1.075e+04  &     3675.409     &     2.925  &         0.003        &     3546.037    &      1.8e+04     \\\\\n",
       "\\textbf{7685.0}           &    9783.4865  &     3575.982     &     2.736  &         0.006        &     2774.018    &     1.68e+04     \\\\\n",
       "\\textbf{7692.0}           &    5094.7382  &     3102.002     &     1.642  &         0.101        &     -985.659    &     1.12e+04     \\\\\n",
       "\\textbf{7762.0}           &    1.038e+04  &     3464.947     &     2.995  &         0.003        &     3586.497    &     1.72e+04     \\\\\n",
       "\\textbf{7772.0}           &   -1371.3064  &     3005.129     &    -0.456  &         0.648        &    -7261.817    &     4519.205     \\\\\n",
       "\\textbf{7773.0}           &    9554.4855  &     3463.000     &     2.759  &         0.006        &     2766.478    &     1.63e+04     \\\\\n",
       "\\textbf{7777.0}           &    6152.5489  &     3204.358     &     1.920  &         0.055        &     -128.480    &     1.24e+04     \\\\\n",
       "\\textbf{7835.0}           &    1.054e+04  &     3518.681     &     2.996  &         0.003        &     3643.560    &     1.74e+04     \\\\\n",
       "\\textbf{7873.0}           &   -2813.4654  &     2999.335     &    -0.938  &         0.348        &    -8692.619    &     3065.688     \\\\\n",
       "\\textbf{7883.0}           &    9932.7429  &     3433.142     &     2.893  &         0.004        &     3203.262    &     1.67e+04     \\\\\n",
       "\\textbf{7904.0}           &    9043.3757  &     3486.366     &     2.594  &         0.009        &     2209.568    &     1.59e+04     \\\\\n",
       "\\textbf{7906.0}           &    1.253e+04  &     3521.991     &     3.558  &         0.000        &     5627.449    &     1.94e+04     \\\\\n",
       "\\textbf{7921.0}           &    1.079e+04  &     3466.265     &     3.112  &         0.002        &     3993.016    &     1.76e+04     \\\\\n",
       "\\textbf{7923.0}           &    8674.4178  &     3539.688     &     2.451  &         0.014        &     1736.090    &     1.56e+04     \\\\\n",
       "\\textbf{7935.0}           &    7444.2116  &     3273.992     &     2.274  &         0.023        &     1026.689    &     1.39e+04     \\\\\n",
       "\\textbf{7938.0}           &    8845.2456  &     3467.407     &     2.551  &         0.011        &     2048.600    &     1.56e+04     \\\\\n",
       "\\textbf{7985.0}           &   -1.091e+04  &     3104.873     &    -3.515  &         0.000        &     -1.7e+04    &    -4828.819     \\\\\n",
       "\\textbf{8014.0}           &    7773.1550  &     3358.207     &     2.315  &         0.021        &     1190.559    &     1.44e+04     \\\\\n",
       "\\textbf{8030.0}           &    1.024e+04  &     3432.790     &     2.983  &         0.003        &     3510.766    &      1.7e+04     \\\\\n",
       "\\textbf{8046.0}           &   -2361.1775  &     3083.758     &    -0.766  &         0.444        &    -8405.812    &     3683.457     \\\\\n",
       "\\textbf{8047.0}           &    9758.5162  &     3946.299     &     2.473  &         0.013        &     2023.169    &     1.75e+04     \\\\\n",
       "\\textbf{8062.0}           &    8420.4733  &     3407.990     &     2.471  &         0.013        &     1740.294    &     1.51e+04     \\\\\n",
       "\\textbf{8068.0}           &   -5735.8824  &     3541.043     &    -1.620  &         0.105        &    -1.27e+04    &     1205.100     \\\\\n",
       "\\textbf{8087.0}           &   -6967.8963  &     3297.326     &    -2.113  &         0.035        &    -1.34e+04    &     -504.635     \\\\\n",
       "\\textbf{8095.0}           &    1.036e+04  &     3503.810     &     2.956  &         0.003        &     3490.766    &     1.72e+04     \\\\\n",
       "\\textbf{8096.0}           &    9442.4911  &     3396.980     &     2.780  &         0.005        &     2783.893    &     1.61e+04     \\\\\n",
       "\\textbf{8109.0}           &    1.031e+04  &     3485.190     &     2.957  &         0.003        &     3474.529    &     1.71e+04     \\\\\n",
       "\\textbf{8123.0}           &    9497.2222  &     3457.064     &     2.747  &         0.006        &     2720.850    &     1.63e+04     \\\\\n",
       "\\textbf{8150.0}           &    1.035e+04  &     3499.858     &     2.956  &         0.003        &     3486.945    &     1.72e+04     \\\\\n",
       "\\textbf{8163.0}           &     1.04e+04  &     3545.844     &     2.934  &         0.003        &     3451.826    &     1.74e+04     \\\\\n",
       "\\textbf{8176.0}           &    1380.9099  &     3340.324     &     0.413  &         0.679        &    -5166.633    &     7928.453     \\\\\n",
       "\\textbf{8202.0}           &    8928.8530  &     3496.105     &     2.554  &         0.011        &     2075.956    &     1.58e+04     \\\\\n",
       "\\textbf{8214.0}           &    7951.0174  &     3472.900     &     2.289  &         0.022        &     1143.605    &     1.48e+04     \\\\\n",
       "\\textbf{8215.0}           &    6869.4339  &     3533.851     &     1.944  &         0.052        &      -57.451    &     1.38e+04     \\\\\n",
       "\\textbf{8219.0}           &     1.01e+04  &     3521.286     &     2.868  &         0.004        &     3195.379    &      1.7e+04     \\\\\n",
       "\\textbf{8247.0}           &    7517.0591  &     3412.455     &     2.203  &         0.028        &      828.128    &     1.42e+04     \\\\\n",
       "\\textbf{8253.0}           &   -5960.1192  &     3082.870     &    -1.933  &         0.053        &     -1.2e+04    &       82.775     \\\\\n",
       "\\textbf{8290.0}           &    7523.9306  &     3581.453     &     2.101  &         0.036        &      503.737    &     1.45e+04     \\\\\n",
       "\\textbf{8293.0}           &    1.049e+04  &     3497.907     &     2.999  &         0.003        &     3632.477    &     1.73e+04     \\\\\n",
       "\\textbf{8304.0}           &    1.052e+04  &     3404.658     &     3.090  &         0.002        &     3846.046    &     1.72e+04     \\\\\n",
       "\\textbf{8334.0}           &    8459.9005  &     3390.789     &     2.495  &         0.013        &     1813.438    &     1.51e+04     \\\\\n",
       "\\textbf{8348.0}           &    1.033e+04  &     3541.616     &     2.918  &         0.004        &     3392.106    &     1.73e+04     \\\\\n",
       "\\textbf{8357.0}           &    9916.7234  &     3441.737     &     2.881  &         0.004        &     3170.395    &     1.67e+04     \\\\\n",
       "\\textbf{8358.0}           &    8999.4511  &     3440.726     &     2.616  &         0.009        &     2255.105    &     1.57e+04     \\\\\n",
       "\\textbf{8446.0}           &   -1264.4703  &     3768.625     &    -0.336  &         0.737        &    -8651.548    &     6122.607     \\\\\n",
       "\\textbf{8460.0}           &    1.089e+04  &     3837.834     &     2.838  &         0.005        &     3368.395    &     1.84e+04     \\\\\n",
       "\\textbf{8463.0}           &    9275.8747  &     3429.308     &     2.705  &         0.007        &     2553.909    &      1.6e+04     \\\\\n",
       "\\textbf{8479.0}           &    1.002e+04  &     4084.448     &     2.454  &         0.014        &     2015.762    &      1.8e+04     \\\\\n",
       "\\textbf{8530.0}           &     1.51e+04  &     3389.501     &     4.455  &         0.000        &     8454.778    &     2.17e+04     \\\\\n",
       "\\textbf{8536.0}           &    7703.1720  &     3360.553     &     2.292  &         0.022        &     1115.978    &     1.43e+04     \\\\\n",
       "\\textbf{8543.0}           &    2.968e+04  &     3685.258     &     8.053  &         0.000        &     2.25e+04    &     3.69e+04     \\\\\n",
       "\\textbf{8549.0}           &    -314.6492  &     3224.852     &    -0.098  &         0.922        &    -6635.849    &     6006.551     \\\\\n",
       "\\textbf{8551.0}           &    9873.1363  &     3485.903     &     2.832  &         0.005        &     3040.235    &     1.67e+04     \\\\\n",
       "\\textbf{8559.0}           &    4491.5178  &     3212.952     &     1.398  &         0.162        &    -1806.358    &     1.08e+04     \\\\\n",
       "\\textbf{8573.0}           &   -3760.3739  &     3245.720     &    -1.159  &         0.247        &    -1.01e+04    &     2601.732     \\\\\n",
       "\\textbf{8606.0}           &    1.163e+04  &     3484.800     &     3.338  &         0.001        &     4802.881    &     1.85e+04     \\\\\n",
       "\\textbf{8607.0}           &    9805.6659  &     3477.735     &     2.820  &         0.005        &     2988.777    &     1.66e+04     \\\\\n",
       "\\textbf{8648.0}           &    9658.8439  &     3419.758     &     2.824  &         0.005        &     2955.598    &     1.64e+04     \\\\\n",
       "\\textbf{8657.0}           &    2916.5634  &     3074.140     &     0.949  &         0.343        &    -3109.219    &     8942.345     \\\\\n",
       "\\textbf{8675.0}           &    8484.9073  &     4577.829     &     1.853  &         0.064        &     -488.333    &     1.75e+04     \\\\\n",
       "\\textbf{8681.0}           &    5340.5429  &     3103.645     &     1.721  &         0.085        &     -743.073    &     1.14e+04     \\\\\n",
       "\\textbf{8687.0}           &    7564.8392  &     3485.445     &     2.170  &         0.030        &      732.837    &     1.44e+04     \\\\\n",
       "\\textbf{8692.0}           &    8275.6065  &     3374.689     &     2.452  &         0.014        &     1660.703    &     1.49e+04     \\\\\n",
       "\\textbf{8699.0}           &    1.015e+04  &     3469.228     &     2.926  &         0.003        &     3350.051    &      1.7e+04     \\\\\n",
       "\\textbf{8717.0}           &    1.034e+04  &     3480.922     &     2.970  &         0.003        &     3515.999    &     1.72e+04     \\\\\n",
       "\\textbf{8759.0}           &    3195.8327  &     3002.715     &     1.064  &         0.287        &    -2689.946    &     9081.612     \\\\\n",
       "\\textbf{8762.0}           &    9229.2761  &     3211.469     &     2.874  &         0.004        &     2934.309    &     1.55e+04     \\\\\n",
       "\\textbf{8819.0}           &    1.045e+04  &     3554.911     &     2.941  &         0.003        &     3485.338    &     1.74e+04     \\\\\n",
       "\\textbf{8850.0}           &    1.006e+04  &     3470.331     &     2.900  &         0.004        &     3260.321    &     1.69e+04     \\\\\n",
       "\\textbf{8852.0}           &    1.037e+04  &     3514.880     &     2.952  &         0.003        &     3485.138    &     1.73e+04     \\\\\n",
       "\\textbf{8859.0}           &    9504.3144  &     3432.430     &     2.769  &         0.006        &     2776.229    &     1.62e+04     \\\\\n",
       "\\textbf{8867.0}           &    2804.6769  &     3365.235     &     0.833  &         0.405        &    -3791.695    &     9401.049     \\\\\n",
       "\\textbf{8881.0}           &    8810.4771  &     3373.327     &     2.612  &         0.009        &     2198.244    &     1.54e+04     \\\\\n",
       "\\textbf{8958.0}           &    6294.1180  &     3147.355     &     2.000  &         0.046        &      124.823    &     1.25e+04     \\\\\n",
       "\\textbf{8972.0}           &   -1.304e+04  &     3292.688     &    -3.960  &         0.000        &    -1.95e+04    &    -6584.568     \\\\\n",
       "\\textbf{8990.0}           &   -3233.5185  &     3122.100     &    -1.036  &         0.300        &    -9353.310    &     2886.273     \\\\\n",
       "\\textbf{9004.0}           &    1.028e+04  &     3671.630     &     2.799  &         0.005        &     3078.424    &     1.75e+04     \\\\\n",
       "\\textbf{9016.0}           &    7909.5405  &     3260.736     &     2.426  &         0.015        &     1518.003    &     1.43e+04     \\\\\n",
       "\\textbf{9048.0}           &    8117.5466  &     3282.531     &     2.473  &         0.013        &     1683.285    &     1.46e+04     \\\\\n",
       "\\textbf{9051.0}           &    -285.8187  &     3339.687     &    -0.086  &         0.932        &    -6832.114    &     6260.476     \\\\\n",
       "\\textbf{9071.0}           &    1.047e+04  &     3494.598     &     2.995  &         0.003        &     3616.763    &     1.73e+04     \\\\\n",
       "\\textbf{9112.0}           &    5268.4764  &     3107.901     &     1.695  &         0.090        &     -823.483    &     1.14e+04     \\\\\n",
       "\\textbf{9114.0}           &    6528.6201  &     3498.644     &     1.866  &         0.062        &     -329.255    &     1.34e+04     \\\\\n",
       "\\textbf{9132.0}           &    7679.5102  &     4515.180     &     1.701  &         0.089        &    -1170.930    &     1.65e+04     \\\\\n",
       "\\textbf{9173.0}           &    9871.3241  &     3749.193     &     2.633  &         0.008        &     2522.335    &     1.72e+04     \\\\\n",
       "\\textbf{9180.0}           &    1.023e+04  &     3560.674     &     2.873  &         0.004        &     3251.695    &     1.72e+04     \\\\\n",
       "\\textbf{9186.0}           &    9579.4763  &     3435.987     &     2.788  &         0.005        &     2844.420    &     1.63e+04     \\\\\n",
       "\\textbf{9191.0}           &    7809.2449  &     4335.647     &     1.801  &         0.072        &     -689.283    &     1.63e+04     \\\\\n",
       "\\textbf{9216.0}           &    5465.3263  &     3093.356     &     1.767  &         0.077        &     -598.121    &     1.15e+04     \\\\\n",
       "\\textbf{9217.0}           &    1777.0444  &     3009.252     &     0.591  &         0.555        &    -4121.547    &     7675.635     \\\\\n",
       "\\textbf{9225.0}           &    1.071e+04  &     3503.029     &     3.058  &         0.002        &     3845.352    &     1.76e+04     \\\\\n",
       "\\textbf{9230.0}           &    9410.1365  &     3829.849     &     2.457  &         0.014        &     1903.050    &     1.69e+04     \\\\\n",
       "\\textbf{9259.0}           &    1.043e+04  &     3496.754     &     2.983  &         0.003        &     3576.686    &     1.73e+04     \\\\\n",
       "\\textbf{9293.0}           &    1.034e+04  &     3492.063     &     2.960  &         0.003        &     3491.788    &     1.72e+04     \\\\\n",
       "\\textbf{9299.0}           &    6897.8306  &     3506.720     &     1.967  &         0.049        &       24.126    &     1.38e+04     \\\\\n",
       "\\textbf{9308.0}           &    1741.3332  &     3024.635     &     0.576  &         0.565        &    -4187.411    &     7670.078     \\\\\n",
       "\\textbf{9311.0}           &    5296.9888  &     4107.271     &     1.290  &         0.197        &    -2753.887    &     1.33e+04     \\\\\n",
       "\\textbf{9313.0}           &    3977.4922  &     3104.734     &     1.281  &         0.200        &    -2108.259    &     1.01e+04     \\\\\n",
       "\\textbf{9325.0}           &    9578.9967  &     3442.888     &     2.782  &         0.005        &     2830.413    &     1.63e+04     \\\\\n",
       "\\textbf{9332.0}           &    1.001e+04  &     3470.784     &     2.883  &         0.004        &     3203.517    &     1.68e+04     \\\\\n",
       "\\textbf{9340.0}           &   -2.037e+04  &     5011.621     &    -4.064  &         0.000        &    -3.02e+04    &    -1.05e+04     \\\\\n",
       "\\textbf{9372.0}           &    9148.2969  &     3621.080     &     2.526  &         0.012        &     2050.429    &     1.62e+04     \\\\\n",
       "\\textbf{9411.0}           &    8088.5107  &     3524.109     &     2.295  &         0.022        &     1180.721    &      1.5e+04     \\\\\n",
       "\\textbf{9459.0}           &    -666.6739  &     3356.430     &    -0.199  &         0.843        &    -7245.788    &     5912.440     \\\\\n",
       "\\textbf{9465.0}           &    1.471e+04  &     3419.252     &     4.303  &         0.000        &     8010.932    &     2.14e+04     \\\\\n",
       "\\textbf{9472.0}           &    3955.3257  &     3021.325     &     1.309  &         0.191        &    -1966.930    &     9877.582     \\\\\n",
       "\\textbf{9483.0}           &   -4488.5082  &     3083.214     &    -1.456  &         0.145        &    -1.05e+04    &     1555.060     \\\\\n",
       "\\textbf{9563.0}           &   -1.564e+04  &     4431.982     &    -3.530  &         0.000        &    -2.43e+04    &    -6956.363     \\\\\n",
       "\\textbf{9590.0}           &    4752.8554  &     3056.595     &     1.555  &         0.120        &    -1238.535    &     1.07e+04     \\\\\n",
       "\\textbf{9598.0}           &    2876.1361  &     3734.665     &     0.770  &         0.441        &    -4444.375    &     1.02e+04     \\\\\n",
       "\\textbf{9599.0}           &    1309.9612  &     2948.034     &     0.444  &         0.657        &    -4468.634    &     7088.556     \\\\\n",
       "\\textbf{9602.0}           &    2279.3103  &     4077.970     &     0.559  &         0.576        &    -5714.132    &     1.03e+04     \\\\\n",
       "\\textbf{9619.0}           &    8879.0387  &     3376.513     &     2.630  &         0.009        &     2260.559    &     1.55e+04     \\\\\n",
       "\\textbf{9643.0}           &    1.012e+04  &     3558.028     &     2.844  &         0.004        &     3146.293    &     1.71e+04     \\\\\n",
       "\\textbf{9650.0}           &    1.011e+04  &     3465.651     &     2.917  &         0.004        &     3316.603    &     1.69e+04     \\\\\n",
       "\\textbf{9653.0}           &   -6037.0683  &     5294.152     &    -1.140  &         0.254        &    -1.64e+04    &     4340.275     \\\\\n",
       "\\textbf{9667.0}           &    9385.3308  &     3422.969     &     2.742  &         0.006        &     2675.790    &     1.61e+04     \\\\\n",
       "\\textbf{9698.0}           &    7457.0200  &     3229.332     &     2.309  &         0.021        &     1127.039    &     1.38e+04     \\\\\n",
       "\\textbf{9699.0}           &    9848.2959  &     3312.269     &     2.973  &         0.003        &     3355.744    &     1.63e+04     \\\\\n",
       "\\textbf{9719.0}           &     768.4108  &     2945.395     &     0.261  &         0.794        &    -5005.011    &     6541.833     \\\\\n",
       "\\textbf{9742.0}           &   -2580.5961  &     3079.909     &    -0.838  &         0.402        &    -8617.686    &     3456.494     \\\\\n",
       "\\textbf{9761.0}           &    1.046e+04  &     3530.077     &     2.963  &         0.003        &     3539.740    &     1.74e+04     \\\\\n",
       "\\textbf{9771.0}           &    3049.1857  &     3017.146     &     1.011  &         0.312        &    -2864.879    &     8963.251     \\\\\n",
       "\\textbf{9772.0}           &    9577.3307  &     3397.420     &     2.819  &         0.005        &     2917.871    &     1.62e+04     \\\\\n",
       "\\textbf{9778.0}           &    8834.8250  &     3321.328     &     2.660  &         0.008        &     2324.517    &     1.53e+04     \\\\\n",
       "\\textbf{9799.0}           &    2300.8569  &     3111.941     &     0.739  &         0.460        &    -3799.021    &     8400.734     \\\\\n",
       "\\textbf{9815.0}           &    1.041e+04  &     3600.344     &     2.891  &         0.004        &     3349.939    &     1.75e+04     \\\\\n",
       "\\textbf{9818.0}           &   -3.633e+04  &     3152.499     &   -11.524  &         0.000        &    -4.25e+04    &    -3.02e+04     \\\\\n",
       "\\textbf{9837.0}           &    1.034e+04  &     3554.223     &     2.908  &         0.004        &     3370.486    &     1.73e+04     \\\\\n",
       "\\textbf{9922.0}           &    1086.6389  &     2947.216     &     0.369  &         0.712        &    -4690.353    &     6863.631     \\\\\n",
       "\\textbf{9954.0}           &    9757.9764  &     3958.884     &     2.465  &         0.014        &     1997.961    &     1.75e+04     \\\\\n",
       "\\textbf{9963.0}           &    9335.3174  &     3515.814     &     2.655  &         0.008        &     2443.787    &     1.62e+04     \\\\\n",
       "\\textbf{9988.0}           &    9912.8554  &     3503.871     &     2.829  &         0.005        &     3044.736    &     1.68e+04     \\\\\n",
       "\\textbf{gspillsicIVX1981} &      -0.0427  &        0.122     &    -0.349  &         0.727        &       -0.283    &        0.197     \\\\\n",
       "\\textbf{gspillsicIVX1982} &      -0.0526  &        0.121     &    -0.436  &         0.663        &       -0.289    &        0.184     \\\\\n",
       "\\textbf{gspillsicIVX1983} &      -0.0757  &        0.119     &    -0.638  &         0.523        &       -0.308    &        0.157     \\\\\n",
       "\\textbf{gspillsicIVX1984} &      -0.1432  &        0.117     &    -1.222  &         0.222        &       -0.373    &        0.087     \\\\\n",
       "\\textbf{gspillsicIVX1985} &      -0.1742  &        0.117     &    -1.488  &         0.137        &       -0.404    &        0.055     \\\\\n",
       "\\textbf{gspillsicIVX1986} &      -0.2309  &        0.117     &    -1.976  &         0.048        &       -0.460    &       -0.002     \\\\\n",
       "\\textbf{gspillsicIVX1987} &      -0.2508  &        0.117     &    -2.143  &         0.032        &       -0.480    &       -0.021     \\\\\n",
       "\\textbf{gspillsicIVX1988} &      -0.2858  &        0.117     &    -2.434  &         0.015        &       -0.516    &       -0.056     \\\\\n",
       "\\textbf{gspillsicIVX1989} &      -0.2845  &        0.118     &    -2.410  &         0.016        &       -0.516    &       -0.053     \\\\\n",
       "\\textbf{gspillsicIVX1990} &      -0.3120  &        0.119     &    -2.624  &         0.009        &       -0.545    &       -0.079     \\\\\n",
       "\\textbf{gspillsicIVX1991} &      -0.2929  &        0.120     &    -2.445  &         0.014        &       -0.528    &       -0.058     \\\\\n",
       "\\textbf{gspillsicIVX1992} &      -0.3439  &        0.121     &    -2.850  &         0.004        &       -0.580    &       -0.107     \\\\\n",
       "\\textbf{gspillsicIVX1993} &      -0.3505  &        0.122     &    -2.873  &         0.004        &       -0.590    &       -0.111     \\\\\n",
       "\\textbf{gspillsicIVX1994} &      -0.3584  &        0.123     &    -2.913  &         0.004        &       -0.600    &       -0.117     \\\\\n",
       "\\textbf{gspillsicIVX1995} &      -0.3304  &        0.125     &    -2.644  &         0.008        &       -0.575    &       -0.085     \\\\\n",
       "\\textbf{gspillsicIVX1996} &      -0.3542  &        0.127     &    -2.785  &         0.005        &       -0.603    &       -0.105     \\\\\n",
       "\\textbf{gspillsicIVX1997} &      -0.3330  &        0.130     &    -2.565  &         0.010        &       -0.587    &       -0.078     \\\\\n",
       "\\textbf{gspillsicIVX1998} &      -0.2876  &        0.133     &    -2.170  &         0.030        &       -0.547    &       -0.028     \\\\\n",
       "\\textbf{gspillsicIVX1999} &      -0.2297  &        0.135     &    -1.700  &         0.089        &       -0.495    &        0.035     \\\\\n",
       "\\textbf{gspillsicIVX2000} &      -0.2065  &        0.137     &    -1.504  &         0.133        &       -0.476    &        0.063     \\\\\n",
       "\\textbf{gspillsicIVX2001} &      -0.4205  &        0.139     &    -3.027  &         0.002        &       -0.693    &       -0.148     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 21719.318 & \\textbf{  Durbin-Watson:     } &      0.545    \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 50384141.785  \\\\\n",
       "\\textbf{Skew:}          &   10.195  & \\textbf{  Prob(JB):          } &       0.00    \\\\\n",
       "\\textbf{Kurtosis:}      &  302.876  & \\textbf{  Cond. No.          } &   9.49e+06    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 9.49e+06. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 rmkvaf   R-squared:                       0.666\n",
       "Model:                            OLS   Adj. R-squared:                  0.646\n",
       "Method:                 Least Squares   F-statistic:                     32.48\n",
       "Date:                Wed, 16 Oct 2024   Prob (F-statistic):               0.00\n",
       "Time:                        02:03:03   Log-Likelihood:            -1.4155e+05\n",
       "No. Observations:               13385   AIC:                         2.847e+05\n",
       "Df Residuals:                   12609   BIC:                         2.905e+05\n",
       "Df Model:                         775                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================\n",
       "                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const            -1.059e+04   2858.143     -3.705      0.000   -1.62e+04   -4986.834\n",
       "gspillsicIV          0.8770      0.199      4.412      0.000       0.487       1.267\n",
       "pat_count          -29.6820      1.861    -15.948      0.000     -33.330     -26.034\n",
       "rsales               0.7906      0.037     21.566      0.000       0.719       0.862\n",
       "rppent               0.6416      0.084      7.603      0.000       0.476       0.807\n",
       "emp                 12.1477      7.031      1.728      0.084      -1.633      25.929\n",
       "rxrd                18.7054      0.614     30.479      0.000      17.502      19.908\n",
       "1981              -323.9459    749.124     -0.432      0.665   -1792.342    1144.450\n",
       "1982              -175.3555    747.071     -0.235      0.814   -1639.729    1289.018\n",
       "1983              -106.8254    738.681     -0.145      0.885   -1554.753    1341.102\n",
       "1984              -358.0670    734.939     -0.487      0.626   -1798.660    1082.526\n",
       "1985              -339.9200    734.792     -0.463      0.644   -1780.223    1100.383\n",
       "1986              -333.0589    729.764     -0.456      0.648   -1763.507    1097.389\n",
       "1987              -418.4831    726.215     -0.576      0.564   -1841.975    1005.009\n",
       "1988              -537.8235    725.635     -0.741      0.459   -1960.179     884.531\n",
       "1989              -394.3985    723.074     -0.545      0.585   -1811.734    1022.937\n",
       "1990              -723.8705    719.942     -1.005      0.315   -2135.066     687.325\n",
       "1991              -405.2199    718.078     -0.564      0.573   -1812.763    1002.323\n",
       "1992              -200.8668    716.324     -0.280      0.779   -1604.970    1203.236\n",
       "1993               -60.0251    714.587     -0.084      0.933   -1460.725    1340.675\n",
       "1994              -213.6207    716.211     -0.298      0.766   -1617.503    1190.261\n",
       "1995               162.8964    716.020      0.228      0.820   -1240.612    1566.405\n",
       "1996               673.3490    718.687      0.937      0.349    -735.387    2082.085\n",
       "1997               981.4864    721.476      1.360      0.174    -432.716    2395.689\n",
       "1998               793.8169    725.956      1.093      0.274    -629.167    2216.801\n",
       "1999              1042.6699    732.548      1.423      0.155    -393.235    2478.575\n",
       "2000               473.6464    750.844      0.631      0.528    -998.123    1945.415\n",
       "2001               461.9434    779.875      0.592      0.554   -1066.731    1990.618\n",
       "10005.0           9659.3047   3429.640      2.816      0.005    2936.688    1.64e+04\n",
       "10006.0           9654.2887   3848.511      2.509      0.012    2110.621    1.72e+04\n",
       "10008.0           9065.5709   3369.549      2.690      0.007    2460.743    1.57e+04\n",
       "10016.0           1.032e+04   3471.639      2.971      0.003    3510.375    1.71e+04\n",
       "10030.0           1.032e+04   3484.519      2.962      0.003    3490.881    1.72e+04\n",
       "1004.0             1.04e+04   3516.374      2.956      0.003    3502.541    1.73e+04\n",
       "10056.0           8114.8321   3313.170      2.449      0.014    1620.514    1.46e+04\n",
       "10085.0           2996.3025   2984.784      1.004      0.315   -2854.329    8846.934\n",
       "10092.0               1e+04   5579.277      1.793      0.073    -932.045    2.09e+04\n",
       "10097.0           2913.9955   3124.866      0.933      0.351   -3211.217    9039.208\n",
       "1010.0             1.01e+04   5629.803      1.794      0.073    -935.137    2.11e+04\n",
       "10109.0           1.061e+04   3495.834      3.035      0.002    3757.360    1.75e+04\n",
       "10115.0           7108.6747   3121.346      2.277      0.023     990.362    1.32e+04\n",
       "10124.0           1.071e+04   3505.368      3.055      0.002    3837.145    1.76e+04\n",
       "1013.0            5204.0952   3088.093      1.685      0.092    -849.037    1.13e+04\n",
       "10150.0           3126.6112   3528.442      0.886      0.376   -3789.672       1e+04\n",
       "10159.0          -3942.8076   4398.018     -0.896      0.370   -1.26e+04    4677.977\n",
       "10174.0           1.059e+04   3728.330      2.840      0.005    3281.435    1.79e+04\n",
       "10185.0           6933.6784   3401.405      2.038      0.042     266.407    1.36e+04\n",
       "10195.0          -2419.7055   2991.651     -0.809      0.419   -8283.797    3444.386\n",
       "10198.0            1.04e+04   3490.406      2.980      0.003    3559.496    1.72e+04\n",
       "10215.0           1.047e+04   3502.877      2.989      0.003    3603.939    1.73e+04\n",
       "10232.0           3412.4149   3139.375      1.087      0.277   -2741.238    9566.068\n",
       "10236.0           9729.1672   3425.098      2.841      0.005    3015.455    1.64e+04\n",
       "10286.0           9948.2372   3494.179      2.847      0.004    3099.114    1.68e+04\n",
       "10301.0          -1.568e+04   3092.129     -5.072      0.000   -2.17e+04   -9623.073\n",
       "10312.0           1.037e+04   3515.620      2.951      0.003    3483.546    1.73e+04\n",
       "10332.0            1.05e+04   4320.420      2.430      0.015    2028.470     1.9e+04\n",
       "1036.0            6315.6838   3359.575      1.880      0.060    -269.594    1.29e+04\n",
       "10374.0           8620.1908   3326.131      2.592      0.010    2100.469    1.51e+04\n",
       "10386.0           6270.3101   3177.083      1.974      0.048      42.745    1.25e+04\n",
       "10391.0           3673.5113   3158.308      1.163      0.245   -2517.254    9864.276\n",
       "10407.0           3552.8502   2995.765      1.186      0.236   -2319.306    9425.006\n",
       "10420.0           7609.5888   3141.904      2.422      0.015    1450.979    1.38e+04\n",
       "10422.0           7032.1079   3412.655      2.061      0.039     342.784    1.37e+04\n",
       "10426.0           8969.2563   3539.383      2.534      0.011    2031.527    1.59e+04\n",
       "10441.0            1.01e+04   3465.889      2.913      0.004    3301.710    1.69e+04\n",
       "1045.0             263.6677   3653.113      0.072      0.942   -6896.990    7424.325\n",
       "10453.0           6252.1220   3193.893      1.958      0.050      -8.395    1.25e+04\n",
       "10482.0          -1.574e+04   3499.420     -4.499      0.000   -2.26e+04   -8883.519\n",
       "10498.0           9516.6376   3463.766      2.747      0.006    2727.130    1.63e+04\n",
       "10499.0           1905.7296   3036.020      0.628      0.530   -4045.331    7856.790\n",
       "10511.0           1.036e+04   3584.248      2.890      0.004    3331.750    1.74e+04\n",
       "10519.0          -6995.2218   2994.130     -2.336      0.019   -1.29e+04   -1126.271\n",
       "10530.0           7992.8778   3302.808      2.420      0.016    1518.871    1.45e+04\n",
       "10537.0           7966.2215   3612.886      2.205      0.027     884.416     1.5e+04\n",
       "10540.0           8942.0924   3329.146      2.686      0.007    2416.459    1.55e+04\n",
       "10541.0            1.02e+04   3607.567      2.826      0.005    3125.335    1.73e+04\n",
       "10550.0           7172.0022   6120.797      1.172      0.241   -4825.691    1.92e+04\n",
       "10553.0           4910.8820   3309.233      1.484      0.138   -1575.719    1.14e+04\n",
       "10565.0           1.095e+04   3502.584      3.125      0.002    4080.237    1.78e+04\n",
       "10580.0           1.074e+04   3496.131      3.071      0.002    3883.162    1.76e+04\n",
       "10581.0           8286.5040   3446.834      2.404      0.016    1530.184     1.5e+04\n",
       "10588.0           -175.8462   2944.608     -0.060      0.952   -5947.726    5596.034\n",
       "10597.0           9656.1515   3467.223      2.785      0.005    2859.867    1.65e+04\n",
       "10599.0           1.023e+04   3528.354      2.900      0.004    3315.237    1.71e+04\n",
       "10618.0           9074.0716   3399.306      2.669      0.008    2410.914    1.57e+04\n",
       "10656.0           1.046e+04   3495.542      2.993      0.003    3610.766    1.73e+04\n",
       "10658.0           1.045e+04   3496.829      2.987      0.003    3590.733    1.73e+04\n",
       "10726.0           1.227e+04   3523.531      3.481      0.001    5359.204    1.92e+04\n",
       "10734.0           7760.9997   3836.548      2.023      0.043     240.781    1.53e+04\n",
       "10735.0           9750.1704   3465.560      2.813      0.005    2957.146    1.65e+04\n",
       "10764.0            1.03e+04   3584.360      2.874      0.004    3274.591    1.73e+04\n",
       "10777.0           1.038e+04   3498.480      2.966      0.003    3519.464    1.72e+04\n",
       "1078.0             536.0262   3370.038      0.159      0.874   -6069.762    7141.814\n",
       "10793.0           8667.7588   3500.015      2.476      0.013    1807.197    1.55e+04\n",
       "10816.0           8810.4118   3495.532      2.520      0.012    1958.637    1.57e+04\n",
       "10839.0           1.057e+04   3494.111      3.026      0.002    3724.674    1.74e+04\n",
       "10857.0            267.8678   3201.082      0.084      0.933   -6006.741    6542.476\n",
       "10867.0           4873.3848   3659.413      1.332      0.183   -2299.621     1.2e+04\n",
       "10906.0           9336.4858   3395.787      2.749      0.006    2680.227     1.6e+04\n",
       "10950.0           1.032e+04   4451.546      2.319      0.020    1598.643    1.91e+04\n",
       "10983.0          -2.185e+04   3294.297     -6.633      0.000   -2.83e+04   -1.54e+04\n",
       "1099.0            9612.8017   3450.007      2.786      0.005    2850.264    1.64e+04\n",
       "10991.0           4300.1083   3524.484      1.220      0.222   -2608.417    1.12e+04\n",
       "11012.0           7003.0744   3245.625      2.158      0.031     641.155    1.34e+04\n",
       "11038.0           1584.9939   3243.140      0.489      0.625   -4772.054    7942.042\n",
       "1104.0            9087.1045   3363.632      2.702      0.007    2493.873    1.57e+04\n",
       "11060.0           9940.4467   3499.969      2.840      0.005    3079.975    1.68e+04\n",
       "11094.0           9322.6430   3404.847      2.738      0.006    2648.624     1.6e+04\n",
       "11096.0           8070.0491   3306.301      2.441      0.015    1589.196    1.46e+04\n",
       "11113.0           9320.6769   3620.346      2.575      0.010    2224.247    1.64e+04\n",
       "1115.0            8436.8675   3367.347      2.505      0.012    1836.355     1.5e+04\n",
       "11161.0           6021.3457   3130.858      1.923      0.054    -115.612    1.22e+04\n",
       "11225.0           1.019e+04   3520.620      2.896      0.004    3294.007    1.71e+04\n",
       "11228.0           1.058e+04   3455.736      3.062      0.002    3807.075    1.74e+04\n",
       "11236.0           8487.3536   4807.653      1.765      0.078    -936.377    1.79e+04\n",
       "11288.0          -4285.1438   3433.102     -1.248      0.212    -1.1e+04    2444.258\n",
       "11312.0          -1488.0720   3086.728     -0.482      0.630   -7538.528    4562.384\n",
       "11361.0            1.05e+04   3501.988      2.998      0.003    3632.949    1.74e+04\n",
       "11399.0           2347.5209   3077.670      0.763      0.446   -3685.180    8380.222\n",
       "114303.0         -9571.3693   5399.892     -1.773      0.076   -2.02e+04    1013.241\n",
       "11456.0           5586.1634   3461.222      1.614      0.107   -1198.358    1.24e+04\n",
       "11465.0           4659.0121   3374.297      1.381      0.167   -1955.123    1.13e+04\n",
       "11502.0           8668.1917   3397.049      2.552      0.011    2009.459    1.53e+04\n",
       "11506.0           6603.9028   3378.886      1.954      0.051     -19.228    1.32e+04\n",
       "11537.0           1.023e+04   3479.312      2.939      0.003    3407.335     1.7e+04\n",
       "11566.0           1.036e+04   3491.031      2.968      0.003    3518.073    1.72e+04\n",
       "11573.0           9761.2159   3429.072      2.847      0.004    3039.712    1.65e+04\n",
       "11580.0           7866.4408   3666.583      2.145      0.032     679.380    1.51e+04\n",
       "11600.0           9665.2370   3432.503      2.816      0.005    2937.009    1.64e+04\n",
       "11609.0           1.356e+04   3467.960      3.911      0.000    6766.557    2.04e+04\n",
       "1161.0            -480.8907   2968.996     -0.162      0.871   -6300.575    5338.793\n",
       "11636.0          -1.146e+04   3207.307     -3.574      0.000   -1.78e+04   -5177.201\n",
       "11670.0           9809.1236   3451.905      2.842      0.004    3042.865    1.66e+04\n",
       "11678.0          -3033.7459   3115.647     -0.974      0.330   -9140.887    3073.396\n",
       "11682.0            1.06e+04   3611.779      2.935      0.003    3521.726    1.77e+04\n",
       "11694.0           1.041e+04   3619.335      2.877      0.004    3317.257    1.75e+04\n",
       "11720.0           -762.2755   3867.445     -0.197      0.844   -8343.057    6818.506\n",
       "11721.0          -9711.2464   3644.527     -2.665      0.008   -1.69e+04   -2567.419\n",
       "11722.0           9216.8593   3613.105      2.551      0.011    2134.624    1.63e+04\n",
       "11793.0           9959.7365   6257.374      1.592      0.111   -2305.669    2.22e+04\n",
       "11797.0           1.057e+04   3841.148      2.752      0.006    3040.013    1.81e+04\n",
       "11914.0           9570.5529   4017.374      2.382      0.017    1695.889    1.74e+04\n",
       "1209.0            8145.5808   3258.527      2.500      0.012    1758.373    1.45e+04\n",
       "12136.0          -8658.1861   3530.681     -2.452      0.014   -1.56e+04   -1737.514\n",
       "12141.0           8.845e+04   3265.400     27.087      0.000     8.2e+04    9.48e+04\n",
       "12181.0           7785.2247   4432.555      1.756      0.079    -903.258    1.65e+04\n",
       "12215.0          -1543.0409   3214.227     -0.480      0.631   -7843.415    4757.334\n",
       "12216.0           1926.4346   3215.418      0.599      0.549   -4376.274    8229.143\n",
       "12256.0           2154.7127   3244.711      0.664      0.507   -4205.414    8514.839\n",
       "12262.0           8366.2379   3552.028      2.355      0.019    1403.724    1.53e+04\n",
       "12389.0           1.212e+04   3710.289      3.266      0.001    4843.769    1.94e+04\n",
       "1239.0            6859.3033   3194.471      2.147      0.032     597.654    1.31e+04\n",
       "12390.0           8144.3311   3827.435      2.128      0.033     641.976    1.56e+04\n",
       "12397.0           7091.0057   5435.028      1.305      0.192   -3562.476    1.77e+04\n",
       "1243.0            4740.1661   3505.474      1.352      0.176   -2131.096    1.16e+04\n",
       "12548.0           9622.3915   3913.321      2.459      0.014    1951.687    1.73e+04\n",
       "12570.0           8478.9178   3680.537      2.304      0.021    1264.506    1.57e+04\n",
       "12581.0           8072.2136   3959.674      2.039      0.042     310.651    1.58e+04\n",
       "12592.0           7180.3739   3562.401      2.016      0.044     197.525    1.42e+04\n",
       "12604.0           6302.1541   6167.016      1.022      0.307   -5786.135    1.84e+04\n",
       "12656.0           9831.1782   3729.089      2.636      0.008    2521.597    1.71e+04\n",
       "12679.0          -1.207e+04   3575.885     -3.374      0.001   -1.91e+04   -5057.142\n",
       "1278.0            9272.1183   3554.622      2.608      0.009    2304.519    1.62e+04\n",
       "12788.0          -1.081e+04   3704.281     -2.917      0.004   -1.81e+04   -3545.393\n",
       "1283.0            8454.8366   3314.507      2.551      0.011    1957.900     1.5e+04\n",
       "1297.0            9358.7113   3457.490      2.707      0.007    2581.505    1.61e+04\n",
       "12992.0           1.015e+04   3767.816      2.693      0.007    2762.823    1.75e+04\n",
       "13135.0           5448.9172   3564.090      1.529      0.126   -1537.242    1.24e+04\n",
       "1327.0           -1643.9351   3038.632     -0.541      0.589   -7600.117    4312.247\n",
       "13282.0           4445.8876   5417.023      0.821      0.412   -6172.301    1.51e+04\n",
       "1334.0           -5548.4156   3198.783     -1.735      0.083   -1.18e+04     721.687\n",
       "13351.0           6518.8735   4245.842      1.535      0.125   -1803.622    1.48e+04\n",
       "13365.0          -1.383e+04   4121.711     -3.356      0.001   -2.19e+04   -5754.663\n",
       "13369.0           9139.3518   3706.917      2.465      0.014    1873.230    1.64e+04\n",
       "13406.0           9939.8470   3720.720      2.671      0.008    2646.670    1.72e+04\n",
       "13407.0           2417.5901   3312.421      0.730      0.465   -4075.258    8910.439\n",
       "13417.0           9899.0300   3860.871      2.564      0.010    2331.135    1.75e+04\n",
       "13525.0          -2774.8050   3294.923     -0.842      0.400   -9233.355    3683.745\n",
       "13554.0            1.04e+04   3768.695      2.759      0.006    3009.340    1.78e+04\n",
       "1359.0           -7579.7543   3532.881     -2.145      0.032   -1.45e+04    -654.770\n",
       "13623.0           7282.1206   3530.867      2.062      0.039     361.084    1.42e+04\n",
       "1372.0           -1024.5227   2952.643     -0.347      0.729   -6812.153    4763.107\n",
       "1380.0             933.6613   3077.529      0.303      0.762   -5098.765    6966.087\n",
       "13923.0           9652.5025   4001.225      2.412      0.016    1809.493    1.75e+04\n",
       "13932.0           9693.4571   4463.929      2.172      0.030     943.476    1.84e+04\n",
       "13941.0          -1234.2215   3351.972     -0.368      0.713   -7804.596    5336.153\n",
       "1397.0            8978.2149   3604.283      2.491      0.013    1913.271     1.6e+04\n",
       "14064.0           6496.0923   3427.729      1.895      0.058    -222.777    1.32e+04\n",
       "14084.0           9938.4584   3773.668      2.634      0.008    2541.495    1.73e+04\n",
       "14324.0           -448.3298   3367.341     -0.133      0.894   -7048.831    6152.171\n",
       "14462.0           9253.3530   3776.161      2.450      0.014    1851.504    1.67e+04\n",
       "1447.0            1.462e+04   4904.563      2.981      0.003    5007.306    2.42e+04\n",
       "14531.0           8818.8233   1.01e+04      0.870      0.385   -1.11e+04    2.87e+04\n",
       "14593.0           9416.8572   3820.901      2.465      0.014    1927.310    1.69e+04\n",
       "14622.0           1.034e+04   7410.721      1.395      0.163   -4188.065    2.49e+04\n",
       "1465.0            8580.7095   3881.320      2.211      0.027     972.732    1.62e+04\n",
       "1468.0            1.025e+04   3913.165      2.618      0.009    2575.683    1.79e+04\n",
       "14897.0           8143.8094   5481.373      1.486      0.137   -2600.516    1.89e+04\n",
       "14954.0           8879.7142   3787.153      2.345      0.019    1456.318    1.63e+04\n",
       "1496.0            1.056e+04   3504.246      3.014      0.003    3691.980    1.74e+04\n",
       "15267.0           8769.1983   3768.994      2.327      0.020    1381.397    1.62e+04\n",
       "15354.0           4573.2807   3644.445      1.255      0.210   -2570.386    1.17e+04\n",
       "1542.0            9271.0954   3433.382      2.700      0.007    2541.144     1.6e+04\n",
       "15459.0           8320.3467   3771.104      2.206      0.027     928.408    1.57e+04\n",
       "1554.0            1.037e+04   3482.162      2.977      0.003    3539.471    1.72e+04\n",
       "15708.0          -1.427e+04   4364.184     -3.269      0.001   -2.28e+04   -5713.790\n",
       "15711.0           8305.0899   3828.438      2.169      0.030     800.770    1.58e+04\n",
       "15761.0           9943.8050   4432.762      2.243      0.025    1254.916    1.86e+04\n",
       "1581.0           -1.894e+04   4232.063     -4.476      0.000   -2.72e+04   -1.06e+04\n",
       "1593.0            9341.0130   3412.180      2.738      0.006    2652.621     1.6e+04\n",
       "1602.0            1.675e+04   3401.742      4.925      0.000    1.01e+04    2.34e+04\n",
       "1613.0            9426.6340   3409.769      2.765      0.006    2742.967    1.61e+04\n",
       "16188.0           5067.9193   3620.452      1.400      0.162   -2028.717    1.22e+04\n",
       "1632.0            1695.0991   2948.865      0.575      0.565   -4085.124    7475.322\n",
       "1633.0            7476.1562   3246.672      2.303      0.021    1112.185    1.38e+04\n",
       "1635.0           -9570.9410   3483.344     -2.748      0.006   -1.64e+04   -2743.057\n",
       "16401.0            154.0311   3522.272      0.044      0.965   -6750.159    7058.221\n",
       "16437.0           3278.1817   4055.398      0.808      0.419   -4671.016    1.12e+04\n",
       "1651.0            4853.9574   3050.757      1.591      0.112   -1125.990    1.08e+04\n",
       "1655.0            1.039e+04   3500.961      2.969      0.003    3531.377    1.73e+04\n",
       "1663.0            1.499e+04   3493.288      4.290      0.000    8137.781    2.18e+04\n",
       "16710.0           4680.4781   3643.868      1.284      0.199   -2462.057    1.18e+04\n",
       "16729.0           3279.1787   3496.933      0.938      0.348   -3575.341    1.01e+04\n",
       "1690.0            -1.29e+04   3126.881     -4.125      0.000    -1.9e+04   -6769.159\n",
       "1703.0            8580.7670   3451.436      2.486      0.013    1815.426    1.53e+04\n",
       "17101.0          -2.018e+04   1.11e+04     -1.826      0.068   -4.19e+04    1486.592\n",
       "17202.0           7809.9940   3719.926      2.100      0.036     518.372    1.51e+04\n",
       "1722.0            8452.8790   3492.118      2.421      0.016    1607.796    1.53e+04\n",
       "1728.0            1.027e+04   3498.334      2.935      0.003    3409.289    1.71e+04\n",
       "1743.0            8906.5506   4307.712      2.068      0.039     462.779    1.74e+04\n",
       "1754.0            9834.3119   3546.280      2.773      0.006    2883.064    1.68e+04\n",
       "1762.0            7435.8004   3401.849      2.186      0.029     767.658    1.41e+04\n",
       "1773.0            8891.4429   3479.243      2.556      0.011    2071.596    1.57e+04\n",
       "1786.0           -3461.9294   3081.001     -1.124      0.261   -9501.160    2577.301\n",
       "18100.0           6947.5759   3657.295      1.900      0.058    -221.279    1.41e+04\n",
       "1820.0            5465.5030   3097.241      1.765      0.078    -605.561    1.15e+04\n",
       "1848.0           -1886.5500   3530.970     -0.534      0.593   -8807.788    5034.688\n",
       "18654.0           9606.5788   4646.011      2.068      0.039     499.691    1.87e+04\n",
       "1875.0            3764.7167   4537.792      0.830      0.407   -5130.045    1.27e+04\n",
       "1884.0            8893.5871   3449.113      2.579      0.010    2132.801    1.57e+04\n",
       "1913.0            8167.5315   3325.843      2.456      0.014    1648.373    1.47e+04\n",
       "1919.0            9105.8754   3573.766      2.548      0.011    2100.751    1.61e+04\n",
       "1920.0            7091.9048   3175.931      2.233      0.026     866.596    1.33e+04\n",
       "1968.0            9763.7597   3432.129      2.845      0.004    3036.264    1.65e+04\n",
       "1976.0            1.072e+04   3385.906      3.167      0.002    4085.647    1.74e+04\n",
       "1981.0            9179.0457   3386.527      2.710      0.007    2540.938    1.58e+04\n",
       "1988.0            4480.5582   4066.528      1.102      0.271   -3490.456    1.25e+04\n",
       "1992.0            1.038e+04   3487.074      2.977      0.003    3545.153    1.72e+04\n",
       "2008.0            9454.9287   3367.152      2.808      0.005    2854.798    1.61e+04\n",
       "2033.0            9562.2420   3880.534      2.464      0.014    1955.805    1.72e+04\n",
       "2044.0            7026.6396   3177.011      2.212      0.027     799.214    1.33e+04\n",
       "2049.0            9228.3043   3397.286      2.716      0.007    2569.107    1.59e+04\n",
       "2061.0            1.047e+04   3505.208      2.986      0.003    3594.353    1.73e+04\n",
       "20779.0           5.253e+04   3656.911     14.363      0.000    4.54e+04    5.97e+04\n",
       "2085.0           -8160.7724   3333.012     -2.448      0.014   -1.47e+04   -1627.563\n",
       "2086.0            7159.5200   3250.961      2.202      0.028     787.141    1.35e+04\n",
       "2111.0            7686.8490   3220.397      2.387      0.017    1374.380     1.4e+04\n",
       "21204.0            384.4065   3516.538      0.109      0.913   -6508.543    7277.356\n",
       "21238.0           8142.5703   3798.818      2.143      0.032     696.310    1.56e+04\n",
       "2124.0            9948.2780   3545.061      2.806      0.005    2999.419    1.69e+04\n",
       "2146.0            1.384e+04   3784.701      3.656      0.000    6419.923    2.13e+04\n",
       "21496.0          -1.592e+04   4394.801     -3.622      0.000   -2.45e+04   -7302.277\n",
       "2154.0            9073.6815   3398.606      2.670      0.008    2411.896    1.57e+04\n",
       "2176.0            4.929e+04   3844.965     12.820      0.000    4.18e+04    5.68e+04\n",
       "2188.0            1.044e+04   3575.743      2.921      0.004    3434.167    1.75e+04\n",
       "2189.0            4635.5911   3442.148      1.347      0.178   -2111.542    1.14e+04\n",
       "2220.0            9562.6108   3447.803      2.774      0.006    2804.392    1.63e+04\n",
       "22205.0           1.025e+04   3981.986      2.575      0.010    2448.797    1.81e+04\n",
       "2226.0            6832.7747   6127.983      1.115      0.265   -5179.003    1.88e+04\n",
       "2230.0            8034.0356   3473.326      2.313      0.021    1225.788    1.48e+04\n",
       "22325.0          -1992.9417   3644.785     -0.547      0.585   -9137.275    5151.392\n",
       "2255.0            8674.1665   3467.604      2.501      0.012    1877.134    1.55e+04\n",
       "22619.0           9952.2455   4149.838      2.398      0.016    1817.932    1.81e+04\n",
       "2267.0            -805.0746   3029.653     -0.266      0.790   -6743.655    5133.505\n",
       "22815.0           4148.6717   3597.517      1.153      0.249   -2903.008    1.12e+04\n",
       "2285.0           -1.943e+04   3290.519     -5.906      0.000   -2.59e+04    -1.3e+04\n",
       "2290.0            5559.2176   3411.874      1.629      0.103   -1128.575    1.22e+04\n",
       "2295.0            1.008e+04   4816.118      2.093      0.036     640.045    1.95e+04\n",
       "2316.0            5178.5854   3441.299      1.505      0.132   -1566.883    1.19e+04\n",
       "23220.0           8950.6027   3920.186      2.283      0.022    1266.442    1.66e+04\n",
       "23224.0          -9039.2043   3854.373     -2.345      0.019   -1.66e+04   -1484.047\n",
       "2343.0           -4325.9227   5346.106     -0.809      0.418   -1.48e+04    6153.259\n",
       "2352.0            1.003e+04   3595.998      2.790      0.005    2983.330    1.71e+04\n",
       "23700.0          -2370.0832   4559.670     -0.520      0.603   -1.13e+04    6567.565\n",
       "2390.0            1.048e+04   3494.768      2.997      0.003    3624.802    1.73e+04\n",
       "2393.0            8653.3534   3349.179      2.584      0.010    2088.453    1.52e+04\n",
       "2403.0            9935.9405   3390.435      2.931      0.003    3290.172    1.66e+04\n",
       "2435.0            1.183e+04   3523.767      3.357      0.001    4921.422    1.87e+04\n",
       "2444.0            8334.6736   3418.105      2.438      0.015    1634.667     1.5e+04\n",
       "2448.0            8079.7608   3280.572      2.463      0.014    1649.340    1.45e+04\n",
       "2469.0            9319.5696   4619.391      2.017      0.044     264.861    1.84e+04\n",
       "24720.0           1.002e+04   4081.147      2.455      0.014    2018.551     1.8e+04\n",
       "24800.0          -2368.8265   4068.738     -0.582      0.560   -1.03e+04    5606.520\n",
       "2482.0            1.054e+04   3503.339      3.009      0.003    3675.623    1.74e+04\n",
       "24969.0           9638.1580   4606.058      2.092      0.036     609.584    1.87e+04\n",
       "2498.0            1009.6808   3154.855      0.320      0.749   -5174.316    7193.678\n",
       "2504.0            -596.5201   3326.842     -0.179      0.858   -7117.636    5924.596\n",
       "2508.0            1.021e+04   3663.087      2.788      0.005    3034.214    1.74e+04\n",
       "25124.0           9907.2768   4155.005      2.384      0.017    1762.836    1.81e+04\n",
       "2518.0             1.02e+04   3499.266      2.914      0.004    3337.153    1.71e+04\n",
       "25224.0           1.085e+04   7432.065      1.459      0.145   -3722.732    2.54e+04\n",
       "25279.0           7235.2571   3849.249      1.880      0.060    -309.857    1.48e+04\n",
       "2537.0           -6799.0456   3253.919     -2.089      0.037   -1.32e+04    -420.869\n",
       "2538.0            9856.5848   4266.509      2.310      0.021    1493.578    1.82e+04\n",
       "25389.0           9764.1891   6252.235      1.562      0.118   -2491.144     2.2e+04\n",
       "2547.0            3557.0844   3568.631      0.997      0.319   -3437.976    1.06e+04\n",
       "2553.0            8873.3357   3380.697      2.625      0.009    2246.655    1.55e+04\n",
       "2574.0            2969.8852   3914.372      0.759      0.448   -4702.880    1.06e+04\n",
       "25747.0           9693.6935   4140.844      2.341      0.019    1577.010    1.78e+04\n",
       "2577.0            8965.0512   3358.735      2.669      0.008    2381.419    1.55e+04\n",
       "2593.0            9363.1907   3451.006      2.713      0.007    2598.694    1.61e+04\n",
       "2596.0            4211.3081   3037.853      1.386      0.166   -1743.346    1.02e+04\n",
       "2663.0            1.312e+04   3506.203      3.743      0.000    6250.849       2e+04\n",
       "2771.0            4584.3418   3064.314      1.496      0.135   -1422.180    1.06e+04\n",
       "2787.0            9312.7499   3431.787      2.714      0.007    2585.925     1.6e+04\n",
       "2797.0           -3679.5566   3112.073     -1.182      0.237   -9779.693    2420.580\n",
       "2802.0            1.005e+04   3460.168      2.905      0.004    3267.980    1.68e+04\n",
       "2817.0            2283.4997   3416.197      0.668      0.504   -4412.766    8979.766\n",
       "28678.0          -9271.0040   4084.380     -2.270      0.023   -1.73e+04   -1264.999\n",
       "28701.0           7716.1013   3370.690      2.289      0.022    1109.037    1.43e+04\n",
       "28742.0          -9267.6116   4027.724     -2.301      0.021   -1.72e+04   -1372.660\n",
       "2888.0            1.007e+04   3594.695      2.802      0.005    3025.153    1.71e+04\n",
       "2897.0            1.066e+04   4157.956      2.564      0.010    2511.181    1.88e+04\n",
       "2917.0            4909.1440   3468.408      1.415      0.157   -1889.463    1.17e+04\n",
       "29392.0          -2347.8239   3892.943     -0.603      0.546   -9978.585    5282.937\n",
       "2950.0           -1.893e+04   4792.383     -3.951      0.000   -2.83e+04   -9540.080\n",
       "2951.0            1.016e+04   3824.854      2.656      0.008    2663.376    1.77e+04\n",
       "2953.0            9261.6627   3377.554      2.742      0.006    2641.143    1.59e+04\n",
       "2960.0            8815.4539   4097.258      2.152      0.031     784.204    1.68e+04\n",
       "2975.0            2967.2731   3055.386      0.971      0.331   -3021.748    8956.294\n",
       "2982.0            8378.8566   3335.690      2.512      0.012    1840.397    1.49e+04\n",
       "2991.0           -3542.1410   3614.739     -0.980      0.327   -1.06e+04    3543.297\n",
       "3011.0           -4619.7729   3181.777     -1.452      0.147   -1.09e+04    1616.993\n",
       "3015.0            1.033e+04   3445.213      2.998      0.003    3575.672    1.71e+04\n",
       "3026.0            8115.5273   3298.350      2.460      0.014    1650.259    1.46e+04\n",
       "3031.0            -1.31e+04   4243.942     -3.087      0.002   -2.14e+04   -4783.026\n",
       "3062.0            1.218e+04   3636.770      3.350      0.001    5056.024    1.93e+04\n",
       "3093.0            1154.0983   3273.649      0.353      0.724   -5262.751    7570.948\n",
       "3107.0            1.003e+04   4887.548      2.052      0.040     449.992    1.96e+04\n",
       "3121.0            1.131e+04   3424.546      3.304      0.001    4601.086     1.8e+04\n",
       "3126.0            1.029e+04   3500.672      2.940      0.003    3430.420    1.72e+04\n",
       "3144.0            6.226e+04   3492.741     17.824      0.000    5.54e+04    6.91e+04\n",
       "3156.0            8439.9508   3686.328      2.290      0.022    1214.187    1.57e+04\n",
       "3157.0            8909.5602   3367.099      2.646      0.008    2309.534    1.55e+04\n",
       "3170.0            1.049e+04   3163.430      3.317      0.001    4292.419    1.67e+04\n",
       "3178.0             476.3824   3058.630      0.156      0.876   -5518.997    6471.762\n",
       "3206.0            6647.9203   3682.551      1.805      0.071    -570.439    1.39e+04\n",
       "3229.0            4699.0928   3154.197      1.490      0.136   -1483.614    1.09e+04\n",
       "3235.0            1.043e+04   3672.530      2.840      0.005    3232.613    1.76e+04\n",
       "3246.0            8999.9332   3441.630      2.615      0.009    2253.815    1.57e+04\n",
       "3248.0            1.017e+04   3536.773      2.875      0.004    3235.421    1.71e+04\n",
       "3282.0           -2.157e+04   3279.699     -6.578      0.000    -2.8e+04   -1.51e+04\n",
       "3362.0            2423.5362   3770.430      0.643      0.520   -4967.079    9814.152\n",
       "3372.0            9627.7372   3873.154      2.486      0.013    2035.766    1.72e+04\n",
       "3422.0            9480.0410   3437.691      2.758      0.006    2741.643    1.62e+04\n",
       "3497.0            5252.3369   3141.534      1.672      0.095    -905.547    1.14e+04\n",
       "3502.0            3238.2765   3011.798      1.075      0.282   -2665.307    9141.860\n",
       "3504.0            8995.7334   3981.420      2.259      0.024    1191.544    1.68e+04\n",
       "3505.0            9143.3764   3461.357      2.642      0.008    2358.589    1.59e+04\n",
       "3532.0            1.003e+04   3245.402      3.091      0.002    3669.185    1.64e+04\n",
       "3574.0            1.069e+04   5195.998      2.058      0.040     509.765    2.09e+04\n",
       "3580.0            6068.2467   3187.564      1.904      0.057    -179.864    1.23e+04\n",
       "3612.0             1.02e+04   3452.504      2.955      0.003    3436.160     1.7e+04\n",
       "3619.0            7934.3369   3447.904      2.301      0.021    1175.921    1.47e+04\n",
       "3622.0            1.043e+04   3580.387      2.913      0.004    3411.652    1.74e+04\n",
       "3639.0           -1512.7133   2960.576     -0.511      0.609   -7315.893    4290.467\n",
       "3650.0            3115.5849   3263.925      0.955      0.340   -3282.206    9513.375\n",
       "3662.0            1.006e+04   3445.692      2.919      0.004    3304.170    1.68e+04\n",
       "3734.0           -7319.5906   3017.118     -2.426      0.015   -1.32e+04   -1405.580\n",
       "3735.0            9975.8401   3692.019      2.702      0.007    2738.921    1.72e+04\n",
       "3761.0            6349.2844   3193.756      1.988      0.047      89.036    1.26e+04\n",
       "3779.0           -8181.0780   3599.088     -2.273      0.023   -1.52e+04   -1126.317\n",
       "3781.0            -618.0552   3532.836     -0.175      0.861   -7542.951    6306.840\n",
       "3782.0           -3555.6311   3135.968     -1.134      0.257   -9702.606    2591.344\n",
       "3786.0            7499.2694   3240.654      2.314      0.021    1147.094    1.39e+04\n",
       "3796.0           -9059.0070   3588.977     -2.524      0.012   -1.61e+04   -2024.065\n",
       "3821.0            9604.8814   3519.854      2.729      0.006    2705.432    1.65e+04\n",
       "3835.0            3076.1366   3410.513      0.902      0.367   -3608.988    9761.262\n",
       "3839.0            8341.2256   4032.054      2.069      0.039     437.786    1.62e+04\n",
       "3840.0            -969.3453   2951.072     -0.328      0.743   -6753.895    4815.204\n",
       "3895.0            9871.3541   3445.298      2.865      0.004    3118.046    1.66e+04\n",
       "3908.0            3036.1131   4046.355      0.750      0.453   -4895.359     1.1e+04\n",
       "3911.0            6304.1002   3262.976      1.932      0.053     -91.829    1.27e+04\n",
       "3917.0            9590.5921   3473.229      2.761      0.006    2782.534    1.64e+04\n",
       "3946.0            1.011e+04   3473.206      2.910      0.004    3300.234    1.69e+04\n",
       "3971.0            9572.0036   3496.097      2.738      0.006    2719.122    1.64e+04\n",
       "3980.0            1.938e+04   3338.403      5.804      0.000    1.28e+04    2.59e+04\n",
       "4034.0            7772.7623   3372.613      2.305      0.021    1161.929    1.44e+04\n",
       "4036.0             1.05e+04   3491.567      3.008      0.003    3659.649    1.73e+04\n",
       "4040.0            1558.3206   2961.665      0.526      0.599   -4246.994    7363.635\n",
       "4058.0            8856.4981   3314.736      2.672      0.008    2359.111    1.54e+04\n",
       "4060.0           -6647.6270   3194.633     -2.081      0.037   -1.29e+04    -385.661\n",
       "4062.0            1.204e+04   3470.424      3.469      0.001    5237.107    1.88e+04\n",
       "4077.0            7312.3454   4387.204      1.667      0.096   -1287.241    1.59e+04\n",
       "4087.0           -1.765e+04   3493.298     -5.052      0.000   -2.45e+04   -1.08e+04\n",
       "4091.0            7653.2198   3705.070      2.066      0.039     390.718    1.49e+04\n",
       "4127.0            3525.2756   3006.526      1.173      0.241   -2367.972    9418.523\n",
       "4138.0            1.059e+04   4042.581      2.620      0.009    2666.318    1.85e+04\n",
       "4162.0            9146.0786   3916.758      2.335      0.020    1468.636    1.68e+04\n",
       "4186.0             1.05e+04   3488.695      3.009      0.003    3658.523    1.73e+04\n",
       "4194.0            -120.8974   3365.019     -0.036      0.971   -6716.846    6475.051\n",
       "4199.0           -2262.4254   2969.652     -0.762      0.446   -8083.396    3558.545\n",
       "4213.0            1.047e+04   3437.887      3.045      0.002    3728.458    1.72e+04\n",
       "4222.0           -4565.7812   3081.867     -1.481      0.138   -1.06e+04    1475.148\n",
       "4223.0            9852.3412   3425.086      2.877      0.004    3138.651    1.66e+04\n",
       "4251.0            1.021e+04   3471.456      2.940      0.003    3402.138     1.7e+04\n",
       "4265.0            7812.5812   3452.112      2.263      0.024    1045.916    1.46e+04\n",
       "4274.0            9412.2701   3542.496      2.657      0.008    2468.439    1.64e+04\n",
       "4321.0            9164.6583   3367.888      2.721      0.007    2563.085    1.58e+04\n",
       "4335.0            8190.6964   4783.515      1.712      0.087   -1185.720    1.76e+04\n",
       "4340.0            6767.9340   3214.191      2.106      0.035     467.630    1.31e+04\n",
       "4371.0            8758.7466   3469.648      2.524      0.012    1957.708    1.56e+04\n",
       "4415.0            9054.9012   3466.775      2.612      0.009    2259.495    1.59e+04\n",
       "4450.0            7004.7705   3192.340      2.194      0.028     747.298    1.33e+04\n",
       "4476.0            -537.3034   3155.397     -0.170      0.865   -6722.362    5647.755\n",
       "4510.0            5574.8780   3357.655      1.660      0.097   -1006.636    1.22e+04\n",
       "4520.0            1.049e+04   3503.110      2.996      0.003    3627.664    1.74e+04\n",
       "4551.0            7925.1651   7335.131      1.080      0.280   -6452.807    2.23e+04\n",
       "4568.0            1.039e+04   3626.066      2.865      0.004    3281.685    1.75e+04\n",
       "4579.0             1.01e+04   3454.228      2.924      0.003    3327.659    1.69e+04\n",
       "4585.0            1.023e+04   3532.907      2.895      0.004    3303.805    1.72e+04\n",
       "4595.0            7017.4950   3203.870      2.190      0.029     737.422    1.33e+04\n",
       "4600.0           -2913.4675   3188.550     -0.914      0.361   -9163.510    3336.575\n",
       "4607.0             1.01e+04   3461.607      2.918      0.004    3314.622    1.69e+04\n",
       "4608.0           -2467.3480   2996.140     -0.824      0.410   -8340.238    3405.542\n",
       "4622.0            8052.4631   3317.026      2.428      0.015    1550.588    1.46e+04\n",
       "4623.0            8958.3286   3477.238      2.576      0.010    2142.413    1.58e+04\n",
       "4768.0            9434.6639   3524.936      2.677      0.007    2525.254    1.63e+04\n",
       "4771.0            1.027e+04   3483.066      2.949      0.003    3445.939    1.71e+04\n",
       "4800.0            8501.1614   3530.706      2.408      0.016    1580.441    1.54e+04\n",
       "4802.0            1.039e+04   3495.460      2.971      0.003    3535.120    1.72e+04\n",
       "4807.0            1.021e+04   3570.231      2.859      0.004    3209.789    1.72e+04\n",
       "4839.0           -1.356e+05   4533.275    -29.907      0.000   -1.44e+05   -1.27e+05\n",
       "4843.0           -8167.6858   3490.884     -2.340      0.019    -1.5e+04   -1325.021\n",
       "4881.0            8989.8462   3358.611      2.677      0.007    2406.458    1.56e+04\n",
       "4900.0            7258.8031   3240.831      2.240      0.025     906.282    1.36e+04\n",
       "4926.0            9046.0784   3409.453      2.653      0.008    2363.032    1.57e+04\n",
       "4941.0            8846.7949   3497.185      2.530      0.011    1991.780    1.57e+04\n",
       "4961.0           -7068.5016   3842.536     -1.840      0.066   -1.46e+04     463.454\n",
       "4988.0            1.557e+04   3453.820      4.509      0.000    8802.201    2.23e+04\n",
       "4993.0            1.045e+04   3497.931      2.988      0.003    3594.338    1.73e+04\n",
       "5018.0            7179.5577   3370.938      2.130      0.033     572.005    1.38e+04\n",
       "5020.0           -5446.9435   3426.169     -1.590      0.112   -1.22e+04    1268.869\n",
       "5027.0            5845.9537   3181.465      1.838      0.066    -390.201    1.21e+04\n",
       "5032.0            9857.7384   3470.574      2.840      0.005    3054.886    1.67e+04\n",
       "5043.0            5610.0072   3176.436      1.766      0.077    -616.291    1.18e+04\n",
       "5046.0           -1341.8874   3062.059     -0.438      0.661   -7343.989    4660.214\n",
       "5047.0            5.447e+04   4066.247     13.397      0.000    4.65e+04    6.24e+04\n",
       "5065.0            1.006e+04   3830.583      2.625      0.009    2547.975    1.76e+04\n",
       "5071.0            9512.9670   3843.083      2.475      0.013    1979.940     1.7e+04\n",
       "5073.0           -2.008e+05   6312.374    -31.807      0.000   -2.13e+05   -1.88e+05\n",
       "5087.0            4797.4041   3156.086      1.520      0.129   -1389.005     1.1e+04\n",
       "5109.0            1.013e+04   3496.987      2.898      0.004    3278.094     1.7e+04\n",
       "5116.0           -3937.9453   3224.186     -1.221      0.222   -1.03e+04    2381.951\n",
       "5122.0            5517.2539   3154.948      1.749      0.080    -666.925    1.17e+04\n",
       "5134.0            3884.2245   3594.240      1.081      0.280   -3161.033    1.09e+04\n",
       "5142.0            6838.4021   3976.061      1.720      0.085    -955.281    1.46e+04\n",
       "5165.0            4497.2071   3311.638      1.358      0.174   -1994.108     1.1e+04\n",
       "5169.0            1.916e+04   3342.007      5.734      0.000    1.26e+04    2.57e+04\n",
       "5174.0            6293.7590   3406.431      1.848      0.065    -383.364     1.3e+04\n",
       "5179.0            9463.8885   3387.278      2.794      0.005    2824.307    1.61e+04\n",
       "5181.0            1.047e+04   3593.512      2.915      0.004    3430.631    1.75e+04\n",
       "5187.0            1.028e+04   3835.771      2.680      0.007    2762.068    1.78e+04\n",
       "5229.0            3372.9553   3100.033      1.088      0.277   -2703.582    9449.492\n",
       "5234.0           -2373.8375   3557.256     -0.667      0.505   -9346.600    4598.925\n",
       "5237.0            9274.2111   3376.757      2.746      0.006    2655.255    1.59e+04\n",
       "5252.0            9398.7830   3395.434      2.768      0.006    2743.215    1.61e+04\n",
       "5254.0            9895.7852   3468.171      2.853      0.004    3097.642    1.67e+04\n",
       "5306.0            9991.3769   3380.819      2.955      0.003    3364.457    1.66e+04\n",
       "5338.0            1.017e+04   3461.718      2.937      0.003    3379.876     1.7e+04\n",
       "5377.0            1.013e+04   3504.604      2.891      0.004    3263.929     1.7e+04\n",
       "5439.0            7549.1619   3383.094      2.231      0.026     917.783    1.42e+04\n",
       "5456.0            1.043e+04   3529.032      2.955      0.003    3510.479    1.73e+04\n",
       "5464.0            9243.4459   4033.476      2.292      0.022    1337.220    1.71e+04\n",
       "5476.0            9316.0567   3359.046      2.773      0.006    2731.816    1.59e+04\n",
       "5492.0           -1.083e+04   3431.449     -3.157      0.002   -1.76e+04   -4106.851\n",
       "5496.0            9305.5943   3421.714      2.720      0.007    2598.514     1.6e+04\n",
       "5505.0            9887.5592   3496.472      2.828      0.005    3033.942    1.67e+04\n",
       "5518.0            8043.4491   3674.641      2.189      0.029     840.594    1.52e+04\n",
       "5520.0            7086.8585   3241.750      2.186      0.029     732.536    1.34e+04\n",
       "5545.0            1.061e+04   3626.502      2.926      0.003    3503.952    1.77e+04\n",
       "5568.0            1.329e+04   3515.859      3.779      0.000    6394.215    2.02e+04\n",
       "5569.0            1.038e+04   3521.677      2.947      0.003    3474.427    1.73e+04\n",
       "5578.0            9794.0748   3412.408      2.870      0.004    3105.235    1.65e+04\n",
       "5581.0            9726.6032   3379.409      2.878      0.004    3102.448    1.64e+04\n",
       "5589.0            4310.6488   3071.264      1.404      0.160   -1709.495    1.03e+04\n",
       "5597.0            1.225e+04   3985.852      3.073      0.002    4436.767    2.01e+04\n",
       "5606.0           -2.818e+04   3156.085     -8.928      0.000   -3.44e+04    -2.2e+04\n",
       "5639.0            1.079e+04   3475.889      3.103      0.002    3972.430    1.76e+04\n",
       "5667.0            4408.3690   3081.766      1.430      0.153   -1632.362    1.04e+04\n",
       "5690.0            1.044e+04   3495.787      2.986      0.003    3586.243    1.73e+04\n",
       "5709.0            9489.9110   3484.902      2.723      0.006    2658.972    1.63e+04\n",
       "5726.0            8843.5967   3350.233      2.640      0.008    2276.631    1.54e+04\n",
       "5764.0            9607.0617   3330.031      2.885      0.004    3079.694    1.61e+04\n",
       "5772.0            1.009e+04   3477.233      2.901      0.004    3272.074    1.69e+04\n",
       "5860.0           -2.137e+04   3183.508     -6.714      0.000   -2.76e+04   -1.51e+04\n",
       "5878.0            1.288e+04   3373.262      3.819      0.000    6271.821    1.95e+04\n",
       "5903.0            3056.9152   3093.343      0.988      0.323   -3006.507    9120.338\n",
       "5905.0            8382.9179   3504.017      2.392      0.017    1514.512    1.53e+04\n",
       "5959.0            6222.0789   3427.640      1.815      0.070    -496.618    1.29e+04\n",
       "6008.0            2.945e+04   3053.695      9.643      0.000    2.35e+04    3.54e+04\n",
       "6034.0            1.011e+04   3585.086      2.820      0.005    3081.561    1.71e+04\n",
       "6035.0            1483.7234   3506.418      0.423      0.672   -5389.389    8356.836\n",
       "6036.0             221.6732   2991.364      0.074      0.941   -5641.855    6085.201\n",
       "6039.0            9544.5704   3432.253      2.781      0.005    2816.832    1.63e+04\n",
       "6044.0            1.016e+04   3681.651      2.760      0.006    2942.992    1.74e+04\n",
       "6066.0           -2.346e+04   4482.310     -5.233      0.000   -3.22e+04   -1.47e+04\n",
       "6078.0            9706.7472   3249.116      2.988      0.003    3337.985    1.61e+04\n",
       "6081.0           -1.153e+04   3146.489     -3.664      0.000   -1.77e+04   -5361.713\n",
       "60893.0           -1.28e+04   4529.427     -2.826      0.005   -2.17e+04   -3921.010\n",
       "6097.0            9210.7131   3343.462      2.755      0.006    2657.018    1.58e+04\n",
       "6102.0            9066.2170   3480.579      2.605      0.009    2243.752    1.59e+04\n",
       "6104.0             232.2733   3423.042      0.068      0.946   -6477.409    6941.956\n",
       "6109.0            1010.3528   2947.059      0.343      0.732   -4766.331    6787.037\n",
       "6127.0            4701.1928   3757.688      1.251      0.211   -2664.447    1.21e+04\n",
       "61552.0          -4432.5561   4243.880     -1.044      0.296   -1.28e+04    3886.094\n",
       "6158.0            6664.1488   3326.127      2.004      0.045     144.434    1.32e+04\n",
       "6171.0            9441.2528   3383.795      2.790      0.005    2808.500    1.61e+04\n",
       "61780.0           9463.2455   5197.187      1.821      0.069    -724.032    1.97e+04\n",
       "6207.0            8858.3816   3348.787      2.645      0.008    2294.250    1.54e+04\n",
       "6214.0            1.051e+04   3500.804      3.002      0.003    3647.047    1.74e+04\n",
       "6216.0            9305.6197   3433.501      2.710      0.007    2575.436     1.6e+04\n",
       "62221.0           9636.3504   4648.363      2.073      0.038     524.852    1.87e+04\n",
       "6259.0            1362.8316   3271.611      0.417      0.677   -5050.023    7775.686\n",
       "62599.0          -2.068e+04   4912.844     -4.210      0.000   -3.03e+04   -1.11e+04\n",
       "6266.0            5707.1489   3198.854      1.784      0.074    -563.092     1.2e+04\n",
       "6268.0            5194.0633   3510.777      1.479      0.139   -1687.594    1.21e+04\n",
       "6288.0            9663.1685   3447.626      2.803      0.005    2905.297    1.64e+04\n",
       "6297.0            1.056e+04   3595.867      2.937      0.003    3511.586    1.76e+04\n",
       "6307.0           -1.092e+04   4063.132     -2.688      0.007   -1.89e+04   -2956.996\n",
       "6313.0            8891.8083   4582.140      1.941      0.052     -89.882    1.79e+04\n",
       "6314.0            9311.8048   3390.368      2.747      0.006    2666.168     1.6e+04\n",
       "6326.0            6072.5560   3163.322      1.920      0.055    -128.036    1.23e+04\n",
       "6349.0            9207.5642   3389.098      2.717      0.007    2564.416    1.59e+04\n",
       "6357.0            1.024e+04   3585.280      2.857      0.004    3216.274    1.73e+04\n",
       "6375.0            1.444e+04   3464.702      4.168      0.000    7650.844    2.12e+04\n",
       "6376.0            9678.6118   3484.646      2.778      0.005    2848.175    1.65e+04\n",
       "6379.0              26.6996   6333.401      0.004      0.997   -1.24e+04    1.24e+04\n",
       "6386.0            1.048e+04   3498.806      2.994      0.003    3618.477    1.73e+04\n",
       "6403.0            4977.6386   3259.208      1.527      0.127   -1410.906    1.14e+04\n",
       "6410.0             1.05e+04   3527.394      2.977      0.003    3586.498    1.74e+04\n",
       "6416.0            2347.3189   3090.659      0.759      0.448   -3710.844    8405.482\n",
       "6424.0            9621.0992   3436.334      2.800      0.005    2885.363    1.64e+04\n",
       "6433.0            8661.7986   3343.259      2.591      0.010    2108.503    1.52e+04\n",
       "6435.0            1.183e+04   3360.879      3.519      0.000    5240.102    1.84e+04\n",
       "6492.0            8974.9088   3432.021      2.615      0.009    2247.626    1.57e+04\n",
       "6497.0           -3053.3190   3013.220     -1.013      0.311   -8959.689    2853.051\n",
       "6500.0            6150.1570   7269.626      0.846      0.398   -8099.415    2.04e+04\n",
       "6509.0            9556.9483   3435.340      2.782      0.005    2823.159    1.63e+04\n",
       "6527.0            1.047e+04   3729.698      2.807      0.005    3158.019    1.78e+04\n",
       "6528.0            8269.4671   3499.188      2.363      0.018    1410.526    1.51e+04\n",
       "6531.0            3609.6244   3468.218      1.041      0.298   -3188.611    1.04e+04\n",
       "6532.0            5989.2996   3226.740      1.856      0.063    -335.601    1.23e+04\n",
       "6543.0            1.019e+04   3488.637      2.922      0.003    3355.752     1.7e+04\n",
       "6548.0            9642.4815   3462.712      2.785      0.005    2855.040    1.64e+04\n",
       "6550.0            1.029e+04   3711.831      2.772      0.006    3013.513    1.76e+04\n",
       "6552.0            9481.6941   3579.910      2.649      0.008    2464.527    1.65e+04\n",
       "6565.0            8646.7354   3544.299      2.440      0.015    1699.371    1.56e+04\n",
       "6571.0            9933.4396   3461.508      2.870      0.004    3148.358    1.67e+04\n",
       "6573.0            9777.5503   3422.534      2.857      0.004    3068.863    1.65e+04\n",
       "6641.0            6780.5002   5403.773      1.255      0.210   -3811.717    1.74e+04\n",
       "6649.0            1.063e+04   3496.881      3.041      0.002    3779.297    1.75e+04\n",
       "6730.0            3503.1057   3291.625      1.064      0.287   -2948.979    9955.191\n",
       "6731.0            9345.3044   3489.365      2.678      0.007    2505.618    1.62e+04\n",
       "6742.0            9178.2023   4815.789      1.906      0.057    -261.476    1.86e+04\n",
       "6745.0            1.058e+04   3564.650      2.968      0.003    3592.600    1.76e+04\n",
       "6756.0            9146.2023   3364.036      2.719      0.007    2552.180    1.57e+04\n",
       "6765.0           -6705.0520   3084.029     -2.174      0.030   -1.28e+04    -659.886\n",
       "6768.0            1.103e+04   3514.640      3.140      0.002    4145.597    1.79e+04\n",
       "6774.0           -1.801e+04   3543.800     -5.083      0.000    -2.5e+04   -1.11e+04\n",
       "6797.0            1.026e+04   3843.351      2.670      0.008    2729.513    1.78e+04\n",
       "6803.0            9607.2274   3435.986      2.796      0.005    2872.172    1.63e+04\n",
       "6821.0            9500.5977   3458.877      2.747      0.006    2720.672    1.63e+04\n",
       "6830.0            7437.6025   3255.407      2.285      0.022    1056.509    1.38e+04\n",
       "6845.0            9803.0965   3438.070      2.851      0.004    3063.956    1.65e+04\n",
       "6848.0            8333.6403   3496.836      2.383      0.017    1479.309    1.52e+04\n",
       "6873.0            6772.6473   4025.271      1.683      0.092   -1117.495    1.47e+04\n",
       "6900.0            9887.2618   3448.768      2.867      0.004    3127.151    1.66e+04\n",
       "6908.0            9670.9945   3412.881      2.834      0.005    2981.228    1.64e+04\n",
       "6994.0            7027.9999   3190.940      2.202      0.028     773.272    1.33e+04\n",
       "7045.0           -1365.3935   3609.225     -0.378      0.705   -8440.023    5709.236\n",
       "7065.0            1.611e+04   3506.227      4.596      0.000    9241.876     2.3e+04\n",
       "7085.0            1.278e+04   3490.166      3.663      0.000    5943.361    1.96e+04\n",
       "7107.0            1.016e+04   3620.898      2.807      0.005    3065.851    1.73e+04\n",
       "7116.0            9950.6944   3458.392      2.877      0.004    3171.720    1.67e+04\n",
       "7117.0            9962.5160   4171.308      2.388      0.017    1786.117    1.81e+04\n",
       "7121.0            8806.0934   3370.809      2.612      0.009    2198.794    1.54e+04\n",
       "7127.0            5683.8420   3400.609      1.671      0.095    -981.868    1.23e+04\n",
       "7139.0            9692.1878   3477.874      2.787      0.005    2875.026    1.65e+04\n",
       "7146.0            1.012e+04   3465.747      2.919      0.004    3323.612    1.69e+04\n",
       "7163.0             1.31e+04   3467.196      3.777      0.000    6299.856    1.99e+04\n",
       "7180.0            7580.6776   3426.253      2.213      0.027     864.700    1.43e+04\n",
       "7183.0            9122.0661   3445.881      2.647      0.008    2367.616    1.59e+04\n",
       "7228.0            1.691e+04   3336.789      5.067      0.000    1.04e+04    2.34e+04\n",
       "7232.0            6459.3309   4154.287      1.555      0.120   -1683.704    1.46e+04\n",
       "7250.0            5903.5751   3433.908      1.719      0.086    -827.408    1.26e+04\n",
       "7257.0            2.454e+04   3288.256      7.464      0.000    1.81e+04     3.1e+04\n",
       "7260.0            9960.5244   3417.721      2.914      0.004    3261.272    1.67e+04\n",
       "7267.0            8789.4484   3543.555      2.480      0.013    1843.541    1.57e+04\n",
       "7268.0           -4864.0786   3160.501     -1.539      0.124   -1.11e+04    1330.984\n",
       "7281.0            1.024e+04   4245.639      2.411      0.016    1915.827    1.86e+04\n",
       "7291.0            9738.4799   3446.044      2.826      0.005    2983.710    1.65e+04\n",
       "7343.0            2811.3153   3688.762      0.762      0.446   -4419.220       1e+04\n",
       "7346.0             510.6111   2979.289      0.171      0.864   -5329.248    6350.470\n",
       "7401.0            1.037e+04   3532.412      2.935      0.003    3442.141    1.73e+04\n",
       "7409.0            9957.6138   3452.343      2.884      0.004    3190.497    1.67e+04\n",
       "7420.0            7030.1192   3222.589      2.182      0.029     713.354    1.33e+04\n",
       "7435.0            9960.7449   3257.812      3.057      0.002    3574.937    1.63e+04\n",
       "7466.0            8121.3534   3464.126      2.344      0.019    1331.140    1.49e+04\n",
       "7486.0           -3690.8571   3031.916     -1.217      0.223   -9633.873    2252.159\n",
       "7503.0            9995.7891   4813.311      2.077      0.038     560.967    1.94e+04\n",
       "7506.0            1.114e+04   3408.724      3.269      0.001    4461.736    1.78e+04\n",
       "7537.0            9056.8261   3396.774      2.666      0.008    2398.632    1.57e+04\n",
       "7549.0            9421.4226   3438.414      2.740      0.006    2681.608    1.62e+04\n",
       "7554.0            9079.1061   3444.815      2.636      0.008    2326.745    1.58e+04\n",
       "7557.0            9586.5450   3506.696      2.734      0.006    2712.887    1.65e+04\n",
       "7585.0           -1.767e+04   3412.507     -5.179      0.000   -2.44e+04    -1.1e+04\n",
       "7602.0            8879.0584   3347.634      2.652      0.008    2317.187    1.54e+04\n",
       "7620.0            4824.0196   3317.870      1.454      0.146   -1679.510    1.13e+04\n",
       "7636.0            9195.8774   3376.389      2.724      0.006    2577.642    1.58e+04\n",
       "7646.0            9544.3596   3443.343      2.772      0.006    2794.883    1.63e+04\n",
       "7658.0            8846.6138   3378.292      2.619      0.009    2224.648    1.55e+04\n",
       "7683.0            1.075e+04   3675.409      2.925      0.003    3546.037     1.8e+04\n",
       "7685.0            9783.4865   3575.982      2.736      0.006    2774.018    1.68e+04\n",
       "7692.0            5094.7382   3102.002      1.642      0.101    -985.659    1.12e+04\n",
       "7762.0            1.038e+04   3464.947      2.995      0.003    3586.497    1.72e+04\n",
       "7772.0           -1371.3064   3005.129     -0.456      0.648   -7261.817    4519.205\n",
       "7773.0            9554.4855   3463.000      2.759      0.006    2766.478    1.63e+04\n",
       "7777.0            6152.5489   3204.358      1.920      0.055    -128.480    1.24e+04\n",
       "7835.0            1.054e+04   3518.681      2.996      0.003    3643.560    1.74e+04\n",
       "7873.0           -2813.4654   2999.335     -0.938      0.348   -8692.619    3065.688\n",
       "7883.0            9932.7429   3433.142      2.893      0.004    3203.262    1.67e+04\n",
       "7904.0            9043.3757   3486.366      2.594      0.009    2209.568    1.59e+04\n",
       "7906.0            1.253e+04   3521.991      3.558      0.000    5627.449    1.94e+04\n",
       "7921.0            1.079e+04   3466.265      3.112      0.002    3993.016    1.76e+04\n",
       "7923.0            8674.4178   3539.688      2.451      0.014    1736.090    1.56e+04\n",
       "7935.0            7444.2116   3273.992      2.274      0.023    1026.689    1.39e+04\n",
       "7938.0            8845.2456   3467.407      2.551      0.011    2048.600    1.56e+04\n",
       "7985.0           -1.091e+04   3104.873     -3.515      0.000    -1.7e+04   -4828.819\n",
       "8014.0            7773.1550   3358.207      2.315      0.021    1190.559    1.44e+04\n",
       "8030.0            1.024e+04   3432.790      2.983      0.003    3510.766     1.7e+04\n",
       "8046.0           -2361.1775   3083.758     -0.766      0.444   -8405.812    3683.457\n",
       "8047.0            9758.5162   3946.299      2.473      0.013    2023.169    1.75e+04\n",
       "8062.0            8420.4733   3407.990      2.471      0.013    1740.294    1.51e+04\n",
       "8068.0           -5735.8824   3541.043     -1.620      0.105   -1.27e+04    1205.100\n",
       "8087.0           -6967.8963   3297.326     -2.113      0.035   -1.34e+04    -504.635\n",
       "8095.0            1.036e+04   3503.810      2.956      0.003    3490.766    1.72e+04\n",
       "8096.0            9442.4911   3396.980      2.780      0.005    2783.893    1.61e+04\n",
       "8109.0            1.031e+04   3485.190      2.957      0.003    3474.529    1.71e+04\n",
       "8123.0            9497.2222   3457.064      2.747      0.006    2720.850    1.63e+04\n",
       "8150.0            1.035e+04   3499.858      2.956      0.003    3486.945    1.72e+04\n",
       "8163.0             1.04e+04   3545.844      2.934      0.003    3451.826    1.74e+04\n",
       "8176.0            1380.9099   3340.324      0.413      0.679   -5166.633    7928.453\n",
       "8202.0            8928.8530   3496.105      2.554      0.011    2075.956    1.58e+04\n",
       "8214.0            7951.0174   3472.900      2.289      0.022    1143.605    1.48e+04\n",
       "8215.0            6869.4339   3533.851      1.944      0.052     -57.451    1.38e+04\n",
       "8219.0             1.01e+04   3521.286      2.868      0.004    3195.379     1.7e+04\n",
       "8247.0            7517.0591   3412.455      2.203      0.028     828.128    1.42e+04\n",
       "8253.0           -5960.1192   3082.870     -1.933      0.053    -1.2e+04      82.775\n",
       "8290.0            7523.9306   3581.453      2.101      0.036     503.737    1.45e+04\n",
       "8293.0            1.049e+04   3497.907      2.999      0.003    3632.477    1.73e+04\n",
       "8304.0            1.052e+04   3404.658      3.090      0.002    3846.046    1.72e+04\n",
       "8334.0            8459.9005   3390.789      2.495      0.013    1813.438    1.51e+04\n",
       "8348.0            1.033e+04   3541.616      2.918      0.004    3392.106    1.73e+04\n",
       "8357.0            9916.7234   3441.737      2.881      0.004    3170.395    1.67e+04\n",
       "8358.0            8999.4511   3440.726      2.616      0.009    2255.105    1.57e+04\n",
       "8446.0           -1264.4703   3768.625     -0.336      0.737   -8651.548    6122.607\n",
       "8460.0            1.089e+04   3837.834      2.838      0.005    3368.395    1.84e+04\n",
       "8463.0            9275.8747   3429.308      2.705      0.007    2553.909     1.6e+04\n",
       "8479.0            1.002e+04   4084.448      2.454      0.014    2015.762     1.8e+04\n",
       "8530.0             1.51e+04   3389.501      4.455      0.000    8454.778    2.17e+04\n",
       "8536.0            7703.1720   3360.553      2.292      0.022    1115.978    1.43e+04\n",
       "8543.0            2.968e+04   3685.258      8.053      0.000    2.25e+04    3.69e+04\n",
       "8549.0            -314.6492   3224.852     -0.098      0.922   -6635.849    6006.551\n",
       "8551.0            9873.1363   3485.903      2.832      0.005    3040.235    1.67e+04\n",
       "8559.0            4491.5178   3212.952      1.398      0.162   -1806.358    1.08e+04\n",
       "8573.0           -3760.3739   3245.720     -1.159      0.247   -1.01e+04    2601.732\n",
       "8606.0            1.163e+04   3484.800      3.338      0.001    4802.881    1.85e+04\n",
       "8607.0            9805.6659   3477.735      2.820      0.005    2988.777    1.66e+04\n",
       "8648.0            9658.8439   3419.758      2.824      0.005    2955.598    1.64e+04\n",
       "8657.0            2916.5634   3074.140      0.949      0.343   -3109.219    8942.345\n",
       "8675.0            8484.9073   4577.829      1.853      0.064    -488.333    1.75e+04\n",
       "8681.0            5340.5429   3103.645      1.721      0.085    -743.073    1.14e+04\n",
       "8687.0            7564.8392   3485.445      2.170      0.030     732.837    1.44e+04\n",
       "8692.0            8275.6065   3374.689      2.452      0.014    1660.703    1.49e+04\n",
       "8699.0            1.015e+04   3469.228      2.926      0.003    3350.051     1.7e+04\n",
       "8717.0            1.034e+04   3480.922      2.970      0.003    3515.999    1.72e+04\n",
       "8759.0            3195.8327   3002.715      1.064      0.287   -2689.946    9081.612\n",
       "8762.0            9229.2761   3211.469      2.874      0.004    2934.309    1.55e+04\n",
       "8819.0            1.045e+04   3554.911      2.941      0.003    3485.338    1.74e+04\n",
       "8850.0            1.006e+04   3470.331      2.900      0.004    3260.321    1.69e+04\n",
       "8852.0            1.037e+04   3514.880      2.952      0.003    3485.138    1.73e+04\n",
       "8859.0            9504.3144   3432.430      2.769      0.006    2776.229    1.62e+04\n",
       "8867.0            2804.6769   3365.235      0.833      0.405   -3791.695    9401.049\n",
       "8881.0            8810.4771   3373.327      2.612      0.009    2198.244    1.54e+04\n",
       "8958.0            6294.1180   3147.355      2.000      0.046     124.823    1.25e+04\n",
       "8972.0           -1.304e+04   3292.688     -3.960      0.000   -1.95e+04   -6584.568\n",
       "8990.0           -3233.5185   3122.100     -1.036      0.300   -9353.310    2886.273\n",
       "9004.0            1.028e+04   3671.630      2.799      0.005    3078.424    1.75e+04\n",
       "9016.0            7909.5405   3260.736      2.426      0.015    1518.003    1.43e+04\n",
       "9048.0            8117.5466   3282.531      2.473      0.013    1683.285    1.46e+04\n",
       "9051.0            -285.8187   3339.687     -0.086      0.932   -6832.114    6260.476\n",
       "9071.0            1.047e+04   3494.598      2.995      0.003    3616.763    1.73e+04\n",
       "9112.0            5268.4764   3107.901      1.695      0.090    -823.483    1.14e+04\n",
       "9114.0            6528.6201   3498.644      1.866      0.062    -329.255    1.34e+04\n",
       "9132.0            7679.5102   4515.180      1.701      0.089   -1170.930    1.65e+04\n",
       "9173.0            9871.3241   3749.193      2.633      0.008    2522.335    1.72e+04\n",
       "9180.0            1.023e+04   3560.674      2.873      0.004    3251.695    1.72e+04\n",
       "9186.0            9579.4763   3435.987      2.788      0.005    2844.420    1.63e+04\n",
       "9191.0            7809.2449   4335.647      1.801      0.072    -689.283    1.63e+04\n",
       "9216.0            5465.3263   3093.356      1.767      0.077    -598.121    1.15e+04\n",
       "9217.0            1777.0444   3009.252      0.591      0.555   -4121.547    7675.635\n",
       "9225.0            1.071e+04   3503.029      3.058      0.002    3845.352    1.76e+04\n",
       "9230.0            9410.1365   3829.849      2.457      0.014    1903.050    1.69e+04\n",
       "9259.0            1.043e+04   3496.754      2.983      0.003    3576.686    1.73e+04\n",
       "9293.0            1.034e+04   3492.063      2.960      0.003    3491.788    1.72e+04\n",
       "9299.0            6897.8306   3506.720      1.967      0.049      24.126    1.38e+04\n",
       "9308.0            1741.3332   3024.635      0.576      0.565   -4187.411    7670.078\n",
       "9311.0            5296.9888   4107.271      1.290      0.197   -2753.887    1.33e+04\n",
       "9313.0            3977.4922   3104.734      1.281      0.200   -2108.259    1.01e+04\n",
       "9325.0            9578.9967   3442.888      2.782      0.005    2830.413    1.63e+04\n",
       "9332.0            1.001e+04   3470.784      2.883      0.004    3203.517    1.68e+04\n",
       "9340.0           -2.037e+04   5011.621     -4.064      0.000   -3.02e+04   -1.05e+04\n",
       "9372.0            9148.2969   3621.080      2.526      0.012    2050.429    1.62e+04\n",
       "9411.0            8088.5107   3524.109      2.295      0.022    1180.721     1.5e+04\n",
       "9459.0            -666.6739   3356.430     -0.199      0.843   -7245.788    5912.440\n",
       "9465.0            1.471e+04   3419.252      4.303      0.000    8010.932    2.14e+04\n",
       "9472.0            3955.3257   3021.325      1.309      0.191   -1966.930    9877.582\n",
       "9483.0           -4488.5082   3083.214     -1.456      0.145   -1.05e+04    1555.060\n",
       "9563.0           -1.564e+04   4431.982     -3.530      0.000   -2.43e+04   -6956.363\n",
       "9590.0            4752.8554   3056.595      1.555      0.120   -1238.535    1.07e+04\n",
       "9598.0            2876.1361   3734.665      0.770      0.441   -4444.375    1.02e+04\n",
       "9599.0            1309.9612   2948.034      0.444      0.657   -4468.634    7088.556\n",
       "9602.0            2279.3103   4077.970      0.559      0.576   -5714.132    1.03e+04\n",
       "9619.0            8879.0387   3376.513      2.630      0.009    2260.559    1.55e+04\n",
       "9643.0            1.012e+04   3558.028      2.844      0.004    3146.293    1.71e+04\n",
       "9650.0            1.011e+04   3465.651      2.917      0.004    3316.603    1.69e+04\n",
       "9653.0           -6037.0683   5294.152     -1.140      0.254   -1.64e+04    4340.275\n",
       "9667.0            9385.3308   3422.969      2.742      0.006    2675.790    1.61e+04\n",
       "9698.0            7457.0200   3229.332      2.309      0.021    1127.039    1.38e+04\n",
       "9699.0            9848.2959   3312.269      2.973      0.003    3355.744    1.63e+04\n",
       "9719.0             768.4108   2945.395      0.261      0.794   -5005.011    6541.833\n",
       "9742.0           -2580.5961   3079.909     -0.838      0.402   -8617.686    3456.494\n",
       "9761.0            1.046e+04   3530.077      2.963      0.003    3539.740    1.74e+04\n",
       "9771.0            3049.1857   3017.146      1.011      0.312   -2864.879    8963.251\n",
       "9772.0            9577.3307   3397.420      2.819      0.005    2917.871    1.62e+04\n",
       "9778.0            8834.8250   3321.328      2.660      0.008    2324.517    1.53e+04\n",
       "9799.0            2300.8569   3111.941      0.739      0.460   -3799.021    8400.734\n",
       "9815.0            1.041e+04   3600.344      2.891      0.004    3349.939    1.75e+04\n",
       "9818.0           -3.633e+04   3152.499    -11.524      0.000   -4.25e+04   -3.02e+04\n",
       "9837.0            1.034e+04   3554.223      2.908      0.004    3370.486    1.73e+04\n",
       "9922.0            1086.6389   2947.216      0.369      0.712   -4690.353    6863.631\n",
       "9954.0            9757.9764   3958.884      2.465      0.014    1997.961    1.75e+04\n",
       "9963.0            9335.3174   3515.814      2.655      0.008    2443.787    1.62e+04\n",
       "9988.0            9912.8554   3503.871      2.829      0.005    3044.736    1.68e+04\n",
       "gspillsicIVX1981    -0.0427      0.122     -0.349      0.727      -0.283       0.197\n",
       "gspillsicIVX1982    -0.0526      0.121     -0.436      0.663      -0.289       0.184\n",
       "gspillsicIVX1983    -0.0757      0.119     -0.638      0.523      -0.308       0.157\n",
       "gspillsicIVX1984    -0.1432      0.117     -1.222      0.222      -0.373       0.087\n",
       "gspillsicIVX1985    -0.1742      0.117     -1.488      0.137      -0.404       0.055\n",
       "gspillsicIVX1986    -0.2309      0.117     -1.976      0.048      -0.460      -0.002\n",
       "gspillsicIVX1987    -0.2508      0.117     -2.143      0.032      -0.480      -0.021\n",
       "gspillsicIVX1988    -0.2858      0.117     -2.434      0.015      -0.516      -0.056\n",
       "gspillsicIVX1989    -0.2845      0.118     -2.410      0.016      -0.516      -0.053\n",
       "gspillsicIVX1990    -0.3120      0.119     -2.624      0.009      -0.545      -0.079\n",
       "gspillsicIVX1991    -0.2929      0.120     -2.445      0.014      -0.528      -0.058\n",
       "gspillsicIVX1992    -0.3439      0.121     -2.850      0.004      -0.580      -0.107\n",
       "gspillsicIVX1993    -0.3505      0.122     -2.873      0.004      -0.590      -0.111\n",
       "gspillsicIVX1994    -0.3584      0.123     -2.913      0.004      -0.600      -0.117\n",
       "gspillsicIVX1995    -0.3304      0.125     -2.644      0.008      -0.575      -0.085\n",
       "gspillsicIVX1996    -0.3542      0.127     -2.785      0.005      -0.603      -0.105\n",
       "gspillsicIVX1997    -0.3330      0.130     -2.565      0.010      -0.587      -0.078\n",
       "gspillsicIVX1998    -0.2876      0.133     -2.170      0.030      -0.547      -0.028\n",
       "gspillsicIVX1999    -0.2297      0.135     -1.700      0.089      -0.495       0.035\n",
       "gspillsicIVX2000    -0.2065      0.137     -1.504      0.133      -0.476       0.063\n",
       "gspillsicIVX2001    -0.4205      0.139     -3.027      0.002      -0.693      -0.148\n",
       "==============================================================================\n",
       "Omnibus:                    21719.318   Durbin-Watson:                   0.545\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         50384141.785\n",
       "Skew:                          10.195   Prob(JB):                         0.00\n",
       "Kurtosis:                     302.876   Cond. No.                     9.49e+06\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.49e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Product spillovers model, firm FE's\n",
    "x_vars_fe = x_vars.drop(columns=drop_columns)\n",
    "\n",
    "year_model5 = sm.OLS(y_var,x_vars_fe).fit()\n",
    "year_model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d70dcb93-017d-4292-ac78-1e52c59abadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab estimates. Add non-interactive term to each year coefficient\n",
    "# Build Dataframe of year, coefficient, conf intervals\n",
    "\n",
    "# use years to interate through\n",
    "# years = np.append(years, '1981')\n",
    "# years = years.sort_values()\n",
    "\n",
    "ref_year = '1980'\n",
    "time_coefs_sic = {\n",
    "    'year': np.append(years,ref_year),\n",
    "    'coef': [np.nan for i in range(0,len(years)+1)],\n",
    "    'CI_l': [np.nan for i in range(0,len(years)+1)],\n",
    "    'CI_h': [np.nan for i in range(0,len(years)+1)],\n",
    "    'se': [np.nan for i in range(0,len(years)+1)]\n",
    "}\n",
    "\n",
    "time_coefs_sic = pd.DataFrame(time_coefs_sic)\n",
    "time_coefs_sic = time_coefs_sic.sort_values(by='year').reset_index(drop=True)\n",
    "\n",
    "conf_intervals = year_model5.conf_int(alpha=0.05, cols=None)\n",
    "s_errors = year_model5.HC0_se\n",
    "\n",
    "# grab coefficients and confidence intervals\n",
    "ref_year = '1980'\n",
    "coef_ref = year_model5.params['gspillsicIV'] # coef for ref year category\n",
    "l_ref = conf_intervals.loc[conf_intervals.index == 'gspillsicIV',0].values[0] # conf interval for ref cat\n",
    "h_ref = conf_intervals.loc[conf_intervals.index == 'gspillsicIV',1].values[0]\n",
    "\n",
    "for year in time_coefs_sic['year'].unique():\n",
    "    if year == ref_year:\n",
    "        coef = coef_ref\n",
    "        ci_l = 0 \n",
    "        ci_h = 0\n",
    "        \n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'coef'] = coef\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_l'] = ci_l\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_h'] = ci_h\n",
    "        \n",
    "    else:\n",
    "        col_name = f\"gspillsicIVX{year}\"\n",
    "        coef = year_model5.params[col_name] + coef_ref\n",
    "        ci_l = conf_intervals.loc[conf_intervals.index == col_name,0].values[0] + coef_ref\n",
    "        ci_h = conf_intervals.loc[conf_intervals.index == col_name,1].values[0] + coef_ref\n",
    "        \n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'coef'] = coef\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_l'] = ci_l\n",
    "        time_coefs_sic.loc[time_coefs_sic['year'] == year, 'CI_h'] = ci_h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a13d487-f1f8-42ea-af72-6aa9a3b84519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_coefs_sic['upper'] = time_coefs_sic['CI_h'] - time_coefs_sic['coef']\n",
    "time_coefs_sic['lower'] = time_coefs_sic['coef'] - time_coefs_sic['CI_l']\n",
    "time_coefs_sic.loc[time_coefs_sic['year'] == ref_year, ['upper','lower']] = 0\n",
    "\n",
    "cis = time_coefs_sic[['lower','upper']].T\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.errorbar(x=time_coefs_sic['year'],y=time_coefs_sic['coef'],yerr=cis, capsize=5, marker='o')\n",
    "plt.axhline(y=coef_ref, linestyle='dashed', color='r')\n",
    "plt.axhline(y=l_ref, linestyle='dotted', color='r')\n",
    "plt.axhline(y=h_ref, linestyle='dotted', color='r')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Coefficient')\n",
    "plt.title('Effect of Product Market Spillovers Over Time')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620daffe-ce4c-4423-8e2b-08b236e23f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cb59ca5-5fca-4adf-bdd6-f013ae965e66",
   "metadata": {},
   "source": [
    "# DAG model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "938b1200-939b-4733-b44d-d4fd27e5ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DAG3 = \"\"\"graph [\n",
    "directed 1\n",
    "\n",
    "node [\n",
    "    id 0\n",
    "    label \"rmkvaf\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 1\n",
    "    label \"rxrd\"\n",
    "    ]\n",
    "    \n",
    "node [\n",
    "    id 2\n",
    "    label \"rsales\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 3\n",
    "    label \"gspilltecIV\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 4\n",
    "    label \"gspillsicIV\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 5\n",
    "    label \"pat_count\"\n",
    "    ]\n",
    "\n",
    "node [\n",
    "    id 7\n",
    "    label \"rppent\"\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 0\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 5\n",
    "    target 7\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 3\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 4\n",
    "    target 1\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 4\n",
    "    target 2\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 7\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 1\n",
    "    target 5\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 2\n",
    "    ]\n",
    "\n",
    "\n",
    "edge [\n",
    "    source 2\n",
    "    target 0\n",
    "    ]\n",
    "\n",
    "edge [\n",
    "    source 7\n",
    "    target 0\n",
    "    ]\n",
    "\n",
    "]\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ff4fa94-6a7f-44b4-8c2d-1c2592fa852d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CausalModel(\n",
    "    data=df,\n",
    "    treatment='gspilltecIV',\n",
    "    outcome='rmkvaf',\n",
    "    graph=DAG3)\n",
    "\n",
    "model.view_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5750d61e-8f45-4335-a771-e7279042f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimand type: EstimandType.NONPARAMETRIC_ATE\n",
      "\n",
      "### Estimand : 1\n",
      "Estimand name: backdoor\n",
      "Estimand expression:\n",
      "      d                  \n",
      "──────────────(E[rmkvaf])\n",
      "d[gspilltecIV]           \n",
      "Estimand assumption 1, Unconfoundedness: If U→{gspilltecIV} and U→rmkvaf then P(rmkvaf|gspilltecIV,,U) = P(rmkvaf|gspilltecIV,)\n",
      "\n",
      "### Estimand : 2\n",
      "Estimand name: iv\n",
      "No such variable(s) found!\n",
      "\n",
      "### Estimand : 3\n",
      "Estimand name: frontdoor\n",
      "Estimand expression:\n",
      " ⎡                d                                ∂                          \n",
      "E⎢──────────────────────────────────(rmkvaf)⋅──────────────([pat_count  rppent\n",
      " ⎣d[pat_count  rppent  rsales  rxrd]         ∂[gspilltecIV]                   \n",
      "\n",
      "                ⎤\n",
      "  rsales  rxrd])⎥\n",
      "                ⎦\n",
      "Estimand assumption 1, Full-mediation: pat_count,rppent,rsales,rxrd intercepts (blocks) all directed paths from gspilltecIV to r,m,k,v,a,f.\n",
      "Estimand assumption 2, First-stage-unconfoundedness: If U→{gspilltecIV} and U→{pat_count,rppent,rsales,rxrd} then P(pat_count,rppent,rsales,rxrd|gspilltecIV,U) = P(pat_count,rppent,rsales,rxrd|gspilltecIV)\n",
      "Estimand assumption 3, Second-stage-unconfoundedness: If U→{pat_count,rppent,rsales,rxrd} and U→rmkvaf then P(rmkvaf|pat_count,rppent,rsales,rxrd, gspilltecIV, U) = P(rmkvaf|pat_count,rppent,rsales,rxrd, gspilltecIV)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify estimand\n",
    "estimand = model.identify_effect()\n",
    "print(estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "006f5b87-4a1f-471e-b67a-12b1b9ffc9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12296998685320659\n"
     ]
    }
   ],
   "source": [
    "# obtain estimates\n",
    "estimate = model.estimate_effect(\n",
    "    identified_estimand=estimand,\n",
    "    method_name='backdoor.linear_regression')\n",
    "\n",
    "print(estimate.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f27e0e80-ebcb-4543-b1b5-8eedafa7ac0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refute: Use a subset of data\n",
      "Estimated effect:0.12296998685320659\n",
      "New effect:0.12347458135323676\n",
      "p value:0.9199999999999999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# refutation test\n",
    "refute_subset = model.refute_estimate(\n",
    "    estimand=estimand,\n",
    "    estimate=estimate,\n",
    "    method_name=\"data_subset_refuter\",\n",
    "    subset_fraction=0.4)\n",
    "\n",
    "print(refute_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755e48a0-25a6-4faf-8a5a-2db6ba5e8635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39600fbc-c968-438a-b2d4-1758b578a563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0c591-95c7-42e6-b42f-8d3ba9b0f447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1c3ef64-ca2f-4b0e-ae15-74b57b958650",
   "metadata": {},
   "source": [
    "# Scrap Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce3ff2-b571-4e3c-9b57-c7c6210b0ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56cc964a-8741-4ebf-8c7c-ea8c06631143",
   "metadata": {},
   "source": [
    "X_spills = df[['gspilltecIV', 'gspillsicIV']]\n",
    "\n",
    "(X_trainsp,\n",
    " X_testsp,\n",
    " y_trainsp,\n",
    " y_testsp) = skm.train_test_split(X_spills,\n",
    "                                df['rmkvaf'],\n",
    "                                test_size=0.3,\n",
    "                                random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1704c099-cfef-4cc6-8905-38615eb18d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try a neural network\n",
    "# one_layers = [\n",
    "#     (1),\n",
    "#     (2),\n",
    "#     (3),\n",
    "#     (4),\n",
    "#     (5)\n",
    "# ]\n",
    "\n",
    "# two_layers = [\n",
    "#     (1,1),(1,2),(1,3),\n",
    "#     (2,1),(2,2),(2,3),(2,4),\n",
    "#     (3,1),(3,2),(3,3),(3,4),(3,5),\n",
    "#     (4,1),(4,2),(4,3),(4,4),(4,5),\n",
    "#     (5,1),(5,2),(5,3),(5,4),(5,5)\n",
    "# ]\n",
    "\n",
    "# # Use DML with a PLR equation, keeping spillovers entering linearly\n",
    "# # Use Random Forest as the ML model\n",
    "\n",
    "# # Specify doubleML data model\n",
    "\n",
    "# x_vars = ['pat_count','rsales','rppent','emp','rxrd']\n",
    "# data_dml_tec = dml.DoubleMLData(df,\n",
    "#                                  y_col='rmkvaf',\n",
    "#                                  d_cols='gspilltec',\n",
    "#                                  x_cols=x_vars)\n",
    "\n",
    "# RF_DML = RF(max_features=3, random_state=0)\n",
    "\n",
    "# # Implement PLR DML estimation with gspilltec linear\n",
    "\n",
    "# dml_plr_tec = dml.DoubleMLPLR(data_dml_tec,\n",
    "#                                 ml_l = RF_DML,\n",
    "#                                 ml_m = RF_DML,\n",
    "#                                 n_folds = 3)\n",
    "\n",
    "# dml_plr_tec.fit(store_predictions=True)\n",
    "# print(dml_plr_tec.summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
